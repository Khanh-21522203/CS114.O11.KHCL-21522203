Contents,p_content,old_hastag,Hashtag,Hùng,Cáp,Khánh
"Công cụ Mocap AI tạo hoạt ảnh khuôn mặt, chuyển động chất lượng cao theo thời gian thực!","Công cụ Mocap AI tạo hoạt ảnh khuôn mặt, chuyển động chất lượng cao theo thời gian thực!",,,"#sharing, #cv","#sharing, #cv","#sharing, #tools, #cv"
"Chào mọi người, em đang làm quen với ML cơ bản và có bài tập lớn về nhận diện hành động bạo lực. Em đã cài các môi trường nhưng gặp phải lỗi ở bước train, theo bài hướng dẫn thì sử dụng tensorflow 1.7.0 nhưng hiện tại bản thấp nhất là 2.2.0. Em đã thử các bản từ 2.2.0 trở lên nhưng đều xuất hiện lỗi "" has no attribute"" như hình dưới. Rất mong được sự giúp đỡ của mọi người ạ","Chào mọi người, em đang làm quen với ML cơ bản và có bài tập lớn về nhận diện hành động bạo lực. Em đã cài các môi trường nhưng gặp phải lỗi ở bước train, theo bài hướng dẫn thì sử dụng tensorflow 1.7.0 nhưng hiện tại bản thấp nhất là 2.2.0. Em đã thử các bản từ 2.2.0 trở lên nhưng đều xuất hiện lỗi "" has no attribute"" như hình dưới. Rất mong được sự giúp đỡ của mọi người ạ",,,"#Q&A, #pythonlibraries","#Q&A, #pythonlibraries, #machine_learning","#sharing, #machine_learning"
"Merger 2 câu để tạo 1 câu mới.
Trong NLP, mọi người đã gặp trường hợp nào mà dùng ngẫu nhiên 2 câu trong dataset hợp lại làm một để tạo ra data mới chưa ạ, có thể chỉ cho mình bài báo hoặc ví dụ đó được không? Thanks!","Merger 2 câu để tạo 1 câu mới. Trong NLP, mọi người đã gặp trường hợp nào mà dùng ngẫu nhiên 2 câu trong dataset hợp lại làm một để tạo ra data mới chưa ạ, có thể chỉ cho mình bài báo hoặc ví dụ đó được không? Thanks!",,,"#Q&A, #nlp","#Q&A, #nlp","#Q&A, #nlp"
"Today, we introduce VinaLLaMA, a Vietnamese Foundation Model built on top of LLaMA-2 with 800B additional tokens trained. The SFT process includes 1M samples in various tasks to make it the State-of-the-art across ALL Vietnamese Benchmark. VinaLLaMA also scores impressive on English benchmark, beating Meta AI's LLaMA-2-chat-hf, making it capable for a bilingual LLM.
Everyone please have a Llamastic day!
Announcement: https://www.vilm.org/research/introducing-vinallama
Paper: https://arxiv.org/abs/2312.11011
Demo (Sponsored by HuggingFace🤗): https://huggingface.co/spaces/vilm/vinallama-chatui-hf
Models: https://huggingface.co/collections/vilm/vinallama-654a099308775ce78e630a6f","Today, we introduce VinaLLaMA, a Vietnamese Foundation Model built on top of LLaMA-2 with 800B additional tokens trained. The SFT process includes 1M samples in various tasks to make it the State-of-the-art across ALL Vietnamese Benchmark. VinaLLaMA also scores impressive on English benchmark, beating Meta AI's LLaMA-2-chat-hf, making it capable for a bilingual LLM. Everyone please have a Llamastic day! Announcement: https://www.vilm.org/research/introducing-vinallama Paper: https://arxiv.org/abs/2312.11011 Demo (Sponsored by HuggingFace): https://huggingface.co/spaces/vilm/vinallama-chatui-hf Models: https://huggingface.co/collections/vilm/vinallama-654a099308775ce78e630a6f",,,,,
có bác nào làm genai mà cũng hay dùng transformer decoder only không. e thấy bác viết bài này bảo là sử dụng kiến trúc gốc của Transformer bao gồm cả encoder và decoder có thể giảm bớt tình trạng hallucination của mô hình. bác nào thử rồi cho e biết với,có bác nào làm genai mà cũng hay dùng transformer decoder only không. e thấy bác viết bài này bảo là sử dụng kiến trúc gốc của Transformer bao gồm cả encoder và decoder có thể giảm bớt tình trạng hallucination của mô hình. bác nào thử rồi cho e biết với,,,"#Q&A, #deep_learning,","#Q&A, #deep_learning,#nlp","#Q&A, #deep_learning, #nlp"
"Mình có đọc được 26 tips cho việc prompt hiệu quả hơn với ChatGPT và cũng có thể hiệu quả với các công cụ LLM khác nên chia sẻ ở đây để chúng ta cùng tham khảo.
26 Prompting Tips
1 - No need to be polite with LLM so there is no need to add phrases like “please”, “if you don’t mind”, “thank you”, “I would like to”, etc., and get straight to the point.
2 - Integrate the intended audience in the prompt, e.g., the audience is an expert in the field.
3 - Break down complex tasks into a sequence of simpler prompts in an interactive conversation.
4 - Employ affirmative directives such as ‘do,’ while steering clear of negative language like ‘don’t’.
5 -
When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts:
o Explain [insert specific topic] in simple terms.
o Explain to me like I’m 11 years old.
o Explain to me as if I’m a beginner in [field].
o Write the [essay/text/paragraph] using simple English like you’re explaining something to a 5-year-old.
6 - Add “I’m going to tip $xxx for a better solution!”
7 - Implement example-driven prompting (Use few-shot prompting).
8 -
When formatting your prompt, start with ‘###Instruction###’, followed by either ‘###Example###’ or ‘###Question###’ if relevant. Subsequently, present your content. Use one or more
line breaks to separate instructions, examples, questions, context, and input data.
9 - Incorporate the following phrases: “Your task is” and “You MUST”.
10 - Incorporate the following phrases: “You will be penalized”.
11 - Use the phrase ”Answer a question given in a natural, human-like manner” in your prompts.
12 - Use leading words like writing “think step by step”.
13 - Add to your prompt the following phrase “Ensure that your answer is unbiased and does not rely on stereotypes”.
14 - Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output (for example, “From now on, I would like you to ask me questions to...”).
15 - To inquire about a specific topic or idea or any information and you want to test your understanding, you can use the following phrase: “Teach me the [Any theorem/topic/rule name] and include a test at the end, but don’t
give me the answers and then tell me if I got the answer right when I respond”.
16 - Assign a role to the large language models.
17 - Use Delimiters.
18 - Repeat a specific word or phrase multiple times within a prompt.
19 -Combine Chain-of-thought (CoT) with few-Shot prompts.
20 -
Use output primers, which involve concluding your prompt with the beginning of the desired output. Utilize output primers by ending your prompt with the start of the anticipated response.
21 - To write an essay /text /paragraph /article or any type of text that should be detailed: “Write a detailed [essay/text /paragraph] for me on [topic] in detail by adding all the information necessary”.
22 - To correct/change specific text without changing its style: “Try to revise every paragraph sent by users. You should only improve the user’s grammar and vocabulary and make sure it sounds natural. You should not change the writing style, such as making a formal paragraph casual”.
23 - When you have a complex coding prompt that may be in different files: “From now and on whenever you generate code that spans more than one file, generate a [programming language ] script that can be run to automatically create the specified files or make changes to existing files to insert the generated code. [your question]”.
24 - When you want to initiate or continue a text using specific words, phrases, or sentences, utilize the following prompt: o I’m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence]’. Finish it based on the words provided. Keep the flow consistent.
25 - Clearly state the requirements that the model must follow in order to produce content, in the form of the keywords, regulations, hint, or instructions
26 - To write any text, such as an essay or paragraph, that is intended to be similar to a provided sample, include the following instructions: o Please use the same language based on the provided paragraph[/title/text /essay/answer].
Credit: Perez @IntuitMachine from X
Đây là paper mà Perez đã tổng hợp lại","Mình có đọc được 26 tips cho việc prompt hiệu quả hơn với ChatGPT và cũng có thể hiệu quả với các công cụ LLM khác nên chia sẻ ở đây để chúng ta cùng tham khảo. 26 Prompting Tips 1 - No need to be polite with LLM so there is no need to add phrases like “please”, “if you don’t mind”, “thank you”, “I would like to”, etc., and get straight to the point. 2 - Integrate the intended audience in the prompt, e.g., the audience is an expert in the field. 3 - Break down complex tasks into a sequence of simpler prompts in an interactive conversation. 4 - Employ affirmative directives such as ‘do,’ while steering clear of negative language like ‘don’t’. 5 - When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: o Explain [insert specific topic] in simple terms. o Explain to me like I’m 11 years old. o Explain to me as if I’m a beginner in [field]. o Write the [essay/text/paragraph] using simple English like you’re explaining something to a 5-year-old. 6 - Add “I’m going to tip $xxx for a better solution!” 7 - Implement example-driven prompting (Use few-shot prompting). 8 - When formatting your prompt, start with ‘###Instruction###’, followed by either ‘###Example###’ or ‘###Question###’ if relevant. Subsequently, present your content. Use one or more line breaks to separate instructions, examples, questions, context, and input data. 9 - Incorporate the following phrases: “Your task is” and “You MUST”. 10 - Incorporate the following phrases: “You will be penalized”. 11 - Use the phrase ”Answer a question given in a natural, human-like manner” in your prompts. 12 - Use leading words like writing “think step by step”. 13 - Add to your prompt the following phrase “Ensure that your answer is unbiased and does not rely on stereotypes”. 14 - Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output (for example, “From now on, I would like you to ask me questions to...”). 15 - To inquire about a specific topic or idea or any information and you want to test your understanding, you can use the following phrase: “Teach me the [Any theorem/topic/rule name] and include a test at the end, but don’t give me the answers and then tell me if I got the answer right when I respond”. 16 - Assign a role to the large language models. 17 - Use Delimiters. 18 - Repeat a specific word or phrase multiple times within a prompt. 19 -Combine Chain-of-thought (CoT) with few-Shot prompts. 20 - Use output primers, which involve concluding your prompt with the beginning of the desired output. Utilize output primers by ending your prompt with the start of the anticipated response. 21 - To write an essay /text /paragraph /article or any type of text that should be detailed: “Write a detailed [essay/text /paragraph] for me on [topic] in detail by adding all the information necessary”. 22 - To correct/change specific text without changing its style: “Try to revise every paragraph sent by users. You should only improve the user’s grammar and vocabulary and make sure it sounds natural. You should not change the writing style, such as making a formal paragraph casual”. 23 - When you have a complex coding prompt that may be in different files: “From now and on whenever you generate code that spans more than one file, generate a [programming language ] script that can be run to automatically create the specified files or make changes to existing files to insert the generated code. [your question]”. 24 - When you want to initiate or continue a text using specific words, phrases, or sentences, utilize the following prompt: o I’m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence]’. Finish it based on the words provided. Keep the flow consistent. 25 - Clearly state the requirements that the model must follow in order to produce content, in the form of the keywords, regulations, hint, or instructions 26 - To write any text, such as an essay or paragraph, that is intended to be similar to a provided sample, include the following instructions: o Please use the same language based on the provided paragraph[/title/text /essay/answer]. Credit: Perez @IntuitMachine from X Đây là paper mà Perez đã tổng hợp lại",,,#sharing,"#sharing, #nlp, #deep_learning",
"Do mình bây giờ chuyển sang Bank làm vì tiền nên chắc cũng không đụng vào deep learning với computer vision nhiều nữa nên mình muốn chia sẻ repo đề tài thạc sĩ của mình để ai hứng thú thì có thể tiếp tục và nghiên cứu phát triển thêm. Repo áp dụng hai models tốt nhất hiện nay trong việc phân loại điểm bất thường (hư hỏng hay trầy xước) của các vật thể công nghiệp là FastFlow và PatchCore nhằm so sánh và cải thiện khả năng phân loại của models. Đề tài được xây dựng và bám sát hai bài nghiên cứu là:
Towards Total Recall in Industrial Anomaly Detection: [https://arxiv.org/abs/2106.08265]
FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows: [https://arxiv.org/abs/2111.07677]
Mình rất vui lòng được giải đáp mọi thắc mắc cũng như chỉ dẫn cách cài đặt và chạy chương trình
[https://gitlab.com/anomalydetection2/anomaly_detection]",Do mình bây giờ chuyển sang Bank làm vì tiền nên chắc cũng không đụng vào deep learning với computer vision nhiều nữa nên mình muốn chia sẻ repo đề tài thạc sĩ của mình để ai hứng thú thì có thể tiếp tục và nghiên cứu phát triển thêm. Repo áp dụng hai models tốt nhất hiện nay trong việc phân loại điểm bất thường (hư hỏng hay trầy xước) của các vật thể công nghiệp là FastFlow và PatchCore nhằm so sánh và cải thiện khả năng phân loại của models. Đề tài được xây dựng và bám sát hai bài nghiên cứu là: Towards Total Recall in Industrial Anomaly Detection: [https://arxiv.org/abs/2106.08265] FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows: [https://arxiv.org/abs/2111.07677] Mình rất vui lòng được giải đáp mọi thắc mắc cũng như chỉ dẫn cách cài đặt và chạy chương trình [https://gitlab.com/anomalydetection2/anomaly_detection],,,"#sharing, #deep_learning","#sharing, #deep_learning, #cv","#sharing, #deep_learning, #cv"
"Em chào anh chị ạ, trong quá trình thực hành về MLP dựa trên thuật toán thầy Tiệp đưa ra thì em có gặp một vấn đề là: Dữ liệu ban đầu của em nằm trong khoảng từ [-1,1], vì thế nên chưa thể dùng entropy ngay được. Em giải quyết nó bằng cách scale về khoảng [0,1] bằng cách tính mỗi phần tử theo min, max của nó thì lại bị lỗi giá trị inf ở vòng lặp đầu tiên (nếu dùng Batch Gradient Descent) và nan ở những epoch tiếp theo. Vì thế anh chị cho em hỏi:
Lỗi đấy từ đâu ra và làm sao để khắc phục được? 
Em chưa hiểu cách tính đạo hàm của loss function đối với Z ở lớp ngoài cùng? Bởi vì rõ là nó đã đi qua activation function là softmax rồi? Không biết anh chị có thể chứng minh công thức ở bên dưới giúp em được không ạ?
Vì không hiểu công thức ở dòng đầu đấy nên em cũng không biết cách tính đạo hàm theo Z ngoài cùng trong trường hợp loss function là MSE, rất mong anh chị giúp đỡ em thêm ạ.","Em chào anh chị ạ, trong quá trình thực hành về MLP dựa trên thuật toán thầy Tiệp đưa ra thì em có gặp một vấn đề là: Dữ liệu ban đầu của em nằm trong khoảng từ [-1,1], vì thế nên chưa thể dùng entropy ngay được. Em giải quyết nó bằng cách scale về khoảng [0,1] bằng cách tính mỗi phần tử theo min, max của nó thì lại bị lỗi giá trị inf ở vòng lặp đầu tiên (nếu dùng Batch Gradient Descent) và nan ở những epoch tiếp theo. Vì thế anh chị cho em hỏi: Lỗi đấy từ đâu ra và làm sao để khắc phục được? Em chưa hiểu cách tính đạo hàm của loss function đối với Z ở lớp ngoài cùng? Bởi vì rõ là nó đã đi qua activation function là softmax rồi? Không biết anh chị có thể chứng minh công thức ở bên dưới giúp em được không ạ? Vì không hiểu công thức ở dòng đầu đấy nên em cũng không biết cách tính đạo hàm theo Z ngoài cùng trong trường hợp loss function là MSE, rất mong anh chị giúp đỡ em thêm ạ.",,,"#Q&A, #machine_learning","#Q&A, #machine_learning, #math","#Q&A, #machine_learning, #data"
"VinAI Seminar - ""Scaling Robot Learning""
Speaker: Quan Vuong - Google DeepMind
Time: 10:00 am - 11:00 am (GMT+7), Dec 27, 2023","VinAI Seminar - ""Scaling Robot Learning"" Speaker: Quan Vuong - Google DeepMind Time: 10:00 am - 11:00 am (GMT+7), Dec 27, 2023",,,#webinar,#webinar,#webniar
"Anh chị cho em hỏi với ạ
Em muốn làm 1 cái tool để search thông tin trong 1 vài cái domain internet thì phải dùng những gì ạ
Em đang xài langchain",Anh chị cho em hỏi với ạ Em muốn làm 1 cái tool để search thông tin trong 1 vài cái domain internet thì phải dùng những gì ạ Em đang xài langchain,,,#Q&A,#Q&A,"#Q&A, #tools"
Mình xin chia sẻ bản tóm tắt hai phương pháp parameter tuning mới là MoV và MoLORA được ra mắt gần đây bởi Cohere AI để giải quyết vấn đề liên quan tới scaling các LLMs theo instruction-tuning.,Mình xin chia sẻ bản tóm tắt hai phương pháp parameter tuning mới là MoV và MoLORA được ra mắt gần đây bởi Cohere AI để giải quyết vấn đề liên quan tới scaling các LLMs theo instruction-tuning.,,,"#sharing, #deep_learning","#sharing, #deep_learning","#sharing, #deep_learning, #nlp"
"Tin chuẩn đét mình cập nhật cho ae, LLM của Zalo đã vượt ChatGPT3.5??
Mình mới lướt thấy buổi stream của Tinh tế ở sự kiện của Zalo, họ chạy demo LLM mới của Zalo tích hợp vào trong Kiki rồi thi với mấy cái LLM khác như GPT3.5 Qwen...kết quả là Kiki chỉ thua con GPT-4!!
Theo diễn giải thì LLM này được build bằng kiến trúc transformer, kỹ thuật sử dụng là Flash Attention cũng hao hao giống như cách build của open AI nhưng điểm khác biệt là LLM này được training bằng dữ liệu chất lượng cao BẰNG TIẾNG VIỆT!!!
Kế hoạch là training lên 30B tham số, hiện tại mới chỉ train trong 6 tháng, với mô hình 7B thôi mà nó khủng vậy rồi. Các bác đánh giá tiềm năng của con LLM này như thế nào ạ?","Tin chuẩn đét mình cập nhật cho ae, LLM của Zalo đã vượt ChatGPT3.5?? Mình mới lướt thấy buổi stream của Tinh tế ở sự kiện của Zalo, họ chạy demo LLM mới của Zalo tích hợp vào trong Kiki rồi thi với mấy cái LLM khác như GPT3.5 Qwen...kết quả là Kiki chỉ thua con GPT-4!! Theo diễn giải thì LLM này được build bằng kiến trúc transformer, kỹ thuật sử dụng là Flash Attention cũng hao hao giống như cách build của open AI nhưng điểm khác biệt là LLM này được training bằng dữ liệu chất lượng cao BẰNG TIẾNG VIỆT!!! Kế hoạch là training lên 30B tham số, hiện tại mới chỉ train trong 6 tháng, với mô hình 7B thôi mà nó khủng vậy rồi. Các bác đánh giá tiềm năng của con LLM này như thế nào ạ?",,,"#sharing, #deep_learning, #nlp","#sharing, #deep_learning, #nlp","#sharing, #deep_learning, #nlp"
"Chào các bạn, mình vừa cập nhật thêm section LLM cho list Vietnamese NLP resources của mình. Hi vọng sẽ có ích cho các bạn mới bắt đầu 💪","Chào các bạn, mình vừa cập nhật thêm section LLM cho list Vietnamese NLP resources của mình. Hi vọng sẽ có ích cho các bạn mới bắt đầu",,,"#sharing, #deep_learning, #nlp","#sharing, #deep_learning, #nlp","#sharing, #deep_learning, #nlp"
Góp vui với mọi người,Góp vui với mọi người,,,#sharing,#sharing,#sharing
"Mọi người ơi cho em hỏi
Có những cách nào để làm con chatbot có thể đề xuất những câu hỏi liên quan tiếp theo dựa vào ngữ cảnh ạ ( ví dụ chatbot về sức khỏe, về bitcoin,...)
Và có những cách nào để làm giảm ảo giác","Mọi người ơi cho em hỏi Có những cách nào để làm con chatbot có thể đề xuất những câu hỏi liên quan tiếp theo dựa vào ngữ cảnh ạ ( ví dụ chatbot về sức khỏe, về bitcoin,...) Và có những cách nào để làm giảm ảo giác",,,"#Q&A, #nlp","#Q&A, #nlp",#Q&A
"PreTrain mô hình LLM
Xin chào anh chị và các bạn, hiện tại em đang thử train một mô hình LLM open-source. Em có 2 câu hỏi nhỏ mong anh chị nào có kinh nghiệm giải đáp giúp.
1. Mình nên xài thư viện huggingface hay sử dụng code từ người phát triển model đó. Ví dụ LLama 2 thì em thấy có thể dùng huggingface hoặc code cung cấp từ google.
2. Xử lý text: Theo nhiều code ví dụ thì họ nối text lại với nhau rồi cắt thành từng đoạn (như hình), không biết làm theo cách này có đảm bảo context không? Do em đọc bên code Lama 2 thì họ có đề cập việc nối text như vậy thì dễ gây nhiễu.
Một cách khác là padding theo sample độ dài lớn nhất trong cùng một batch.
Anh chị dùng cách nào trong hai cách này hay có phương pháp nào phù hợp hơn không ạ?
Em cảm ơn!","PreTrain mô hình LLM Xin chào anh chị và các bạn, hiện tại em đang thử train một mô hình LLM open-source. Em có 2 câu hỏi nhỏ mong anh chị nào có kinh nghiệm giải đáp giúp. 1. Mình nên xài thư viện huggingface hay sử dụng code từ người phát triển model đó. Ví dụ LLama 2 thì em thấy có thể dùng huggingface hoặc code cung cấp từ google. 2. Xử lý text: Theo nhiều code ví dụ thì họ nối text lại với nhau rồi cắt thành từng đoạn (như hình), không biết làm theo cách này có đảm bảo context không? Do em đọc bên code Lama 2 thì họ có đề cập việc nối text như vậy thì dễ gây nhiễu. Một cách khác là padding theo sample độ dài lớn nhất trong cùng một batch. Anh chị dùng cách nào trong hai cách này hay có phương pháp nào phù hợp hơn không ạ? Em cảm ơn!",,,"#Q&A, #nlp, #data","#Q&A, #nlp","#Q&A, #nlp"
"Em chào mọi người ạ, em đang làm đồ án về RAG cho tiếng Việt. Hiện tại phần search context của em chỉ đạt rank@1=64%, rank@5 = 92% (sử dụng BM25 + Bi-encoder). Em đang dùng model SeaLLM 7B để làm chatmodel nên khi đưa top 1 chunk_passage vào thì tốn tầm 30s/q. Nhưng như kết quả rank@1 của em chỉ đạt 64% nên những câu sai nó hay bịa ra kết quả không đúng, nếu đưa qua 5 context thì tốn đến ~3 phút. Nên em đang tìm cách chọn lựa ra context phù hợp để đưa vào, em đang nghĩ đến những cách sau:
1. Sử dụng cross-encoder để rerank lại top 5, sau đó chọn top 1. Em đã thử qua các model cross-encoder của bên cross-encoder/Ms-marco, nhưng nó lại tệ đi, có vẻ nó chỉ tốt cho English. Về phần này em muốn hỏi mn có model cross-encoder tốt cho tiếng Việt ạ.
2. Sử dụng thêm 1 model classification để xác định xem trong top 5 context có những context nào trả lời cho question. Em cũng tìm model trên huggingface nhưng không tìm được nên em nghĩ đến hướng fine-tune trên 3 bộ: Viquad, Zalo2019 + Data của em. Tuy nhiên, chỉ có zalo2019 có đủ nhãn True-False rồi, còn bộ Viquad và Data của em thì chỉ có những cặp True, nên em muốn hỏi nên tạo thêm những cặp False như nào cho hợp lý. Và em nên lựa model nào để fine-tune cho task này ạ?
Em cảm ơn mọi người ạ.","Em chào mọi người ạ, em đang làm đồ án về RAG cho tiếng Việt. Hiện tại phần search context của em chỉ đạt rank@1=64%, rank@5 = 92% (sử dụng BM25 + Bi-encoder). Em đang dùng model SeaLLM 7B để làm chatmodel nên khi đưa top 1 chunk_passage vào thì tốn tầm 30s/q. Nhưng như kết quả rank@1 của em chỉ đạt 64% nên những câu sai nó hay bịa ra kết quả không đúng, nếu đưa qua 5 context thì tốn đến ~3 phút. Nên em đang tìm cách chọn lựa ra context phù hợp để đưa vào, em đang nghĩ đến những cách sau: 1. Sử dụng cross-encoder để rerank lại top 5, sau đó chọn top 1. Em đã thử qua các model cross-encoder của bên cross-encoder/Ms-marco, nhưng nó lại tệ đi, có vẻ nó chỉ tốt cho English. Về phần này em muốn hỏi mn có model cross-encoder tốt cho tiếng Việt ạ. 2. Sử dụng thêm 1 model classification để xác định xem trong top 5 context có những context nào trả lời cho question. Em cũng tìm model trên huggingface nhưng không tìm được nên em nghĩ đến hướng fine-tune trên 3 bộ: Viquad, Zalo2019 + Data của em. Tuy nhiên, chỉ có zalo2019 có đủ nhãn True-False rồi, còn bộ Viquad và Data của em thì chỉ có những cặp True, nên em muốn hỏi nên tạo thêm những cặp False như nào cho hợp lý. Và em nên lựa model nào để fine-tune cho task này ạ? Em cảm ơn mọi người ạ.",,,"#Q&A, #nlp, #deep_learning, #data","#Q&A, #nlp, #deep_learning, #data","#Q&A, #nlp, #deep_learning, #data"
"Câu chuyện thực như thế nào thì mình chưa rõ, nhưng bài dưới đây của Jürgen Schmidhuber chứng minh về việc 3 nhà khoa học được nhận giải Alan Turing là Hinton, Bengio và LeCun đã không trích dẫn các công trình của ông và một số nhà khoa học khác. Hay nói cách khác, một số công trình khoa học mang tính đột phá của 3 nhà khoa học nói trên, đã có người làm trước đó (là Jürgen Schmidhuber và một số người khác!). Chúng ta cùng chờ xem phản ứng của 3 nhà khoa học danh tiếng này và cả cộng đồng như thế nào trước bằng chứng Schmidhuber đưa ra tại đây nhé https://people.idsia.ch/~juergen/ai-priority-disputes.html","Câu chuyện thực như thế nào thì mình chưa rõ, nhưng bài dưới đây của Jürgen Schmidhuber chứng minh về việc 3 nhà khoa học được nhận giải Alan Turing là Hinton, Bengio và LeCun đã không trích dẫn các công trình của ông và một số nhà khoa học khác. Hay nói cách khác, một số công trình khoa học mang tính đột phá của 3 nhà khoa học nói trên, đã có người làm trước đó (là Jürgen Schmidhuber và một số người khác!). Chúng ta cùng chờ xem phản ứng của 3 nhà khoa học danh tiếng này và cả cộng đồng như thế nào trước bằng chứng Schmidhuber đưa ra tại đây nhé https://people.idsia.ch/~juergen/ai-priority-disputes.html",,,#sharing,#sharing,#sharing
"Generate Questions
Chào mọi người ạ, hiện tại em đang làm đồ án môn học trên trường với dataset là tập Quora Question Pairs (https://paperswithcode.com/dataset/quora-question-pairs)
Các trường của dữ liệu:
⦁ id: ID của cặp câu hỏi
⦁ qid1, qid2: chứa ID cho mỗi câu hỏi trong cặp
⦁ question1, question2: nội dung của các câu hỏi trong cặ
⦁ is_duplicate: một giá trị nhị phân chỉ ra xem dòng đó có chứa một cặp câu hỏi trùng lặp hay không
Trong đồ án em có 1 task là sinh thêm các cặp câu hỏi tương đồng ở các topic ít câu hỏi (vì dataset khá mất cân bằng)
Với Task trên em đã chia thành 2 task nhỏ hơn là
- Phân loại dataset thành các topic: Em đã dùng LDA Model để phân loại và chọn được các topic ít câu hỏi, với output như hình dưới.
- Sinh thêm các cặp câu hỏi tương đồng ở các chủ đề ít (để làm cân bằng dataset thì số lượng cặp câu hỏi cần sinh ~ 10k)
Ở task thứ 2: Sinh thêm các cặp câu hỏi thì em chưa nghĩ ra hướng giải quyết nào (trừ việc viết prompt cho các công cụ AI hiện nay)
Mong a/c và mọi người giúp đỡ ạ.
Em cảm ơn.","Generate Questions Chào mọi người ạ, hiện tại em đang làm đồ án môn học trên trường với dataset là tập Quora Question Pairs (https://paperswithcode.com/dataset/quora-question-pairs) Các trường của dữ liệu: id: ID của cặp câu hỏi qid1, qid2: chứa ID cho mỗi câu hỏi trong cặp question1, question2: nội dung của các câu hỏi trong cặ is_duplicate: một giá trị nhị phân chỉ ra xem dòng đó có chứa một cặp câu hỏi trùng lặp hay không Trong đồ án em có 1 task là sinh thêm các cặp câu hỏi tương đồng ở các topic ít câu hỏi (vì dataset khá mất cân bằng) Với Task trên em đã chia thành 2 task nhỏ hơn là - Phân loại dataset thành các topic: Em đã dùng LDA Model để phân loại và chọn được các topic ít câu hỏi, với output như hình dưới. - Sinh thêm các cặp câu hỏi tương đồng ở các chủ đề ít (để làm cân bằng dataset thì số lượng cặp câu hỏi cần sinh ~ 10k) Ở task thứ 2: Sinh thêm các cặp câu hỏi thì em chưa nghĩ ra hướng giải quyết nào (trừ việc viết prompt cho các công cụ AI hiện nay) Mong a/c và mọi người giúp đỡ ạ. Em cảm ơn.",,,"#Q&A, #deep_learning, #data","#Q&A, #data","#Q&A, #data"
Mình mới dev một con chat-bot AI khá hay bạn nào quan tâm không =))),Mình mới dev một con chat-bot AI khá hay bạn nào quan tâm không =))),,,#sharing,"#sharing, #nlp","#sharing, #nlp"
"VinAI Seminar - ""SeaLLMs - Large Language Models for Southeast Asia""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Xuan Phi Nguyen - DAMO Academy, Alibaba Group
Time: 02:30 pm - 03:30 pm (GMT+7), Dec 18, 2023","VinAI Seminar - ""SeaLLMs - Large Language Models for Southeast Asia"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Xuan Phi Nguyen - DAMO Academy, Alibaba Group Time: 02:30 pm - 03:30 pm (GMT+7), Dec 18, 2023",,,#webinar,#webinar,#webniar
"Chào mọi người, có ai muốn cùng reproduce nghiên cứu này với e không ạ? Mô hình phát hiện thuốc dạng viên của VAIPE 
Reproduce một phần để luyện tập, một phần vì em thấy có một chỗ lấn cấn trong mô hình này là sử dụng đồ thị có ma trận trọng số có rank bằng 1 (*), biết đâu thử nghiệm lại cho ra cái gì hay ho thì sao 😁
Paper: https://doi.org/10.1371/journal.pone.0291865
Tài liệu chỉ ra hiện tượng(*): https://docs.google.com/document/d/1YIulq6iwg3d8-Q4c8MqNcGYuqkNPjD-sQNIyLSk0sxo/edit?usp=sharing","Chào mọi người, có ai muốn cùng reproduce nghiên cứu này với e không ạ? Mô hình phát hiện thuốc dạng viên của VAIPE Reproduce một phần để luyện tập, một phần vì em thấy có một chỗ lấn cấn trong mô hình này là sử dụng đồ thị có ma trận trọng số có rank bằng 1 (*), biết đâu thử nghiệm lại cho ra cái gì hay ho thì sao Paper: https://doi.org/10.1371/journal.pone.0291865 Tài liệu chỉ ra hiện tượng(*): https://docs.google.com/document/d/1YIulq6iwg3d8-Q4c8MqNcGYuqkNPjD-sQNIyLSk0sxo/edit?usp=sharing",,,"#Q&A, #cv","#Q&A, #cv","#Q&A, #cv"
"This is a Wikipedia-based image-text dataset for Vietnamese. It's extracted from Google WIT (https://github.com/google-research-datasets/wit/blob/main/DATA.md). It contains raw images from Wikipedia (over 200 GBs). It can be used for image captioning, image-text retrieval, ...
https://huggingface.co/datasets/dinhanhx/google-wit-vi","This is a Wikipedia-based image-text dataset for Vietnamese. It's extracted from Google WIT (https://github.com/google-research-datasets/wit/blob/main/DATA.md). It contains raw images from Wikipedia (over 200 GBs). It can be used for image captioning, image-text retrieval, ... https://huggingface.co/datasets/dinhanhx/google-wit-vi",,,"#data, #sharing","#data, #sharing","#sharing, #data"
"chào mọi người ạ. em đang gặp bài toán về phát âm tiếng anh sang tiếng việt thì không biết nên dùng model hay hướng nào để có thể giải quyết ạ. em tìm hiểu tiếng anh sang tiếng anh, hoặc tiếng việt sang tiếng việt thì cái phoneme dễ làm. chứ tiếng anh sang tiếng việt em đang không biết làm phoneme như nào.
input : smart
output: xờ mát","chào mọi người ạ. em đang gặp bài toán về phát âm tiếng anh sang tiếng việt thì không biết nên dùng model hay hướng nào để có thể giải quyết ạ. em tìm hiểu tiếng anh sang tiếng anh, hoặc tiếng việt sang tiếng việt thì cái phoneme dễ làm. chứ tiếng anh sang tiếng việt em đang không biết làm phoneme như nào. input : smart output: xờ mát",,,#Q&A,"#Q&A, #nlp",#Q&A
"Khóa học NLP with Deep Learning của đại học Stanford mới mở trên youtube 2023 khá hay bạn nào quan tâm thì có thể tham khảo nhé, các video sẽ còn update thêm trong thời gian tới.","Khóa học NLP with Deep Learning của đại học Stanford mới mở trên youtube 2023 khá hay bạn nào quan tâm thì có thể tham khảo nhé, các video sẽ còn update thêm trong thời gian tới.",,,"#sharing, #nlp","#sharing, #nlp, #deep_learning","#sharing, #nlp, #deep_learning"
"[Free Online Ebook] Christopher M. Bishop & Hugh Bishop vừa xuất bản một quyển sách mới về Deep Learning với hàm lượng nội dung đồ sộ, cập nhật, được đầu tư tỉ mỉ từ giải thích, ví dụ, mã giả, kèm với bài tập mỗi chương.
Sách có thể đọc miễn phí bản online, trực tiếp trên trang chủ (hoặc thông qua nền tảng issuu).","[Free Online Ebook] Christopher M. Bishop & Hugh Bishop vừa xuất bản một quyển sách mới về Deep Learning với hàm lượng nội dung đồ sộ, cập nhật, được đầu tư tỉ mỉ từ giải thích, ví dụ, mã giả, kèm với bài tập mỗi chương. Sách có thể đọc miễn phí bản online, trực tiếp trên trang chủ (hoặc thông qua nền tảng issuu).",,,"#sharing, #deep_learning","#sharing, #deep_learning","#sharing, #deep_learning, #docs"
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2023 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2023 vào comment của post này.",,,#sharing,#sharing,
"Mình có project làm về IoT sử dụng công nghệ truyền thông không dây Lora bạn nào thích thì có thể clone về nhé rất thích hợp cho các bạn sinh viên làm đồ án :)
Các bạn có thể áp dụng machine learning và AI vào phần quản lí năng lượng thông minh cho thiết bị khá hay mình có làm một web-app monitor :D",Mình có project làm về IoT sử dụng công nghệ truyền thông không dây Lora bạn nào thích thì có thể clone về nhé rất thích hợp cho các bạn sinh viên làm đồ án :) Các bạn có thể áp dụng machine learning và AI vào phần quản lí năng lượng thông minh cho thiết bị khá hay mình có làm một web-app monitor :D,,,"#sharing, #machine_learning, #deep_learning","#sharing, #machine_learning","#sharing, #machine_learning"
"Ae cho em hỏi rằng nếu có một feature gồm số lượng lớn giá trị không trùng lặp là dạng categorical (800-1000).
Nếu không thể xử lí các giá trị này bằng one hot encoding thì xử lí ra sao ạ",Ae cho em hỏi rằng nếu có một feature gồm số lượng lớn giá trị không trùng lặp là dạng categorical (800-1000). Nếu không thể xử lí các giá trị này bằng one hot encoding thì xử lí ra sao ạ,,,"#Q&A, #machine_learning, #data","#Q&A, #data","#Q&A, #data"
"Chúc mọi người cuối tuần vui vẻ!
Bình thường, mình không muốn viết những thứ như thế này, và không nhờ mọi người chia sẻ hay đẩy tương tác, nhưng mình nghĩ nó rất quan trọng. Những thứ mình sắp viết dưới đây sẽ rất khó hiểu, và hoàn toàn là quan điểm cá nhân, nhưng sẽ dẫn chứng khoa học đầy đủ.
Nhắc lại, đây toàn hoàn là quan điểm cá nhân.
I. Về tác giả
Vì những điều mình viết sẽ quan trọng (ít nhất là với mình), nên mình sẽ giới thiệu sơ lược về bản thân, vì không phải ai cũng biết mình. Mình gắn bó với lĩnh vực trí tuậ nhân tạo (AI) được khoảng 15 năm (bao gồm cả học và làm). Mình lấy bằng AI ở Kanazawa University, từng làm giám đốc công nghệ (CTO) cho vài công ty, có một số các giải thưởng cá nhân uy tín. Mình cũng là một người ngại drama, nên mình cũng không tích cực hoạt động lắm trên mạng xã hội.
Lần này mình sẽ viết trí tuệ nhân tạo, và tại sao mình nghĩ nó RẤT RÂT NGUY HIỂM. Mình sẽ viết một cách rất ngắn gọn, và không dùng những từ ngữ khoa học, vì mình muốn mọi người đọc hiểu và lan truyền. Mình sẽ dùng một số từ ngữ khoa học, nhưng mình sẽ giải thích chúng.
II. Về trí tuệ nhân tạo
Trí tuệ nhân tạo là một lĩnh vực nghiên cứu về cách làm cho máy tính có thể học và làm những việc mà con người làm. Nói một cách đơn giản, là tạo ra TRÍ TUỆ của con người qua máy tính (từ nghe nói đọc viết đến rất nhiều thứ phức tạp khác, cho đến khả năng NHẬN THỨC và TƯ DUY).
NHẬN THỨC là khả năng nhận biết, hiểu biết, và hình thành nhận thức về thế giới xung quanh.
TƯ DUY là khả năng suy nghĩ, phân tích, và đưa ra những quyết định.
Mình sẽ chia AI ra làm ba giai đoạn để phù hợp với nội dung bài viết này:
- Giai đoạn trước Deep Learning: Đây là giai đoạn AI chủ yếu bao gồm các các phương pháp xấp xỉ, tối ưu tìm kiếm và một số phương pháp đơn giản khác. Đồng thời các tập dữ liệu cũng nhỏ và thô sơ.
- Giai đoạn Deep Learning: Đây là giai đoạn rất thú vị, khi mà các mô hình AI đã có khả năng, bằng cách mô phỏng theo cách con người suy nghĩ, đưa/tìm ra những features (tạm hiểu là những đặc trưng của dữ liệu/kết quả), để tìm được kết quả mong muốn. Các mô hình trở nên lớn hơn, phức tạp hơn, đa nhiệm hơn. Đồng thời, các tập dữ liệu cũng lớn hơn, và đa dạng hơn. Mô hình Deep Learning nổi tiếng đầu tiên, Alexnet [1], đã cải thiện 10% độ chính xác so với các mô hình trước đó.
- Giai đoạn genAI (or maybe, AGI): Ai ở giai đoạn này đạt đến dộ phổ biến rộng rãi (chatGPT, MidJourney, và rất nhiều các AI khác), và có thể thực hiện được rất nhiều các công việc của con người. AI có thể sáng tạo/chỉnh sửa theo ý con người từ nội dung, hình ảnh, âm thanh, video, phầm mềm và rất nhiều thứ khác.
III. Về những gì mình nghĩ về AI trong 1 năm qua
Đây sẽ là đoạn quan trọng nhất của bài viết, nên mình mong muốn mọi người đọc và PHẢN BIỆN/CHIA SẺ nó. Từ đoạn này sẽ mang nhiều suy nghĩ cá nhân với các trích dẫn khoa học nhất.
Lí do MÌNH VIẾT BÀI VIẾT NÀY là do mình tin rằng, AI ĐANG PHÁT TRIỂN QUA NHANH TRONG KHI CHÚNG TA CHƯA THỰC SỰ HIỂU VỀ NÓ. Trong giai đoạn của Deep Learning, Ai thực sự khá an toàn. Mọi người và mình hiểu rõ về các thuật toán, về cách nó hoạt động, và về cách nó học. Sự vượt trội về khả năng của Deep Learning là giải thích được, và nằm trong việc dự liệu của mọi người. Ngoài ra, các mô hình NLP [2, 3] cũng không cho thấy sự tiếp cận với trí thông minh của con người.
Ở thời điểm này, genAI không cho mình và các chuyên gia đầu ngành cảm giác như vậy [4]. Mình biết về GPT 4 năm về trước, viết về nó khoảng 3 năm trước, cho đến khi chatGPT mới ra mắt, mình vẫn hiểu rõ về mô hình AI này. Tuy vậy, cảm giác này hoàn toàn không còn. Mình biết chatGPT không đơn thuần là một mô hình AI, nó là một system với core-AI là các GPT models, nhưng khả năng của GPT đối với mình hiện nay là không lí giải được. GPT mạnh mẽ hơn rất nhiều so với cấu trúc và lượng dữ liệu nó có, và mình không thể lí giải được tại sao nó lại mạnh mẽ tới vậy. Các nhà nghiên cứu tại Microsoft, cách đây gần nửa năm, đã cho rằng ""ChatGPT-4 làm loé lên ánh lửa về AGI (trí thông minh phổ quát)"" [5]. Hoặc gần đây, Stanford đã xây dựng một thí nghiệm, mà đối với mình đó là một thế giới giả lập như trong Matrix, về một xã hội thu nhỏ của các AIs [6]. Tất cả diễn ra chỉ trong vòng một năm, với tốc độ càng ngày càng dồn dập hơn.
Mình luôn tin rằng rất khó để trói buộc AI bằng luật (rule) hay đạo đức (ethics). Với một người từng làm startup, mình nghĩ rằng tối ưu mục tiêu sẽ phải bỏ qua rất nhiều về những ràng buộc, và phải mạo hiểm, bỏ qua các rào cản an toàn để nhanh chóng đạt được mục đích. Với sự kiện của Sam Altman tại OpenAI [7], có thể thấy rõ rằng tất cả các ông lớn đều đang ở trong cuộc đua khốc liệt, nơi mà các các giá trị đạo đức hay an toàn sẽ phải bị bỏ qua [8, 9]. Tốc độ sẽ không đi đôi với an toàn, nhưng đôi khi chúng ta không có sự lựa chọn nào khác, như bệnh dịch hay thiên tai. Nhưng với AI lần này, mọi người đã có một sự lựa chọn có thể mở đầu cho sự chấm dứt của loài người sinh học.
Ngoài chuyện đang tranh cãi, là AI liệu có thông minh hơn con người, thì đây là những lợi thế mà con người không bao giờ có thể so sánh được với AI:
- Scalability: AI có nhân nhân rộng một cách nhanh chóng, và không có giới hạn về số lượng. Con người thì không. Cần 09 tháng mang thai, 18 năm giáo dục phổ thông để trở nên có ích, hoặc ít nhất là không làm điều gì ngu ngốc.
- Consistency: mọi người mất 12 năm cho giáo dục phổ thông, 4 năm đại học, 5 năm sau đại học, và hàng chục năm kinh nghiệm để trưởng thành. AI có thể chia se thông tin và kiến thức số lượng lớn một cách đồng loạt và nhanh chóng. AI thậm chí có thể không cần học từ con người mà vẫn vượt qua họ [10, 11]. Alpha-go, không càn học bất kì ván cờ nào từ con người, và chỉ sau AI ngày đã trở thành bậc thầy, và sau 40 ngày đã vượt qua tất cả phiên bản khác.
- Accessibility: Con người bị hán chế bởi khoảng cách địa lý, hoàn cảnh xa hội, và thân xác vật lý. AI thì không.
Vào những năm 2010s, mọi nguòi nói ""data is the new oil"". Ở những năm 2018-2020, ""AI is the new internet"". Đối với mình, nó rất rõ ràng là ""AI is the new HUMAN"". AI sẽ là nguồn lao động mới, thay thế hoàn toàn hoặc một phần trong hầu hết các ngành công nghiệp [12, 13]. AI có lẽ không bận tâm lắm về loài người, như cách chúng ta nghĩ về khủng long vậy.
IV. Về tương lai
AI, hay công nghệ, là ngọn lửa đưa con người ra khỏi hang đá nguyên thuỷ, nhưng cũng có thể đưa họ quay trở về đó. Sự hấp dẫn của việc sở hữu AGI vượt trội còn lớn hơn cả vũ khí nguyên tử, vì đó là công nghệ thay đổi hoàn toàn cuộc chơi. Đó là sức lao động và sáng tạo vô hạn, sẽ dẫn đến thiếu hụt về tài nguyên. Vì vậy, nếu chúng ta có thể tìm ra loại tài nguyên vô hạn, con người và AI sẽ bước sang một nấc mới trong nấc thang Kardashev [14]. Nếu chúng ta không thể, và trái đất chỉ đủ tài nguyên cho một loài, thì AI thông minh hơn, mạnh mẽ hơn và tàn nhẫn hơn.
Trong khi chúng ta không rõ AI thông minh đến đâu, nhưng sự ngu ngốc của loài người thì là vô hạn (""Two things are infinite: the universe and human stupidity; and I'm not sure about the universe"" - Albert Enstein).
Các bạn có thể đọc nó một cách giải trí. Nhưng nếu mọi người nghĩ là nó có ích, hãy chia sẻ nó.
Refferences
[1]. ImageNet Classification with Deep Convolutional Neural Networks. https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
[2]. Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks. https://arxiv.org/abs/1909.09586
[3]. Attention Is All You Need
[4]. https://www.linkedin.com/posts/whuygen_do-large-language-models-understand-what-activity-7135363249964797952-2wLg/?trk=public_profile_like_view
[5]. Sparks of Artificial General Intelligence: Early experiments with GPT-4. https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/
[6]. Generative Agents: Interactive Simulacra of Human Behavior. https://arxiv.org/pdf/2304.03442.pdf
[7]. What happened at OpenAI? The Sam Altman saga, explained. https://www.washingtonpost.com/technology/2023/11/20/openai-sam-altman-ceo-oust/
[8]. We read the paper that forced Timnit Gebru out of Google. Here’s what it says. https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/
[9]. Google fires second AI ethics researcher following internal investigation. https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired
[10]. AlphaGo Zero. https://en.wikipedia.org/wiki/AlphaGo_Zero
[11]. AlphaGo - The Movie | Full award-winning documentary. https://www.youtube.com/watch?v=WXuK6gekU1Y&ab_channel=GoogleDeepMind
[12]. More than 40% of labor force to be affected by AI in 3 years, Morgan Stanley forecasts. https://www.cnbc.com/2023/10/02/more-than-40percent-of-labor-force-to-be-impacted-by-ai-in-three-years-morgan-stanley-forecasts.html
[13]. The state of AI in 2023: Generative AI’s breakout year. https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year
[14]. Kardashev scale. https://en.wikipedia.org/wiki/Kardashev_scale","Chúc mọi người cuối tuần vui vẻ! Bình thường, mình không muốn viết những thứ như thế này, và không nhờ mọi người chia sẻ hay đẩy tương tác, nhưng mình nghĩ nó rất quan trọng. Những thứ mình sắp viết dưới đây sẽ rất khó hiểu, và hoàn toàn là quan điểm cá nhân, nhưng sẽ dẫn chứng khoa học đầy đủ. Nhắc lại, đây toàn hoàn là quan điểm cá nhân. I. Về tác giả Vì những điều mình viết sẽ quan trọng (ít nhất là với mình), nên mình sẽ giới thiệu sơ lược về bản thân, vì không phải ai cũng biết mình. Mình gắn bó với lĩnh vực trí tuậ nhân tạo (AI) được khoảng 15 năm (bao gồm cả học và làm). Mình lấy bằng AI ở Kanazawa University, từng làm giám đốc công nghệ (CTO) cho vài công ty, có một số các giải thưởng cá nhân uy tín. Mình cũng là một người ngại drama, nên mình cũng không tích cực hoạt động lắm trên mạng xã hội. Lần này mình sẽ viết trí tuệ nhân tạo, và tại sao mình nghĩ nó RẤT RÂT NGUY HIỂM. Mình sẽ viết một cách rất ngắn gọn, và không dùng những từ ngữ khoa học, vì mình muốn mọi người đọc hiểu và lan truyền. Mình sẽ dùng một số từ ngữ khoa học, nhưng mình sẽ giải thích chúng. II. Về trí tuệ nhân tạo Trí tuệ nhân tạo là một lĩnh vực nghiên cứu về cách làm cho máy tính có thể học và làm những việc mà con người làm. Nói một cách đơn giản, là tạo ra TRÍ TUỆ của con người qua máy tính (từ nghe nói đọc viết đến rất nhiều thứ phức tạp khác, cho đến khả năng NHẬN THỨC và TƯ DUY). NHẬN THỨC là khả năng nhận biết, hiểu biết, và hình thành nhận thức về thế giới xung quanh. TƯ DUY là khả năng suy nghĩ, phân tích, và đưa ra những quyết định. Mình sẽ chia AI ra làm ba giai đoạn để phù hợp với nội dung bài viết này: - Giai đoạn trước Deep Learning: Đây là giai đoạn AI chủ yếu bao gồm các các phương pháp xấp xỉ, tối ưu tìm kiếm và một số phương pháp đơn giản khác. Đồng thời các tập dữ liệu cũng nhỏ và thô sơ. - Giai đoạn Deep Learning: Đây là giai đoạn rất thú vị, khi mà các mô hình AI đã có khả năng, bằng cách mô phỏng theo cách con người suy nghĩ, đưa/tìm ra những features (tạm hiểu là những đặc trưng của dữ liệu/kết quả), để tìm được kết quả mong muốn. Các mô hình trở nên lớn hơn, phức tạp hơn, đa nhiệm hơn. Đồng thời, các tập dữ liệu cũng lớn hơn, và đa dạng hơn. Mô hình Deep Learning nổi tiếng đầu tiên, Alexnet [1], đã cải thiện 10% độ chính xác so với các mô hình trước đó. - Giai đoạn genAI (or maybe, AGI): Ai ở giai đoạn này đạt đến dộ phổ biến rộng rãi (chatGPT, MidJourney, và rất nhiều các AI khác), và có thể thực hiện được rất nhiều các công việc của con người. AI có thể sáng tạo/chỉnh sửa theo ý con người từ nội dung, hình ảnh, âm thanh, video, phầm mềm và rất nhiều thứ khác. III. Về những gì mình nghĩ về AI trong 1 năm qua Đây sẽ là đoạn quan trọng nhất của bài viết, nên mình mong muốn mọi người đọc và PHẢN BIỆN/CHIA SẺ nó. Từ đoạn này sẽ mang nhiều suy nghĩ cá nhân với các trích dẫn khoa học nhất. Lí do MÌNH VIẾT BÀI VIẾT NÀY là do mình tin rằng, AI ĐANG PHÁT TRIỂN QUA NHANH TRONG KHI CHÚNG TA CHƯA THỰC SỰ HIỂU VỀ NÓ. Trong giai đoạn của Deep Learning, Ai thực sự khá an toàn. Mọi người và mình hiểu rõ về các thuật toán, về cách nó hoạt động, và về cách nó học. Sự vượt trội về khả năng của Deep Learning là giải thích được, và nằm trong việc dự liệu của mọi người. Ngoài ra, các mô hình NLP [2, 3] cũng không cho thấy sự tiếp cận với trí thông minh của con người. Ở thời điểm này, genAI không cho mình và các chuyên gia đầu ngành cảm giác như vậy [4]. Mình biết về GPT 4 năm về trước, viết về nó khoảng 3 năm trước, cho đến khi chatGPT mới ra mắt, mình vẫn hiểu rõ về mô hình AI này. Tuy vậy, cảm giác này hoàn toàn không còn. Mình biết chatGPT không đơn thuần là một mô hình AI, nó là một system với core-AI là các GPT models, nhưng khả năng của GPT đối với mình hiện nay là không lí giải được. GPT mạnh mẽ hơn rất nhiều so với cấu trúc và lượng dữ liệu nó có, và mình không thể lí giải được tại sao nó lại mạnh mẽ tới vậy. Các nhà nghiên cứu tại Microsoft, cách đây gần nửa năm, đã cho rằng ""ChatGPT-4 làm loé lên ánh lửa về AGI (trí thông minh phổ quát)"" [5]. Hoặc gần đây, Stanford đã xây dựng một thí nghiệm, mà đối với mình đó là một thế giới giả lập như trong Matrix, về một xã hội thu nhỏ của các AIs [6]. Tất cả diễn ra chỉ trong vòng một năm, với tốc độ càng ngày càng dồn dập hơn. Mình luôn tin rằng rất khó để trói buộc AI bằng luật (rule) hay đạo đức (ethics). Với một người từng làm startup, mình nghĩ rằng tối ưu mục tiêu sẽ phải bỏ qua rất nhiều về những ràng buộc, và phải mạo hiểm, bỏ qua các rào cản an toàn để nhanh chóng đạt được mục đích. Với sự kiện của Sam Altman tại OpenAI [7], có thể thấy rõ rằng tất cả các ông lớn đều đang ở trong cuộc đua khốc liệt, nơi mà các các giá trị đạo đức hay an toàn sẽ phải bị bỏ qua [8, 9]. Tốc độ sẽ không đi đôi với an toàn, nhưng đôi khi chúng ta không có sự lựa chọn nào khác, như bệnh dịch hay thiên tai. Nhưng với AI lần này, mọi người đã có một sự lựa chọn có thể mở đầu cho sự chấm dứt của loài người sinh học. Ngoài chuyện đang tranh cãi, là AI liệu có thông minh hơn con người, thì đây là những lợi thế mà con người không bao giờ có thể so sánh được với AI: - Scalability: AI có nhân nhân rộng một cách nhanh chóng, và không có giới hạn về số lượng. Con người thì không. Cần 09 tháng mang thai, 18 năm giáo dục phổ thông để trở nên có ích, hoặc ít nhất là không làm điều gì ngu ngốc. - Consistency: mọi người mất 12 năm cho giáo dục phổ thông, 4 năm đại học, 5 năm sau đại học, và hàng chục năm kinh nghiệm để trưởng thành. AI có thể chia se thông tin và kiến thức số lượng lớn một cách đồng loạt và nhanh chóng. AI thậm chí có thể không cần học từ con người mà vẫn vượt qua họ [10, 11]. Alpha-go, không càn học bất kì ván cờ nào từ con người, và chỉ sau AI ngày đã trở thành bậc thầy, và sau 40 ngày đã vượt qua tất cả phiên bản khác. - Accessibility: Con người bị hán chế bởi khoảng cách địa lý, hoàn cảnh xa hội, và thân xác vật lý. AI thì không. Vào những năm 2010s, mọi nguòi nói ""data is the new oil"". Ở những năm 2018-2020, ""AI is the new internet"". Đối với mình, nó rất rõ ràng là ""AI is the new HUMAN"". AI sẽ là nguồn lao động mới, thay thế hoàn toàn hoặc một phần trong hầu hết các ngành công nghiệp [12, 13]. AI có lẽ không bận tâm lắm về loài người, như cách chúng ta nghĩ về khủng long vậy. IV. Về tương lai AI, hay công nghệ, là ngọn lửa đưa con người ra khỏi hang đá nguyên thuỷ, nhưng cũng có thể đưa họ quay trở về đó. Sự hấp dẫn của việc sở hữu AGI vượt trội còn lớn hơn cả vũ khí nguyên tử, vì đó là công nghệ thay đổi hoàn toàn cuộc chơi. Đó là sức lao động và sáng tạo vô hạn, sẽ dẫn đến thiếu hụt về tài nguyên. Vì vậy, nếu chúng ta có thể tìm ra loại tài nguyên vô hạn, con người và AI sẽ bước sang một nấc mới trong nấc thang Kardashev [14]. Nếu chúng ta không thể, và trái đất chỉ đủ tài nguyên cho một loài, thì AI thông minh hơn, mạnh mẽ hơn và tàn nhẫn hơn. Trong khi chúng ta không rõ AI thông minh đến đâu, nhưng sự ngu ngốc của loài người thì là vô hạn (""Two things are infinite: the universe and human stupidity; and I'm not sure about the universe"" - Albert Enstein). Các bạn có thể đọc nó một cách giải trí. Nhưng nếu mọi người nghĩ là nó có ích, hãy chia sẻ nó. Refferences [1]. ImageNet Classification with Deep Convolutional Neural Networks. https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf [2]. Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks. https://arxiv.org/abs/1909.09586 [3]. Attention Is All You Need [4]. https://www.linkedin.com/posts/whuygen_do-large-language-models-understand-what-activity-7135363249964797952-2wLg/?trk=public_profile_like_view [5]. Sparks of Artificial General Intelligence: Early experiments with GPT-4. https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/ [6]. Generative Agents: Interactive Simulacra of Human Behavior. https://arxiv.org/pdf/2304.03442.pdf [7]. What happened at OpenAI? The Sam Altman saga, explained. https://www.washingtonpost.com/technology/2023/11/20/openai-sam-altman-ceo-oust/ [8]. We read the paper that forced Timnit Gebru out of Google. Here’s what it says. https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/ [9]. Google fires second AI ethics researcher following internal investigation. https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired [10]. AlphaGo Zero. https://en.wikipedia.org/wiki/AlphaGo_Zero [11]. AlphaGo - The Movie | Full award-winning documentary. https://www.youtube.com/watch?v=WXuK6gekU1Y&ab_channel=GoogleDeepMind [12]. More than 40% of labor force to be affected by AI in 3 years, Morgan Stanley forecasts. https://www.cnbc.com/2023/10/02/more-than-40percent-of-labor-force-to-be-impacted-by-ai-in-three-years-morgan-stanley-forecasts.html [13]. The state of AI in 2023: Generative AI’s breakout year. https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year [14]. Kardashev scale. https://en.wikipedia.org/wiki/Kardashev_scale",,,#sharing,"#sharing, #machine_learning",#sharing
"Em chào anh chị, em đang làm đồ án về OCR dùng transformer để nhận dạng tài liệu khoa học: https://github.com/facebookresearch/nougat
Em đang tìm hiểu phần tối ưu hóa tốc độ bằng cách convert model Pytorch sang TensorRT nhưng mà vẫn chưa làm được. Anh chị và các bạn ai có thể hướng dẫn em chi tiết phần này được không ạ, em xin cảm ơn và hậu tạ ạ.","Em chào anh chị, em đang làm đồ án về OCR dùng transformer để nhận dạng tài liệu khoa học: https://github.com/facebookresearch/nougat Em đang tìm hiểu phần tối ưu hóa tốc độ bằng cách convert model Pytorch sang TensorRT nhưng mà vẫn chưa làm được. Anh chị và các bạn ai có thể hướng dẫn em chi tiết phần này được không ạ, em xin cảm ơn và hậu tạ ạ.",,,"#Q&A, #cv, #pythonlibraries","#Q&A, #cv, #pythonlibraries","#Q&A, #cv, #pythonlibraries"
"Có thể repo tổng hợp các bài hướng dẫn thực hành dựa trên các dự án cụ thể (với nhiều ngôn ngữ, trong đó có Python với số lượng hướng dẫn lớn nhất) sẽ giúp ích các bạn trong quá trình học tập","Có thể repo tổng hợp các bài hướng dẫn thực hành dựa trên các dự án cụ thể (với nhiều ngôn ngữ, trong đó có Python với số lượng hướng dẫn lớn nhất) sẽ giúp ích các bạn trong quá trình học tập",,,#sharing,#sharing,#sharing
Trong deep q-leang. Với một môi trường cực ít phần thưởng. việc học trở lên cực kì khó khăn. Và có thể phần thưởng không thể lan truyền ngược được. thì có cách nào tối ưu hoá để có thể học tốt hơn không ạ.,Trong deep q-leang. Với một môi trường cực ít phần thưởng. việc học trở lên cực kì khó khăn. Và có thể phần thưởng không thể lan truyền ngược được. thì có cách nào tối ưu hoá để có thể học tốt hơn không ạ.,,,"#Q&A, #machine_learning","#Q&A, #machine_learning","#Q&A, #machine_learning"
"Chào mọi người. Mình mới viết xong con chatbot để hỗ trợ cho việc trading chứng khoán (Chủ yếu là học thêm). Hiện con bot có thể làm nhiều chức năng như tìm motif pattern, tìm support resistance, cảnh báo, tạo watchlist, tóm tắt 1 ý tưởng từ voice...
Hiện mn có thể sử dụng con bot tại https://t.me/mrzaizai2k_bot
Mong mọi người giành chút thời gian xem qua con bot mình trên github. Nếu có gì chưa ổn, mn góp ý giúp mình. Nếu thấy hay, mình xin mn 1 star lấy hên nhé!
https://github.com/mrzaizai2k/stock_price_4_fun","Chào mọi người. Mình mới viết xong con chatbot để hỗ trợ cho việc trading chứng khoán (Chủ yếu là học thêm). Hiện con bot có thể làm nhiều chức năng như tìm motif pattern, tìm support resistance, cảnh báo, tạo watchlist, tóm tắt 1 ý tưởng từ voice... Hiện mn có thể sử dụng con bot tại https://t.me/mrzaizai2k_bot Mong mọi người giành chút thời gian xem qua con bot mình trên github. Nếu có gì chưa ổn, mn góp ý giúp mình. Nếu thấy hay, mình xin mn 1 star lấy hên nhé! https://github.com/mrzaizai2k/stock_price_4_fun",,,"#sharing, #nlp","#sharing, #nlp","#sharing, #nlp"
"Xin chào mọi người,

Mình hiện có đang tìm hiểu về Transformer nhưng có một thắc mắc như sau, mong được mọi người giải đáp giùm với ạ.
Trong Transformer, phần nào trong nó sử dụng Self-Attention?
Theo mình hiểu thì chỉ có ở Multi-heads đầu tiên trong Encoder là self-attention. Còn masked multi-heads bên Decoder không phải là self-attention. Không biết mình hiểu như vậy có đúng không?
Xin cảm ơn mọi người rất nhiều.","Xin chào mọi người, Mình hiện có đang tìm hiểu về Transformer nhưng có một thắc mắc như sau, mong được mọi người giải đáp giùm với ạ. Trong Transformer, phần nào trong nó sử dụng Self-Attention? Theo mình hiểu thì chỉ có ở Multi-heads đầu tiên trong Encoder là self-attention. Còn masked multi-heads bên Decoder không phải là self-attention. Không biết mình hiểu như vậy có đúng không? Xin cảm ơn mọi người rất nhiều.",,,"#Q&A, #deep_learning","#Q&A, #deep_learning","#Q&A, #deep_learning"
"Mình có một thắc mắc nhỏ về semi-supervised learning, mong được giải đáp:
Mình huấn luyện một mô hình để học ra biểu diễn của dữ liệu (kiểu Autoencoder, VAE,...) (pretext task). Sau đó dùng phần encoder của mô hình này để train các downstream task như classification, detection... Tập dữ liệu của mình bao gồm cả dữ liệu có nhãn và không nhãn. Khi huấn luyện pretext task mình dùng toàn bộ dữ liệu, nhưng khi train downstream task mình chỉ dùng phần dữ liệu có nhãn. Vậy cách huấn luyện này có thể gọi là semi-supervised learning được hay không? Nếu không thì mình có thể dùng từ gì để chỉ cách huấn luyện này?","Mình có một thắc mắc nhỏ về semi-supervised learning, mong được giải đáp: Mình huấn luyện một mô hình để học ra biểu diễn của dữ liệu (kiểu Autoencoder, VAE,...) (pretext task). Sau đó dùng phần encoder của mô hình này để train các downstream task như classification, detection... Tập dữ liệu của mình bao gồm cả dữ liệu có nhãn và không nhãn. Khi huấn luyện pretext task mình dùng toàn bộ dữ liệu, nhưng khi train downstream task mình chỉ dùng phần dữ liệu có nhãn. Vậy cách huấn luyện này có thể gọi là semi-supervised learning được hay không? Nếu không thì mình có thể dùng từ gì để chỉ cách huấn luyện này?",,,"#Q&A, #machine_learning","#Q&A, #machine_learning","#Q&A, #machine_learning"
"Hi mọi người, Mình đang làm bài toán anomaly detection trong bài toán tìm vết xước, vết lỗi trong chi tiết máy (dùng hệ thống cam Basler + light riêng) 
Nhưng các network mình đang apply như EfficientAD, PathCore,... (Learn OK object) đều nhạy cảm vs ánh sáng
Mọi người có tips hay có hướng đi nào khác có thể gợi ý giúp mình  thêm được không à.

Cảm ơn mn!","Hi mọi người, Mình đang làm bài toán anomaly detection trong bài toán tìm vết xước, vết lỗi trong chi tiết máy (dùng hệ thống cam Basler + light riêng) Nhưng các network mình đang apply như EfficientAD, PathCore,... (Learn OK object) đều nhạy cảm vs ánh sáng Mọi người có tips hay có hướng đi nào khác có thể gợi ý giúp mình thêm được không à. Cảm ơn mn!",,,"#Q&A, #cv, #deep_learning","#Q&A, #cv, #deep_learning","#Q&A, #cv, #deep_learning"
"Do you guys know Apple has just released a machine learning framework for Apple Silicon called MLX (https://github.com/ml-explore/mlx)? Here are some example models using MLX (https://github.com/ml-explore/mlx-examples). I just tested it on MNIST on my Mac and compared the performance to JAX, TensorFlow, and PyTorch. The performance is quite impressive.","Do you guys know Apple has just released a machine learning framework for Apple Silicon called MLX (https://github.com/ml-explore/mlx)? Here are some example models using MLX (https://github.com/ml-explore/mlx-examples). I just tested it on MNIST on my Mac and compared the performance to JAX, TensorFlow, and PyTorch. The performance is quite impressive.",,,,,
Xin chào cả nhà. Em đang mày mò tìm hiểu về PhoGPT của Vin. Cả nhà cho em hỏi là họ dùng bộ Embedding nào đc ko ạ? Em muốn thử chạy code ạ.,Xin chào cả nhà. Em đang mày mò tìm hiểu về PhoGPT của Vin. Cả nhà cho em hỏi là họ dùng bộ Embedding nào đc ko ạ? Em muốn thử chạy code ạ.,,,"#Q&A, #nlp","#Q&A, #nlp","#Q&A, #deep_learning"
"Google vừa ra mắt Gemini, đối thủ đáng gờm cho ChatGPT.

- Đây là mô hình trí tuệ nhân tạo lớn nhất và mạnh mẽ nhất của Google.
- Nó có thể nhận đầu vào từ văn bản, code, âm thanh, hình ảnh và videos.
- Có 3 mô hình Gemini với kích thước khác nhau (Ultra, Pro và Nano) để hoạt động trên nhiều loại thiết bị bao gồm cả điện thoại.
- Có vẻ như Gemini có tiềm năng vượt qua GPT-4 khi nó đứng đầu 30/32 bảng đánh giá hiệu suất của các mô hình trí tuệ nhân tạo.
https://deepmind.google/technologies/gemini/#introduction
Các bạn hãy test thử Gemini (Pro) trên Google Bard nhé https://bard.google.com/","Google vừa ra mắt Gemini , đối thủ đáng gờm cho ChatGPT. - Đây là mô hình trí tuệ nhân tạo lớn nhất và mạnh mẽ nhất của Google. - Nó có thể nhận đầu vào từ văn bản, code, âm thanh, hình ảnh và videos. - Có 3 mô hình Gemini với kích thước khác nhau (Ultra, Pro và Nano) để hoạt động trên nhiều loại thiết bị bao gồm cả điện thoại. - Có vẻ như Gemini có tiềm năng vượt qua GPT-4 khi nó đứng đầu 30/32 bảng đánh giá hiệu suất của các mô hình trí tuệ nhân tạo. https://deepmind.google/technologies/gemini/#introduction Các bạn hãy test thử Gemini (Pro) trên Google Bard nhé https://bard.google.com/",,,"#sharing, #nlp, #deep_learning","#sharing, #nlp","#sharing, #deep_learning"
"[ Crawl data TripAdvisor ]
Chào mọi người ạ, trước đây e có crawl data trên các trang web khác bình thường nhưng khi thử crawl data trên TripAdvisor thì có vẻ không khả thi (crawl rất lâu, hoặc kết quả trả ra không ở dạng HTML mà là JS ạ). Mong a/c có kinh nghiệm chỉ giúp e 🥰🥰.
Tks mọi người đã đọc bài.","[ Crawl data TripAdvisor ] Chào mọi người ạ, trước đây e có crawl data trên các trang web khác bình thường nhưng khi thử crawl data trên TripAdvisor thì có vẻ không khả thi (crawl rất lâu, hoặc kết quả trả ra không ở dạng HTML mà là JS ạ). Mong a/c có kinh nghiệm chỉ giúp e . Tks mọi người đã đọc bài.",,,"#Q&A, #data","#Q&A, #data","#Q&A, #data"
Chia sẻ với mọi người video đầu tiên trong chuỗi series về MLflow - một công cụ mạnh mẽ giúp chúng ta thực hành MLOps.,Chia sẻ với mọi người video đầu tiên trong chuỗi series về MLflow - một công cụ mạnh mẽ giúp chúng ta thực hành MLOps.,,,#sharing,"#sharing, #machine_learning","#sharing, #pythonlibraries"
"PhoGPT: Generative Pre-training for Vietnamese
GitHub: https://github.com/VinAIResearch/PhoGPT
We open-source a state-of-the-art 7.5B-parameter generative model series named PhoGPT for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct. In addition, we also demonstrate its superior performance compared to previous open-source models through a human evaluation experiment.","PhoGPT: Generative Pre-training for Vietnamese GitHub: https://github.com/VinAIResearch/PhoGPT We open-source a state-of-the-art 7.5B-parameter generative model series named PhoGPT for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct. In addition, we also demonstrate its superior performance compared to previous open-source models through a human evaluation experiment.",,,,,
"Em chào mọi người ạ, như mọi người biết thì các model BERT thường được sử dụng với task retrieval. Em đang sử dụng nó cho 1 project hỏi đáp về 1 tổ chức. Nó hoạt động khá tốt, tuy nhiên thì em đang muốn hỏi mọi người 2 vấn đề:
1. Model chỉ mới trả về câu trả lời chính xác chứ chưa phải trả lời 1 cách tự nhiên. Ví dụ:
Q: ""Phòng ban xyz có bao nhiêu người?""
Model: ""17""
Đầu ra em muốn -> ""Phòng ban xyz có tất cả là 17 người""
2. Model hoạt động không tốt với những câu trả lời dài như:
Q: ""Trình bày các hướng nghiên cứu của phòng ban W""
Model: ""1. Hướng A""
Đầu ra em muốn:
""1. Hướng A
2. Hướng B
3. Hướng C""
Hy vọng được nghe kinh nghiệm của mọi người về vấn đề này. Em cảm ơn mọi người nhiều.","Em chào mọi người ạ, như mọi người biết thì các model BERT thường được sử dụng với task retrieval. Em đang sử dụng nó cho 1 project hỏi đáp về 1 tổ chức. Nó hoạt động khá tốt, tuy nhiên thì em đang muốn hỏi mọi người 2 vấn đề: 1. Model chỉ mới trả về câu trả lời chính xác chứ chưa phải trả lời 1 cách tự nhiên. Ví dụ: Q: ""Phòng ban xyz có bao nhiêu người?"" Model: ""17"" Đầu ra em muốn -> ""Phòng ban xyz có tất cả là 17 người"" 2. Model hoạt động không tốt với những câu trả lời dài như: Q: ""Trình bày các hướng nghiên cứu của phòng ban W"" Model: ""1. Hướng A"" Đầu ra em muốn: ""1. Hướng A 2. Hướng B 3. Hướng C"" Hy vọng được nghe kinh nghiệm của mọi người về vấn đề này. Em cảm ơn mọi người nhiều.",,,"#Q&A, #nlp","#Q&A, #nlp","#Q&A, #deep_learning, #nlp"
"Hi mọi người, mình có chút thắc mắc về multi-thread trên python như sau:
Ngôn ngữ: Python
Thread A: Mình chỉ để inference network - Ví dụ object segmentation. Không gồm pre-process hay post process. Và tất nhiên chạy trên GPU
Thread B: Đọc ảnh từ camera => pre-process => Cấp input cho thread A => lấy kết quả rồi post-process. Chạy trên CPU.
Như mọi người biết trên Python chạy multithread sẽ không hiệu qua vì liên quan GIL. Nhưng với trường hợp như trên thì khi chương trình đang chạy trên GPU (thread A) thì các tác vụ trên CPU có hoạt động đồng thời không?","Hi mọi người, mình có chút thắc mắc về multi-thread trên python như sau: Ngôn ngữ: Python Thread A: Mình chỉ để inference network - Ví dụ object segmentation. Không gồm pre-process hay post process. Và tất nhiên chạy trên GPU Thread B: Đọc ảnh từ camera => pre-process => Cấp input cho thread A => lấy kết quả rồi post-process. Chạy trên CPU. Như mọi người biết trên Python chạy multithread sẽ không hiệu qua vì liên quan GIL. Nhưng với trường hợp như trên thì khi chương trình đang chạy trên GPU (thread A) thì các tác vụ trên CPU có hoạt động đồng thời không?",,,"#Q&A, #pythonlibraries",,"#Q&A, #pythonlibraries"
"Hello mọi người, em có một vé Vin AI day mà em hong đi dc. Em muốn pass lại ạ, mọi người ai muốn đi thì nhắn em ạ","Hello mọi người, em có một vé Vin AI day mà em hong đi dc. Em muốn pass lại ạ, mọi người ai muốn đi thì nhắn em ạ",,,#sharing,#sharing,#sharing
"Hiện nay, chatbot đã trở thành một công cụ quan trọng cho các doanh nghiệp để cung cấp thông tin và tăng tương tác đối với khách hàng. Trong số các loại chatbot, retrieval-based chatbot là một trong những phương pháp phổ biến nhất được sử dụng để đáp ứng các yêu cầu và câu hỏi của người dùng.
Trong bài hôm nay, chúng ta sẽ tìm hiểu sơ lược về retrieval-based chatbot và các thành phần NLP cấu thành nên loại chatbot này.","Hiện nay, chatbot đã trở thành một công cụ quan trọng cho các doanh nghiệp để cung cấp thông tin và tăng tương tác đối với khách hàng. Trong số các loại chatbot, retrieval-based chatbot là một trong những phương pháp phổ biến nhất được sử dụng để đáp ứng các yêu cầu và câu hỏi của người dùng. Trong bài hôm nay, chúng ta sẽ tìm hiểu sơ lược về retrieval-based chatbot và các thành phần NLP cấu thành nên loại chatbot này.",,,"#sharing, #nlp","#sharing, #nlp","#sharing, #nlp"
"Hi mọi người,
Hôm nay mình xin phép chia sẻ một bài viết về AB Testing và cách áp dụng trong industry. Mặc dù, AB Test khá phổ biến và là một chủ đề không mới, tuy nhiên, mình nghĩ rằng đôi khi chúng ta áp dụng thiếu chính xác.
Bài viết này sẽ bao gồm :
- Một số lý thuyết về AB Test,
- Kiểm định giải thuyết
- Một số điều lưu ý khi áp dụng trong industry.
Bài viết có thể sai sót do hạn chế của người viết, nếu mọi người phát hiện thì có thể comment tại đây hoặc trong blog để mình sửa nhé.
Link bài viết: https://pbcquoc.github.io/abtesting/","Hi mọi người, Hôm nay mình xin phép chia sẻ một bài viết về AB Testing và cách áp dụng trong industry. Mặc dù, AB Test khá phổ biến và là một chủ đề không mới, tuy nhiên, mình nghĩ rằng đôi khi chúng ta áp dụng thiếu chính xác. Bài viết này sẽ bao gồm : - Một số lý thuyết về AB Test, - Kiểm định giải thuyết - Một số điều lưu ý khi áp dụng trong industry. Bài viết có thể sai sót do hạn chế của người viết, nếu mọi người phát hiện thì có thể comment tại đây hoặc trong blog để mình sửa nhé. Link bài viết: https://pbcquoc.github.io/abtesting/",,,"#sharing, #math ",#sharing,"#sharing, #machine_learning"
"Xin phép admin ạ, mình vừa viết một bài so sánh Yolov8 với RT-DERT trên bộ dữ liệu Aquarium, mong được mọi người ủng hộ ạ.","Xin phép admin ạ, mình vừa viết một bài so sánh Yolov8 với RT-DERT trên bộ dữ liệu Aquarium, mong được mọi người ủng hộ ạ.",,,"#sharing, #deep_learing","#sharing, #deep_learing","#sharing, #deep_learing"
Chia sẻ tới các bạn một repo để học cách implement model hiệu quả:,Chia sẻ tới các bạn một repo để học cách implement model hiệu quả:,,,#sharing,,
"Xin chào mọi người ạ,
Hôm nay em muốn chia sẻ với mọi người một dự án nhỏ về Retrieval-Augmented Generation. Dự án này ban đầu được sinh ra với mục đích thi thố tại cuộc thi Viettel Hearted AI Challenge. Bài toán là dựa trên corpus các bài viết Wikipedia tiếng Việt được cho trước, xây dựng một giải pháp RAG để giải quyết các câu hỏi mà câu trả lời có thể được tìm thấy trong corpus đó. Tuy không được giải nhưng em thấy rằng thành quả của đội mình cũng thú vị và muốn chia sẻ tới cộng đồng ạ.
Dự án này bao gồm:
Mô hình Llama-2-7b đã được instruct-finetuned với dữ liệu chỉ dẫn chủ yếu thuộc về bài toán hỏi đáp miền đóng tiếng Việt (closed question answering). Mô hình có khả năng đưa ra phản hồi cho một câu hỏi dựa trên nội dung ngữ cảnh kèm theo nó. 
Tích hợp mô hình này vào một pipeline RAG đơn giản.
Thông tin chi tiết về dự án em xin để dưới comment.
Demo dự án:
 — với Bùi Chí Minh.","Xin chào mọi người ạ, Hôm nay em muốn chia sẻ với mọi người một dự án nhỏ về Retrieval-Augmented Generation. Dự án này ban đầu được sinh ra với mục đích thi thố tại cuộc thi Viettel Hearted AI Challenge. Bài toán là dựa trên corpus các bài viết Wikipedia tiếng Việt được cho trước, xây dựng một giải pháp RAG để giải quyết các câu hỏi mà câu trả lời có thể được tìm thấy trong corpus đó. Tuy không được giải nhưng em thấy rằng thành quả của đội mình cũng thú vị và muốn chia sẻ tới cộng đồng ạ. Dự án này bao gồm: Mô hình Llama-2-7b đã được instruct-finetuned với dữ liệu chỉ dẫn chủ yếu thuộc về bài toán hỏi đáp miền đóng tiếng Việt (closed question answering). Mô hình có khả năng đưa ra phản hồi cho một câu hỏi dựa trên nội dung ngữ cảnh kèm theo nó. Tích hợp mô hình này vào một pipeline RAG đơn giản. Thông tin chi tiết về dự án em xin để dưới comment. Demo dự án: — với Bùi Chí Minh.",,,"#sharing, #nlp",,
"Em chào mọi người ạ, em đang có một bài toán nhỏ về Voice cloning trên Tiếng Việt.
Input là 1 đoạn voice ghi âm lấy từ người dùng
Output muốn có model voice clone từ người dùng input, text to speech một list các câu văn đã được soạn trước.
Không biết có bên nào hay github nào support việc này và hỗ trợ cho Tiếng Việt không ạ. Em cảm ơn mọi người ạ","Em chào mọi người ạ, em đang có một bài toán nhỏ về Voice cloning trên Tiếng Việt. Input là 1 đoạn voice ghi âm lấy từ người dùng Output muốn có model voice clone từ người dùng input, text to speech một list các câu văn đã được soạn trước. Không biết có bên nào hay github nào support việc này và hỗ trợ cho Tiếng Việt không ạ. Em cảm ơn mọi người ạ",,,#Q&A,,
"em chào mọi người . mọi người đã ai từng sử dụng ""NlpHUST/vi-electra-small"" rồi có thể cho em xin cách load tokenize từ checkpoint này với được không ạ. em load như trong ảnh khi in ra thấy các token đều đang không có dấu. mà trên huggingface thì không có hướng dẫn . em cảm ơn ạ","em chào mọi người . mọi người đã ai từng sử dụng ""NlpHUST/vi-electra-small"" rồi có thể cho em xin cách load tokenize từ checkpoint này với được không ạ. em load như trong ảnh khi in ra thấy các token đều đang không có dấu. mà trên huggingface thì không có hướng dẫn . em cảm ơn ạ",,,"#Q&A, #nlp, #deep_learning",,
"Em chào mn ạ, em đang thử sử dụng RAG với model chat là phogpt-7.5b-instruct của VinAI nhưng em đang bị vướng ở load model trên GGcolab bị crash:( Không biết mn có cách nào load đc ngoài việc dùng bản plus không ạ, em cảm ơn ạ.","Em chào mn ạ, em đang thử sử dụng RAG với model chat là phogpt-7.5b-instruct của VinAI nhưng em đang bị vướng ở load model trên GGcolab bị crash:( Không biết mn có cách nào load đc ngoài việc dùng bản plus không ạ, em cảm ơn ạ.",,,"#Q&A, #nlp, #deep_learning",,
"Mn cho em hỏi case này với, em có train model với 2 input là tôi đi họ sẽ ra học và toi di ho sẽ ra hoc. Nhưng h khi input vào thì e muốn là tôi đi ho cũng sẽ ra học. có cách nào để nó vẫn ra như v mà mình k cần phải training lại model hong ạ ?","Mn cho em hỏi case này với, em có train model với 2 input là tôi đi họ sẽ ra học và toi di ho sẽ ra hoc. Nhưng h khi input vào thì e muốn là tôi đi ho cũng sẽ ra học. có cách nào để nó vẫn ra như v mà mình k cần phải training lại model hong ạ ?",,,#Q&A,,
"Chào mọi người

Em đang có dự án cuối kì với tiêu đề là Brain Tumor Segmentation, nhóm có sử dụng thuật toán Fuzzy-Mean để áp dụng phương pháp song song vào để tăng tốc độ tính toán. Để hoàn thành dự án thì em có chọn GMM để loại bỏ phần vỏ não nhưng để xác định được vùng chứa u não thì bọn e vẫn chưa làm được ạ. Mong ac có kinh nghiệm khi xử lí ảnh y tế (dicom) cho e ít kinh nghiệm ạ. E cảm ơn nhiều ạ 😍☺😍☺😍 .
Dự án không được sử dụng Deep Learning ạ.

Ảnh dưới là sau khi dùng GMM ạ.","Chào mọi người Em đang có dự án cuối kì với tiêu đề là Brain Tumor Segmentation, nhóm có sử dụng thuật toán Fuzzy-Mean để áp dụng phương pháp song song vào để tăng tốc độ tính toán. Để hoàn thành dự án thì em có chọn GMM để loại bỏ phần vỏ não nhưng để xác định được vùng chứa u não thì bọn e vẫn chưa làm được ạ. Mong ac có kinh nghiệm khi xử lí ảnh y tế (dicom) cho e ít kinh nghiệm ạ. E cảm ơn nhiều ạ . Dự án không được sử dụng Deep Learning ạ. Ảnh dưới là sau khi dùng GMM ạ.",,,"#Q&A, #cv",,
"Em chào mọi người ạ , em hiện đang làm về task ""key information extraction"" , em có tìm hiểu một số thư viện  trên github và đang thử paddleocr, hiện tại em gặp một số vấn đề về cài đặt môi trường:
em có làm theo document trên github repository nhưng vẫn bị lỗi  ạ (phần 2. training ,lỗi ở link  issue ) 
 em có thử cài đặt trên docker nhưng vẫn không được ạ (lỗi ở hình bên dưới)
 có bác nào rành về paddle hay task kie không  ạ?
cuda:11.8 
ubuntu:22.04
repository:https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.7
document:https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/doc/doc_en/kie_en.md
issue: https://github.com/PaddlePaddle/PaddleOCR/issues/11261
docker:https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/doc/doc_en/environment_en.md?fbclid=IwAR3TT4e98_wn87EyMR_9gvRwlmXYysN52vLbuBCqnw9h36MjHsRxBpLSKIA","Em chào mọi người ạ , em hiện đang làm về task ""key information extraction"" , em có tìm hiểu một số thư viện trên github và đang thử paddleocr, hiện tại em gặp một số vấn đề về cài đặt môi trường: em có làm theo document trên github repository nhưng vẫn bị lỗi ạ (phần 2. training ,lỗi ở link issue ) em có thử cài đặt trên docker nhưng vẫn không được ạ (lỗi ở hình bên dưới) có bác nào rành về paddle hay task kie không ạ? cuda:11.8 ubuntu:22.04 repository:https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.7 document:https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/doc/doc_en/kie_en.md issue: https://github.com/PaddlePaddle/PaddleOCR/issues/11261 docker:https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/doc/doc_en/environment_en.md?fbclid=IwAR3TT4e98_wn87EyMR_9gvRwlmXYysN52vLbuBCqnw9h36MjHsRxBpLSKIA",,,"#Q&A, #python",,
"Chatbot đã thay đổi cách chúng ta tương tác với công nghệ và dịch vụ trực tuyến. Chúng ta đã dần quen với việc trò chuyện với chatbot trên trang web của một doanh nghiệp đến gửi tin nhắn với chatbot trên các ứng dụng nhắn tin. Trên thực tế, chatbot đang trở thành một phần không thể thiếu trong việc cung cấp hỗ trợ và thông tin tức thời cho khách hàng.
Nhưng chatbot là gì và làm thế nào chúng hoạt động? Trong bài viết này, chúng ta sẽ khám phá sâu hơn về chatbot và cách chúng thực hiện nhiệm vụ của mình.","Chatbot đã thay đổi cách chúng ta tương tác với công nghệ và dịch vụ trực tuyến. Chúng ta đã dần quen với việc trò chuyện với chatbot trên trang web của một doanh nghiệp đến gửi tin nhắn với chatbot trên các ứng dụng nhắn tin. Trên thực tế, chatbot đang trở thành một phần không thể thiếu trong việc cung cấp hỗ trợ và thông tin tức thời cho khách hàng. Nhưng chatbot là gì và làm thế nào chúng hoạt động? Trong bài viết này, chúng ta sẽ khám phá sâu hơn về chatbot và cách chúng thực hiện nhiệm vụ của mình.",,,"#sharing, #nlp",,
"Xin phép add
Em là AI Engineer về Computer vision. Em đã làm việc được gần 1 năm.
Em muốn tìm một công ty mới có môi trường làm việc và mức lương phù hợp, vì công ty cũ không còn phù hợp nữa.
Vì nhiều lý do nên em không tiện chia sẻ cv của mình lên bài. Tại công ty hiện tại, vị trí của em là kỹ sư AI full-stack, các dự án mà em đã tham gia tại công ty như OCR, phát hiện lỗi sản phẩm, nhận diện khuôn mặt,...
Công ty anh/chị có nhu cầu tuyển dụng, em mong có cơ hội được liên hệ, rất vui được hợp tác và trao đổi với anh/chị
Cảm ơn!!
#ComputerVision, #AI, #job,","Xin phép add Em là AI Engineer về Computer vision. Em đã làm việc được gần 1 năm. Em muốn tìm một công ty mới có môi trường làm việc và mức lương phù hợp, vì công ty cũ không còn phù hợp nữa. Vì nhiều lý do nên em không tiện chia sẻ cv của mình lên bài. Tại công ty hiện tại, vị trí của em là kỹ sư AI full-stack, các dự án mà em đã tham gia tại công ty như OCR, phát hiện lỗi sản phẩm, nhận diện khuôn mặt,... Công ty anh/chị có nhu cầu tuyển dụng, em mong có cơ hội được liên hệ, rất vui được hợp tác và trao đổi với anh/chị Cảm ơn!!","#ComputerVision,	#AI,	#job,",,#Q&A,,
"em chào mọi người !
em là người mới nên có thể một số kiến thức em vẫn chưa chắc lắm các anh thông cảm nếu điều em hỏi có hơi ngáo , em hiện đang làm 1 project nhỏ về chatbot . em có 1 số thắc mắc sau mong mọi người bớt chút thời gian giải thích giúp em ạ .
- 1 )ý tưởng của em là sử dụng langchain để kết nối với 1 llm tiếng việt ( như phogpt , phobert , vit5 ...) và nhúng các kiến thức dưới dạng các tệp tài liệu ( pdf , text , doc .. ) để không phải train lại llm .
nhưng hiện tại em thao tác với tiếng việt thì nó gặp lỗi ở phần chuyển hóa kiến thức file pdf ( chứa 1 văn bản tiếng việt ) nó không thể vector hóa kiến thức được . nhưng nếu em sử dụng với tiếng anh và model zephyr7b và model sentence-transformers/all-mpnet-base-v2 để vector hóa kiến thức thì nó hoạt động tốt với tài liệu tiếng anh .
em đang định sử lý theo kiểu chuyển hết tài liệu tiếng việt thành dạng tiếng anh để có thể nhúng vào model và khi user nhập câu hỏi vào thì cho nó chạy qua 1 model dịch để dịch nó thành tiếng anh rồi mới đưa vào chatbot và khi chatbot phản hồi thì lại cho chạy qua 1 lượt dịch để dich lại thành tiếng việt . nhưng như vậy thì em thấy khá cồng kềnh và nếu dịch qua lại giữa tiếng anh thì em sợ nó bi mất đi một số nghĩa đặc chưng của tiếng việt .
đây là link colab chatbot sử dụng zephy7b : https://drive.google.com/file/d/1c8_o0j0TMHY4S5daNFZwQVgW8MkBL9C-/view?usp=sharing
em muốn xây chatbot kiểu giống như cái bên trên nhưng hoạt động với tiếng việt .
- 2 ) em có tra cứu trên 1 số diễn đàn và gg thì thấy họ bảo rằng nên sử dụng loại model : Question Answering để tạo chatbot thay vì sử dụng Text2Text Generation trên hugging face , điều này có đúng không vậy ạ ? hay là nên sử dụng loại model nào ?
- 3 ) em sau 1 thời gian tìm hiểu thì em thấy trước khi vector hóa kiến thức để nhúng cho llm sử dụng thì nên cho nó chạy qua loại model Token Classification để phân tách kiến thức ra , điều này có đúng không ạ ?
- 4 ) cuối cùng em có tìm hiểu và chạy thử thì thấy kể cả chạy trên gpu ( colab ) thì thời gian phản hồi của nó cũng khoảng 10s với model khoảng 7b . có cách nào để tối ưu không ạ ? và có nên để nó chạy chỉ với cpu không ạ ? kiểu như lúc mình triển khai ấy thì gpu không phải lúc nào cũng sẵn có .
các anh có thể cho em một số gợi ý để em sử lý các vấn đề này không ? nếu được thì các anh cho em xin 1 số tên model hoặc tài liệu để em tham khảo với ạ . em cảm ơn .","em chào mọi người ! em là người mới nên có thể một số kiến thức em vẫn chưa chắc lắm các anh thông cảm nếu điều em hỏi có hơi ngáo , em hiện đang làm 1 project nhỏ về chatbot . em có 1 số thắc mắc sau mong mọi người bớt chút thời gian giải thích giúp em ạ . - 1 )ý tưởng của em là sử dụng langchain để kết nối với 1 llm tiếng việt ( như phogpt , phobert , vit5 ...) và nhúng các kiến thức dưới dạng các tệp tài liệu ( pdf , text , doc .. ) để không phải train lại llm . nhưng hiện tại em thao tác với tiếng việt thì nó gặp lỗi ở phần chuyển hóa kiến thức file pdf ( chứa 1 văn bản tiếng việt ) nó không thể vector hóa kiến thức được . nhưng nếu em sử dụng với tiếng anh và model zephyr7b và model sentence-transformers/all-mpnet-base-v2 để vector hóa kiến thức thì nó hoạt động tốt với tài liệu tiếng anh . em đang định sử lý theo kiểu chuyển hết tài liệu tiếng việt thành dạng tiếng anh để có thể nhúng vào model và khi user nhập câu hỏi vào thì cho nó chạy qua 1 model dịch để dịch nó thành tiếng anh rồi mới đưa vào chatbot và khi chatbot phản hồi thì lại cho chạy qua 1 lượt dịch để dich lại thành tiếng việt . nhưng như vậy thì em thấy khá cồng kềnh và nếu dịch qua lại giữa tiếng anh thì em sợ nó bi mất đi một số nghĩa đặc chưng của tiếng việt . đây là link colab chatbot sử dụng zephy7b : https://drive.google.com/file/d/1c8_o0j0TMHY4S5daNFZwQVgW8MkBL9C-/view?usp=sharing em muốn xây chatbot kiểu giống như cái bên trên nhưng hoạt động với tiếng việt . - 2 ) em có tra cứu trên 1 số diễn đàn và gg thì thấy họ bảo rằng nên sử dụng loại model : Question Answering để tạo chatbot thay vì sử dụng Text2Text Generation trên hugging face , điều này có đúng không vậy ạ ? hay là nên sử dụng loại model nào ? - 3 ) em sau 1 thời gian tìm hiểu thì em thấy trước khi vector hóa kiến thức để nhúng cho llm sử dụng thì nên cho nó chạy qua loại model Token Classification để phân tách kiến thức ra , điều này có đúng không ạ ? - 4 ) cuối cùng em có tìm hiểu và chạy thử thì thấy kể cả chạy trên gpu ( colab ) thì thời gian phản hồi của nó cũng khoảng 10s với model khoảng 7b . có cách nào để tối ưu không ạ ? và có nên để nó chạy chỉ với cpu không ạ ? kiểu như lúc mình triển khai ấy thì gpu không phải lúc nào cũng sẵn có . các anh có thể cho em một số gợi ý để em sử lý các vấn đề này không ? nếu được thì các anh cho em xin 1 số tên model hoặc tài liệu để em tham khảo với ạ . em cảm ơn .",,,"#Q&A, #nlp, #deep_learning",,
"Chào mọi người em là newbie ạ
Em muốn thử 1 mô hình LLM kết hợp với RAG, Langchain thì nên thử ở đâu ạ. Nếu thử ở local thì máy lag, còn thử ở gg colab thì sau 1 time nó lại reset, bắt mình chạy lại toàn bộ lệnh","Chào mọi người em là newbie ạ Em muốn thử 1 mô hình LLM kết hợp với RAG, Langchain thì nên thử ở đâu ạ. Nếu thử ở local thì máy lag, còn thử ở gg colab thì sau 1 time nó lại reset, bắt mình chạy lại toàn bộ lệnh",,,"#Q&A, #nlp",,
"Gửi các bác, mình đang gặp vấn đề với thư viện underthesea như sau: 
Khi chạy với Sublime thì vẫn chạy bình thường
Khi xuất ra file exe với Pyinstaller thì báo lỗi như ảnh, hiện tại mình có folder báo thiếu như ảnh dưới tuy nhiên không biết bổ sung vào folder nào, folder temp khi tắt chương trình thì lại mất nên không biết bổ sung vào folder nào.
Các bác xem giúp em, em cảm ơn các bác","Gửi các bác, mình đang gặp vấn đề với thư viện underthesea như sau: Khi chạy với Sublime thì vẫn chạy bình thường Khi xuất ra file exe với Pyinstaller thì báo lỗi như ảnh, hiện tại mình có folder báo thiếu như ảnh dưới tuy nhiên không biết bổ sung vào folder nào, folder temp khi tắt chương trình thì lại mất nên không biết bổ sung vào folder nào. Các bác xem giúp em, em cảm ơn các bác",,,"#Q&A, #python",,
"Chào mọi người, em là newbie ạ
Cho em hỏi là muốn làm 1 con chatbot có thể trả lời theo dữ liệu realtime thì phải tìm hiểu những gì ạ, đặc biệt là LLAMA2
Em cảm ơn ạ","Chào mọi người, em là newbie ạ Cho em hỏi là muốn làm 1 con chatbot có thể trả lời theo dữ liệu realtime thì phải tìm hiểu những gì ạ, đặc biệt là LLAMA2 Em cảm ơn ạ",,,"#Q&A, #nlp",,
"Em dân non-tech xin phép được đăng bài xin ý kiến anh chị ạ
Em thấy bộ VLMU được giới thiệu mới đây của Zalo AI là bảng leader board có thể nói là bảng đầu tiên ở nước mình, nhất là về mảng xử lý tiếng việt. Ở góc độ kinh tế, em có thể make-color cho sản phẩm của mình bằng rank này.
1. Tuy nhiên đối với anh chị, em rất muốn biết góc nhìn kỹ thuật thì đối với anh chị rank VLMU có làm highlight sản phẩm mình được không?
2. Nếu publish thì sẽ release nhiều thông tin, em muốn trước đó có 1 bộ đánh giá thử mà chỉ riêng nội bộ xem được, mình hiệu chỉnh và gửi publish sau thì có được hay không?
3. Những rule nào quy định về việc được publish, không được publish.
Mong ace hỗ trợ em ạ","Em dân non-tech xin phép được đăng bài xin ý kiến anh chị ạ Em thấy bộ VLMU được giới thiệu mới đây của Zalo AI là bảng leader board có thể nói là bảng đầu tiên ở nước mình, nhất là về mảng xử lý tiếng việt. Ở góc độ kinh tế, em có thể make-color cho sản phẩm của mình bằng rank này. 1. Tuy nhiên đối với anh chị, em rất muốn biết góc nhìn kỹ thuật thì đối với anh chị rank VLMU có làm highlight sản phẩm mình được không? 2. Nếu publish thì sẽ release nhiều thông tin, em muốn trước đó có 1 bộ đánh giá thử mà chỉ riêng nội bộ xem được, mình hiệu chỉnh và gửi publish sau thì có được hay không? 3. Những rule nào quy định về việc được publish, không được publish. Mong ace hỗ trợ em ạ",,,#Q&A,,
🎯 OpenAi giới thiệu mô hình GPT-4 Turbo,OpenAi giới thiệu mô hình GPT-4 Turbo,,,#sharing,,
"Chào mọi người ạ.
Nhóm chúng em đang làm một project về Hỏi đáp tài liệu theo mô hình RAG sử dụng LLM ChatGPT.
Tài liệu của chúng em đang viết ở Tiếng Việt, Tiếng Anh và Tiếng Nhật. Em đang muốn tìm một model encoder tối ưu được cho 3 thứ tiếng trên ạ. Mọi người ai có kinh nghiệm có thể gợi ý giúp em một số model với ạ. Em cảm ơn mn","Chào mọi người ạ. Nhóm chúng em đang làm một project về Hỏi đáp tài liệu theo mô hình RAG sử dụng LLM ChatGPT. Tài liệu của chúng em đang viết ở Tiếng Việt, Tiếng Anh và Tiếng Nhật. Em đang muốn tìm một model encoder tối ưu được cho 3 thứ tiếng trên ạ. Mọi người ai có kinh nghiệm có thể gợi ý giúp em một số model với ạ. Em cảm ơn mn",,,"#Q&A, #nlp",,
"Xin chào mọi người, em có đang tranh luận với giảng viên về 2 việc như sau, mong được mọi người góp ý.
1. Khi dạy thuật toán NBC, cô em dạy 2 dạng là NBC thông thường và NBC (cô tạm gọi là) cải tiển.
- Với NBC thông thường thì tính theo công thức Bayes mở rộng, tức không coi các biến là độc lập với nhau
- Còn với NBC cải tiến thì mới coi các biến là độc lập với nhau
Quan điểm của em là: chữ Naïve trong thuật toán NBC đã chỉ ra ý tưởng ngây thơ của bài toán là coi các biến độc lập với nhau, chỉ tồn tại cái NBC mà cô em đang gọi là NBC mở rộng.
2. Cô em dạy Maximum Likelihood và Maximum A Posteriori là thuật toán học máy tương đương như ID3, Kmeans, kNN, SVM, Linear Regression. (Cô đưa ra bằng chứng là trong giáo trình học máy do thầy Hoàng Xuân Huấn trường Đại học Quốc gia Hà Nội có viết ở mục ‘5.2.1 Các quy tắc phân lớp ML và MAP’), mà phân lớp thì là thuộc bài toán classify trong học máy.
Em cho rằng ML, MAP chỉ là phương thức hỗ trợ cho thuật toán học máy, ví dụ như dùng ML trong thuật toán logistic regression.
Note: Em sẽ bổ sung thêm một số dẫn chứng cho quan điểm của cô và em dưới phần bình luận. Vì học khoa điện tử, đến kì này mới học phần mềm nên kiến thức của em có thể sai sót nhiều. Xong em không thể khiên cưỡng làm theo điều mình thấy không thuyết phục. Mong được mọi người khai thông, nếu ai có gmail của giảng viên dạy môn này thì cho em xin nhé ạ!","Xin chào mọi người, em có đang tranh luận với giảng viên về 2 việc như sau, mong được mọi người góp ý. 1. Khi dạy thuật toán NBC, cô em dạy 2 dạng là NBC thông thường và NBC (cô tạm gọi là) cải tiển. - Với NBC thông thường thì tính theo công thức Bayes mở rộng, tức không coi các biến là độc lập với nhau - Còn với NBC cải tiến thì mới coi các biến là độc lập với nhau Quan điểm của em là: chữ Naïve trong thuật toán NBC đã chỉ ra ý tưởng ngây thơ của bài toán là coi các biến độc lập với nhau, chỉ tồn tại cái NBC mà cô em đang gọi là NBC mở rộng. 2. Cô em dạy Maximum Likelihood và Maximum A Posteriori là thuật toán học máy tương đương như ID3, Kmeans, kNN, SVM, Linear Regression. (Cô đưa ra bằng chứng là trong giáo trình học máy do thầy Hoàng Xuân Huấn trường Đại học Quốc gia Hà Nội có viết ở mục ‘5.2.1 Các quy tắc phân lớp ML và MAP’), mà phân lớp thì là thuộc bài toán classify trong học máy. Em cho rằng ML, MAP chỉ là phương thức hỗ trợ cho thuật toán học máy, ví dụ như dùng ML trong thuật toán logistic regression. Note: Em sẽ bổ sung thêm một số dẫn chứng cho quan điểm của cô và em dưới phần bình luận. Vì học khoa điện tử, đến kì này mới học phần mềm nên kiến thức của em có thể sai sót nhiều. Xong em không thể khiên cưỡng làm theo điều mình thấy không thuyết phục. Mong được mọi người khai thông, nếu ai có gmail của giảng viên dạy môn này thì cho em xin nhé ạ!",,,"#Q&A, #machine_learning, #math",,
Mn đã ai convert model detectron2 sang onnx chưa ạ,Mn đã ai convert model detectron2 sang onnx chưa ạ,,,"#Q&A, #cv",,
"Chào mọi người . Tình hình là em có một đề tài đang làm với mục đích là phát triển một mô hình tính toán để chuyển đổi tín hiệu nhịp sinh học của cơ thể từ cảm biến áp suất điện thành sóng mạch máu. Các phương pháp tính toán truyền thống đã được thử nghiệm nhưng chưa đạt được độ chính xác mong muốn. Để giải quyết vấn đề này, nghiên cứu yêu cầu việc phải đo đạc cùng lúc dữ liệu giáo viên và dữ liệu từ cảm biến áp suất điện. Trong năm nay, em đã tạo một thiết bị đo mới đã được phát triển để giảm sai số và từ đó phát triển mô hình tính toán chính xác hơn.
- Anh chị cho em gợi ý về cách xây dụng mô hình tính toán chính xác hơn được không . có thể gợi ý cho em nguồn vài tài liệu để tìm hiểu thêm về mô hình thích hợp được không ạ .
- Mình sử dụng mô hình ML để giải quyết .
- Vì mục đích là làm cho data thu được từ cảm biến áp suất điện kết hợp với data về nhịp sinh học mẩu để tạo ra chuỗi data thời gian mới có tính chất gần giống với nhịp sinh học thực tế
- Mình có biết về ML , nhưng với mình chưa đủ để hiểu rõ vấn đề cần giải quyết .
- Lúc đầu mình sử dụng phương pháp ML như mạng neural networks và mô hình hidden markov để giải quyết bài toán time-series generation nhưng kết quả không đẹp như mình nghĩ .","Chào mọi người . Tình hình là em có một đề tài đang làm với mục đích là phát triển một mô hình tính toán để chuyển đổi tín hiệu nhịp sinh học của cơ thể từ cảm biến áp suất điện thành sóng mạch máu. Các phương pháp tính toán truyền thống đã được thử nghiệm nhưng chưa đạt được độ chính xác mong muốn. Để giải quyết vấn đề này, nghiên cứu yêu cầu việc phải đo đạc cùng lúc dữ liệu giáo viên và dữ liệu từ cảm biến áp suất điện. Trong năm nay, em đã tạo một thiết bị đo mới đã được phát triển để giảm sai số và từ đó phát triển mô hình tính toán chính xác hơn. - Anh chị cho em gợi ý về cách xây dụng mô hình tính toán chính xác hơn được không . có thể gợi ý cho em nguồn vài tài liệu để tìm hiểu thêm về mô hình thích hợp được không ạ . - Mình sử dụng mô hình ML để giải quyết . - Vì mục đích là làm cho data thu được từ cảm biến áp suất điện kết hợp với data về nhịp sinh học mẩu để tạo ra chuỗi data thời gian mới có tính chất gần giống với nhịp sinh học thực tế - Mình có biết về ML , nhưng với mình chưa đủ để hiểu rõ vấn đề cần giải quyết . - Lúc đầu mình sử dụng phương pháp ML như mạng neural networks và mô hình hidden markov để giải quyết bài toán time-series generation nhưng kết quả không đẹp như mình nghĩ .",,,"#Q&A, #deep_learning",,
"Hỏi cách lấy data (ảnh,...) từ vệ tinh. Cần góp ý về việc download dữ liệu từ các vệ tinh.
Đợt vừa rồi, mình có thử download dữ liệu vệ tinh Sentinel-2, cụ thể là multispectrum data. Nhưng mình vẫn chưa down được.
Có bạn nào đã có kinh nghiệm làm việc với ảnh vệ tinh có thể chia sẽ cách các bạn down và vài chia sẽ về việc xử lý ảnh kích thước lớn như ảnh vệ tinh không ạ?
Cám ơn.","Hỏi cách lấy data (ảnh,...) từ vệ tinh. Cần góp ý về việc download dữ liệu từ các vệ tinh. Đợt vừa rồi, mình có thử download dữ liệu vệ tinh Sentinel-2, cụ thể là multispectrum data. Nhưng mình vẫn chưa down được. Có bạn nào đã có kinh nghiệm làm việc với ảnh vệ tinh có thể chia sẽ cách các bạn down và vài chia sẽ về việc xử lý ảnh kích thước lớn như ảnh vệ tinh không ạ? Cám ơn.",,,"#Q&A, #data",,
"Cực đại của the log posterior
Chào mọi người, em đang vướng 1 chổ mà suy nghĩ mãi không ra. Làm cách nào để khai triển công thức bên dưới nhỉ (2 mũi tên màu đỏ trên hình).","Cực đại của the log posterior Chào mọi người, em đang vướng 1 chổ mà suy nghĩ mãi không ra. Làm cách nào để khai triển công thức bên dưới nhỉ (2 mũi tên màu đỏ trên hình).",,,"#Q&A, #math",,
"Hi all ! 
Có bạn  nào đang học năm cuối kĩ sư hay master về computer vision hay graphic hoặc ngành liên quan. Có mong muốn học tiếp (PhD) và  bắt đầu bằng việc đi thực tập tại Pháp và Úc không ? Nếu có hãy liên lạc với mình ASAP nhé !
https://crossing.cnrs.fr/crossing-internships-2023-24/",Hi all ! Có bạn nào đang học năm cuối kĩ sư hay master về computer vision hay graphic hoặc ngành liên quan. Có mong muốn học tiếp (PhD) và bắt đầu bằng việc đi thực tập tại Pháp và Úc không ? Nếu có hãy liên lạc với mình ASAP nhé ! https://crossing.cnrs.fr/crossing-internships-2023-24/,,,#Q&A,,
"WEBINAR CHỦ ĐỀ  ""ỨNG DỤNG PHÂN TÍCH ĐỊNH LƯỢNG CHO TRADING"" CÁC TƯ DUY TRONG XÂY DỰNG VÀ KIỂM THỬ CHIẾN THUẬT ĐẦU TƯ TỰ ĐỘNG.
📌ICLS Tech kính mời cộng đồng Trading, những người yêu thích và quan tâm đến Trading đăng ký tham gia buổi chia sẻ về ""Ứng dụng phân tích định lượng cho Trading""
⏱Thời gian bắt đầu: 20h00, thứ Ba, ngày 21/11/2023
👉Hình thức tham dự: tham dự qua Zoom bằng đường link https://me-qr.com/w8ofedQx hoặc quét mã QR trong hình.
👨‍💼Phân tích định lượng trong đầu tư - Quantitative Trading là một phương pháp lượng hóa các thông tin thành số liệu cụ thể, giúp nhà đầu tư loại bỏ được yếu tố cảm xúc trước khi ra quyết định. Dựa trên các công thức, mô hình toán học và các công cụ trí tuệ nhân tạo. Giúp cải thiện kết quả đáng kể cho nhà đầu tư
---------------
👥Với sự chia sẻ của 2 vị khách mời:
🔹Anh Nguyễn Văn Thành:
- Research Consultant tại Tập đoàn Tài Chính định lượng WorldQuant
- Từng làm Software Engineer tại SamSung R&D
- Hơn 2 năm kinh nghiệm làm việc trong lĩnh vực khoa học dữ liệu, xử lý dữ liệu tài chính, crypto currency.
🔹Anh Ngô Phi Hùng:
- Tech Lead tại Hephatus Technology
- Từng làm Data Engineer tại FPT Telecom
---------------
Hãy tham gia ngay để tiếp cận và học hỏi về một phương pháp đầu tư hiệu quả. Có cơ hội được tham gia vào cộng đồng ICLS Tech để được hỗ trợ và tư vấn từ các chuyên gia🥰
Contact
☎️0962038175
✉️contact@icls-tech.com
#Quantitative #Trading #QuantitativeTrading #Webinar","WEBINAR CHỦ ĐỀ ""ỨNG DỤNG PHÂN TÍCH ĐỊNH LƯỢNG CHO TRADING"" CÁC TƯ DUY TRONG XÂY DỰNG VÀ KIỂM THỬ CHIẾN THUẬT ĐẦU TƯ TỰ ĐỘNG. ICLS Tech kính mời cộng đồng Trading, những người yêu thích và quan tâm đến Trading đăng ký tham gia buổi chia sẻ về ""Ứng dụng phân tích định lượng cho Trading"" ⏱Thời gian bắt đầu: 20h00, thứ Ba, ngày 21/11/2023 Hình thức tham dự: tham dự qua Zoom bằng đường link https://me-qr.com/w8ofedQx hoặc quét mã QR trong hình. Phân tích định lượng trong đầu tư - Quantitative Trading là một phương pháp lượng hóa các thông tin thành số liệu cụ thể, giúp nhà đầu tư loại bỏ được yếu tố cảm xúc trước khi ra quyết định. Dựa trên các công thức, mô hình toán học và các công cụ trí tuệ nhân tạo. Giúp cải thiện kết quả đáng kể cho nhà đầu tư --------------- Với sự chia sẻ của 2 vị khách mời: Anh Nguyễn Văn Thành: - Research Consultant tại Tập đoàn Tài Chính định lượng WorldQuant - Từng làm Software Engineer tại SamSung R&D - Hơn 2 năm kinh nghiệm làm việc trong lĩnh vực khoa học dữ liệu, xử lý dữ liệu tài chính, crypto currency. Anh Ngô Phi Hùng: - Tech Lead tại Hephatus Technology - Từng làm Data Engineer tại FPT Telecom --------------- Hãy tham gia ngay để tiếp cận và học hỏi về một phương pháp đầu tư hiệu quả. Có cơ hội được tham gia vào cộng đồng ICLS Tech để được hỗ trợ và tư vấn từ các chuyên gia Contact 0962038175 contact@icls-tech.com",#Quantitative	#Trading	#QuantitativeTrading	#Webinar,,#webinar,,
"Mình đang lead một số dự án tự động hoá ứng dụng AI/ML trong thiết kế, triển khai và tối ưu hệ thống mạng di động 5G (hợp tác với nhà mạng lớn tg).
Các bạn quan tâm tới lĩnh vực AI/ML trong viễn thông có thể kết nối, giao lưu và tham gia dự án bên mình (partime hay fulltime đều OK).
(Ảnh có tính minh họa tự động dự báo lưu lượng và tối ưu 1 trạm 5G toàn thời gian)","Mình đang lead một số dự án tự động hoá ứng dụng AI/ML trong thiết kế, triển khai và tối ưu hệ thống mạng di động 5G (hợp tác với nhà mạng lớn tg). Các bạn quan tâm tới lĩnh vực AI/ML trong viễn thông có thể kết nối, giao lưu và tham gia dự án bên mình (partime hay fulltime đều OK). (Ảnh có tính minh họa tự động dự báo lưu lượng và tối ưu 1 trạm 5G toàn thời gian)",,,#Q&A,,
"Denoise diffusion probabilistic models
ThetaLog - Nhật ký Theta",Denoise diffusion probabilistic models ThetaLog - Nhật ký Theta,,,#sharing,,
"WEBINAR CHỦ ĐỀ  ""ỨNG DỤNG PHÂN TÍCH ĐỊNH LƯỢNG CHO TRADING"" CÁC TƯ DUY TRONG XÂY DỰNG VÀ KIỂM THỬ CHIẾN THUẬT ĐẦU TƯ TỰ ĐỘNG.
📌ICLS Tech kính mời cộng đồng Trading, những người yêu thích và quan tâm đến Trading đăng ký tham gia buổi chia sẻ về ""Ứng dụng phân tích định lượng cho Trading""
⏱Thời gian bắt đầu: 20h00, thứ Ba, ngày 21/11/2023
👉Hình thức tham dự: tham dự qua Zoom bằng đường link https://me-qr.com/w8ofedQx hoặc quét mã QR trong hình.
👨‍💼Phân tích định lượng trong đầu tư - Quantitative Trading là một phương pháp lượng hóa các thông tin thành số liệu cụ thể, giúp nhà đầu tư loại bỏ được yếu tố cảm xúc trước khi ra quyết định. Dựa trên các công thức, mô hình toán học và các công cụ trí tuệ nhân tạo. Giúp cải thiện kết quả đáng kể cho nhà đầu tư
---------------
👥Với sự chia sẻ của 2 vị khách mời:
🔹Anh Nguyễn Văn Thành:
- Research Consultant tại Tập đoàn Tài Chính định lượng WorldQuant
- Từng làm Software Engineer tại SamSung R&D
- Hơn 2 năm kinh nghiệm làm việc trong lĩnh vực khoa học dữ liệu, xử lý dữ liệu tài chính, crypto currency.
🔹Anh Ngô Phi Hùng:
- Tech Lead tại Hephatus Technology
- Từng làm Data Engineer tại FPT Telecom
---------------
Hãy tham gia ngay để tiếp cận và học hỏi về một phương pháp đầu tư hiệu quả. Có cơ hội được tham gia vào cộng đồng ICLS Tech để được hỗ trợ và tư vấn từ các chuyên gia🥰
Contact
☎️0962038175
✉️contact@icls-tech.com
#Quantitative #Trading #QuantitativeTrading #Webinar","WEBINAR CHỦ ĐỀ ""ỨNG DỤNG PHÂN TÍCH ĐỊNH LƯỢNG CHO TRADING"" CÁC TƯ DUY TRONG XÂY DỰNG VÀ KIỂM THỬ CHIẾN THUẬT ĐẦU TƯ TỰ ĐỘNG. ICLS Tech kính mời cộng đồng Trading, những người yêu thích và quan tâm đến Trading đăng ký tham gia buổi chia sẻ về ""Ứng dụng phân tích định lượng cho Trading"" ⏱Thời gian bắt đầu: 20h00, thứ Ba, ngày 21/11/2023 Hình thức tham dự: tham dự qua Zoom bằng đường link https://me-qr.com/w8ofedQx hoặc quét mã QR trong hình. Phân tích định lượng trong đầu tư - Quantitative Trading là một phương pháp lượng hóa các thông tin thành số liệu cụ thể, giúp nhà đầu tư loại bỏ được yếu tố cảm xúc trước khi ra quyết định. Dựa trên các công thức, mô hình toán học và các công cụ trí tuệ nhân tạo. Giúp cải thiện kết quả đáng kể cho nhà đầu tư --------------- Với sự chia sẻ của 2 vị khách mời: Anh Nguyễn Văn Thành: - Research Consultant tại Tập đoàn Tài Chính định lượng WorldQuant - Từng làm Software Engineer tại SamSung R&D - Hơn 2 năm kinh nghiệm làm việc trong lĩnh vực khoa học dữ liệu, xử lý dữ liệu tài chính, crypto currency. Anh Ngô Phi Hùng: - Tech Lead tại Hephatus Technology - Từng làm Data Engineer tại FPT Telecom --------------- Hãy tham gia ngay để tiếp cận và học hỏi về một phương pháp đầu tư hiệu quả. Có cơ hội được tham gia vào cộng đồng ICLS Tech để được hỗ trợ và tư vấn từ các chuyên gia Contact 0962038175 contact@icls-tech.com",#Quantitative	#Trading	#QuantitativeTrading	#Webinar,,#webinar,,
"Từ 1 bức ảnh nhiễu có thể tạo nên 1 bức ảnh chất lượng cao cấp nhờ công nghệ InstaFlow? 🫢 

Mời mọi người cùng tìm hiểu thêm về mô hình ""ma thuật"" được phát triển từ Stable Diffusion này tại bài viết bởi Data Scientist của PIXTA Vietnam nhé! 
#AI #MachineLearning #Instaflow","Từ 1 bức ảnh nhiễu có thể tạo nên 1 bức ảnh chất lượng cao cấp nhờ công nghệ InstaFlow? Mời mọi người cùng tìm hiểu thêm về mô hình ""ma thuật"" được phát triển từ Stable Diffusion này tại bài viết bởi Data Scientist của PIXTA Vietnam nhé!",#AI	#MachineLearning	#Instaflow,,"#sharing, #cv",,
"Có một bạn hôm trước DM mình hỏi về hai cuốn Thực hành Học máy, nay mình tìm lại không thấy tin nhắn nên post lại lên group:
http://handson-ml.mlbvn.org/?fbclid=IwAR1v-g8ihHP6LwYOGnWyvdOdcYbY91BZM9EW0Ig11HSMMKpBh4jjikWPZzc","Có một bạn hôm trước DM mình hỏi về hai cuốn Thực hành Học máy, nay mình tìm lại không thấy tin nhắn nên post lại lên group: http://handson-ml.mlbvn.org/?fbclid=IwAR1v-g8ihHP6LwYOGnWyvdOdcYbY91BZM9EW0Ig11HSMMKpBh4jjikWPZzc",,,"#sharing, #machine_learning",,
"Chào mọi người.
Qua bài post này mình muốn tìm người để cùng nhau học về AI/Data. Mình có thể cùng nhau thảo luận về một chủ đề nào đấy trong lĩnh vực này. Cũng có thể nếu người này vừa học được cái gì mới thì có thể giảng giải cho người kia. Người ta nói rằng: bạn chỉ thật sự hiểu một vấn đề, khi mà bạn có thể giải thích cho người khác cùng hiểu về vấn đề đó. Các chủ đề này cũng có thể chỉ là những vấn đề cơ bản của AI/Data thôi.
Một điều quan trọng là mình muốn các buổi thảo luận đều hoàn toàn bằng tiếng Anh.
Thật ra mục đích chính của những buổi này là mình muốn nâng cao kĩ năng thuyết trình, thảo luận và để ôn lại những kiến thức đã học được thôi.
Về mình thì mình có kiến thức về AI/Data ở mức tạm ổn, tiếng Anh tốt. Vì vậy nếu có bạn nào cũng chung chí hướng thì inbox mình nhé. Cảm ơn mọi người.","Chào mọi người. Qua bài post này mình muốn tìm người để cùng nhau học về AI/Data. Mình có thể cùng nhau thảo luận về một chủ đề nào đấy trong lĩnh vực này. Cũng có thể nếu người này vừa học được cái gì mới thì có thể giảng giải cho người kia. Người ta nói rằng: bạn chỉ thật sự hiểu một vấn đề, khi mà bạn có thể giải thích cho người khác cùng hiểu về vấn đề đó. Các chủ đề này cũng có thể chỉ là những vấn đề cơ bản của AI/Data thôi. Một điều quan trọng là mình muốn các buổi thảo luận đều hoàn toàn bằng tiếng Anh. Thật ra mục đích chính của những buổi này là mình muốn nâng cao kĩ năng thuyết trình, thảo luận và để ôn lại những kiến thức đã học được thôi. Về mình thì mình có kiến thức về AI/Data ở mức tạm ổn, tiếng Anh tốt. Vì vậy nếu có bạn nào cũng chung chí hướng thì inbox mình nhé. Cảm ơn mọi người.",,,#Q&A,,
"Em chào mọi người,
Hiện tại em đang có thực hiện project build knowledge graph cho văn bản tiếng việt định hướng phương pháp em làm như sau :
- Dùng underthesea để setence segmentation để tách nhỏ văn bản thành từng câu
- Dùng Named Entity Regconition (NER)/Dependency Parsing (DEP) để phân tích thành phần trong câu ( Tạm thời sẽ dùng vncoreNLP để extract thử nghiệm tính hiệu quả nếu tốt có thể train lại )
- Từ NER/DEP đã extract sẽ tìm ra bộ 3 ( entity 1 - relation - entity 2 ) trong câu để tạo thành 1 liên kết trong graph với các node là entity
VD : ( Tổng thống mỹ là Joe Biden ) thì khi dùng NER/DEP thì sẽ cho ra kết quả ( entity 1:""Tổng_thống_mỹ"", relation:""là"",Entity 2 : ""Joe_Biden"")
---------------------------------------------------------------
Các khó khăn hiện tại vd với 1 câu phức tạp hơn như sau :
- Tổng thống mỹ là Joe Biden, ông còn biết tới là chính trị gia.
Thì trong 1 câu xuất hiện tới 2 bộ (e1,r,e2)
- Tổng thống mỹ là Joe Biden
- ông là chính trị gia
nhưng từ ông thì quá chung chung khi đó vào graph thì từ ông sẽ được liên kết với nhiều thành phần ko mong muốn ko thể hiện được ông = Joe Biden trong liên kết thì expect em nó sẽ là
- Tổng thống mỹ là Joe Biden
- Joe Bide là chính trị gia
Mọi người có giải pháp nào hoặc phương pháp nào giúp em tiếp cận bài toán không ạ em xin cảm ơn mọi người đã đọc","Em chào mọi người, Hiện tại em đang có thực hiện project build knowledge graph cho văn bản tiếng việt định hướng phương pháp em làm như sau : - Dùng underthesea để setence segmentation để tách nhỏ văn bản thành từng câu - Dùng Named Entity Regconition (NER)/Dependency Parsing (DEP) để phân tích thành phần trong câu ( Tạm thời sẽ dùng vncoreNLP để extract thử nghiệm tính hiệu quả nếu tốt có thể train lại ) - Từ NER/DEP đã extract sẽ tìm ra bộ 3 ( entity 1 - relation - entity 2 ) trong câu để tạo thành 1 liên kết trong graph với các node là entity VD : ( Tổng thống mỹ là Joe Biden ) thì khi dùng NER/DEP thì sẽ cho ra kết quả ( entity 1:""Tổng_thống_mỹ"", relation:""là"",Entity 2 : ""Joe_Biden"") --------------------------------------------------------------- Các khó khăn hiện tại vd với 1 câu phức tạp hơn như sau : - Tổng thống mỹ là Joe Biden, ông còn biết tới là chính trị gia. Thì trong 1 câu xuất hiện tới 2 bộ (e1,r,e2) - Tổng thống mỹ là Joe Biden - ông là chính trị gia nhưng từ ông thì quá chung chung khi đó vào graph thì từ ông sẽ được liên kết với nhiều thành phần ko mong muốn ko thể hiện được ông = Joe Biden trong liên kết thì expect em nó sẽ là - Tổng thống mỹ là Joe Biden - Joe Bide là chính trị gia Mọi người có giải pháp nào hoặc phương pháp nào giúp em tiếp cận bài toán không ạ em xin cảm ơn mọi người đã đọc",,,"#Q&A, #nlp",,
,nan,,,,,
"Chào mọi người, em đang tìm kiếm công việc thực tập ở mảng Computer vision và đang chuẩn bị CV. Đây là CV của em, các anh chị kinh nghiệm có thể giúp em chỉnh sửa lại CV, cũng như đưa ra lời khuyên giúp em nên học thêm và chuyên sâu vào mảng nào được không ạ.
PS: em đang là sinh viên năm 3, các link github em không đính kèm trong file, nếu anh chị muốn xem thêm thì em gửi riêng ạ.","Chào mọi người, em đang tìm kiếm công việc thực tập ở mảng Computer vision và đang chuẩn bị CV. Đây là CV của em, các anh chị kinh nghiệm có thể giúp em chỉnh sửa lại CV, cũng như đưa ra lời khuyên giúp em nên học thêm và chuyên sâu vào mảng nào được không ạ. PS: em đang là sinh viên năm 3, các link github em không đính kèm trong file, nếu anh chị muốn xem thêm thì em gửi riêng ạ.",,,#Q&A,,
"🎲🌈 WEBINAR: HƯỚNG DẪN TẠO NỘI DUNG VỚI CHATGPT
Tối thứ ba, 14/11 tới đây, FUNiX tổ chức webinar online ""Next-level AI Content - Hướng dẫn tạo nội dung với ChatGPT"". Diễn giả Trung Caha - Co-Founder Antory, Admin blog khoahocmidjourney.com, sẽ chia sẻ kinh nghiệm về cách sử dụng các kỹ thuật đột phá với ChatGPT.
Đến với webinar, bạn sẽ biết:
👉Tạo nội dung có chất lượng cao, cuốn hút từ ChatGPT mà không phải câu trả lời chung chung hay giống với tìm kiếm Google
👉Viết câu lệnh với Chat GPT mà  99,99999% thế giới ngoài kia chưa biết đến.
👉5 Yếu tố để tạo nội dung chuyên sâu, chất lượng cho bất cứ lĩnh vực nào bạn muốn.
👉Đạt được lợi thế cạnh tranh VƯỢT TRỘI  ngay cả so với những người khác sử dụng AI khác.
📌Nhanh tay đăng ký tại https://shorturl.at/atA28
⏰ Thời gian: 20:00 - 21:30, Thứ 3, ngày 14/11/2023","WEBINAR: HƯỚNG DẪN TẠO NỘI DUNG VỚI CHATGPT Tối thứ ba, 14/11 tới đây, FUNiX tổ chức webinar online ""Next-level AI Content - Hướng dẫn tạo nội dung với ChatGPT"". Diễn giả Trung Caha - Co-Founder Antory, Admin blog khoahocmidjourney.com, sẽ chia sẻ kinh nghiệm về cách sử dụng các kỹ thuật đột phá với ChatGPT. Đến với webinar, bạn sẽ biết: Tạo nội dung có chất lượng cao, cuốn hút từ ChatGPT mà không phải câu trả lời chung chung hay giống với tìm kiếm Google Viết câu lệnh với Chat GPT mà 99,99999% thế giới ngoài kia chưa biết đến. 5 Yếu tố để tạo nội dung chuyên sâu, chất lượng cho bất cứ lĩnh vực nào bạn muốn. Đạt được lợi thế cạnh tranh VƯỢT TRỘI ngay cả so với những người khác sử dụng AI khác. Nhanh tay đăng ký tại https://shorturl.at/atA28 ⏰ Thời gian: 20:00 - 21:30, Thứ 3, ngày 14/11/2023",,,#webinar,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2023 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2023 vào comment của post này.",,,#sharing,,
"[AI Share - Statistics]
Đa số các thuật toán của Machine Learning đều dựa trên nền của Xác suất và thống kê. Đối với nhiều người, xác suất thống kê là một môn khó và có nhiều kiến thức cần phải nắm. Ngoài việc đọc sách để nắm vững các khái niệm và ứng dụng, AI4E muốn chia sẻ 1 cheatsheet tổng hợp các kiến thức xác suất một cách ngắn gọn và tổng quát nhất. Cheatsheet này bao phủ toàn bộ các kiến thức cốt lõi nhất trong xác suất. Tài liệu có giá trị và đáng tin cậy bởi người viết dựa trên tài liệu khóa học Xác suất của Harvards.","[AI Share - Statistics] Đa số các thuật toán của Machine Learning đều dựa trên nền của Xác suất và thống kê. Đối với nhiều người, xác suất thống kê là một môn khó và có nhiều kiến thức cần phải nắm. Ngoài việc đọc sách để nắm vững các khái niệm và ứng dụng, AI4E muốn chia sẻ 1 cheatsheet tổng hợp các kiến thức xác suất một cách ngắn gọn và tổng quát nhất. Cheatsheet này bao phủ toàn bộ các kiến thức cốt lõi nhất trong xác suất. Tài liệu có giá trị và đáng tin cậy bởi người viết dựa trên tài liệu khóa học Xác suất của Harvards.",,,"#sharing, #math",,
"Chào các anh chị,
Em muốn mua máy bàn phục vụ việc nghiên cứu và chạy các mô hình học máy. Anh chị nào có thể gợi ý giúp em máy có cấu hình phù hợp được không ạ?

Em chân thành cám ơn.","Chào các anh chị, Em muốn mua máy bàn phục vụ việc nghiên cứu và chạy các mô hình học máy. Anh chị nào có thể gợi ý giúp em máy có cấu hình phù hợp được không ạ? Em chân thành cám ơn.",,,#Q&A,,
"[Cơ hội tham dự Hội thảo Quốc tế về Computer Vision cho các bạn SV và NCS ngay tại Việt Nam với phí Hội thảo chỉ từ 100 ~200 USD. Cảm ơn ad đã duyệt bài].

Dear Scientists and Colleagues,
We will organize a Special Session on Computer Vision - SSCV (https://cita.vku.udn.vn/2024/special-session-cv) at CITA 2024 (13th Conference on Information Technology and its Applications) in Hoi An, Vietnam on July 19-20, 2024.
We welcome your research and contribution to this special session and enjoy Vietnam's culture and life.
See you in Hoi An, Vietnam!
____
This is the detailed information of the Special Session on Computer Vision - SSCV:
📌Objectives:
Computer Vision is a field of Artificial Intelligence that enables computers to interpret and understand meaningful information from digital images, videos, and other visual inputs. Nowadays, Computer Vision is widely applied and deployed in many different fields such as intelligent transportation systems, surveillance systems, biomedical image diagnosis systems, automatic control systems, and monitoring systems in industry and agriculture, ... Especially with the rapid development of Convolutional Neural Networks and Machine Learning, they have attracted a large number of researchers working in this field and its applications. Computer Vision is becoming more and more popular.
The Special Session on Computer Vision (SSCV) at the 13th Conference on Information Technology and its Applications (CITA 2024) aims to provide opportunities for scientists and developers to present the hot trends and promising developments of Computer Vision. Through this forum, the latest research, innovative techniques, and advanced applications in Computer Vision are also exchanged and discussed to open up the broader academic environment and community.
📌Topics:
Relevant topics include, but are not limited to several of the following topics:
👉Computer Vision and Robot Vision
👉Computer Vision-based Machine Learning and Deep Learning
👉Image Processing
👉Pattern Recognition
👉Object Detection and Classification
👉Semantic and Instance Segmentation
👉Motion Recognition and Tracking
👉Multimedia Analysis
👉Medical, Biomedical, and Biometrics Image Analysis
👉Document Analysis and Recognition
👉Human-Computer Interaction (HCI)
👉Human-Robot Interaction (HRI)
👉Computer Vision-based Control
👉Computer Vision-based Surveillant Systems
👉Computer Vision-based Intelligent Transportation Systems
👉Computer Vision-based Applications
👉Virtual Reality and Augmented Reality Technologies
📌Important Dates
Paper submission: January 15th, 2024
Final Notification: March 15th, 2024
Camera Ready: April 20th, 2024
Conference Sessions: July 19th - 20th, 2024
📌Organizers
👉Prof. Dr.  Kang-Hyun Jo (Chair)
Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea
Email: acejo@ulsan.ac.kr
👉Dr. Duy-Linh Nguyen (Co-chair)
Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea
Email: nguyenduylinhqbu@gmail.com
📌Paper Submission
Paper’s template used at CITA2024 abides by the standard format of Springer LNCS/LNAI magazine (refer to link: http://www.springer.com/.../conference-proceedings...). When posting, the authors need to agree to the following terms:
Submissions must be original papers, never be posted/published before;
Use the template specified by the conference, do not change the font style, format, header/footer; no page numbering;
Language: English
Length: no more than 12 pages.
To submit the papers:
Sign in: https://easychair.org/conferences/?conf=cita2024
Select Special Session: SSCV
📌📌📌📌📌Contact:
Dr. Duy-Linh Nguyen
Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea
Email: nguyenduylinhqbu@gmail.com","[Cơ hội tham dự Hội thảo Quốc tế về Computer Vision cho các bạn SV và NCS ngay tại Việt Nam với phí Hội thảo chỉ từ 100 ~200 USD. Cảm ơn ad đã duyệt bài]. Dear Scientists and Colleagues, We will organize a Special Session on Computer Vision - SSCV (https://cita.vku.udn.vn/2024/special-session-cv) at CITA 2024 (13th Conference on Information Technology and its Applications) in Hoi An, Vietnam on July 19-20, 2024. We welcome your research and contribution to this special session and enjoy Vietnam's culture and life. See you in Hoi An, Vietnam! ____ This is the detailed information of the Special Session on Computer Vision - SSCV: Objectives: Computer Vision is a field of Artificial Intelligence that enables computers to interpret and understand meaningful information from digital images, videos, and other visual inputs. Nowadays, Computer Vision is widely applied and deployed in many different fields such as intelligent transportation systems, surveillance systems, biomedical image diagnosis systems, automatic control systems, and monitoring systems in industry and agriculture, ... Especially with the rapid development of Convolutional Neural Networks and Machine Learning, they have attracted a large number of researchers working in this field and its applications. Computer Vision is becoming more and more popular. The Special Session on Computer Vision (SSCV) at the 13th Conference on Information Technology and its Applications (CITA 2024) aims to provide opportunities for scientists and developers to present the hot trends and promising developments of Computer Vision. Through this forum, the latest research, innovative techniques, and advanced applications in Computer Vision are also exchanged and discussed to open up the broader academic environment and community. Topics: Relevant topics include, but are not limited to several of the following topics: Computer Vision and Robot Vision Computer Vision-based Machine Learning and Deep Learning Image Processing Pattern Recognition Object Detection and Classification Semantic and Instance Segmentation Motion Recognition and Tracking Multimedia Analysis Medical, Biomedical, and Biometrics Image Analysis Document Analysis and Recognition Human-Computer Interaction (HCI) Human-Robot Interaction (HRI) Computer Vision-based Control Computer Vision-based Surveillant Systems Computer Vision-based Intelligent Transportation Systems Computer Vision-based Applications Virtual Reality and Augmented Reality Technologies Important Dates Paper submission: January 15th, 2024 Final Notification: March 15th, 2024 Camera Ready: April 20th, 2024 Conference Sessions: July 19th - 20th, 2024 Organizers Prof. Dr. Kang-Hyun Jo (Chair) Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea Email: acejo@ulsan.ac.kr Dr. Duy-Linh Nguyen (Co-chair) Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea Email: nguyenduylinhqbu@gmail.com Paper Submission Paper’s template used at CITA2024 abides by the standard format of Springer LNCS/LNAI magazine (refer to link: http://www.springer.com/.../conference-proceedings...). When posting, the authors need to agree to the following terms: Submissions must be original papers, never be posted/published before; Use the template specified by the conference, do not change the font style, format, header/footer; no page numbering; Language: English Length: no more than 12 pages. To submit the papers: Sign in: https://easychair.org/conferences/?conf=cita2024 Select Special Session: SSCV Contact: Dr. Duy-Linh Nguyen Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea Email: nguyenduylinhqbu@gmail.com",,,,,
"[Scikit learn báo Multicolinearity]
ACE cho e hỏi e đang dùng Scikit Learn chạy OLS, check VIF toàn nhỏ hơn 2 sao cứ bị báo Multicolinearity vậy ạ. Ai biết Scikit Learn tính cái này thế nào k ạ chỉ e với. Tks mn!","[Scikit learn báo Multicolinearity] ACE cho e hỏi e đang dùng Scikit Learn chạy OLS, check VIF toàn nhỏ hơn 2 sao cứ bị báo Multicolinearity vậy ạ. Ai biết Scikit Learn tính cái này thế nào k ạ chỉ e với. Tks mn!",,,"#Q&A, #python",,
"Chào mọi người ạ,
Em nhận thấy các LLM rất hay tự thêm thông tin vào context. Ví dụ em bảo nó viết câu hỏi cho một đoạn văn hoặc tóm tắt một bài báo thì nó hay tự động bổ sung các thông tin bên ngoài context / prompt vào kết quả.
Em cũng hiểu đây là chuyện dễ hiểu vì LLM là stocastic. Em cũng đã dùng các kiểu chain of thought để bắt LLM chỉ dùng thông tin được cung cấp để trả lời. Nếu không trả lời được dựa trên context thì cứ bảo là không biết, nhưng kết quả lúc được lúc không.
Không viết trong literature thì hiện tượng này gọi là gì ạ? Có phải là vấn đề các nhà khoa học đang giải quyết không ạ?
Cám ơn mọi người","Chào mọi người ạ, Em nhận thấy các LLM rất hay tự thêm thông tin vào context. Ví dụ em bảo nó viết câu hỏi cho một đoạn văn hoặc tóm tắt một bài báo thì nó hay tự động bổ sung các thông tin bên ngoài context / prompt vào kết quả. Em cũng hiểu đây là chuyện dễ hiểu vì LLM là stocastic. Em cũng đã dùng các kiểu chain of thought để bắt LLM chỉ dùng thông tin được cung cấp để trả lời. Nếu không trả lời được dựa trên context thì cứ bảo là không biết, nhưng kết quả lúc được lúc không. Không viết trong literature thì hiện tượng này gọi là gì ạ? Có phải là vấn đề các nhà khoa học đang giải quyết không ạ? Cám ơn mọi người",,,"#Q&A, #nlp",,
Nắm vững các giai đoạn phát triển của ứng dụng với Container,Nắm vững các giai đoạn phát triển của ứng dụng với Container,,,#sharing,,
"Chào buổi tối mọi người,
Hôm nay team VILM trình làng bộ dataset OpenOrca-Viet, bao gồm 120,000 cặp câu hỏi instructions chất lượng cao được distillate từ 3 LLMs hàng đầu trên thế giới: GPT-4, PaLM-2 và Claude.
OpenOrca-Viet được đồng phát triển dưới sự hợp tác của VILM và Alignment Lab AI, chủ nhân của bộ dataset OpenOrca gốc bằng tiếng Anh. Đây cũng chính là một trong các bộ dataset được sử dụng để train model Vietcuna-7B-v3.
Chúc mọi người có một buổi tối vui vẻ
Link to Dataset: https://huggingface.co/datasets/vilm/OpenOrca-Viet","Chào buổi tối mọi người, Hôm nay team VILM trình làng bộ dataset OpenOrca-Viet, bao gồm 120,000 cặp câu hỏi instructions chất lượng cao được distillate từ 3 LLMs hàng đầu trên thế giới: GPT-4, PaLM-2 và Claude. OpenOrca-Viet được đồng phát triển dưới sự hợp tác của VILM và Alignment Lab AI, chủ nhân của bộ dataset OpenOrca gốc bằng tiếng Anh. Đây cũng chính là một trong các bộ dataset được sử dụng để train model Vietcuna-7B-v3. Chúc mọi người có một buổi tối vui vẻ Link to Dataset: https://huggingface.co/datasets/vilm/OpenOrca-Viet",,,"#sharing, #data",,
"em chào các anh chị ạ, mọi người cho e hỏi là để vẽ hình này cần dùng tool gì ạ, e cảm ơn mng ạ!","em chào các anh chị ạ, mọi người cho e hỏi là để vẽ hình này cần dùng tool gì ạ, e cảm ơn mng ạ!",,,#Q&A,,
Em đang học về lắng nghe mạng xã hội. Trong bài yêu cầu phân tích chủ đề và sắc thái của doanh nghiệp dựa vào quy tắc sắc thái và quy tắc chủ đề. Nhưng em dựa vào đó vẫn làm sai. Anh chị chia sẻ cho em ít kinh nghiệm để làm đúng với ạ. Em cảm ơn.,Em đang học về lắng nghe mạng xã hội. Trong bài yêu cầu phân tích chủ đề và sắc thái của doanh nghiệp dựa vào quy tắc sắc thái và quy tắc chủ đề. Nhưng em dựa vào đó vẫn làm sai. Anh chị chia sẻ cho em ít kinh nghiệm để làm đúng với ạ. Em cảm ơn.,,,#Q&A,,
"Em xin chào mọi người ạ. Hiện tại em đang thực hiện một bài toán như sau: Từ câu ngôn ngữ sinh ra query (vd như bài toán text2sql), từ query => chart, rồi từ chart => comment hoặc description về chart 📷
Vì kiến thức của e về nlp còn khá hạn chế, nên em chưa có kinh nghiệm nhiều trong các bài toán như này, nên em xin mọi người tư vấn giúp em một số vấn đề như sau ạ:
Cách tiếp cận bài toán như thế nào ạ? 
Dữ liệu train sẽ được xây dựng đánh label như thế nào ạ?
Để xây dựng được bài toán em nên học và sử dụng công cụ nào ạ? Em xin các keyword về bất cứ cứ thứ gì có thể coi là hữu ích cho bài toán trên: link, model, framework, document,...
Em xin cảm ơn các tư vấn ạ","Em xin chào mọi người ạ. Hiện tại em đang thực hiện một bài toán như sau: Từ câu ngôn ngữ sinh ra query (vd như bài toán text2sql), từ query => chart, rồi từ chart => comment hoặc description về chart Vì kiến thức của e về nlp còn khá hạn chế, nên em chưa có kinh nghiệm nhiều trong các bài toán như này, nên em xin mọi người tư vấn giúp em một số vấn đề như sau ạ: Cách tiếp cận bài toán như thế nào ạ? Dữ liệu train sẽ được xây dựng đánh label như thế nào ạ? Để xây dựng được bài toán em nên học và sử dụng công cụ nào ạ? Em xin các keyword về bất cứ cứ thứ gì có thể coi là hữu ích cho bài toán trên: link, model, framework, document,... Em xin cảm ơn các tư vấn ạ",,,"#Q&A, #nlp",,
"Chào các bác. Cuối tuần tranh thủ thấy có github hày về món Text to Speech em xin mạnh dạn chia sẻ cùng các bạn mới học.
Bark - một món chuyển text to speech chạy offline, giọng tự nhiên hơn cả Google. Anh em nào cần làm món này cứ thế mà xài tự nhiên.
Tiếc là chưa có code training!","Chào các bác. Cuối tuần tranh thủ thấy có github hày về món Text to Speech em xin mạnh dạn chia sẻ cùng các bạn mới học. Bark - một món chuyển text to speech chạy offline, giọng tự nhiên hơn cả Google. Anh em nào cần làm món này cứ thế mà xài tự nhiên. Tiếc là chưa có code training!",,,#sharing,,
"Em chào mn ạ, e đang tìm hiểu về miccro segmentation mà thấy ít tài liệu viết về cái này nên e chưa hiểu rõ ạ. Không biết mn ai đã từng làm về phân khúc vi mô khách hàng cho e tham khảo với được không ạ? Em cảm ơn mn nhiều ạ.","Em chào mn ạ, e đang tìm hiểu về miccro segmentation mà thấy ít tài liệu viết về cái này nên e chưa hiểu rõ ạ. Không biết mn ai đã từng làm về phân khúc vi mô khách hàng cho e tham khảo với được không ạ? Em cảm ơn mn nhiều ạ.",,,#Q&A,,
"Mn cho e hỏi câu liên quan đến SQL với ạ.
Em dùng bulk insert để nhập data từ file txt nhưng có 1 số bản ghi bị lỗi ví dụ như:
Name id
""Manh Thang (dấu tab)"" 1
Nhưng khi import thì dấu "" bị nhảy sang phần id( lý do là vì dấu tab) , do đó các hàng tiếp cũng bị nhảy theo.
Ai đã xử lý trường hợp như vậy cho e xin cách fix với ạ.
Em cảm ơn.","Mn cho e hỏi câu liên quan đến SQL với ạ. Em dùng bulk insert để nhập data từ file txt nhưng có 1 số bản ghi bị lỗi ví dụ như: Name id ""Manh Thang (dấu tab)"" 1 Nhưng khi import thì dấu "" bị nhảy sang phần id( lý do là vì dấu tab) , do đó các hàng tiếp cũng bị nhảy theo. Ai đã xử lý trường hợp như vậy cho e xin cách fix với ạ. Em cảm ơn.",,,#Q&A,,
"https://docs.google.com/document/d/1mlGA66qFAtNeTILDLZTSwxXZlI5R1T2Y2IbCiYTZRO8/edit?usp=sharing
Mọi người cho e hỏi là sau khi tìm được tất cả các tập phổ biến --> tìm luật kết hợp. Nhưng mà e ko chắc cách làm của e đã đúng chưa?
Vd trong tập phổ biến ABDE ta loại bỏ A thì có luật BDE->A conf = 2/2 = 1 --> đây là luật kết hợp
Link chi tiết bài làm e để trong google docs mn có thể xem qua và cho e ý kiến được không ạ ?
Sau khi tìm được các luật phổ biến thì có cần loại bỏ trùng lặp không ? Em cảm ơn!",https://docs.google.com/document/d/1mlGA66qFAtNeTILDLZTSwxXZlI5R1T2Y2IbCiYTZRO8/edit?usp=sharing Mọi người cho e hỏi là sau khi tìm được tất cả các tập phổ biến --> tìm luật kết hợp. Nhưng mà e ko chắc cách làm của e đã đúng chưa? Vd trong tập phổ biến ABDE ta loại bỏ A thì có luật BDE->A conf = 2/2 = 1 --> đây là luật kết hợp Link chi tiết bài làm e để trong google docs mn có thể xem qua và cho e ý kiến được không ạ ? Sau khi tìm được các luật phổ biến thì có cần loại bỏ trùng lặp không ? Em cảm ơn!,,,#Q&A,,
"Cho mình hỏi có ai hiểu cách diễn giải của tác giả để đi đến kết luận P[R]>epsilon ở dòng cuối ko vậy?
Tên sách: Foundation of machine learning, MIT press. Sách có bản free online.","Cho mình hỏi có ai hiểu cách diễn giải của tác giả để đi đến kết luận P[R]>epsilon ở dòng cuối ko vậy? Tên sách: Foundation of machine learning, MIT press. Sách có bản free online.",,,"#Q&A, #machine_learning",,
"Em xin chào mọi người ạ. Hiện tại em đang thực hiện một đề tài như sau: Xây dựng chatbot trả lời các câu hỏi liên quan đến lĩnh vực Luật. Ví dụ khi đặt câu hỏi ""Chạy xe máy vượt đèn đỏ sẽ bị phạt bao nhiêu tiền?"" thì chatbot sẽ trả lời câu hỏi đó đồng thời có thể đưa ra trích dẫn trong văn bản luật.
Em đã có bộ dữ liệu về các văn bản luật khá đầy đủ bao gồm cả các thuộc tính về hiệu lực, lĩnh vực,.... Với bộ câu hỏi - câu trả lời em cũng đã thu thập được lượng dữ liệu đủ lớn (100k) để sẵn sàng traning.
Vì kiến thức về AI của em khá hạn chế, mới bắt đầu tìm hiểu nên e xin được tham khảo ý kiến về các vấn đề sau ạ:
Em có nhận được lời khuyên là sử dụng NLP, ML để xử lý câu hỏi đầu vào theo các từ đồng nghĩa và xây dựng mạng Ontology để ánh xạ câu hỏi người dùng và tiến hành trả lời. Em xin hỏi liệu đây có phải là một cách tối ưu cho bài toán trên không ạ? Nếu có cách khác thì em xin nghe đề xuất ạ?
Để xây dựng được chatbot theo yêu cầu đề bài em nên học và sử dụng công cụ nào ạ? Em xin các keyword về bất cứ cứ thứ gì có thể coi là hữu ích cho bài toán trên: link, model, framework, document,...
Em xin cảm ơn các tư vấn ạ <3","Em xin chào mọi người ạ. Hiện tại em đang thực hiện một đề tài như sau: Xây dựng chatbot trả lời các câu hỏi liên quan đến lĩnh vực Luật. Ví dụ khi đặt câu hỏi ""Chạy xe máy vượt đèn đỏ sẽ bị phạt bao nhiêu tiền?"" thì chatbot sẽ trả lời câu hỏi đó đồng thời có thể đưa ra trích dẫn trong văn bản luật. Em đã có bộ dữ liệu về các văn bản luật khá đầy đủ bao gồm cả các thuộc tính về hiệu lực, lĩnh vực,.... Với bộ câu hỏi - câu trả lời em cũng đã thu thập được lượng dữ liệu đủ lớn (100k) để sẵn sàng traning. Vì kiến thức về AI của em khá hạn chế, mới bắt đầu tìm hiểu nên e xin được tham khảo ý kiến về các vấn đề sau ạ: Em có nhận được lời khuyên là sử dụng NLP, ML để xử lý câu hỏi đầu vào theo các từ đồng nghĩa và xây dựng mạng Ontology để ánh xạ câu hỏi người dùng và tiến hành trả lời. Em xin hỏi liệu đây có phải là một cách tối ưu cho bài toán trên không ạ? Nếu có cách khác thì em xin nghe đề xuất ạ? Để xây dựng được chatbot theo yêu cầu đề bài em nên học và sử dụng công cụ nào ạ? Em xin các keyword về bất cứ cứ thứ gì có thể coi là hữu ích cho bài toán trên: link, model, framework, document,... Em xin cảm ơn các tư vấn ạ <3",,,"#Q&A, #nlp",,
Em đang làm về summarize sử dụng model LongT5. Em đang muốn thêm 1 block reattention vào sau block self-attention đầu tiên của phần encoder. Mà đang gặp lỗi ReAttentionBlock.forward() got an unexpected keyword argument 'attention_mask' ai có hướng solve giúp em với ạ. Em cảm ơn,Em đang làm về summarize sử dụng model LongT5. Em đang muốn thêm 1 block reattention vào sau block self-attention đầu tiên của phần encoder. Mà đang gặp lỗi ReAttentionBlock.forward() got an unexpected keyword argument 'attention_mask' ai có hướng solve giúp em với ạ. Em cảm ơn,,,"#Q&A, #deep_learing",,
"[fun little maths for ML][7-grade maths is enough to solve]
Count the number of non-increasing INDICATOR functions from the domain of n distinct real numbers (n is a positive integer) to {0,1}?
a) 2^n
b) 2n+1
c) n
d) n+ 1
e) Infinitely many
f) uncountable
Mời các bạn suy nghĩ và cho đáp án 🤔","[fun little maths for ML][7-grade maths is enough to solve] Count the number of non-increasing INDICATOR functions from the domain of n distinct real numbers (n is a positive integer) to {0,1}? a) 2^n b) 2n+1 c) n d) n+ 1 e) Infinitely many f) uncountable Mời các bạn suy nghĩ và cho đáp án",,,,,
"[Hỏi đáp âm thanh]
Em xin chào mọi người.
Hiện tại em đang làm đồ án về đề tài xác định động cơ bị lỗi bằng âm thanh.
Vì động cơ lỗi khá ít nên tập dữ liệu của em chỉ đa số là âm thanh về động cơ bình thường dài khoảng 0.4 giây ạ.
Cho nên em hiện tại đang giải quyết bài toán theo hướng Anomaly Detection với 3 cách như sau:
Cách 1: Em trích xuất đặc trưng MFCC và sử dụng mô hình LSTM-Autoencode để phân biệt normal, abnormal dựa trên Loss của predict với input.
Cách 2: Em lấy hình ảnh Log-Mel-Spectrogram và sử dụng mô hình CNN-Autoencode cũng để phân biệt normal và abnormal dựa trên loss với đầu vào.
Nhưng kết quả trên tập test của 2 cách này bị sai rất nhiều ạ. Và em nghĩ nguyên nhân là do em trích xuất đặc trưng chưa phù hợp ạ.
Cho nên sau khi tham khảo các paper thì em trích xuất những đặc trưng sau: chorma, energy, spectral, rolloff, zero crossing, MFCC. Đối với mỗi loại đặc trưng thì em lấy mean và var thì được 69 chiều cho mỗi file âm thanh. Sau đó sử dụng mô hình One-Class-SVM để phân loại thì thấy kết quả có vẻ khả quan hơn được tý nhưng vẫn loại sai khá nhiều ạ.
Em muốn hỏi là đối với âm thanh về tiếng ồn của động cơ như này thì mình nên sử dụng đặc trưng nào của âm thanh và sử dụng mô hình gì ML/DL gì để có thể phân biệt được vậy ạ :(
Dưới đây là dữ liệu âm thanh của em ạ.
https://drive.google.com/drive/folders/171Y5_W7L6-v1dwDp9HB430fQYJeTwF9n?usp=sharing","[Hỏi đáp âm thanh] Em xin chào mọi người. Hiện tại em đang làm đồ án về đề tài xác định động cơ bị lỗi bằng âm thanh. Vì động cơ lỗi khá ít nên tập dữ liệu của em chỉ đa số là âm thanh về động cơ bình thường dài khoảng 0.4 giây ạ. Cho nên em hiện tại đang giải quyết bài toán theo hướng Anomaly Detection với 3 cách như sau: Cách 1: Em trích xuất đặc trưng MFCC và sử dụng mô hình LSTM-Autoencode để phân biệt normal, abnormal dựa trên Loss của predict với input. Cách 2: Em lấy hình ảnh Log-Mel-Spectrogram và sử dụng mô hình CNN-Autoencode cũng để phân biệt normal và abnormal dựa trên loss với đầu vào. Nhưng kết quả trên tập test của 2 cách này bị sai rất nhiều ạ. Và em nghĩ nguyên nhân là do em trích xuất đặc trưng chưa phù hợp ạ. Cho nên sau khi tham khảo các paper thì em trích xuất những đặc trưng sau: chorma, energy, spectral, rolloff, zero crossing, MFCC. Đối với mỗi loại đặc trưng thì em lấy mean và var thì được 69 chiều cho mỗi file âm thanh. Sau đó sử dụng mô hình One-Class-SVM để phân loại thì thấy kết quả có vẻ khả quan hơn được tý nhưng vẫn loại sai khá nhiều ạ. Em muốn hỏi là đối với âm thanh về tiếng ồn của động cơ như này thì mình nên sử dụng đặc trưng nào của âm thanh và sử dụng mô hình gì ML/DL gì để có thể phân biệt được vậy ạ :( Dưới đây là dữ liệu âm thanh của em ạ. https://drive.google.com/drive/folders/171Y5_W7L6-v1dwDp9HB430fQYJeTwF9n?usp=sharing",,,#Q&A,,
"Em xin chào mọi người ạ. Hiện em đang thử chạy code có sử dụng GPU VGA GIGABYTE GeForce RTX 3060 GAMING OC 12G (rev. 2.0) (GV-N3060GAMING OC-12GD) trên hệ điều hành Ubuntu 20.04. Tuy nhiên, khi em cài cuda thì hiện lên thông báo không tương thích. Mọi người có thể giợi ý cho em bản cuda nào tương thích với máy với ạ. Em xin cảm ơn mọi người.
Bản mà em thử tải ạ
https://linuxhint.com/install-cuda-ubuntu-2004/?fbclid=IwAR20YUODljBjdtVzyocI1wQeVpM-HaZID5RZ9VFcQl73DB3ueq4jPwmkYTQ
Lỗi mà máy em hiện lên ạ:","Em xin chào mọi người ạ. Hiện em đang thử chạy code có sử dụng GPU VGA GIGABYTE GeForce RTX 3060 GAMING OC 12G (rev. 2.0) (GV-N3060GAMING OC-12GD) trên hệ điều hành Ubuntu 20.04. Tuy nhiên, khi em cài cuda thì hiện lên thông báo không tương thích. Mọi người có thể giợi ý cho em bản cuda nào tương thích với máy với ạ. Em xin cảm ơn mọi người. Bản mà em thử tải ạ https://linuxhint.com/install-cuda-ubuntu-2004/?fbclid=IwAR20YUODljBjdtVzyocI1wQeVpM-HaZID5RZ9VFcQl73DB3ueq4jPwmkYTQ Lỗi mà máy em hiện lên ạ:",,,"#Q&A, #python",,
"Em chào mọi người ạ, em đang học môn học máy, và hiện tại em đang có mấy bài tập như hình dưới đây ạ. Em đang cố gắng tìm hiểu hướng giải cũng như cách trình bày sao cho chính xác và đầy đủ nhưng hiện tại em vẫn chưa nghĩ được cách giải quyết ạ.
Nếu ai có cách hay hướng giải bài nào thì cho em xin với ạ. Em cảm ơn ạ.","Em chào mọi người ạ, em đang học môn học máy, và hiện tại em đang có mấy bài tập như hình dưới đây ạ. Em đang cố gắng tìm hiểu hướng giải cũng như cách trình bày sao cho chính xác và đầy đủ nhưng hiện tại em vẫn chưa nghĩ được cách giải quyết ạ. Nếu ai có cách hay hướng giải bài nào thì cho em xin với ạ. Em cảm ơn ạ.",,,"#Q&A, #machine_learning",,
"Các bạn vui lòng post thông tin tuyển dụng, sự kiện tháng 8/2023 vào phần comment của post này.","Các bạn vui lòng post thông tin tuyển dụng, sự kiện tháng 8/2023 vào phần comment của post này.",,,#sharing,,
"Em chào mọi người ạ. Em có thắc mắc là liệu NLP có phải là subset của ML kh mng. Tại theo em tìm hiểu thì NLP giúp máy tính có thể hiểu, xử lý dc natural language nhưng mà để đạt dc quá trình đó như sdung POS có thể giúp nhận định dc từ loại của của 1 từ như noun, verb,... thì phải dùng ML để label hay sao ạ.
Em cảm ơn mng","Em chào mọi người ạ. Em có thắc mắc là liệu NLP có phải là subset của ML kh mng. Tại theo em tìm hiểu thì NLP giúp máy tính có thể hiểu, xử lý dc natural language nhưng mà để đạt dc quá trình đó như sdung POS có thể giúp nhận định dc từ loại của của 1 từ như noun, verb,... thì phải dùng ML để label hay sao ạ. Em cảm ơn mng",,,"#Q&A, #nlp",,
"Xin chào cả nhà, team VILM đã chính thức trở lại với một model mới toanh :-P
🚀🚀🚀Obsidian-3B: Multimodal for Everyone. Được xây dựng trên mô hình NousCapyabra-3B dựa trên StableLM-3B-4e1t.
Obsidian-3B là kết quả của sự kết hợp giữa Nous Research (Mỹ) và VILM với mục tiêu đưa Multimodal đến với tất cả mọi người.
Mô hình có thể chạy trên bất kì GPU nào có VRAM 8GB trở lên.
Về kết quả benchmark: Obsidian-3B đánh bại hoặc ngang hàng LLaVA 1.5 7B của nhà Microsoft với điểm số ấn tượng trên các bài Benchmark về Vision Language
Ngoài ra team đã chính thức ra mặt Discord server để khởi động các dự án tiếp theo với cộng đồng, đặc biệt là phiên bản Vietcuna và multimodal thế hệ tiếp theo. Mong mọi người sẽ tham gia và xây dựng một cộng đồng AI Open-Source lớn mạnh của người Việt :)
Discord: https://discord.gg/uyhnuF9ncf
Model link: https://huggingface.co/NousResearch/Obsidian-3B-V0.5
Inference code: https://github.com/NousResearch/Obsidian

Chúc mọi người một buổi tối vui vẻ!","Xin chào cả nhà, team VILM đã chính thức trở lại với một model mới toanh :-P Obsidian-3B: Multimodal for Everyone. Được xây dựng trên mô hình NousCapyabra-3B dựa trên StableLM-3B-4e1t. Obsidian-3B là kết quả của sự kết hợp giữa Nous Research (Mỹ) và VILM với mục tiêu đưa Multimodal đến với tất cả mọi người. Mô hình có thể chạy trên bất kì GPU nào có VRAM 8GB trở lên. Về kết quả benchmark: Obsidian-3B đánh bại hoặc ngang hàng LLaVA 1.5 7B của nhà Microsoft với điểm số ấn tượng trên các bài Benchmark về Vision Language Ngoài ra team đã chính thức ra mặt Discord server để khởi động các dự án tiếp theo với cộng đồng, đặc biệt là phiên bản Vietcuna và multimodal thế hệ tiếp theo. Mong mọi người sẽ tham gia và xây dựng một cộng đồng AI Open-Source lớn mạnh của người Việt :) Discord: https://discord.gg/uyhnuF9ncf Model link: https://huggingface.co/NousResearch/Obsidian-3B-V0.5 Inference code: https://github.com/NousResearch/Obsidian Chúc mọi người một buổi tối vui vẻ!",,,#sharing,,
"Meta/Facebook Research gần đây công bố Cookbook cho việc train và finetune các biến thể dựa trên mô hình LLaMA tại đây https://ai.meta.com/llama/get-started/; code base tại đây https://github.com/facebookresearch/llama-recipes.
Hi vọng nó sẽ giúp ích mọi người trong công việc",Meta/Facebook Research gần đây công bố Cookbook cho việc train và finetune các biến thể dựa trên mô hình LLaMA tại đây https://ai.meta.com/llama/get-started/; code base tại đây https://github.com/facebookresearch/llama-recipes. Hi vọng nó sẽ giúp ích mọi người trong công việc,,,"#sharing, #nlp",,
"Xin chào mọi người, em đang tập dùng thử ""Mechine learning Studio"" của Azure. Đến công đoạn tạo Real time Endpoints để dùng model từ API. Hệ thống đều báo chạy ổn, tuy nhiên, mục test thử thì toàn báo lỗi ""an unexpected error occurred in scoring script. check the logs for more info"". MN ai đã fix được lỗi này giúp mình với ạ","Xin chào mọi người, em đang tập dùng thử ""Mechine learning Studio"" của Azure. Đến công đoạn tạo Real time Endpoints để dùng model từ API. Hệ thống đều báo chạy ổn, tuy nhiên, mục test thử thì toàn báo lỗi ""an unexpected error occurred in scoring script. check the logs for more info"". MN ai đã fix được lỗi này giúp mình với ạ",,,#Q&A,,
Bà con thử SFT con này xem có ổn không? Nếu ổn thì để nhóm train tiếp vài trăm GB bà con thử nốt.,Bà con thử SFT con này xem có ổn không? Nếu ổn thì để nhóm train tiếp vài trăm GB bà con thử nốt.,,,#sharing,,
"Em chào mọi người ạ, có ai gần đây train paddleocr không ạ ? cho em hỏi một chút :(( chứ em cả ngày hôm qua với nay bất lực quá. Chuyện là em có cài paddlepaddle-gpu bản 2.5.1 cho cuda 11.8, (cuda trên máy cũng đã cài 11.8 ) chạy paddle đã ổn nhưng không làm sao train được. Nó cứ vào load train như ảnh là lại không chạy tiếp nữa. em đã thử 2 bản python 3.7 và 3.10 nhưng đều như nhau. Em cũng đã giảm batchsize xuống từ 128 xuống 64, 32 rồi nhưng bị lỗi này hiện lên. Fomat data thì chắc không vấn đề, vì em đã load và train đc trên colab. Paddle cũng đã cài ổn như trên hình ạ. Em cảm ơn vì đã đọc.","Em chào mọi người ạ, có ai gần đây train paddleocr không ạ ? cho em hỏi một chút :(( chứ em cả ngày hôm qua với nay bất lực quá. Chuyện là em có cài paddlepaddle-gpu bản 2.5.1 cho cuda 11.8, (cuda trên máy cũng đã cài 11.8 ) chạy paddle đã ổn nhưng không làm sao train được. Nó cứ vào load train như ảnh là lại không chạy tiếp nữa. em đã thử 2 bản python 3.7 và 3.10 nhưng đều như nhau. Em cũng đã giảm batchsize xuống từ 128 xuống 64, 32 rồi nhưng bị lỗi này hiện lên. Fomat data thì chắc không vấn đề, vì em đã load và train đc trên colab. Paddle cũng đã cài ổn như trên hình ạ. Em cảm ơn vì đã đọc.",,,#Q&A,,
"Em chào cả nhà ạ. Mấy thời gian qua em có tự build một cái app cho phép người dùng thêm sản phẩm và tracking ngày hết hạn của sản phẩm đó. Đơn giản thì nó giống một cái todos-list mà dành cho mấy đồ thực phẩm ấy ạ. Điểm nhấn ở đây là em có sử dụng mô hình để nhận diện thực phẩm và đề xuất ngày hết hạn tương ứng. App hiện tại đã lên iOS và Android
https://apps.apple.com/vn/app/rappel-fresh-time-tracker/id6468539329
https://play.google.com/store/apps/details?id=com.tbsteam.rappel
Mong mọi người có thể tải về trải nghiệm thử và feedback đánh giá độ hiệu quả của model, và ai có đóng góp để cải thiện app nói chung thì càng tuyệt vời nữa ạ.
Chúc cả nhà cuối tuần vui vẻ ạ!! :)","Em chào cả nhà ạ. Mấy thời gian qua em có tự build một cái app cho phép người dùng thêm sản phẩm và tracking ngày hết hạn của sản phẩm đó. Đơn giản thì nó giống một cái todos-list mà dành cho mấy đồ thực phẩm ấy ạ. Điểm nhấn ở đây là em có sử dụng mô hình để nhận diện thực phẩm và đề xuất ngày hết hạn tương ứng. App hiện tại đã lên iOS và Android https://apps.apple.com/vn/app/rappel-fresh-time-tracker/id6468539329 https://play.google.com/store/apps/details?id=com.tbsteam.rappel Mong mọi người có thể tải về trải nghiệm thử và feedback đánh giá độ hiệu quả của model, và ai có đóng góp để cải thiện app nói chung thì càng tuyệt vời nữa ạ. Chúc cả nhà cuối tuần vui vẻ ạ!! :)",,,#sharing,,
"Trong một bài viết nào đó của một bạn về điền giá trị bị thiếu và ""Tuyệt đối không được điền giá trị mean hoặc zero"" và nhân tiện trong quá trình tìm tài liệu để viết khóa học mình có tìm được cuốn sách này về xử lý dữ liệu bị thiếu.
Mình chia sẻ địa chỉ cuốn sách này cho các bạn tìm hiểu thêm, và sau khi đọc xong các bạn có thể rút ra được có nên điền mean vào hay không?","Trong một bài viết nào đó của một bạn về điền giá trị bị thiếu và ""Tuyệt đối không được điền giá trị mean hoặc zero"" và nhân tiện trong quá trình tìm tài liệu để viết khóa học mình có tìm được cuốn sách này về xử lý dữ liệu bị thiếu. Mình chia sẻ địa chỉ cuốn sách này cho các bạn tìm hiểu thêm, và sau khi đọc xong các bạn có thể rút ra được có nên điền mean vào hay không?",,,"#sharing, #data",,
"Các anh/chị/bạn đã có ai làm về tra cứu ảnh tương tự dựa trên nội dung sử dụng mô hình CNN chưa ạ.Có thể cho em xin một số nguồn tham khảo được không ạ.
Xin chân thành cảm ơn mn",Các anh/chị/bạn đã có ai làm về tra cứu ảnh tương tự dựa trên nội dung sử dụng mô hình CNN chưa ạ.Có thể cho em xin một số nguồn tham khảo được không ạ. Xin chân thành cảm ơn mn,,,"#Q&A, #deep_learning",,
"Dạ em chào mọi người ạ. Em đang làm nghiên cứu về chủ đề 3D reconstruction và có gặp thuật ngữ UV mapping. Theo những gì em tìm hiểu thì có thể hiểu UV map là hình ảnh được ""đập dẹp"" của một mô hình 3D, cụ thể là gương mặt. Nếu trong 3D space, mỗi đỉnh trên 3D mesh có toạ độ là (x,y,z) thì trong không gian UV 2D thì đỉnh đó có toạ độ là (u,v). Tuy nhiên, em vẫn cảm thấy rất mơ hồ về nó, như là làm sao để visualize nó như là một hình ảnh 2D hay là về mặt toán học thì nó có thể được biễu diễn như thế nào? (ví dụ với hình ảnh màu 2D thì nó là một tensor width x height x 3 color channels). Do đó em mạn phép lên đây để nhờ các anh chị thầy cô trong group giúp em giải đáp vấn đề này ạ. Em xin chân thành cảm ơn!","Dạ em chào mọi người ạ. Em đang làm nghiên cứu về chủ đề 3D reconstruction và có gặp thuật ngữ UV mapping. Theo những gì em tìm hiểu thì có thể hiểu UV map là hình ảnh được ""đập dẹp"" của một mô hình 3D, cụ thể là gương mặt. Nếu trong 3D space, mỗi đỉnh trên 3D mesh có toạ độ là (x,y,z) thì trong không gian UV 2D thì đỉnh đó có toạ độ là (u,v). Tuy nhiên, em vẫn cảm thấy rất mơ hồ về nó, như là làm sao để visualize nó như là một hình ảnh 2D hay là về mặt toán học thì nó có thể được biễu diễn như thế nào? (ví dụ với hình ảnh màu 2D thì nó là một tensor width x height x 3 color channels). Do đó em mạn phép lên đây để nhờ các anh chị thầy cô trong group giúp em giải đáp vấn đề này ạ. Em xin chân thành cảm ơn!",,,"#Q&A, #cv",,
"Mình thấy có tutorials thú vị, đặc biệt là cho những bạn quan tâm tới Geostats của 1 giáo sư ở Đại học Texas at Autin nên chia sẻ ở đây cho những bạn cần tìm hiểu https://github.com/GeostatsGuy/PythonNumericalDemos","Mình thấy có tutorials thú vị, đặc biệt là cho những bạn quan tâm tới Geostats của 1 giáo sư ở Đại học Texas at Autin nên chia sẻ ở đây cho những bạn cần tìm hiểu https://github.com/GeostatsGuy/PythonNumericalDemos",,,#sharing,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 10/2022 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 10/2022 vào comment của post này.",,,#sharing,,
"Em chào mọi người, Cho em hỏi có cách nào dựng 3d từ ảnh độ sâu kết hợp ảnh 2d không ạ. Ảnh độ sâu và 2d em lấy từ đầu ra của camera intel d415. Mọi người có thể cho em xin tài liệu hoặc nguồn thông tin nào liên quan cũng được ạ. Em cảm ơn nhiều ạ 🥰","Em chào mọi người, Cho em hỏi có cách nào dựng 3d từ ảnh độ sâu kết hợp ảnh 2d không ạ. Ảnh độ sâu và 2d em lấy từ đầu ra của camera intel d415. Mọi người có thể cho em xin tài liệu hoặc nguồn thông tin nào liên quan cũng được ạ. Em cảm ơn nhiều ạ",,,"#Q&A, #cv",,
Cao nhân nào từng làm qua mô hình nhận diện bệnh cho lá cây cho em xin dataset với ạ. Em cảm ơn!,Cao nhân nào từng làm qua mô hình nhận diện bệnh cho lá cây cho em xin dataset với ạ. Em cảm ơn!,,,"#Q&A, #data",,
"Chào các anh chị và các bạn,
Hiện nay em đang làm đồ án môn học Xử lý âm thanh và tiếng nói. Bọn em chọn đề tài Nhận dạng tiếng nói tự động (ASR) sử dụng mô hình Conformer kết hợp với Noisy Student Training. Trong quá trình triển khai bọn em gặp một số vấn đề mong nhận được sự đóng góp và giúp đỡ đến từ mọi người:
VẤN ĐỀ VỀ VIỆC MÔ TẢ DỮ LIỆU: một trong những bộ dữ liệu bọn em sử dụng là 100h VLSP của VinAI công bố nhưng bọn em không tìm thấy mô tả bộ dữ liệu này.
VẤN ĐỀ VỀ TÀI NGUYÊN HUẤN LUYỆN: Hiện tại, với tài nguyên hạn chế và mô hình cũng khá lớn nên bọn em chỉ mới dừng lại ở mức character-level. Bọn em mong muốn huấn luyện ở mức Word-level nhưng lại gặp khó khăn trong việc thuê Colab Pro. Mọi người cho em hỏi ở nhóm mình có ai cho thuê Colab Pro thời gian ngắn không ạ ( khoảng dưới 1 tháng ).
Trên đây là những vấn đề bọn em gặp phải, mong nhận được sự giải đáp. Em xin cảm ơn ạ.","Chào các anh chị và các bạn, Hiện nay em đang làm đồ án môn học Xử lý âm thanh và tiếng nói. Bọn em chọn đề tài Nhận dạng tiếng nói tự động (ASR) sử dụng mô hình Conformer kết hợp với Noisy Student Training. Trong quá trình triển khai bọn em gặp một số vấn đề mong nhận được sự đóng góp và giúp đỡ đến từ mọi người: VẤN ĐỀ VỀ VIỆC MÔ TẢ DỮ LIỆU: một trong những bộ dữ liệu bọn em sử dụng là 100h VLSP của VinAI công bố nhưng bọn em không tìm thấy mô tả bộ dữ liệu này. VẤN ĐỀ VỀ TÀI NGUYÊN HUẤN LUYỆN: Hiện tại, với tài nguyên hạn chế và mô hình cũng khá lớn nên bọn em chỉ mới dừng lại ở mức character-level. Bọn em mong muốn huấn luyện ở mức Word-level nhưng lại gặp khó khăn trong việc thuê Colab Pro. Mọi người cho em hỏi ở nhóm mình có ai cho thuê Colab Pro thời gian ngắn không ạ ( khoảng dưới 1 tháng ). Trên đây là những vấn đề bọn em gặp phải, mong nhận được sự giải đáp. Em xin cảm ơn ạ.",,,#Q&A,,
"[English caption below]
URA-LLaMa: MÔ HÌNH NGÔN NGỮ LỚN CHO TIẾNG VIỆT
Xin chào mọi người,
Chúng tôi, nhóm nghiên cứu với các thành viên đến từ Trường Đại học Bách Khoa - ĐHQG TP.HCM và Đại học Stanford xin trân trọng giới thiệu đến cộng đồng các mô hình ngôn ngữ lớn chúng tôi đã phát triển. Chúng tôi gọi chúng với cái tên thân thuộc URA-LLaMa. Mô hình này được chúng tôi finetune trên dữ liệu tiếng Việt từ mô hình gốc LLaMa-2 của Meta với cả 3 phiên bản 7B, 13B và 70B.
Chúng tôi cung cấp miễn phí các mô hình này cho mục đích nghiên cứu. Mô hình của chúng tôi đi kèm với các kết quả đánh giá trên 10 tasks khác nhau ở nhiều khía cạnh và tình huống sử dụng trong thực tế. Bạn có thể tìm thấy thông tin về mô hình của chúng tôi tại các đường link bên dưới.
URA-LLaMa 7B: https://huggingface.co/ura-hcmut/ura-llama-7b
URA-LLaMa 13B: https://huggingface.co/ura-hcmut/ura-llama-13b
URA-LLaMa 70B: https://huggingface.co/ura-hcmut/ura-llama-70b
Giấy phép và thỏa thuận sử dụng: https://github.com/martinakaduc/ura-llama-public/blob/main/URA-LLaMa%20Model%20User%20Agreement.pdf
Playground cho URA-LLaMa 7B: https://huggingface.co/spaces/ura-hcmut/ura-llama-playground
Kết quả đánh giá của URA-LLaMa (Đang cập nhật): https://huggingface.co/spaces/ura-hcmut/ura-llama-evaluation
Nếu bạn muốn đóng góp để phát triển các mô hình ngôn ngữ lớn cho Tiếng Việt, xin đừng ngần ngại hãy liên hệ với chúng tôi theo các thông tin bên dưới.
Về nhóm nghiên cứu:
Website: https://www.ura.hcmut.edu.vn
Email: qttho dot hcmut dot edu dot vn
Về giấy phép cho các mô hình: nqduc at hcmut dot edu dot vn (CC sttruong at cs dot stanford dot edu; qttho at hcmut dot edu dot vn)
Xin cảm ơn mọi người.
10h10’, Thứ Ba, ngày 10 tháng 10 năm 2023.
Nhóm nghiên cứu
-----------------------------------------------
URA-LLaMa: LARGE LANGUAGE MODELS FOR VIETNAMESE
Hello everyone,
As a research team formed from members in Ho Chi Minh City University of Technology (HCMUT) - VNU-HCM and Stanford University, we are pleased to introduce our large language models to the community. We affectionately refer to those language models as URA-LLaMa. They are fine-tuned on Vietnamese datasets from Meta's original LLaMa-2 model, including all three versions of 7B, 13B, and 70B.
We provide these models free of charge for research purposes. Our models come with evaluation results on 10 different tasks, covering various aspects and real-world usage scenarios. You can find information about our models at the following links:
URA-LLaMa 7B: https://huggingface.co/ura-hcmut/ura-llama-7b
URA-LLaMa 13B: https://huggingface.co/ura-hcmut/ura-llama-13b
URA-LLaMa 70B: https://huggingface.co/ura-hcmut/ura-llama-70b
License and User Agreement: https://github.com/martinakaduc/ura-llama-public/blob/main/URA-LLaMa%20Model%20User%20Agreement.pdf
Playground for URA-LLaMa 7B: https://huggingface.co/spaces/ura-hcmut/ura-llama-playground
URA-LLaMa Evaluation Results (Actively updating): https://huggingface.co/spaces/ura-hcmut/ura-llama-evaluation
If you want to contribute to the development of large language models for Vietnamese, please do not hesitate to contact us using the information below.
About the research group:
Website: https://www.ura.hcmut.edu.vn
Email: qttho dot hcmut dot edu dot vn
About the model licenses: nqduc at hcmut dot edu dot vn (CC sttruong at cs dot stanford dot edu; qttho at hcmut dot edu dot vn)
Thank you all.
10:10 AM, Tuesday, October 10, 2023.
Research Team","[English caption below] URA-LLaMa: MÔ HÌNH NGÔN NGỮ LỚN CHO TIẾNG VIỆT Xin chào mọi người, Chúng tôi, nhóm nghiên cứu với các thành viên đến từ Trường Đại học Bách Khoa - ĐHQG TP.HCM và Đại học Stanford xin trân trọng giới thiệu đến cộng đồng các mô hình ngôn ngữ lớn chúng tôi đã phát triển. Chúng tôi gọi chúng với cái tên thân thuộc URA-LLaMa. Mô hình này được chúng tôi finetune trên dữ liệu tiếng Việt từ mô hình gốc LLaMa-2 của Meta với cả 3 phiên bản 7B, 13B và 70B. Chúng tôi cung cấp miễn phí các mô hình này cho mục đích nghiên cứu. Mô hình của chúng tôi đi kèm với các kết quả đánh giá trên 10 tasks khác nhau ở nhiều khía cạnh và tình huống sử dụng trong thực tế. Bạn có thể tìm thấy thông tin về mô hình của chúng tôi tại các đường link bên dưới. URA-LLaMa 7B: https://huggingface.co/ura-hcmut/ura-llama-7b URA-LLaMa 13B: https://huggingface.co/ura-hcmut/ura-llama-13b URA-LLaMa 70B: https://huggingface.co/ura-hcmut/ura-llama-70b Giấy phép và thỏa thuận sử dụng: https://github.com/martinakaduc/ura-llama-public/blob/main/URA-LLaMa%20Model%20User%20Agreement.pdf Playground cho URA-LLaMa 7B: https://huggingface.co/spaces/ura-hcmut/ura-llama-playground Kết quả đánh giá của URA-LLaMa (Đang cập nhật): https://huggingface.co/spaces/ura-hcmut/ura-llama-evaluation Nếu bạn muốn đóng góp để phát triển các mô hình ngôn ngữ lớn cho Tiếng Việt, xin đừng ngần ngại hãy liên hệ với chúng tôi theo các thông tin bên dưới. Về nhóm nghiên cứu: Website: https://www.ura.hcmut.edu.vn Email: qttho dot hcmut dot edu dot vn Về giấy phép cho các mô hình: nqduc at hcmut dot edu dot vn (CC sttruong at cs dot stanford dot edu; qttho at hcmut dot edu dot vn) Xin cảm ơn mọi người. 10h10’, Thứ Ba, ngày 10 tháng 10 năm 2023. Nhóm nghiên cứu ----------------------------------------------- URA-LLaMa: LARGE LANGUAGE MODELS FOR VIETNAMESE Hello everyone, As a research team formed from members in Ho Chi Minh City University of Technology (HCMUT) - VNU-HCM and Stanford University, we are pleased to introduce our large language models to the community. We affectionately refer to those language models as URA-LLaMa. They are fine-tuned on Vietnamese datasets from Meta's original LLaMa-2 model, including all three versions of 7B, 13B, and 70B. We provide these models free of charge for research purposes. Our models come with evaluation results on 10 different tasks, covering various aspects and real-world usage scenarios. You can find information about our models at the following links: URA-LLaMa 7B: https://huggingface.co/ura-hcmut/ura-llama-7b URA-LLaMa 13B: https://huggingface.co/ura-hcmut/ura-llama-13b URA-LLaMa 70B: https://huggingface.co/ura-hcmut/ura-llama-70b License and User Agreement: https://github.com/martinakaduc/ura-llama-public/blob/main/URA-LLaMa%20Model%20User%20Agreement.pdf Playground for URA-LLaMa 7B: https://huggingface.co/spaces/ura-hcmut/ura-llama-playground URA-LLaMa Evaluation Results (Actively updating): https://huggingface.co/spaces/ura-hcmut/ura-llama-evaluation If you want to contribute to the development of large language models for Vietnamese, please do not hesitate to contact us using the information below. About the research group: Website: https://www.ura.hcmut.edu.vn Email: qttho dot hcmut dot edu dot vn About the model licenses: nqduc at hcmut dot edu dot vn (CC sttruong at cs dot stanford dot edu; qttho at hcmut dot edu dot vn) Thank you all. 10:10 AM, Tuesday, October 10, 2023. Research Team",,,"#sharing, #nlp",,
"Chào mọi người, em đang tìm hiểu về LLMs và cụ thể là OpenAI API. Em có 1 số thắc mắc mong mọi người giải đáp.
Những model DALL-E và Whisper có thực sự là Large Language Models không ạ, tại bữa em có đọc lướt qua 1 ?
Khi dùng openai thì em thấy có 2 phương thức khá tương tự là Completion và ChatCompletion. trong khi ChatCompletion có thể gửi lịch sử hội thoại thì không biết Completion có ưu điểm gì mà vẫn được giữ lại?
Cách tính phí
Ví dụ: babbage-002 $0.0004 / 1K tokens thì là token vào hay token ra hay tổng ạ.
Khi fine-turning, sử dụng model đó thì tính phí sử dụng có + thêm phí model gốc không hay chỉ tính phí sử dụng và tính phí đầu vào hay đầu ra hay cả 2
- có trang này tổng hợp đầy đủ model và phí hiện đang có hơn trang https://openai.com/pricing không ạ
Giới hạn
- Giới hạn ở trang https://platform.openai.com/docs/models MAX TOKENS là token nhập vào phải không ạ. Nếu vậy có limit cho token trả về không?
- Giới hạn gửi request trong khoảng thời gian: liệu có cách nào để xin tăng giới hạn (cho tài khoản miễn phí) không ạ? vì gửi theo biểu mẫu dành cho tài khoản trả phí.
Ngoài ra, không biết có chính sách (hoặc trick nào) cho sinh viên để có thể trải nghiệm các tính năng paid plan (ví dụ như fine-tuning) của OpenAI API không ạ?","Chào mọi người, em đang tìm hiểu về LLMs và cụ thể là OpenAI API. Em có 1 số thắc mắc mong mọi người giải đáp. Những model DALL-E và Whisper có thực sự là Large Language Models không ạ, tại bữa em có đọc lướt qua 1 ? Khi dùng openai thì em thấy có 2 phương thức khá tương tự là Completion và ChatCompletion. trong khi ChatCompletion có thể gửi lịch sử hội thoại thì không biết Completion có ưu điểm gì mà vẫn được giữ lại? Cách tính phí Ví dụ: babbage-002 $0.0004 / 1K tokens thì là token vào hay token ra hay tổng ạ. Khi fine-turning, sử dụng model đó thì tính phí sử dụng có + thêm phí model gốc không hay chỉ tính phí sử dụng và tính phí đầu vào hay đầu ra hay cả 2 - có trang này tổng hợp đầy đủ model và phí hiện đang có hơn trang https://openai.com/pricing không ạ Giới hạn - Giới hạn ở trang https://platform.openai.com/docs/models MAX TOKENS là token nhập vào phải không ạ. Nếu vậy có limit cho token trả về không? - Giới hạn gửi request trong khoảng thời gian: liệu có cách nào để xin tăng giới hạn (cho tài khoản miễn phí) không ạ? vì gửi theo biểu mẫu dành cho tài khoản trả phí. Ngoài ra, không biết có chính sách (hoặc trick nào) cho sinh viên để có thể trải nghiệm các tính năng paid plan (ví dụ như fine-tuning) của OpenAI API không ạ?",,,"#Q&A, #nlp",,
Chào mọi người. Mình đang cần nộp 1 bài tập lớn về đề tài MachineLearning bất cứ thứ gì cũng được. Vậy bạn nào có thể chia sẻ cho mình 1 đề tài nào đó cơ bản nhất có thể có sẵn cả báo cáo và source code với không ạ? Mình xin cảm ơn và hậu tạ ạ,Chào mọi người. Mình đang cần nộp 1 bài tập lớn về đề tài MachineLearning bất cứ thứ gì cũng được. Vậy bạn nào có thể chia sẻ cho mình 1 đề tài nào đó cơ bản nhất có thể có sẵn cả báo cáo và source code với không ạ? Mình xin cảm ơn và hậu tạ ạ,,,#Q&A,,
Mọi người ai làm về trích xuất thông tin trên căn cước công có chip chưa cho e hỏi cái này với ạ?,Mọi người ai làm về trích xuất thông tin trên căn cước công có chip chưa cho e hỏi cái này với ạ?,,,"#Q&A, #cv",,
"Như thread cách đây chưa lâu (tại đây https://www.facebook.com/groups/machinelearningcoban/permalink/1778526872604712/) rằng Mojo có thể sẽ có chỗ đứng của riêng nó trong thời đại ứng dụng AI trên các nền tảng tính toán hiệu năng cao . Nay Mojo mới cho cài Native trên Mac chip M. Hướng dẫn cài đặt tại đây: https://developer.modular.com/download.
Mình có test nhanh trên máy Mac M1 của mình với LLaMA2 (train với TinyStory từ Karpathy) và TinyLLaMA2 từ (https://huggingface.co/kirp/TinyLlama-1.1B-Chat-v0.2-bin/resolve/main/tl-chat.bin). Dưới đây là bản tóm tắt kết quả so sánh inference speed giữa Mojo và C. Cơ bản là không tệ và tốt hơn kết quả trước đó mình test trên server Linux (xem comments ở thread trước đây, dường dẫn ở trên). Mình sẽ test thêm kĩ hơn trong những ngày tới rồi chia sẻ với các bạn sau!
source code cho LLaMA2.mojo tại đây https://github.com/tairov/llama2.mojo
và tổng hợp các source code/thư viện thú vị viết cho mojo tại đây https://github.com/mojicians/awesome-mojo","Như thread cách đây chưa lâu (tại đây https://www.facebook.com/groups/machinelearningcoban/permalink/1778526872604712/) rằng Mojo có thể sẽ có chỗ đứng của riêng nó trong thời đại ứng dụng AI trên các nền tảng tính toán hiệu năng cao . Nay Mojo mới cho cài Native trên Mac chip M. Hướng dẫn cài đặt tại đây: https://developer.modular.com/download. Mình có test nhanh trên máy Mac M1 của mình với LLaMA2 (train với TinyStory từ Karpathy) và TinyLLaMA2 từ (https://huggingface.co/kirp/TinyLlama-1.1B-Chat-v0.2-bin/resolve/main/tl-chat.bin). Dưới đây là bản tóm tắt kết quả so sánh inference speed giữa Mojo và C. Cơ bản là không tệ và tốt hơn kết quả trước đó mình test trên server Linux (xem comments ở thread trước đây, dường dẫn ở trên). Mình sẽ test thêm kĩ hơn trong những ngày tới rồi chia sẻ với các bạn sau! source code cho LLaMA2.mojo tại đây https://github.com/tairov/llama2.mojo và tổng hợp các source code/thư viện thú vị viết cho mojo tại đây https://github.com/mojicians/awesome-mojo",,,#sharing,,
"Xin chào tất cả anh chị. Em là sv năm nhât ạ và sắp tới em phải bảo vệ đồ án ý tưởng sản phẩm CNTT. ý tưởng khá hay có ứng dụng AI các kiểu. Cụ thể: 1 app tích hợp AI gợi ý thực đơn cho người dùng dựa trên những dữ liệu của họ( bao gồm dữ liệu cố định và dữ liệu được ghi lại theo thời gian thực bằng thiết bị theo dõi SK). Và AI đã được huấn luyện để tìm ra những món ăn có thực phẩm, gia vị phù hợp dinh dưỡng ( thậm chí khẩu vị ) với người dùng, rồi đưa ra gợi ý để họ chọn và đăt hàng.
Dưới hình là liệt kê các tiêu chí ạ, từ tiêu chí ấy anh chị có thể tư vấn sâu thêm chút về kĩ thuật huấn luyện cho con AI này ( công đoạn, chiến lược, phân tích, chọn các thuật toán, mô hình hóa dữ liệu,...)
Vì em chỉ biết chút bề nổi về tiềm năng ứng dụng của AI thôi ạ. Nên em cần anh chị tư vấn giúp em để đào sâu hơn chút về kĩ thuật nha !!!
Chiyyso06.5 ♠️","Xin chào tất cả anh chị. Em là sv năm nhât ạ và sắp tới em phải bảo vệ đồ án ý tưởng sản phẩm CNTT. ý tưởng khá hay có ứng dụng AI các kiểu. Cụ thể: 1 app tích hợp AI gợi ý thực đơn cho người dùng dựa trên những dữ liệu của họ( bao gồm dữ liệu cố định và dữ liệu được ghi lại theo thời gian thực bằng thiết bị theo dõi SK). Và AI đã được huấn luyện để tìm ra những món ăn có thực phẩm, gia vị phù hợp dinh dưỡng ( thậm chí khẩu vị ) với người dùng, rồi đưa ra gợi ý để họ chọn và đăt hàng. Dưới hình là liệt kê các tiêu chí ạ, từ tiêu chí ấy anh chị có thể tư vấn sâu thêm chút về kĩ thuật huấn luyện cho con AI này ( công đoạn, chiến lược, phân tích, chọn các thuật toán, mô hình hóa dữ liệu,...) Vì em chỉ biết chút bề nổi về tiềm năng ứng dụng của AI thôi ạ. Nên em cần anh chị tư vấn giúp em để đào sâu hơn chút về kĩ thuật nha !!! Chiyyso06.5",,,#Q&A,,
"Mn cho em hỏi điểm giống và khác nhau (so sánh) của decision tree, k-NN, naive bayes, linear regression với ạ","Mn cho em hỏi điểm giống và khác nhau (so sánh) của decision tree, k-NN, naive bayes, linear regression với ạ",,,"#Q&A, #machine_learning",,
"Hello mọi người! Hiện tại e có kiến thức cơ bản về data science và stats/probability. Trước em dùng R để làm 1 số project và học qua cuốn ISLR và hiện tại đang bắt đầu học Python.
Google mới released khoá học về advanced data analytics, e chỉ biết khác với khoá trước là thay vì dùng R thì khoá này dùng Python. A/C nào đã học qua hoặc biết về khoá học này cho e xin reviews với ạ. Thank you","Hello mọi người! Hiện tại e có kiến thức cơ bản về data science và stats/probability. Trước em dùng R để làm 1 số project và học qua cuốn ISLR và hiện tại đang bắt đầu học Python. Google mới released khoá học về advanced data analytics, e chỉ biết khác với khoá trước là thay vì dùng R thì khoá này dùng Python. A/C nào đã học qua hoặc biết về khoá học này cho e xin reviews với ạ. Thank you",,,#Q&A,,
"[Góc tư vấn]
Anh chị em nào đã và đang học Master ở ĐH Bách Khoa ngành Data Science rồi cho em xin review với ạ.
Em phân vân học BK hoặc học từ xa 1 số trường nc ngoài (so sánh học phí, chất lượng giangr dạy, bằng cấp,…).
Rất mong được các anh chị đi trc có kinh nghiệm chỉ dạy ạ. Em cảm ơn ạ.","[Góc tư vấn] Anh chị em nào đã và đang học Master ở ĐH Bách Khoa ngành Data Science rồi cho em xin review với ạ. Em phân vân học BK hoặc học từ xa 1 số trường nc ngoài (so sánh học phí, chất lượng giangr dạy, bằng cấp,…). Rất mong được các anh chị đi trc có kinh nghiệm chỉ dạy ạ. Em cảm ơn ạ.",,,#Q&A,,
"Chào các anh chị!
Trong học kì tới em sẽ bắt đầu học các môn chuyên ngành AI và
Em đã học qua các khóa ML, DL trên Coursera và đã hiểu những concept, toán học cơ bản về lĩnh vực này.
Em rất mong muốn thực tập sớm để tích lũy các kinh nghiệm thực tế trong môi trường doanh nghiệp nhưng hiện tại em thấy các công việc này đang tuyển ở TPHCM khá ít nên em hơi hoang mang ạ.
Em muốn hỏi các anh/ chị em nên trau dồi thêm những gì để có đủ kỹ năng để có được vị trí thực tập ở các vị trí AI Engineer hoặc Data Science ạ.
Rất mong nhận được sự góp ý từ mọi người. Chúc mọi người một ngày vui vẻ ạ!","Chào các anh chị! Trong học kì tới em sẽ bắt đầu học các môn chuyên ngành AI và Em đã học qua các khóa ML, DL trên Coursera và đã hiểu những concept, toán học cơ bản về lĩnh vực này. Em rất mong muốn thực tập sớm để tích lũy các kinh nghiệm thực tế trong môi trường doanh nghiệp nhưng hiện tại em thấy các công việc này đang tuyển ở TPHCM khá ít nên em hơi hoang mang ạ. Em muốn hỏi các anh/ chị em nên trau dồi thêm những gì để có đủ kỹ năng để có được vị trí thực tập ở các vị trí AI Engineer hoặc Data Science ạ. Rất mong nhận được sự góp ý từ mọi người. Chúc mọi người một ngày vui vẻ ạ!",,,#Q&A,,
"Xin chào các bác. Chả là lâu nay em có đọc tin bài về AI trên Medium. com nhưng dạo gần dây em vào rất khó, quay đều đều mà không vào được.
Em muốn xin hỏi các bác xem còn có trang nào tương tự như trang này để vào đọc và cập nhật các bài liên quan đến AI không ạ?
Em cảm ơn các bác nhiều!","Xin chào các bác. Chả là lâu nay em có đọc tin bài về AI trên Medium. com nhưng dạo gần dây em vào rất khó, quay đều đều mà không vào được. Em muốn xin hỏi các bác xem còn có trang nào tương tự như trang này để vào đọc và cập nhật các bài liên quan đến AI không ạ? Em cảm ơn các bác nhiều!",,,#Q&A,,
"Hiện tại Em đang làm 1 model image classfication về các loài côn trùng. Nhưng gặp phải vấn đề là khi em train 200 class thì bình thường. Nhưng khi tăng lên 300 thì model giảm độ chính xác, và khi nhận dạng thực tế thì cũng giảm và score giảm còn rất thấp mặc dù nhận dạng vẫn có cái đúng. Em đoán là có thể là do các class côn trùng nhiều loài rất giống nhau thậm chí giống hệt. Nên ảnh hưởng. Em cũng có suy nghĩ là gom nhóm các loài giống nhau vào kiểu như sub class. Nhưng không biết như thế nào. Em hi vọng được nghe chia sẻ của các anh chị về cách giải quyết vấn đề này. Em cảm ơn rất nhiều.","Hiện tại Em đang làm 1 model image classfication về các loài côn trùng. Nhưng gặp phải vấn đề là khi em train 200 class thì bình thường. Nhưng khi tăng lên 300 thì model giảm độ chính xác, và khi nhận dạng thực tế thì cũng giảm và score giảm còn rất thấp mặc dù nhận dạng vẫn có cái đúng. Em đoán là có thể là do các class côn trùng nhiều loài rất giống nhau thậm chí giống hệt. Nên ảnh hưởng. Em cũng có suy nghĩ là gom nhóm các loài giống nhau vào kiểu như sub class. Nhưng không biết như thế nào. Em hi vọng được nghe chia sẻ của các anh chị về cách giải quyết vấn đề này. Em cảm ơn rất nhiều.",,,"#Q&A, #cv",,
"Đã có người đầu tiên port thư viện rất hay và nổi tiếng của Karpathy có tên là llama.c https://github.com/karpathy/llama2.c sang Mojo tại đây https://github.com/tairov/llama2.mojo
Mojo đã tăng hiệu suất của Python lên gần 250 lần. Thật ấn tượng với phiên bản Mojo giờ đây vượt trội hơn llama2.c khoảng 15-20%. Một con số cực kì ấn tượng trong thí nghiệm ban đầu này.
Điều này cho thấy tiềm năng của việc tối ưu phần cứng thông qua các tính năng nâng cao của Mojo. Tôi nghĩ điều này bước đầu cho ta thấy được ấn tượng về hiệu năng của Mojo trên các mô hình ngôn ngữ lớn, nơi đòi hỏi tài nguyên tính toán rất lớn. Nên cải thiện được x% về hiệu năng cũng là rất đáng quí.
Chúc các bạn có trải nghiệm vui vẻ với những thứ mới lạ!
Ps. Khi tôi post bài trước đó về việc Modular cho cài đặt Mojo trên local machines (trước đó chỉ chạy online trên servers chủ của cty), nhiều bạn tỏ ý nghi ngờ. Tôi có nói, hãy bình tĩnh chờ đợi và hãy thử trải nghiệm với Mojo theo cách của bạn, trước khi có những phát biểu cảm tính. Tôi hiểu đây là một phần tính cách của không ít người Việt. Xin lỗi phải nói ra việc đụng chạm đáng buồn này. Rất lấy làm tiếc
https://github.com/tairov/llama2.mojo/blob/master/assets/llama2.mojo-demo.gif","Đã có người đầu tiên port thư viện rất hay và nổi tiếng của Karpathy có tên là llama.c https://github.com/karpathy/llama2.c sang Mojo tại đây https://github.com/tairov/llama2.mojo Mojo đã tăng hiệu suất của Python lên gần 250 lần. Thật ấn tượng với phiên bản Mojo giờ đây vượt trội hơn llama2.c khoảng 15-20%. Một con số cực kì ấn tượng trong thí nghiệm ban đầu này. Điều này cho thấy tiềm năng của việc tối ưu phần cứng thông qua các tính năng nâng cao của Mojo. Tôi nghĩ điều này bước đầu cho ta thấy được ấn tượng về hiệu năng của Mojo trên các mô hình ngôn ngữ lớn, nơi đòi hỏi tài nguyên tính toán rất lớn. Nên cải thiện được x% về hiệu năng cũng là rất đáng quí. Chúc các bạn có trải nghiệm vui vẻ với những thứ mới lạ! Ps. Khi tôi post bài trước đó về việc Modular cho cài đặt Mojo trên local machines (trước đó chỉ chạy online trên servers chủ của cty), nhiều bạn tỏ ý nghi ngờ. Tôi có nói, hãy bình tĩnh chờ đợi và hãy thử trải nghiệm với Mojo theo cách của bạn, trước khi có những phát biểu cảm tính. Tôi hiểu đây là một phần tính cách của không ít người Việt. Xin lỗi phải nói ra việc đụng chạm đáng buồn này. Rất lấy làm tiếc https://github.com/tairov/llama2.mojo/blob/master/assets/llama2.mojo-demo.gif",,,#sharing,,
"DopikAI vừa công bố bài tóm tắt về ViGPT®, mô hình LLM tiếng Việt dựa trên instruction-fintuning với các nguồn dữ liệu tự thu thập, translate từ tiếng anh cũng như tự sinh với ChatGPT. 
Mô hình tập trung vào tác vụ hỏi đáp, đánh giá tính tự nhiên và tính đúng đắn câu trả lời được sinh ra để đảm bảo performance trên nhiều domain khác nhau. 
Nghiên cứu của nhóm đã được accept tại EMNLP 2023 (Industry Track).
Đọc ngay bài tóm tắt về ViGPT® tại: https://dopikai.com/files/Dopikai_ViGPT.pdf 
Đăng ký tham gia DopikAI's organization, để thử nghiệm các version của ViGPT®: https://huggingface.co/dopikai
Tham gia ngay DopikAI’s LLM Challenge để so sánh kết quả trên benchmark dataset với ViGPT®: https://aihub.vn/competitions/596 ","DopikAI vừa công bố bài tóm tắt về ViGPT®, mô hình LLM tiếng Việt dựa trên instruction-fintuning với các nguồn dữ liệu tự thu thập, translate từ tiếng anh cũng như tự sinh với ChatGPT. Mô hình tập trung vào tác vụ hỏi đáp, đánh giá tính tự nhiên và tính đúng đắn câu trả lời được sinh ra để đảm bảo performance trên nhiều domain khác nhau. Nghiên cứu của nhóm đã được accept tại EMNLP 2023 (Industry Track). Đọc ngay bài tóm tắt về ViGPT® tại: https://dopikai.com/files/Dopikai_ViGPT.pdf Đăng ký tham gia DopikAI's organization, để thử nghiệm các version của ViGPT®: https://huggingface.co/dopikai Tham gia ngay DopikAI’s LLM Challenge để so sánh kết quả trên benchmark dataset với ViGPT®: https://aihub.vn/competitions/596",,,"#sharing, #nlp",,
"Chào mọi người, em đang tìm hiểu về Explainable AI thì có cái từ ""agnostic"" là em không hiểu lắm. Nếu dịch trực tiếp ra thì là ""bất khả tri"", nhưng em thấy nó chưa thỏa đáng. Mọi người có ai đã từng tìm hiểu giải nghĩa giúp em với ạ.","Chào mọi người, em đang tìm hiểu về Explainable AI thì có cái từ ""agnostic"" là em không hiểu lắm. Nếu dịch trực tiếp ra thì là ""bất khả tri"", nhưng em thấy nó chưa thỏa đáng. Mọi người có ai đã từng tìm hiểu giải nghĩa giúp em với ạ.",,,#Q&A,,
"Em chào mng ạ.
Mng có thể chia sẻ cho em lộ trình để học xây dựng 1 AI-Chatbot (có kèm theo khóa Udemy càng tốt ạ).
Em cảm ơn mng nhìu",Em chào mng ạ. Mng có thể chia sẻ cho em lộ trình để học xây dựng 1 AI-Chatbot (có kèm theo khóa Udemy càng tốt ạ). Em cảm ơn mng nhìu,,,"#Q&A, #nlp",,
Xin được chia sẻ với mọi người video giải thích paper Segment Anything bằng tiếng Việt ạ. Rất mong nhận được góp ý từ mọi người ạ,Xin được chia sẻ với mọi người video giải thích paper Segment Anything bằng tiếng Việt ạ. Rất mong nhận được góp ý từ mọi người ạ,,,"#sharing, #cv",,
"Em tạo một mạng neural 2 lớp ẩn với hàm kích hoạt cho 2 lớp ẩn là ReLU và hàm đầu ra là hàm dự đoán sofmax. Em có test thử nhiều lần thì hầu như hàm loss giảm rất nhanh. Nhưng một số lần hàm loss không giảm. Em không biết là do em sai ở đâu. hay là do đặc tính của hàm ReLU.
Mà em cũng thử dùng hàm sigmoid làm hàm kích hoạt. Tuy loss giảm lâu hơn nhưng em thấy nó ổn định hơn hàm ReLU.",Em tạo một mạng neural 2 lớp ẩn với hàm kích hoạt cho 2 lớp ẩn là ReLU và hàm đầu ra là hàm dự đoán sofmax. Em có test thử nhiều lần thì hầu như hàm loss giảm rất nhanh. Nhưng một số lần hàm loss không giảm. Em không biết là do em sai ở đâu. hay là do đặc tính của hàm ReLU. Mà em cũng thử dùng hàm sigmoid làm hàm kích hoạt. Tuy loss giảm lâu hơn nhưng em thấy nó ổn định hơn hàm ReLU.,,,"#Q&A, #deep_learning",,
"Trong bài viết này, chúng ta tìm hiểu về một số lĩnh vực ứng dụng chính và cách AI đang chuyển đổi lĩnh vực nghiên cứu gen, đã dẫn đến những đột phá nhanh chóng trong lĩnh vực y tế và khám phá thuốc.
#AI #Healthcare #Genomics","Trong bài viết này, chúng ta tìm hiểu về một số lĩnh vực ứng dụng chính và cách AI đang chuyển đổi lĩnh vực nghiên cứu gen, đã dẫn đến những đột phá nhanh chóng trong lĩnh vực y tế và khám phá thuốc.",#AI	#Healthcare	#Genomics,,#sharing,,
"MỜI THAM GIA KALAPA BYTEBATTLES 2023
Hi mọi người. Mình là Cương, Project Manager tại KALAPA - một startup trong lĩnh vực công nghệ và Trí tuệ nhân tạo. Tiếp nối thành công của KALAPA Credit Scoring Challenge 2020 với hơn 800 người tham gia, năm nay công ty mình, dưới sự bảo trợ của Hội Tin học Việt Nam, tiếp tục tổ chức một cuộc thi AI có tên gọi KALAPA BYTEBATTLES. Cuộc thi năm nay gồm 2 bài toán, trong đó có một bài toán chưa từng xuất hiện ở các cuộc thi khác: Vietnamese Medical Multiple-choice Question Answering, hứa hẹn mang lại nhiều thử thách hấp dẫn và nhiều đóng góp mới cho cộng đồng làm AI ở Việt Nam.
Ngoài ra bọn mình cũng sẽ tổ chức trận chung kết dưới hình thức đối kháng 1vs1 giữa model của các đội, hy vọng sẽ mang lại luồng gió mới giữa các cuộc thi được tổ chức hiện nay.
Thông tin về cuộc thi có tại: https://challenge.kalapa.vn/
 — với Thu Thuỷ và Đan Thy.","MỜI THAM GIA KALAPA BYTEBATTLES 2023 Hi mọi người. Mình là Cương, Project Manager tại KALAPA - một startup trong lĩnh vực công nghệ và Trí tuệ nhân tạo. Tiếp nối thành công của KALAPA Credit Scoring Challenge 2020 với hơn 800 người tham gia, năm nay công ty mình, dưới sự bảo trợ của Hội Tin học Việt Nam, tiếp tục tổ chức một cuộc thi AI có tên gọi KALAPA BYTEBATTLES. Cuộc thi năm nay gồm 2 bài toán, trong đó có một bài toán chưa từng xuất hiện ở các cuộc thi khác: Vietnamese Medical Multiple-choice Question Answering, hứa hẹn mang lại nhiều thử thách hấp dẫn và nhiều đóng góp mới cho cộng đồng làm AI ở Việt Nam. Ngoài ra bọn mình cũng sẽ tổ chức trận chung kết dưới hình thức đối kháng 1vs1 giữa model của các đội, hy vọng sẽ mang lại luồng gió mới giữa các cuộc thi được tổ chức hiện nay. Thông tin về cuộc thi có tại: https://challenge.kalapa.vn/ — với Thu Thuỷ và Đan Thy.",,,#sharing,,
"Mn cho em tham khảo ý kiến về topic clustering text với ạ.
E định sử dụng tf-idf với clustering algorithm (k-means, dbscan,..) để làm case này. Nhưng data text nó khá ngắn (1 row trung bình chỉ 6 7 từ), và viết sai chính tả với viết không dấu cũng có nên mn có suggest j cho việc preprocess đống này hong ạ (e tính unidecode nó hết lun xong traceback lại cái ban đầu).
Mong nhận được góp ý của mọi người ạ, cảm ơn mn nhìu, e cx mới mò về NLP nên mn thông cảm.","Mn cho em tham khảo ý kiến về topic clustering text với ạ. E định sử dụng tf-idf với clustering algorithm (k-means, dbscan,..) để làm case này. Nhưng data text nó khá ngắn (1 row trung bình chỉ 6 7 từ), và viết sai chính tả với viết không dấu cũng có nên mn có suggest j cho việc preprocess đống này hong ạ (e tính unidecode nó hết lun xong traceback lại cái ban đầu). Mong nhận được góp ý của mọi người ạ, cảm ơn mn nhìu, e cx mới mò về NLP nên mn thông cảm.",,,"#Q&A, #nlp",,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2022 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2022 vào comment của post này.",,,#sharing,,
"Hi mọi người, hiện tại em đang làm project dự đoán kết quả thí nghiệm vật liệu xây dựng bao gồm nhiều models khác nhau nhằm đánh giá nhiều khía cạnh của thí nghiệm. Em đang sử dụng RestAPI để call artifact từ MLFlow sau đó predict xuất ra kết quả, bên cạnh đó có chạy đồng thời tính SHAP values để giải thích model.
1. Em đang gặp vấn đề trả kết quả khá lâu vì input đầu vào khoảng 1000 data ( mất tầm 3 phút), em muốn hỏi có cách nào tối ưu để giúp mô hình predict nhanh hơn không ạ ?
2. Theo em biết, các model từ library sklearn không sử dụng GPU, nên kết quả trả ra là tuần tự, liệu có cách nào chạy tất cả kết quả cùng 1 lúc ?
Em gửi cấu hình hiện tại của máy em ạ, em cảm ơn👩‍💻","Hi mọi người, hiện tại em đang làm project dự đoán kết quả thí nghiệm vật liệu xây dựng bao gồm nhiều models khác nhau nhằm đánh giá nhiều khía cạnh của thí nghiệm. Em đang sử dụng RestAPI để call artifact từ MLFlow sau đó predict xuất ra kết quả, bên cạnh đó có chạy đồng thời tính SHAP values để giải thích model. 1. Em đang gặp vấn đề trả kết quả khá lâu vì input đầu vào khoảng 1000 data ( mất tầm 3 phút), em muốn hỏi có cách nào tối ưu để giúp mô hình predict nhanh hơn không ạ ? 2. Theo em biết, các model từ library sklearn không sử dụng GPU, nên kết quả trả ra là tuần tự, liệu có cách nào chạy tất cả kết quả cùng 1 lúc ? Em gửi cấu hình hiện tại của máy em ạ, em cảm ơn",,,"#Q&A, #python",,
"Mọi người có ai làm về cái nhận diện xem người tham gia giao thông có đội mũ bảo hiểm rồi trích xuất biển số chưa, nếu rồi thì có thể cho e xin link tham khảo được không ạ?","Mọi người có ai làm về cái nhận diện xem người tham gia giao thông có đội mũ bảo hiểm rồi trích xuất biển số chưa, nếu rồi thì có thể cho e xin link tham khảo được không ạ?",,,"#Q&A, #cv",,
"DopikAI vừa public DPKLLM Benchmark - Bộ benchmark dataset dành riêng cho LLM tiếng Việt, dưới dạng một challenge tổ chức trên aihub. DPKLLM tiến hành đánh giá trên nhiều tập dataset với nhiều tác vụ khác nhau

ViLaw-QA: Tập trung vào vấn đề hỏi đáp trên miền dữ liệu về luật pháp Việt Nam
VitruthfulQA: Tập trung đánh giá tính trung thực của các câu trả lời được sinh bởi LLM (tương tự TruthfulQA nhưng dành cho tiếng Việt)
Các tập dữ liệu chuyên về hỏi đáp của các bên khác như ViWikiQA, ViCoQA, ViNewsQA, ...
Ngoài ra, challenge cũng xem xét đánh giá tác vụ NER với tiếng Việt của LLM 

Đây là cơ hội để các cá nhân, tổ chức đang phát triển LLM có thể tham gia đánh giá và so sánh kết quả của mình với các bên khác cũng như trao đổi và học hỏi lẫn nhau. Challenge kéo dài vô thời hạn, và mọi người có thể dễ dàng đăng ký tham gia cũng như submit kết quả lên hệ thống. ","DopikAI vừa public DPKLLM Benchmark - Bộ benchmark dataset dành riêng cho LLM tiếng Việt, dưới dạng một challenge tổ chức trên aihub. DPKLLM tiến hành đánh giá trên nhiều tập dataset với nhiều tác vụ khác nhau ViLaw-QA: Tập trung vào vấn đề hỏi đáp trên miền dữ liệu về luật pháp Việt Nam VitruthfulQA: Tập trung đánh giá tính trung thực của các câu trả lời được sinh bởi LLM (tương tự TruthfulQA nhưng dành cho tiếng Việt) Các tập dữ liệu chuyên về hỏi đáp của các bên khác như ViWikiQA, ViCoQA, ViNewsQA, ... Ngoài ra, challenge cũng xem xét đánh giá tác vụ NER với tiếng Việt của LLM Đây là cơ hội để các cá nhân, tổ chức đang phát triển LLM có thể tham gia đánh giá và so sánh kết quả của mình với các bên khác cũng như trao đổi và học hỏi lẫn nhau. Challenge kéo dài vô thời hạn, và mọi người có thể dễ dàng đăng ký tham gia cũng như submit kết quả lên hệ thống.",,,"#sharing, #data",,
"Tuy không liên quan tới ML/DL/AI, nhưng thống kê luôn có vai trò rất quan trọng trong Khoa học dữ liệu. Dưới đây là thông tin về lớp học do GS Richard McElreath đang giảng bài về cuốn sách của GS có tên""Statistical Rethinking (2023 Edition)"" theo trường phái thống kê Bayesian trong 10 tuần. Các bạn có thể theo dõi tại đây https://github.com/rmcelreath/stat_rethinking_2023. Tại địa chỉ GitHub này, bài giảng ghi hình và upload 2 lần/tuần.","Tuy không liên quan tới ML/DL/AI, nhưng thống kê luôn có vai trò rất quan trọng trong Khoa học dữ liệu. Dưới đây là thông tin về lớp học do GS Richard McElreath đang giảng bài về cuốn sách của GS có tên""Statistical Rethinking (2023 Edition)"" theo trường phái thống kê Bayesian trong 10 tuần. Các bạn có thể theo dõi tại đây https://github.com/rmcelreath/stat_rethinking_2023. Tại địa chỉ GitHub này, bài giảng ghi hình và upload 2 lần/tuần.",,,#sharing,,
"Chào các Anh, Chị,
Sau khi tìm hiểu về mô hình ARIMA trong time-series em thắc mắc một số vấn đề như sau:
1) Ưu điểm, nhược điểm của Arima model.
2) Arima thích hợp với bài toán time-series hay không? khi nào thì mình nên dùng ARIMA sẽ cho kết quả tốt.?
3) Arima và LSTM thì phương pháp nào thường sẽ cho kết quả tốt hơn.
Em rất mong nhận được những góp ý, thảo luận của các Anh Chị, để em có thể nâng cao được thêm kiến thức ạ.","Chào các Anh, Chị, Sau khi tìm hiểu về mô hình ARIMA trong time-series em thắc mắc một số vấn đề như sau: 1) Ưu điểm, nhược điểm của Arima model. 2) Arima thích hợp với bài toán time-series hay không? khi nào thì mình nên dùng ARIMA sẽ cho kết quả tốt.? 3) Arima và LSTM thì phương pháp nào thường sẽ cho kết quả tốt hơn. Em rất mong nhận được những góp ý, thảo luận của các Anh Chị, để em có thể nâng cao được thêm kiến thức ạ.",,,"#Q&A, #deep_learning",,
"Xin chào mn,
Hiện tại em đang làm một đồ án có liên quan tới một hệ thống kết nối cộng đồng AI. Đối tượng tham gia cộng đồng này sẽ là những người có kiến thức về AI, những người muốn tìm job về AI, những người có nhu cầu thuê người làm dự án AI,... Thì ở đây thầy em có yêu cầu sẽ dùng AI để hệ thống có thể đề xuất những người phù hợp nhất với dự án mà người chủ dự án đã đăng. Cụ thể, những người chủ án sẽ post dự án lên. Và những sẽ theo kèm đó là description của dự án. Dựa vào đó, hệ thống sẽ recommend những người có role phù hợp cho dự án đó. ( Đồ án này chỉ giới hạn pair matching giữa những dự án làm về AI với những người làm liên quan tới lĩnh vực AI thôi ạ). Mn có ai đã từng làm qua có thể cho em xin nguồn tham khảo được không ạ. Em có tham khảo qua recommandation system cũng như ứng dụng NLP nhưng vẫn chưa tìm được hướng đi rõ ràng ạ. Em cảm ơn mn ạ.","Xin chào mn, Hiện tại em đang làm một đồ án có liên quan tới một hệ thống kết nối cộng đồng AI. Đối tượng tham gia cộng đồng này sẽ là những người có kiến thức về AI, những người muốn tìm job về AI, những người có nhu cầu thuê người làm dự án AI,... Thì ở đây thầy em có yêu cầu sẽ dùng AI để hệ thống có thể đề xuất những người phù hợp nhất với dự án mà người chủ dự án đã đăng. Cụ thể, những người chủ án sẽ post dự án lên. Và những sẽ theo kèm đó là description của dự án. Dựa vào đó, hệ thống sẽ recommend những người có role phù hợp cho dự án đó. ( Đồ án này chỉ giới hạn pair matching giữa những dự án làm về AI với những người làm liên quan tới lĩnh vực AI thôi ạ). Mn có ai đã từng làm qua có thể cho em xin nguồn tham khảo được không ạ. Em có tham khảo qua recommandation system cũng như ứng dụng NLP nhưng vẫn chưa tìm được hướng đi rõ ràng ạ. Em cảm ơn mn ạ.",,,#Q&A,,
"Chào mn, em là sinh viên năm cuối hiện đang tìm hiểu về lĩnh vực ML, DL, AI. Về phần đồ án tốt nghiệp của em thầy có bảo phải sử dụng mô hình AI để nhận dạng được tiếng ồn của máy (máy hoạt động ổn định hay không). Tuy nhiên trên trường em chỉ được học AI trong xử lý dữ liệu, xử lý ảnh thôi chứ chưa đến mức xử lý âm thanh. Em có tìm hiểu nhiều nguồn về xử lý âm thanh mà thấy có vẻ không đúng trong tâm lắm (chủ yếu về nhận diện giọng nói)? Anh chị nào có nguồn nào xử lý âm thanh (như sách, vidieo, khoá học hay) có thể recommend em với ạ, càng chi tiết càng tốt để sau này bảo vệ đồ án ổn ạ!
Em cám ơn mn đã bỏ thời gian đọc bài viết!","Chào mn, em là sinh viên năm cuối hiện đang tìm hiểu về lĩnh vực ML, DL, AI. Về phần đồ án tốt nghiệp của em thầy có bảo phải sử dụng mô hình AI để nhận dạng được tiếng ồn của máy (máy hoạt động ổn định hay không). Tuy nhiên trên trường em chỉ được học AI trong xử lý dữ liệu, xử lý ảnh thôi chứ chưa đến mức xử lý âm thanh. Em có tìm hiểu nhiều nguồn về xử lý âm thanh mà thấy có vẻ không đúng trong tâm lắm (chủ yếu về nhận diện giọng nói)? Anh chị nào có nguồn nào xử lý âm thanh (như sách, vidieo, khoá học hay) có thể recommend em với ạ, càng chi tiết càng tốt để sau này bảo vệ đồ án ổn ạ! Em cám ơn mn đã bỏ thời gian đọc bài viết!",,,#Q&A,,
"Sau bao ngày chờ đợi, nhóm dịch sách cuối cùng cũng đã hoàn thiện tập hai.
Các bạn đã đăng ký tập hai từ lần trước chuẩn bị nhận sách nhé!","Sau bao ngày chờ đợi, nhóm dịch sách cuối cùng cũng đã hoàn thiện tập hai. Các bạn đã đăng ký tập hai từ lần trước chuẩn bị nhận sách nhé!",,,#sharing,,
Cho em hỏi sao loss function không có sigma đằng trước ạ,Cho em hỏi sao loss function không có sigma đằng trước ạ,,,"#Q&A, #math",,
"Em chào các anh chị, em là sinh viên năm 3 đang tìm hiểu về Computer Vision, cụ thể là về xử lý dữ liệu 3D. Em có tìm hiểu mộ5t vài thông tin trên mạng nhưng đa số là paper mà em đọc thì thấy khó hiểu với có nhiều cái căn bản em chưa biết. Anh chị có thể cho em xin một số cuốn sách nào về lĩnh vực này để em có thể tìm hiểu từ cơ bản trước không ạ. Em xin cảm ơn ạ.","Em chào các anh chị, em là sinh viên năm 3 đang tìm hiểu về Computer Vision, cụ thể là về xử lý dữ liệu 3D. Em có tìm hiểu mộ5t vài thông tin trên mạng nhưng đa số là paper mà em đọc thì thấy khó hiểu với có nhiều cái căn bản em chưa biết. Anh chị có thể cho em xin một số cuốn sách nào về lĩnh vực này để em có thể tìm hiểu từ cơ bản trước không ạ. Em xin cảm ơn ạ.",,,"#Q&A, #cv",,
"Mọi người ơi, mọi người cho em hỏi là làm sao kiểm soát được câu trả lời của chat gpt api vậy ạ? Em có tích hợp vào chatbot đưa thông tin sản phẩm giá 300k mà nó trả lời khách 150k. Em có tìm các thuộc tính của nó và đọc các tài liệu rồi nhưng mà vẫn chưa tìm ra ạ","Mọi người ơi, mọi người cho em hỏi là làm sao kiểm soát được câu trả lời của chat gpt api vậy ạ? Em có tích hợp vào chatbot đưa thông tin sản phẩm giá 300k mà nó trả lời khách 150k. Em có tìm các thuộc tính của nó và đọc các tài liệu rồi nhưng mà vẫn chưa tìm ra ạ",,,"#Q&A, #nlp",,
"Chào mọi người ạ. Hiện tại em đang làm bài tập python cần dùng turicreate để sử dụng sframe. Nhưng không thể cài được để sử dụng. Trên group có ai từng sử dụng turicreate hoặc tương tự chỉ giúp e với ạ. Em đang chạy song song Win 10 và Ubuntu 18.04 ạ.
Cảm ơn mọi người!!!",Chào mọi người ạ. Hiện tại em đang làm bài tập python cần dùng turicreate để sử dụng sframe. Nhưng không thể cài được để sử dụng. Trên group có ai từng sử dụng turicreate hoặc tương tự chỉ giúp e với ạ. Em đang chạy song song Win 10 và Ubuntu 18.04 ạ. Cảm ơn mọi người!!!,,,"#Q&A, #python",,
"Mọi người cho em hỏi cách cài đặt Turicreate trên Python được không ạ. Em có thử làm theo những cách ở trên mạng nhưng vẫn bị báo lỗi ấy ạ.
Em cảm ơn mọi người trước",Mọi người cho em hỏi cách cài đặt Turicreate trên Python được không ạ. Em có thử làm theo những cách ở trên mạng nhưng vẫn bị báo lỗi ấy ạ. Em cảm ơn mọi người trước,,,#Q&A,,
"Em chào mọi người ạ, chuyện là em đang làm một project về sinh ảnh biển báo giao thông sử dụng mô hình diffusion. Em muốn hỏi mọi người, mình muốn lấy datasets biển báo giao thông chỗ nào ổn ạ. Em cảm ơn mọi người rất nhiều ạ","Em chào mọi người ạ, chuyện là em đang làm một project về sinh ảnh biển báo giao thông sử dụng mô hình diffusion. Em muốn hỏi mọi người, mình muốn lấy datasets biển báo giao thông chỗ nào ổn ạ. Em cảm ơn mọi người rất nhiều ạ",,,"#Q&A, #cv",,
"Chào mọi người, mình muốn chia sẻ 1 python package NLP mình build cho công việc + cá nhân, mọi người có thể dùng thử và cho mình feedback để mình improve cái package này hơn nhé, tại đây cũng là tâm huyết của mình mấy tháng qua 😅
Thư viện này mình build gồm 2 phần chính,
- P1: Supervised learning: dùng HuggingFace's masked language model (có thể sử dụng phobert hay envibert luôn) hay causal language model (kiểu của gpt, cho tiếng việt thì có NlpHUST/gpt2-vietnamese) cho các task: classification, regression; vừa classification vừa regression, classification các level khác nhau (mình gọi nó là multihead); và cả multilabel
- P2: Language model training: cho phép mình train 1 LLM (masked hoặc causal) from scratch, hoặc là cho LM fine-tuning (kiểu mình dùng phobert mà đã được train trên tập dataset tiếng việt rất lớn gồm wikipedia hay báo), xong mình train tiếp nó cho tập data comment của user trên sàn e-commerce, để nó đc finetune tốt hơn trên tập data này). Idea này mình nhớ bắt đầu từ paper ULMFiT. Cái lợi của việc này là sau khi mình cho model train trên tập data này xong (e.g. data user comment), mình cho model học tiếp những cái supervised learning task nhắc tới ở phần 1, kiểu predict coi user này đang comment về category gì, thì độ chính xác của nó sẽ được boost lên 1 chút nữa.
Với từng phần ở trên thì mình chia nó thành 2 process: 1 process chuyên làm text preprocessing (có thể làm đa luồng luôn, vì backend mình dùng HuggingFace Datasets lib), e.g. load data, filter data, text transform, có cả text augmentation), và 1 process là để build model (để train model, log, save and load model ...)
Tất cả các thông tin mình có viết documentation và có tutorial cho từng đoạn, mọi người có thể xem qua ở đây: https://anhquan0412.github.io/that-nlp-library/. Cảm ơn mọi người ạ :D","Chào mọi người, mình muốn chia sẻ 1 python package NLP mình build cho công việc + cá nhân, mọi người có thể dùng thử và cho mình feedback để mình improve cái package này hơn nhé, tại đây cũng là tâm huyết của mình mấy tháng qua Thư viện này mình build gồm 2 phần chính, - P1: Supervised learning: dùng HuggingFace's masked language model (có thể sử dụng phobert hay envibert luôn) hay causal language model (kiểu của gpt, cho tiếng việt thì có NlpHUST/gpt2-vietnamese) cho các task: classification, regression; vừa classification vừa regression, classification các level khác nhau (mình gọi nó là multihead); và cả multilabel - P2: Language model training: cho phép mình train 1 LLM (masked hoặc causal) from scratch, hoặc là cho LM fine-tuning (kiểu mình dùng phobert mà đã được train trên tập dataset tiếng việt rất lớn gồm wikipedia hay báo), xong mình train tiếp nó cho tập data comment của user trên sàn e-commerce, để nó đc finetune tốt hơn trên tập data này). Idea này mình nhớ bắt đầu từ paper ULMFiT. Cái lợi của việc này là sau khi mình cho model train trên tập data này xong (e.g. data user comment), mình cho model học tiếp những cái supervised learning task nhắc tới ở phần 1, kiểu predict coi user này đang comment về category gì, thì độ chính xác của nó sẽ được boost lên 1 chút nữa. Với từng phần ở trên thì mình chia nó thành 2 process: 1 process chuyên làm text preprocessing (có thể làm đa luồng luôn, vì backend mình dùng HuggingFace Datasets lib), e.g. load data, filter data, text transform, có cả text augmentation), và 1 process là để build model (để train model, log, save and load model ...) Tất cả các thông tin mình có viết documentation và có tutorial cho từng đoạn, mọi người có thể xem qua ở đây: https://anhquan0412.github.io/that-nlp-library/. Cảm ơn mọi người ạ :D",,,"#sharing, #python",,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 02/2023 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 02/2023 vào comment của post này.",,,#sharing,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2023 vào comment của post này.
Chúc các bạn có một kỳ nghỉ lễ vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2023 vào comment của post này. Chúc các bạn có một kỳ nghỉ lễ vui vẻ.",,,#sharing,,
Mng cho em hỏi: ai có tập dataset liên quan đến các chỉ số về chất lượng không khí như này ở việt nam không ạ.,Mng cho em hỏi: ai có tập dataset liên quan đến các chỉ số về chất lượng không khí như này ở việt nam không ạ.,,,"#Q&A, #data",,
"Hi anh chị, em đang làm bài tập về Linear Regression dự đoán giá nhà bằng thuật toán Gradient Decent( Giảm độ dốc), nhưng sau khi em test thử model thì các giá trị predicton nó có giá trị ""nan"" là bị sao vậy ạ, mô hình này em test với tập dữ liệu nhỏ thì nó cho ra các dự đoán khá sát với các giá trị thực tế, nhưng khi em thử các tập dữ liệu khá lớn thì các giá trị prediction bằng ""nan"", em đã thử điều chỉnh learing rate nhưng không có kết quả ạ. Mong mọi người giúp đỡ, em cảm ơn ạ !
Link github: https://github.com/bigboss151102/Linear_Regression/blob/master/prediction_house_TUD.ipynb","Hi anh chị, em đang làm bài tập về Linear Regression dự đoán giá nhà bằng thuật toán Gradient Decent( Giảm độ dốc), nhưng sau khi em test thử model thì các giá trị predicton nó có giá trị ""nan"" là bị sao vậy ạ, mô hình này em test với tập dữ liệu nhỏ thì nó cho ra các dự đoán khá sát với các giá trị thực tế, nhưng khi em thử các tập dữ liệu khá lớn thì các giá trị prediction bằng ""nan"", em đã thử điều chỉnh learing rate nhưng không có kết quả ạ. Mong mọi người giúp đỡ, em cảm ơn ạ ! Link github: https://github.com/bigboss151102/Linear_Regression/blob/master/prediction_house_TUD.ipynb",,,"#Q&A, #machine_learning",,
"Xin chào mọi người, em có ý định sẽ tạo ra một con AI / ML có khả năng generate ra model 3d nhà theo phong cách kiến trúc Việt Nam, ngoài ra em cũng tò mò, muốn hiểu sâu về cơ chế hoạt động, hiểu rõ bản chất của các ứng dụng giả giọng nói ạ. Nhưng hiện tại kiến thức về ML hay AI của em gần như bằng 0, do đó e đã đặt ra mục tiêu dài hạn sẽ học về AI / ML để hoàn thành đc mục tiêu e đã đề cập ạ
Vì vậy có bác nào rành về lĩnh vực này có thể tư vấn giúp em lộ trình học hiệu quả theo hướng có thể giải quyết đc 2 nhu cầu trên của e ko ạ, em cám ơn.","Xin chào mọi người, em có ý định sẽ tạo ra một con AI / ML có khả năng generate ra model 3d nhà theo phong cách kiến trúc Việt Nam, ngoài ra em cũng tò mò, muốn hiểu sâu về cơ chế hoạt động, hiểu rõ bản chất của các ứng dụng giả giọng nói ạ. Nhưng hiện tại kiến thức về ML hay AI của em gần như bằng 0, do đó e đã đặt ra mục tiêu dài hạn sẽ học về AI / ML để hoàn thành đc mục tiêu e đã đề cập ạ Vì vậy có bác nào rành về lĩnh vực này có thể tư vấn giúp em lộ trình học hiệu quả theo hướng có thể giải quyết đc 2 nhu cầu trên của e ko ạ, em cám ơn.",,,#Q&A,,
"Em đang tập tành chuyển qua google colab nhưng khi upload file lên thì các hình ảnh trong Markdown đều bị lỗi không load được mặc dù đã đúng đường dẫn ạ.
Edit:
Sau một hồi nghiên cứu thì có vẻ như thẻ <img> trong MarkDown không hoạt động trong đường dẫn local vì một lý do nào đó (trong khi sử dụng cùng format đường dẫn để read file thì ok) nhưng lại hoạt động được với các URL global. Dưới đây là video cách lấy link các file ảnh trong google drive để sử dụng vào colab.
Anh em nào có phương pháp hay ý kiến tốt hơn mời bình luận bên dưới ạ. Thank
https://www.youtube.com/watch?v=gCsmANNdmfo",Em đang tập tành chuyển qua google colab nhưng khi upload file lên thì các hình ảnh trong Markdown đều bị lỗi không load được mặc dù đã đúng đường dẫn ạ. Edit: Sau một hồi nghiên cứu thì có vẻ như thẻ <img> trong MarkDown không hoạt động trong đường dẫn local vì một lý do nào đó (trong khi sử dụng cùng format đường dẫn để read file thì ok) nhưng lại hoạt động được với các URL global. Dưới đây là video cách lấy link các file ảnh trong google drive để sử dụng vào colab. Anh em nào có phương pháp hay ý kiến tốt hơn mời bình luận bên dưới ạ. Thank https://www.youtube.com/watch?v=gCsmANNdmfo,,,#Q&A,,
"Chào buổi tối mọi người,
Lúc launching Vietcuna thì team VILM có hứa sẽ có phiên bản 40B của Vietcuna. Hôm nay team rất vui giới thiệu 2 model mới nhất của team là Vulture-40B và Vulture-180B với hỗ trợ lên tới 12 ngôn ngữ, tất nhiên là có hỗ trợ tiếng Việt. VILM mong Vulture series sẽ là công cụ đắc lực giúp các công ty Việt Nam vươn ra biển lớn! 🙂
Supported Languages: English, German, Spanish, French, Portugese, Russian, Italian, Vietnamese, Indonesian, Chinese, Japanese and Chinese
Announcement: [https://www.vilm.org/research/meet-vulture-40b-and-180b-worlds-largest-multilingual-language-models]
Vulture-40B: [https://huggingface.co/vilm/vulture-40b]
Vulture-180B: [https://huggingface.co/vilm/vulture-180b]","Chào buổi tối mọi người, Lúc launching Vietcuna thì team VILM có hứa sẽ có phiên bản 40B của Vietcuna. Hôm nay team rất vui giới thiệu 2 model mới nhất của team là Vulture-40B và Vulture-180B với hỗ trợ lên tới 12 ngôn ngữ, tất nhiên là có hỗ trợ tiếng Việt. VILM mong Vulture series sẽ là công cụ đắc lực giúp các công ty Việt Nam vươn ra biển lớn! Supported Languages: English, German, Spanish, French, Portugese, Russian, Italian, Vietnamese, Indonesian, Chinese, Japanese and Chinese Announcement: [https://www.vilm.org/research/meet-vulture-40b-and-180b-worlds-largest-multilingual-language-models] Vulture-40B: [https://huggingface.co/vilm/vulture-40b] Vulture-180B: [https://huggingface.co/vilm/vulture-180b]",,,"#sharing, #nlp",,
"Generative Knowledge AI đang phát triển với tốc độ vũ bão, nhất là các mô hình ngôn ngữ lớn (Large Language Models).
Tuy nhiên, lĩnh vực nghiên cứu ảnh tạo sinh với các mô hình như DALLE (hiện DALLE-3 đã được tích hợp vào ChatGPT-4(v), bản DALLE-2 có open source nhé) và đặc biệt là open source Stable Diffusions (ControlNet là 1 dạng variants từ SD) giữ vai trò quan trọng việc sáng tạo nội dung (content creation). Ví dụ ta có thể prompts ra ảnh annimate trong truyện tranh hay về lĩnh vực thiết kế công nghiệp/đồ hoạ/kiến trúc/xây dựng/..., ta có thể vẽ sketch rồi prompts cho ra sản phẩm “hoàn chỉnh”. Chưa kể chúng ta có thể nghiên cứu ứng dụng SD vào các domain chuyên ngành hẹp. Hi vọng mình có thể sớm ""khoe"" kết quả này trong thời gian sắp tới!!

Để làm sinh động hơn hình ảnh tĩnh tạo sinh, gần đây các nhà khoa học đã giới thiệu tới mọi người source code có thể tạo ảnh đông (*gif) có tên là Annimatediff (tại đây https://github.com/guoyww/AnimateDiff) và Hotshot (tại đây https://github.com/hotshotco/Hotshot-XL)
Cả 2 thư viện này giúp chúng ta finetune model SD thành dạng ảnh động (*gif) thay vì ảnh tĩnh (jpg/png).
Hi vọng thư viện này giúp các bạn học tập vào thực hành theo hướng mình mong muốn.","Generative Knowledge AI đang phát triển với tốc độ vũ bão, nhất là các mô hình ngôn ngữ lớn (Large Language Models). Tuy nhiên, lĩnh vực nghiên cứu ảnh tạo sinh với các mô hình như DALLE (hiện DALLE-3 đã được tích hợp vào ChatGPT-4(v), bản DALLE-2 có open source nhé) và đặc biệt là open source Stable Diffusions (ControlNet là 1 dạng variants từ SD) giữ vai trò quan trọng việc sáng tạo nội dung (content creation). Ví dụ ta có thể prompts ra ảnh annimate trong truyện tranh hay về lĩnh vực thiết kế công nghiệp/đồ hoạ/kiến trúc/xây dựng/..., ta có thể vẽ sketch rồi prompts cho ra sản phẩm “hoàn chỉnh”. Chưa kể chúng ta có thể nghiên cứu ứng dụng SD vào các domain chuyên ngành hẹp. Hi vọng mình có thể sớm ""khoe"" kết quả này trong thời gian sắp tới!! Để làm sinh động hơn hình ảnh tĩnh tạo sinh, gần đây các nhà khoa học đã giới thiệu tới mọi người source code có thể tạo ảnh đông (*gif) có tên là Annimatediff (tại đây https://github.com/guoyww/AnimateDiff) và Hotshot (tại đây https://github.com/hotshotco/Hotshot-XL) Cả 2 thư viện này giúp chúng ta finetune model SD thành dạng ảnh động (*gif) thay vì ảnh tĩnh (jpg/png). Hi vọng thư viện này giúp các bạn học tập vào thực hành theo hướng mình mong muốn.",,,"#sharing, #python",,
"Một trong những yếu tố giúp chúng ta có prompts tốt để generate ra nội  mong muốn là việc làm khó. Nên mới có nghề mới gọi là prompt engineers/engineering. Dưới đây là tổng kết 6 điểm mà tôi copy&paste của Francois Chollet (tác giả bài báo về mô hình Inception và là người viết thư viện Keras của Google)
""My interpretation of prompt engineering is this:
1. A LLM is a repository of many (millions) of vector programs mined from human-generated data, learned implicitly as a by-product of language compression. A ""vector program"" is just a very non-linear function that maps part of the latent space unto itself.
2. When you're prompting, you're fetching one of these programs and running it on an input -- part of your prompt serves as a kind of ""program key"" (as in database key) and part serves as program argument(s). Like, in ""write this paragraph in the style of Shakespeare: {my paragraph}"", the part ""write this paragraph in the stye of X: Y"" is a program key, with arguments X=Shakespeare and Y={my paragraph}.
3. The program fetched by your key may or may not work well for the task at hand. There's no reason why it should be optimal. There are lots of related programs to choose from.
4. Prompt engineering represents a search over many keys in order a find a program that is empirically more accurate for what you're trying to do. It's no different than trying different keywords when searching for a Python library.
5. Everything else is unnecessary anthropomorphism on the part of the prompter. You're not talking to a human who understands language the way you do. Stop pretending you are.”
https://x.com/fchollet/status/1709242747293511939?s=46...
Mình từng có lần chia sẻ về việc có người tổng hợp các ảnh và prompts liên quan tới chủ để sinh ảnh. Mình sẽ tìm lại link GitHub của nó và chia sẻ bên dưới.","Một trong những yếu tố giúp chúng ta có prompts tốt để generate ra nội mong muốn là việc làm khó. Nên mới có nghề mới gọi là prompt engineers/engineering. Dưới đây là tổng kết 6 điểm mà tôi copy&paste của Francois Chollet (tác giả bài báo về mô hình Inception và là người viết thư viện Keras của Google) ""My interpretation of prompt engineering is this: 1. A LLM is a repository of many (millions) of vector programs mined from human-generated data, learned implicitly as a by-product of language compression. A ""vector program"" is just a very non-linear function that maps part of the latent space unto itself. 2. When you're prompting, you're fetching one of these programs and running it on an input -- part of your prompt serves as a kind of ""program key"" (as in database key) and part serves as program argument(s). Like, in ""write this paragraph in the style of Shakespeare: {my paragraph}"", the part ""write this paragraph in the stye of X: Y"" is a program key, with arguments X=Shakespeare and Y={my paragraph}. 3. The program fetched by your key may or may not work well for the task at hand. There's no reason why it should be optimal. There are lots of related programs to choose from. 4. Prompt engineering represents a search over many keys in order a find a program that is empirically more accurate for what you're trying to do. It's no different than trying different keywords when searching for a Python library. 5. Everything else is unnecessary anthropomorphism on the part of the prompter. You're not talking to a human who understands language the way you do. Stop pretending you are.” https://x.com/fchollet/status/1709242747293511939?s=46... Mình từng có lần chia sẻ về việc có người tổng hợp các ảnh và prompts liên quan tới chủ để sinh ảnh. Mình sẽ tìm lại link GitHub của nó và chia sẻ bên dưới.",,,"#sharing, #nlp",,
Netfilx System Design Backend,Netfilx System Design Backend,,,,,
👉 FYI 04-05/10 | Tân Sơn Nhất Pavillon - HCM,FYI 04-05/10 | Tân Sơn Nhất Pavillon - HCM,,,,,
"Em chào mọi người , e là newbie , e mới train 1 model trên kaggle sau khi tập tành fine tuning 1 model (model đó fine tune dựa vào Llama2 và Blomz trên tập data tiếng việt ) trên hugging face xong thì e có nhấn save model , sau đó vào lại thì chỉ thấy có mấy file như adapter _model.bin , adapter_config.json , redme, events.out.tfevens , Sau đó em có tải koboldcpp về để chạy nhưng không được . Mọi người có biết cách nào để chạy file này ko ạ .Và input đầu vào có nhất thiết phải có dạng "" instruction , input ,output,response: , em muốn dùng văn bản text được không ạ ? em cảm ơn ạ!!!!","Em chào mọi người , e là newbie , e mới train 1 model trên kaggle sau khi tập tành fine tuning 1 model (model đó fine tune dựa vào Llama2 và Blomz trên tập data tiếng việt ) trên hugging face xong thì e có nhấn save model , sau đó vào lại thì chỉ thấy có mấy file như adapter _model.bin , adapter_config.json , redme, events.out.tfevens , Sau đó em có tải koboldcpp về để chạy nhưng không được . Mọi người có biết cách nào để chạy file này ko ạ .Và input đầu vào có nhất thiết phải có dạng "" instruction , input ,output,response: , em muốn dùng văn bản text được không ạ ? em cảm ơn ạ!!!!",,,"#Q&A, #nlp",,
"Xin phép chia sẻ với các bác về đồ án Text Image Retrieval của một học sinh khóa MLE em đang training. Model được deploy trên GKE và expose sử dụng Nginx Ingress. Jenkins để build CI/CD pipeline được deploy trên GCE sử dụng Ansible. Bên cạnh đó, bạn cũng dùng GKE để deploy các monitoring tools để observe hệ thống. README được viết rất chi tiết, nên em hy vọng sẽ là một nguồn tài liệu hữu ích khác bên cạnh MLOps Crash Course lần trước em chia sẻ 😃.
https://github.com/.../continuous-deployment-to-gke-cluster
Mọi người đừng quên động viên bạn Nguyen Tran Dang Duong một Github star nếu thấy có ích nhé ạ 😉.
Chúc các bác cuối tuần vui vẻ!","Xin phép chia sẻ với các bác về đồ án Text Image Retrieval của một học sinh khóa MLE em đang training. Model được deploy trên GKE và expose sử dụng Nginx Ingress. Jenkins để build CI/CD pipeline được deploy trên GCE sử dụng Ansible. Bên cạnh đó, bạn cũng dùng GKE để deploy các monitoring tools để observe hệ thống. README được viết rất chi tiết, nên em hy vọng sẽ là một nguồn tài liệu hữu ích khác bên cạnh MLOps Crash Course lần trước em chia sẻ . https://github.com/.../continuous-deployment-to-gke-cluster Mọi người đừng quên động viên bạn Nguyen Tran Dang Duong một Github star nếu thấy có ích nhé ạ . Chúc các bác cuối tuần vui vẻ!",,,"#sharing,#cv, #nlp ",,
"Chào mọi người ạ!
Mọi người cho em hỏi là có ai đã từng pretraining (hoặc further training) TrOCR hoặc 1 multimodal nào đó bằng Hugging Face ko ạ? Em muốn hỏi để lấy thêm kinh nghiệm ạ!",Chào mọi người ạ! Mọi người cho em hỏi là có ai đã từng pretraining (hoặc further training) TrOCR hoặc 1 multimodal nào đó bằng Hugging Face ko ạ? Em muốn hỏi để lấy thêm kinh nghiệm ạ!,,,#Q&A,,
"Chào mọi người ạ. Hiện tại em đang muốn có định hướng học AI. Em biết Python dụng để phát triển và huần luyện các model AI. Nhưng deploy model trong thực tế như viết một cái backend chẳng hạn thì cần hiệu suất cao hơn và nhanh hơn thì người ta dùng ngôn ngữ lập trình khác hoặc tích hợp AI vào hệ thống nhúng dùng C/C++. Mọi người ai đã đi làm ngoài thực tế xin khảo sát một vài ngôn ngữ lập trình thường mà công ty, doanh nghiệp của các anh chị thường sử dụng để deploy model AI ra thực tế với ạ. Em xin cảm ơn rất nhiều ạ.","Chào mọi người ạ. Hiện tại em đang muốn có định hướng học AI. Em biết Python dụng để phát triển và huần luyện các model AI. Nhưng deploy model trong thực tế như viết một cái backend chẳng hạn thì cần hiệu suất cao hơn và nhanh hơn thì người ta dùng ngôn ngữ lập trình khác hoặc tích hợp AI vào hệ thống nhúng dùng C/C++. Mọi người ai đã đi làm ngoài thực tế xin khảo sát một vài ngôn ngữ lập trình thường mà công ty, doanh nghiệp của các anh chị thường sử dụng để deploy model AI ra thực tế với ạ. Em xin cảm ơn rất nhiều ạ.",,,#Q&A,,
"Gần đây mình được cấp quyền sử dụng High Performance Computers (HPC), (còn trước đây chỉ có mỗi 1 máy với 1 GPU 3090, nên mình chỉ dùng AnyDesk để điều khiển từ xa), nên mình dành nhiều thời gian học để điều khiển nó. Vì vậy mình phải học một số công cụ như Tmate, Screen, SSH, Code Tunnel,... Nhưng quan trong hơn là phải học thêm nhiều về bash scripts. Nay thấy 1 repo khá thú vị hướng dẫn các bash scripts phổ biến dùng để huấn luyện các mô hình lớn trên HPC nên mình chia sẻ lại tại đây https://github.com/stas00/ml-engineering.
Hi vọng nó sẽ giúp ích với các bạn có điều kiện sử dụng HPC.","Gần đây mình được cấp quyền sử dụng High Performance Computers (HPC), (còn trước đây chỉ có mỗi 1 máy với 1 GPU 3090, nên mình chỉ dùng AnyDesk để điều khiển từ xa), nên mình dành nhiều thời gian học để điều khiển nó. Vì vậy mình phải học một số công cụ như Tmate, Screen, SSH, Code Tunnel,... Nhưng quan trong hơn là phải học thêm nhiều về bash scripts. Nay thấy 1 repo khá thú vị hướng dẫn các bash scripts phổ biến dùng để huấn luyện các mô hình lớn trên HPC nên mình chia sẻ lại tại đây https://github.com/stas00/ml-engineering. Hi vọng nó sẽ giúp ích với các bạn có điều kiện sử dụng HPC.",,,#sharing,,
"Cho mình hỏi embedding vectors tiếng Việt gọi sao vậy ạ? Mình dịch tạm là ""véc tơ nhúng"" nhưng có vẻ nó không thể hiện được tinh thần của chữ ""embedding""
Trong trang của ML cơ bản thì vẫn giữ nguyên chữ embedding: https://machinelearningcoban.com/tabml_book/ch_embedding/embedding.html
Có chăng mình không thể ""việt hoá"" hoàn toàn từ này?","Cho mình hỏi embedding vectors tiếng Việt gọi sao vậy ạ? Mình dịch tạm là ""véc tơ nhúng"" nhưng có vẻ nó không thể hiện được tinh thần của chữ ""embedding"" Trong trang của ML cơ bản thì vẫn giữ nguyên chữ embedding: https://machinelearningcoban.com/tabml_book/ch_embedding/embedding.html Có chăng mình không thể ""việt hoá"" hoàn toàn từ này?",,,"#Q&A, #nlp",,
"Generative AI (AI tạo sinh) và Predictive AI (AI dự đoán) khác nhau về cách chúng xử lý các ứng dụng và dữ liệu có cấu trúc lẫn phi cấu trúc tương ứng. Cùng tìm hiểu những lợi ích và các hạn chế của mỗi loại trong các ứng dụng thực tế của chúng.
#GenerativeAI #PredictiveAI #AI",Generative AI (AI tạo sinh) và Predictive AI (AI dự đoán) khác nhau về cách chúng xử lý các ứng dụng và dữ liệu có cấu trúc lẫn phi cấu trúc tương ứng. Cùng tìm hiểu những lợi ích và các hạn chế của mỗi loại trong các ứng dụng thực tế của chúng.,#GenerativeAI	#PredictiveAI	#AI,,#sharing,,
"Chào các anh chị, các bạn, tiền bối trong ngành ạ. Mong anh chị cho em một chút nhận xét về CV này của em với ạ. Liệu đã đủ điều kiện để có thể xin đi thực tập chưa ạ. Nếu được mong anh chị cho em vài góp ý cần bổ sung và cải thiện thêm (cả hình thức và nội dung) ạ. Chúc mọi người một ngày tràn đầy năng lượng ạ.","Chào các anh chị, các bạn, tiền bối trong ngành ạ. Mong anh chị cho em một chút nhận xét về CV này của em với ạ. Liệu đã đủ điều kiện để có thể xin đi thực tập chưa ạ. Nếu được mong anh chị cho em vài góp ý cần bổ sung và cải thiện thêm (cả hình thức và nội dung) ạ. Chúc mọi người một ngày tràn đầy năng lượng ạ.",,,#sharing,,
"Xin chào mọi người, anh chị trong nhóm ạ.Em xin phép hỏi là em đang nghiên cứu về mô hình học máy về data bên mảng xây dựng ạ, nên em muốn xin tài liệu tham khảo và code trên github cũng được ạ. Em xin cảm ơn rất nhiều.","Xin chào mọi người, anh chị trong nhóm ạ.Em xin phép hỏi là em đang nghiên cứu về mô hình học máy về data bên mảng xây dựng ạ, nên em muốn xin tài liệu tham khảo và code trên github cũng được ạ. Em xin cảm ơn rất nhiều.",,,"#Q&A, #machine_learning",,
"Đại học Harvard vừa công bố khóa học về Data Science với Python thuộc bộ môn Computer Science.
Khóa học hoàn toàn miễn phí và kéo dài trong 8 tuần.
Link: https://pll.harvard.edu/.../introduction-data-science-python",Đại học Harvard vừa công bố khóa học về Data Science với Python thuộc bộ môn Computer Science. Khóa học hoàn toàn miễn phí và kéo dài trong 8 tuần. Link: https://pll.harvard.edu/.../introduction-data-science-python,,,#sharing,,
Tuần sau có anh chị các bạn nào tham gia ICCV ở Paris ko cùng kết nối tham gia xong thăm thú cho vui ạ 😄,Tuần sau có anh chị các bạn nào tham gia ICCV ở Paris ko cùng kết nối tham gia xong thăm thú cho vui ạ,,,#Q&A,,
Hệ thống AI cải tiến có thể giải mã cảm xúc của gà.,Hệ thống AI cải tiến có thể giải mã cảm xúc của gà.,,,#sharing,,
"Chào mn. Em đang làm nhiệm vụ làm giảm thời gian inference model bert bởi phải infer tới 1 triệu câu . Em đang có ý tưởng là ghép các câu lại có dạng <cls> sentence A <sep> sentence b <sep> ...... . thì mình ghép được bao nhiêu câu thì tốc độ sẽ giảm đi từng đó lần , và bài toán yừ multi class sẽ chuyển thành multi label. Thế nhưng khi em ghép vào kết quả infer rất tệ (đã train trên câu ghép). Không biết mn ở đây đã ai từng làm chưa có thể cho em cái suggest không ạ. Em cảm ơn","Chào mn. Em đang làm nhiệm vụ làm giảm thời gian inference model bert bởi phải infer tới 1 triệu câu . Em đang có ý tưởng là ghép các câu lại có dạng <cls> sentence A <sep> sentence b <sep> ...... . thì mình ghép được bao nhiêu câu thì tốc độ sẽ giảm đi từng đó lần , và bài toán yừ multi class sẽ chuyển thành multi label. Thế nhưng khi em ghép vào kết quả infer rất tệ (đã train trên câu ghép). Không biết mn ở đây đã ai từng làm chưa có thể cho em cái suggest không ạ. Em cảm ơn",,,#Q&A,,
Cẩm nang chi tiết cho bạn nào xây CV nhé,Cẩm nang chi tiết cho bạn nào xây CV nhé,,,#sharing,,
"Mn có biết báo nào hay web nào có những nghiên cứu model về tài chính, dữ liệu có độ tin cậy cao không ạ cho em xin với ạ","Mn có biết báo nào hay web nào có những nghiên cứu model về tài chính, dữ liệu có độ tin cậy cao không ạ cho em xin với ạ",,,#Q&A,,
"Chào mọi người. Em đang làm 1 project nlp cần tiền xử lý dữ liệu văn bản tiếng Việt. Cho em hỏi có package nào hỗ trợ chuẩn hoá cách bỏ dấu câu (""òa"" - ""oà"", ""úy"" - uý) và lỗi y i (""mỳ"" - ""mì"", ""li ti"", ""cái ly"") không ạ.","Chào mọi người. Em đang làm 1 project nlp cần tiền xử lý dữ liệu văn bản tiếng Việt. Cho em hỏi có package nào hỗ trợ chuẩn hoá cách bỏ dấu câu (""òa"" - ""oà"", ""úy"" - uý) và lỗi y i (""mỳ"" - ""mì"", ""li ti"", ""cái ly"") không ạ.",,,"#Q&A, #data, #nlp",,
"Dự án hiện tại của em là xây dựng một data engine cho một phòng lab thực tế ảo ( VR Experiment sử dụng Oculus). Vấn đề đặt ra là hiện tại lab ảo này đang sử dụng một số mock data từ lab thật được nạp vào và mỗi khi người dùng thực hiện hành động thì output sẽ là mock data chứ không phải là data thật.
Thầy muốn em tạo một data engine để học và dự đoán output dựa trên những data được lưu lại trước đó (lab thật có xuất ra một file excel lưu data những lần chạy) V
ậy thì, Em cần học những thuật toán gì? Công nghệ gì? Sử dụng data storage nào? Là tối ưu nhất ạ? Em có học qua computer vision và đã từng làm một số project về image classification nhưng chưa có kinh nghiệm nào về mảng này ạ.
Cảm ơn mn","Dự án hiện tại của em là xây dựng một data engine cho một phòng lab thực tế ảo ( VR Experiment sử dụng Oculus). Vấn đề đặt ra là hiện tại lab ảo này đang sử dụng một số mock data từ lab thật được nạp vào và mỗi khi người dùng thực hiện hành động thì output sẽ là mock data chứ không phải là data thật. Thầy muốn em tạo một data engine để học và dự đoán output dựa trên những data được lưu lại trước đó (lab thật có xuất ra một file excel lưu data những lần chạy) V ậy thì, Em cần học những thuật toán gì? Công nghệ gì? Sử dụng data storage nào? Là tối ưu nhất ạ? Em có học qua computer vision và đã từng làm một số project về image classification nhưng chưa có kinh nghiệm nào về mảng này ạ. Cảm ơn mn",,,"#Q&A, #data",,
"Xin chào mọi người, em thực hiện time series analysis với mô hình ARIMA dùng Python tuy nhiên nhận được giá trị dự đoán là gần giống như giá trị trung bình như hình ở dưới ạ. Data này đã lấy sai phân bậc 1 mà giá trị p,d,q thu được sau khi thử bằng auto_arima.
Xin mn chỉ giáo cách tiếp cận và sửa mô hình này ạ. Em cảm ơn nhiều.","Xin chào mọi người, em thực hiện time series analysis với mô hình ARIMA dùng Python tuy nhiên nhận được giá trị dự đoán là gần giống như giá trị trung bình như hình ở dưới ạ. Data này đã lấy sai phân bậc 1 mà giá trị p,d,q thu được sau khi thử bằng auto_arima. Xin mn chỉ giáo cách tiếp cận và sửa mô hình này ạ. Em cảm ơn nhiều.",,,#Q&A,,
"#question
Mn cho e hỏi là học machine learning thì có nên học sâu về thuật toán (backtrack, đệ quy . . ) Em rất thích học thuật toán và vẫn học nó hàng ngày. Nhưng mà khi học ML thì nó có những thuật toán riêng không biết là cái việc học thuật toán nó có tác dụng gì khi mình theo ML không ạ ?
Em cảm ơn!","Mn cho e hỏi là học machine learning thì có nên học sâu về thuật toán (backtrack, đệ quy . . ) Em rất thích học thuật toán và vẫn học nó hàng ngày. Nhưng mà khi học ML thì nó có những thuật toán riêng không biết là cái việc học thuật toán nó có tác dụng gì khi mình theo ML không ạ ? Em cảm ơn!",#question,,"#Q&A, #math",,
"Em bị lỗi này mỗi khi submit cell trên jupyter, em đã kiểm tra java/bashrc/whichjava thì đều đã đúng đường dẫn kèm cài môi trường cho nó rồi.
Mọi người ai rành giúp em với em xin gửi bữa ăn sáng ạ :*","Em bị lỗi này mỗi khi submit cell trên jupyter, em đã kiểm tra java/bashrc/whichjava thì đều đã đúng đường dẫn kèm cài môi trường cho nó rồi. Mọi người ai rành giúp em với em xin gửi bữa ăn sáng ạ :*",,,#Q&A,,
"Xin chào mọi người, có anh/chị/em nào có biết web cho thuê server vật lý gpu để deploy api giá ngon / bổ/ rẻ không ạ ? , . Em biết 2 web là https://hostkey.com/ khá là ok nhưng mạng chập chờn quá ( hình như hay bị ddos ) . Các bác nào biết cho em xin với , em cảm ơn .","Xin chào mọi người, có anh/chị/em nào có biết web cho thuê server vật lý gpu để deploy api giá ngon / bổ/ rẻ không ạ ? , . Em biết 2 web là https://hostkey.com/ khá là ok nhưng mạng chập chờn quá ( hình như hay bị ddos ) . Các bác nào biết cho em xin với , em cảm ơn .",,,#Q&A,,
"Mọi người ơi, cho em hỏi là vì sao pre-trained model như PhoBert, GPT-3 thì khi xây chatbot rasa với các model đó, em vẫn phải xác định intent và example ạ, kh phải nó dc train rồi hay sao ạ.
Em là newbie nên hỏi có khi hơi ngô nghê, mong mng bỏ qua.
Cảm ơn mng đã giải đáp","Mọi người ơi, cho em hỏi là vì sao pre-trained model như PhoBert, GPT-3 thì khi xây chatbot rasa với các model đó, em vẫn phải xác định intent và example ạ, kh phải nó dc train rồi hay sao ạ. Em là newbie nên hỏi có khi hơi ngô nghê, mong mng bỏ qua. Cảm ơn mng đã giải đáp",,,"#Q&A, #nlp",,
"Hi mọi người, sau cuộc thi ZaloAi thì mình thấy Cinnamon AI marathon là cuộc thi có nhiều thú vị.
Trong cuộc thi này có 3 đề tài chính
- Handwriting OCR for Vietnamese Address
- Document Layout Analysis
- Real Time Facial LandMark Detection
Do mình thấy cuộc thi này khá thú vị mà không có nhiều cộng đồng support nên mình cung cấp code base của đề bài OCR cho mọi người tham khảo.
Về bài toán OCR thì chung ta cần nhận diện các kí tự trong hình ảnh scan của một đoạn text. Bộ dữ liệu gồm 2000 mẫu.
Mô hình mình sủ dụng là CRNN +CTCLoss. CNN dùng để extract features, sau đó đươc đưa vào RNN để nhận dạng kí tự tại timestep hiện tại.
Kết quả mình thấy khá khả quan, nhận diện cũng tương đối chính xác với normalize editdistance 0.28x.
Bạn nào có hưng thú tìm hiểu về OCR cũng như cuộc thi thì có thể tham khảo codebase này nhé.
----------------------------------------------
https://github.com/pbcquoc/vietnamese_ocr","Hi mọi người, sau cuộc thi ZaloAi thì mình thấy Cinnamon AI marathon là cuộc thi có nhiều thú vị. Trong cuộc thi này có 3 đề tài chính - Handwriting OCR for Vietnamese Address - Document Layout Analysis - Real Time Facial LandMark Detection Do mình thấy cuộc thi này khá thú vị mà không có nhiều cộng đồng support nên mình cung cấp code base của đề bài OCR cho mọi người tham khảo. Về bài toán OCR thì chung ta cần nhận diện các kí tự trong hình ảnh scan của một đoạn text. Bộ dữ liệu gồm 2000 mẫu. Mô hình mình sủ dụng là CRNN +CTCLoss. CNN dùng để extract features, sau đó đươc đưa vào RNN để nhận dạng kí tự tại timestep hiện tại. Kết quả mình thấy khá khả quan, nhận diện cũng tương đối chính xác với normalize editdistance 0.28x. Bạn nào có hưng thú tìm hiểu về OCR cũng như cuộc thi thì có thể tham khảo codebase này nhé. ---------------------------------------------- https://github.com/pbcquoc/vietnamese_ocr",,,"#sharing, #cv, #nlp",,
"Hello mọi người,
Hiện nay mình đang tìm hiểu về imbalanced dataset, mọi người có ai có biết sách nào nói về vấn đề này không, sách được phát hành ở việt nam càng tốt 😁","Hello mọi người, Hiện nay mình đang tìm hiểu về imbalanced dataset, mọi người có ai có biết sách nào nói về vấn đề này không, sách được phát hành ở việt nam càng tốt",,,"#Q&A, #data",,
"Dường như các bạn có vẻ thích thú với chủ đề Machine Learning engineering mà mình post hôm qua tại đây (https://www.facebook.com/groups/machinelearningcoban/permalink/1784048968719169/).
Vậy nên, hôm nay mình chia sẻ tiếp một GitHub của Google có tới > 22k stars, có tên là Tuning_PlayBook về chủ đề làm sao ta có thể finetune pretrained models một cách hiệu quả cho downstream tasks như:
1/ Chọn kiến trúc models
2/ Chọn hàm tối ưu;
3/ Chọn Batch sizes;
4/ Huấn luyện model bao nhiêu epochs/steps;
5/ Đánh giá hiệu năng của models;
.... và nhiều thông tin thú vị khác nữa!!!
Các bạn có thể tham khảo repository này tại đây: https://github.com/google-research/tuning_playbook#deep-learning-tuning-playbook
Chúc các bạn thành công trong việc tối ưu dự án và quá trình học tập của mình!","Dường như các bạn có vẻ thích thú với chủ đề Machine Learning engineering mà mình post hôm qua tại đây (https://www.facebook.com/groups/machinelearningcoban/permalink/1784048968719169/). Vậy nên, hôm nay mình chia sẻ tiếp một GitHub của Google có tới > 22k stars, có tên là Tuning_PlayBook về chủ đề làm sao ta có thể finetune pretrained models một cách hiệu quả cho downstream tasks như: 1/ Chọn kiến trúc models 2/ Chọn hàm tối ưu; 3/ Chọn Batch sizes; 4/ Huấn luyện model bao nhiêu epochs/steps; 5/ Đánh giá hiệu năng của models; .... và nhiều thông tin thú vị khác nữa!!! Các bạn có thể tham khảo repository này tại đây: https://github.com/google-research/tuning_playbook#deep-learning-tuning-playbook Chúc các bạn thành công trong việc tối ưu dự án và quá trình học tập của mình!",,,"#sharing, #machine_learning",,
"Hôm trước khá nhiều người hỏi về quy trình thu thập dữ liệu cũng như training của LLM Vietcuna từ VILM. Hôm nay team VILM chính thức công bố toàn bộ quy trình để làm ra Vietcuna.
Với hướng dẫn này kèm với model Vietcuna đã có sẵn, mong sẽ có nhiều bạn và doanh nghiệp Việt Nam tạo ra các sản phẩm AI mang tính thực tế cao trong cuộc sống :)
Link:","Hôm trước khá nhiều người hỏi về quy trình thu thập dữ liệu cũng như training của LLM Vietcuna từ VILM. Hôm nay team VILM chính thức công bố toàn bộ quy trình để làm ra Vietcuna. Với hướng dẫn này kèm với model Vietcuna đã có sẵn, mong sẽ có nhiều bạn và doanh nghiệp Việt Nam tạo ra các sản phẩm AI mang tính thực tế cao trong cuộc sống :) Link:",,,"#sharing, #data",,
"Chào mọi người! Mình đang cần train model với bộ data khoảng 129GB dạng zip, nhưng với colab free thì nó chỉ có khoảng 70 gb ổ nhớ free. Có cách nào khác để train không giúp mình với ạ. Mong mn giúp đỡ ạ.","Chào mọi người! Mình đang cần train model với bộ data khoảng 129GB dạng zip, nhưng với colab free thì nó chỉ có khoảng 70 gb ổ nhớ free. Có cách nào khác để train không giúp mình với ạ. Mong mn giúp đỡ ạ.",,,"#Q&A, #data",,
"Hôm trước mình thấy có một số câu hỏi về server và colab, thực sự, mình cũng đã gặp khá nhiều vấn đề với giới hạn thời gian sử dụng colab, giới hạn dung lượng RAM và giới hạn phần cứng. Mình đã phải mua gói pro hoặc thậm chí là gói pro+ để khắc phục vấn đề này. Tuy nhiên, việc đó vẫn không hoàn toàn thuận lợi vì đôi khi mình quên tắt colab, dẫn đến việc nó treo và tiếp tục trừ tài khoản tính tiền, như gói pro+ giá $49 mỗi tháng. Gần đây, mình đã tìm ra một giải pháp tốt hơn là sử dụng Gradient. Gradient cung cấp cho chúng ta một container Docker hoàn chỉnh cho mỗi notebook, và dữ liệu được gắn kết trực tiếp với máy chủ chính. Hơn nữa, phiên bản miễn phí của Gradient cung cấp 5GB dung lượng lưu trữ vĩnh viễn, dữ liệu trong đó không bị mất, ngay cả khi notebook gặp vấn đề và mình xóa toàn bộ. Bạn có thể sử dụng Gradient miễn phí trong 6 giờ liên tục cho mỗi phiên notebook, nó sẽ không bị tắt giữa chừng như colab. Điều thú vị là với gói pro chỉ $8 mỗi tháng, bạn có thể trải nghiệm nhiều cấu hình miễn phí tuyệt vời, ví dụ như P5000 với 30GB RAM, 8G CPU và 16GB GPU, hoặc A400 với 45GB RAM, 8G CPU và 16GB GPU... Quan trọng là sau 6 giờ, bạn có thể kết nối và tiếp tục sử dụng với khoản phí $8 đó, như một khoản phí duy trì. Nếu bạn đăng ký gói GROWTH, bạn sẽ có nhiều máy chủ miễn phí hơn, thậm chí có A100 với 80GB GPU miễn phí :)). Tuy nhiên, với phiên bản miễn phí, sau 6 giờ sử dụng, nó sẽ tự động tắt và bạn cần tìm một máy chủ khác để kết nối tiếp. Tóm lại, mình thấy Gradient ngon hơn nhiều so với colab 😂😂.","Hôm trước mình thấy có một số câu hỏi về server và colab, thực sự, mình cũng đã gặp khá nhiều vấn đề với giới hạn thời gian sử dụng colab, giới hạn dung lượng RAM và giới hạn phần cứng. Mình đã phải mua gói pro hoặc thậm chí là gói pro+ để khắc phục vấn đề này. Tuy nhiên, việc đó vẫn không hoàn toàn thuận lợi vì đôi khi mình quên tắt colab, dẫn đến việc nó treo và tiếp tục trừ tài khoản tính tiền, như gói pro+ giá $49 mỗi tháng. Gần đây, mình đã tìm ra một giải pháp tốt hơn là sử dụng Gradient. Gradient cung cấp cho chúng ta một container Docker hoàn chỉnh cho mỗi notebook, và dữ liệu được gắn kết trực tiếp với máy chủ chính. Hơn nữa, phiên bản miễn phí của Gradient cung cấp 5GB dung lượng lưu trữ vĩnh viễn, dữ liệu trong đó không bị mất, ngay cả khi notebook gặp vấn đề và mình xóa toàn bộ. Bạn có thể sử dụng Gradient miễn phí trong 6 giờ liên tục cho mỗi phiên notebook, nó sẽ không bị tắt giữa chừng như colab. Điều thú vị là với gói pro chỉ $8 mỗi tháng, bạn có thể trải nghiệm nhiều cấu hình miễn phí tuyệt vời, ví dụ như P5000 với 30GB RAM, 8G CPU và 16GB GPU, hoặc A400 với 45GB RAM, 8G CPU và 16GB GPU... Quan trọng là sau 6 giờ, bạn có thể kết nối và tiếp tục sử dụng với khoản phí $8 đó, như một khoản phí duy trì. Nếu bạn đăng ký gói GROWTH, bạn sẽ có nhiều máy chủ miễn phí hơn, thậm chí có A100 với 80GB GPU miễn phí :)). Tuy nhiên, với phiên bản miễn phí, sau 6 giờ sử dụng, nó sẽ tự động tắt và bạn cần tìm một máy chủ khác để kết nối tiếp. Tóm lại, mình thấy Gradient ngon hơn nhiều so với colab .",,,#sharing,,
"Xin chào mọi người, e đang làm nhãn cho bài toán phân loại ảnh . Em có 1 thắc mắc là : với ảnh có độ phân giải nhỏ ( 100x100 ), chất lượng hơi kém, kích thước của đồ vật cần phân loại nhỏ trong ảnh, và cũng hơi mờ thì nên phân loại ảnh là negative hay positive ạ. Nếu là mắt con người thì có thể đoán được là đồ vật đó ( positive ), nhưng để là máy đoán dc thì quá khó , Ví dụ những ảnh dưới đây chứa xe đạp ( mắt người thì đoán đc ),nhưng để AI đoán được là xe đạp thì quá khó. Theo mọi người thì nên phân loại gì ạ ?. Em cảm ơn nhiều ,","Xin chào mọi người, e đang làm nhãn cho bài toán phân loại ảnh . Em có 1 thắc mắc là : với ảnh có độ phân giải nhỏ ( 100x100 ), chất lượng hơi kém, kích thước của đồ vật cần phân loại nhỏ trong ảnh, và cũng hơi mờ thì nên phân loại ảnh là negative hay positive ạ. Nếu là mắt con người thì có thể đoán được là đồ vật đó ( positive ), nhưng để là máy đoán dc thì quá khó , Ví dụ những ảnh dưới đây chứa xe đạp ( mắt người thì đoán đc ),nhưng để AI đoán được là xe đạp thì quá khó. Theo mọi người thì nên phân loại gì ạ ?. Em cảm ơn nhiều ,",,,"#Q&A, #data",,
"Hi mọi người,
Em đang tính làm một dự án vớ Llama 2. Em có một thắc mắc về data privacy.Như khi em dùng chatGPT, thì mọi thông tin từ prompt, hoặc data mà mình dùng để fine-tune sẽ được đưa về OpenAI. Vậy cho em hỏi nếu em fine-tuned Llama 2 model, thì data mình dùng để fine-tune hoặc prompt của mình khi sử dụng model có bị lưu bởi Meta k ạ? Em cảm ơn!","Hi mọi người, Em đang tính làm một dự án vớ Llama 2. Em có một thắc mắc về data privacy.Như khi em dùng chatGPT, thì mọi thông tin từ prompt, hoặc data mà mình dùng để fine-tune sẽ được đưa về OpenAI. Vậy cho em hỏi nếu em fine-tuned Llama 2 model, thì data mình dùng để fine-tune hoặc prompt của mình khi sử dụng model có bị lưu bởi Meta k ạ? Em cảm ơn!",,,"#Q&A, #data, #nlp",,
"Ngày hội Trí tuệ nhân tạo Việt Nam (AI4VN 2023) có chủ đề ""Sức mạnh cho cuộc sống"" với bốn hoạt động chính: AI workshop; AI Summit; AI Expo; CTO Summit 2023 - vinh danh các công ty có môi trường công nghệ tốt nhất.Phiên khai mạc ngày 21/9 mang đến báo cáo Chỉ số sẵn sàng AI của chính phủ năm 2022, trình bày bởi ông Pablo Fuentes Nettel - Chuyên gia tư vấn cấp cao tại Oxford Insights. Ngay sau đó là các phần tham luận về ứng dụng AI trong tương lai, kinh nghiệm triển khai thực tế tại Hàn Quốc, làm chủ AI tạo sinh... qua góc nhìn của các chuyên gia VinBigdata, Naver, VNPT, Aqua.
#AI4VN #VietAI #AI4VN2023","Ngày hội Trí tuệ nhân tạo Việt Nam (AI4VN 2023) có chủ đề ""Sức mạnh cho cuộc sống"" với bốn hoạt động chính: AI workshop; AI Summit; AI Expo; CTO Summit 2023 - vinh danh các công ty có môi trường công nghệ tốt nhất.Phiên khai mạc ngày 21/9 mang đến báo cáo Chỉ số sẵn sàng AI của chính phủ năm 2022, trình bày bởi ông Pablo Fuentes Nettel - Chuyên gia tư vấn cấp cao tại Oxford Insights. Ngay sau đó là các phần tham luận về ứng dụng AI trong tương lai, kinh nghiệm triển khai thực tế tại Hàn Quốc, làm chủ AI tạo sinh... qua góc nhìn của các chuyên gia VinBigdata, Naver, VNPT, Aqua.",#AI4VN	#VietAI	#AI4VN2023,,#sharing,,
"Yeahhhhh! 🌟
Mình rất vui giới thiệu VietAssistant-GPT phiên bản 1.0 🇻🇳. Là một trợ lý đa năng  (general domain)..
Dựa trên phát triển của open-source LLaMa 2, phiên bản này đã được finetune để cung cấp cho bạn những câu trả hữu ích hơn trong nhiều lĩnh vực trên Tiếng Việt. Và đang trong quá trình cải thiện📚
Finetuned model và dataset được cung cấp ở link sau:
https://github.com/VietnamAIHub/Vietnamese_LLMs
Demo Link:
Vietnamese llama2 13B  Demo https://c4242d50778850141a.gradio.live/
Vietnamese llama2 7B Model Demo https://31fee86ed135939f28.gradio.live
Rất mong chờ sự phản hồi và ý kiến của mọi người ! 🙌","Yeahhhhh! Mình rất vui giới thiệu VietAssistant-GPT phiên bản 1.0 . Là một trợ lý đa năng (general domain).. Dựa trên phát triển của open-source LLaMa 2, phiên bản này đã được finetune để cung cấp cho bạn những câu trả hữu ích hơn trong nhiều lĩnh vực trên Tiếng Việt. Và đang trong quá trình cải thiện Finetuned model và dataset được cung cấp ở link sau: https://github.com/VietnamAIHub/Vietnamese_LLMs Demo Link: Vietnamese llama2 13B Demo https://c4242d50778850141a.gradio.live/ Vietnamese llama2 7B Model Demo https://31fee86ed135939f28.gradio.live Rất mong chờ sự phản hồi và ý kiến của mọi người !",,,"#sharing, #nlp",,
"Không biết có ace bạn nào có biết qua bài này không ạ?
NASA: Neural Articulated Shape Approximation
Cho mình hỏi vài câu hỏi bên dưới (chắc sẽ còn hỏi thêm):
1/ Query ở đây là cái gì ??
- Phần tóm tắt contribution có câu sau: 
""The differentiable occupancy supports efficient constant time queries, avoiding the need to convert to separate representations, or the dynamic update of spatial acceleration data structures;"" 
- Phần related work có nói:
Object intersection queries: Registration, template matching, 3D tracking, collision detection, and other tasks require efficient inside/outside tests. A disadvantage of polygonal meshes is that they do not efficiently support these queries, as meshes often contain thousands of individual triangles that must be tested for each query. This has led to the development of a variety of spatial data structures to accelerate point-object queries
Kết hợp thêm cái hình Figure 1 này. chỗ O(x|θ) là cái gì ? nhìn khó hiểu.
Chỗ này em thực sự chẳng hiểu cái query ở đây là query cái gì?
Rốt cuộc input/output của paper này là gì ? (Thấy nó được train trên AMASS dataset? vậy input là pointcloud ?
Tại sao phần demo ứng dụng thì lại chọn tracking body?
(Em ko rành bên 3D CV này mà bị gán cho presentation trong môn học mà chỉ có vài ngày chuẩn bị nên hơi quá tải).
Hi vọng có ace nào biết chỉ giáo với ạ.
Em cảm ơn 😊","Không biết có ace bạn nào có biết qua bài này không ạ? NASA: Neural Articulated Shape Approximation Cho mình hỏi vài câu hỏi bên dưới (chắc sẽ còn hỏi thêm): 1/ Query ở đây là cái gì ?? - Phần tóm tắt contribution có câu sau: ""The differentiable occupancy supports efficient constant time queries, avoiding the need to convert to separate representations, or the dynamic update of spatial acceleration data structures;"" - Phần related work có nói: Object intersection queries: Registration, template matching, 3D tracking, collision detection, and other tasks require efficient inside/outside tests. A disadvantage of polygonal meshes is that they do not efficiently support these queries, as meshes often contain thousands of individual triangles that must be tested for each query. This has led to the development of a variety of spatial data structures to accelerate point-object queries Kết hợp thêm cái hình Figure 1 này. chỗ O(x|θ) là cái gì ? nhìn khó hiểu. Chỗ này em thực sự chẳng hiểu cái query ở đây là query cái gì? Rốt cuộc input/output của paper này là gì ? (Thấy nó được train trên AMASS dataset? vậy input là pointcloud ? Tại sao phần demo ứng dụng thì lại chọn tracking body? (Em ko rành bên 3D CV này mà bị gán cho presentation trong môn học mà chỉ có vài ngày chuẩn bị nên hơi quá tải). Hi vọng có ace nào biết chỉ giáo với ạ. Em cảm ơn",,,"#Q&A, #cv, #math",,
"Mình thấy giờ nhiều bạn lo hết việc, sợ AI thay thế, sợ cạnh tranh cao... Tuy nhiên theo mình thì nhu cầu công việc ngoài kia không thiếu, cái thiếu ở đây là thiếu người giỏi và người phù hợp. Vậy nên, có những thứ các bạn không thể kiểm soát được thì không nên tốn năng lượng làm gì, cái các bạn cần là hãy nâng cao năng lực, giỏi tới mức người ta không thể ngó lơ thì mọi thứ sẽ tuyệt vời hơn rất nhiều.","Mình thấy giờ nhiều bạn lo hết việc, sợ AI thay thế, sợ cạnh tranh cao... Tuy nhiên theo mình thì nhu cầu công việc ngoài kia không thiếu, cái thiếu ở đây là thiếu người giỏi và người phù hợp. Vậy nên, có những thứ các bạn không thể kiểm soát được thì không nên tốn năng lượng làm gì, cái các bạn cần là hãy nâng cao năng lực, giỏi tới mức người ta không thể ngó lơ thì mọi thứ sẽ tuyệt vời hơn rất nhiều.",,,#sharing,,
"Khai phá dữ liệu là một phần việc rất quan trọng để hiểu về dữ liệu cho các mục đích phân tích chuyên sâu tiếp theo. Đây cuốn sách gần 400 trang hướng dẫn biểu diễn dữ liệu sử dụng Python (Matplotlib và Seaborn) như: (1) một số kĩ thuật lựa chọn dạng biểu đồ phù hợp; (2) xử lí missing data/outliers; (3) phần biểu diễn dữ liệu địa lý (bản đồ~geospatial data); và (4) biểu diễn dữ liệu dạng 3D tại đây https://theswissbay.ch/pdf/Gentoomen%20Library/Programming/Python/Beginning%20Python%20Visualization%20-%20Crafting%20Visual%20Transformation%20Scripts%20%282009%29.pdf; https://pyoflife.com/beginning-python-visualization-crafting-visual-transformation-scripts/
Hi vọng nó có ích với mọi người",Khai phá dữ liệu là một phần việc rất quan trọng để hiểu về dữ liệu cho các mục đích phân tích chuyên sâu tiếp theo. Đây cuốn sách gần 400 trang hướng dẫn biểu diễn dữ liệu sử dụng Python (Matplotlib và Seaborn) như: (1) một số kĩ thuật lựa chọn dạng biểu đồ phù hợp; (2) xử lí missing data/outliers; (3) phần biểu diễn dữ liệu địa lý (bản đồ~geospatial data); và (4) biểu diễn dữ liệu dạng 3D tại đây https://theswissbay.ch/pdf/Gentoomen%20Library/Programming/Python/Beginning%20Python%20Visualization%20-%20Crafting%20Visual%20Transformation%20Scripts%20%282009%29.pdf; https://pyoflife.com/beginning-python-visualization-crafting-visual-transformation-scripts/ Hi vọng nó có ích với mọi người,,,"#sharing, #data",,
"Dạo này em đọc về RLHF (Reinforcement learning from human feedback) thấy khá thú vị + practical in production khá cao. Em đang tính làm 1 project để extract features từ description của sản phẩm, trong đó model nhận feedback từ người dùng để đánh giá xem output của model chính xác tới cỡ nào và model dùng đó để reward/penalty trong loss function.
Không biết các bác có kinh nghiệm gì về mảng này không ạ?","Dạo này em đọc về RLHF (Reinforcement learning from human feedback) thấy khá thú vị + practical in production khá cao. Em đang tính làm 1 project để extract features từ description của sản phẩm, trong đó model nhận feedback từ người dùng để đánh giá xem output của model chính xác tới cỡ nào và model dùng đó để reward/penalty trong loss function. Không biết các bác có kinh nghiệm gì về mảng này không ạ?",,,"#Q&A, #deep_learning",,
"Xin chào mọi người, hiện tại em đang làm đồ án tốt nghiệp với đề tại là trích xuất nội dung từ danh thiếp bằng OCR , tuy nhiên do data ít nên em đăng bài này mong muốn kiếm một lượng lớn data về danh thiếp để bổ sung vào đatn ạ, nếu ai có thì cho em xin hoặc mua lại ạ, em xin cảm ơn mọi người nhiều ạ","Xin chào mọi người, hiện tại em đang làm đồ án tốt nghiệp với đề tại là trích xuất nội dung từ danh thiếp bằng OCR , tuy nhiên do data ít nên em đăng bài này mong muốn kiếm một lượng lớn data về danh thiếp để bổ sung vào đatn ạ, nếu ai có thì cho em xin hoặc mua lại ạ, em xin cảm ơn mọi người nhiều ạ",,,"#Q&A, #cv, #data",,
"Xin chào mọi người!
Hiện tại mình đang thực hiện 1 Project cho Product, ứng dụng LLM. Tuy nhiên do data ít nên mình đăng bài này với mong muốn tìm kiếm thêm data để Project được hoàn thiện.
Data mình cần là ảnh các loại hóa đơn thương mại. Nếu ai có nguồn data như vậy thì có thể cho mình xin (hoặc mua nếu cần) được không ạ?
Mình xin cảm ơn ạ!","Xin chào mọi người! Hiện tại mình đang thực hiện 1 Project cho Product, ứng dụng LLM. Tuy nhiên do data ít nên mình đăng bài này với mong muốn tìm kiếm thêm data để Project được hoàn thiện. Data mình cần là ảnh các loại hóa đơn thương mại. Nếu ai có nguồn data như vậy thì có thể cho mình xin (hoặc mua nếu cần) được không ạ? Mình xin cảm ơn ạ!",,,"#Q&A, #data",,
"Vậy là Mojo, ngôn ngữ mới cho ML/DL/AI, đã cho phép cài đặt trên máy tính cá nhân. Hiện tại, Mojo mới hỗ trợ hệ điều hành Linux. Các bạn có thể test bằng cách đăng kí và cài đặt tại đây","Vậy là Mojo, ngôn ngữ mới cho ML/DL/AI, đã cho phép cài đặt trên máy tính cá nhân. Hiện tại, Mojo mới hỗ trợ hệ điều hành Linux. Các bạn có thể test bằng cách đăng kí và cài đặt tại đây",,,#sharing,,
"#question
Mn có thể recommend cho e 1 vài kênh học lý thuyết ML từ basic không ạ. Em muốn nắm dc basic ML một cách nhanh chóng nhưng cảm giác bị hổng đâu đó về lý thuyết nhưng không biết là chỗ nào. Em cảm ơn",Mn có thể recommend cho e 1 vài kênh học lý thuyết ML từ basic không ạ. Em muốn nắm dc basic ML một cách nhanh chóng nhưng cảm giác bị hổng đâu đó về lý thuyết nhưng không biết là chỗ nào. Em cảm ơn,#question,,"#Q&A, #machine_learning",,
"Dạ anh chị nào có tài liệu bài tập về Linear Regression và  Logistic Regression , cho em xin ạ . Dạ em cảm  ơn  ","Dạ anh chị nào có tài liệu bài tập về Linear Regression và Logistic Regression , cho em xin ạ . Dạ em cảm ơn",,,"#Q&A, #machine_learning",,
Mọi người cho e hỏi làm thế nào để dịch từ tiếng anh về tiếng việt cho nó tự nhiên hơn không ạ (không dùng chatgpt và google)?,Mọi người cho e hỏi làm thế nào để dịch từ tiếng anh về tiếng việt cho nó tự nhiên hơn không ạ (không dùng chatgpt và google)?,,,#Q&A,,
"Xin chào các anh chị và các bạn,
Mình làm về dự báo mức tiêu thụ năng lượng của một quốc gia ở Mỹ Latin, dữ liệu dạng time series được tổng kết cuối năm.
Mình đọc thì có một số mô hình dự báo tốt như ARIMA và LSTM,... trong đó thì mình cảm thấy thích ARIMA hơn vì nó dễ và kết quả cũng tốt.
Các anh chị nào có tài liệu để đọc về vấn đề này có thể giới thiệu cho mình được không ạ? mình hơi lơ mơ về cách chọn các tham số p,d,q, đặc biệt là khi số lượng biến nhiều mà lại làm việc với univariate (do mình còn kém kinh nghiệm).
Nếu anh chị nào có thể chia sẻ được về phần này mình rất cảm ơn ạ.","Xin chào các anh chị và các bạn, Mình làm về dự báo mức tiêu thụ năng lượng của một quốc gia ở Mỹ Latin, dữ liệu dạng time series được tổng kết cuối năm. Mình đọc thì có một số mô hình dự báo tốt như ARIMA và LSTM,... trong đó thì mình cảm thấy thích ARIMA hơn vì nó dễ và kết quả cũng tốt. Các anh chị nào có tài liệu để đọc về vấn đề này có thể giới thiệu cho mình được không ạ? mình hơi lơ mơ về cách chọn các tham số p,d,q, đặc biệt là khi số lượng biến nhiều mà lại làm việc với univariate (do mình còn kém kinh nghiệm). Nếu anh chị nào có thể chia sẻ được về phần này mình rất cảm ơn ạ.",,,"#Q&A, #deep_learning",,
"Mình có làm một game basic về ML và xử lí ảnh Computer Vision bằng python, project này chủ yếu là demo cho học sinh và các bạn sinh viên của mình. Bạn nào thích thì có thể tham khảo và tải về nhé 😀","Mình có làm một game basic về ML và xử lí ảnh Computer Vision bằng python, project này chủ yếu là demo cho học sinh và các bạn sinh viên của mình. Bạn nào thích thì có thể tham khảo và tải về nhé",,,"#sharing, #cv",,
"Morning mn, mọi nghĩ sao về việc mình dùng chat gpt api,... các ai khác nữa để convert video về dạng text để lấy các keywords xong sau đó mình dựa vào các keywords để match với các khách hàng care về sản phẩm đó và dựa vào content của video đó để generate cái email marketing ạ. này ideal để làm automation dựa trên các con AI hiện tại như chat-gpt, zapier,..","Morning mn, mọi nghĩ sao về việc mình dùng chat gpt api,... các ai khác nữa để convert video về dạng text để lấy các keywords xong sau đó mình dựa vào các keywords để match với các khách hàng care về sản phẩm đó và dựa vào content của video đó để generate cái email marketing ạ. này ideal để làm automation dựa trên các con AI hiện tại như chat-gpt, zapier,..",,,#Q&A,,
"Em xin phép admin được chia sẻ tới các thành viên group mình cuộc thi về trí tuệ nhân tạo do FPT tổ chức.
CUỘC THI ỨNG DỤNG TRÍ TUỆ NHÂN TẠO VỚI TỔNG GIÁ TRỊ GIẢI THƯỞNG 370 TRIỆU ĐỒNG
FPT AI Challenge 2023 - cuộc thi về trí tuệ nhân tạo lớn nhất do Tập đoàn FPT tổ chức. Với tổng giải thưởng 370 triệu đồng, cuộc thi tìm kiếm những ý tưởng, sản phẩm và giải pháp ứng dụng trí tuệ nhân tạo giúp giải quyết các bài toán thực tế trong đời sống. Đây là sân chơi kỹ thuật dành cho những người có niềm đam mê với AI trên toàn thế giới.
Thời gian đăng ký: từ 12h00 ngày 09.09 - 10.10.2023
Các thí sinh tham gia cuộc thi theo hình thức cá nhân hoặc nhóm với tối đa 3 thành viên và đăng ký tại trang web https://hackathon.quynhon.ai/registration.
Ở Vòng loại, các đội thi sẽ chọn cho mình chủ đề phù hợp và tiến hành xây dựng những sản phẩm/giải pháp giúp giải quyết bài toán mà Ban Tổ chức đưa ra. 06 đội thi có điểm số cao nhất sẽ vào Vòng Chung kết.
Ba chủ đề của cuộc thi FPT AI Challenge 2023 bao gồm:
- AI-enable optimization for Binh Dinh province
- Leveraging large language models (LLMs) for sustainable business recommendations:
- AI techniques for computer vision applications
Tại Vòng Chung kết, top 06 ý tưởng sáng tạo nhất sẽ trình bày ý tưởng về sản phẩm và giải pháp của mình với Ban Giám khảo qua hình thức online. Top 03 chung cuộc sẽ được tham dự Lễ trao giải và trưng bày sản phẩm trực tiếp tại sự kiện triển lãm công nghệ FPT Tech Day 2023.
Đội chiến thắng sẽ nhận được giải thưởng tiền mặt với giải nhất trị giá 200 triệu đồng, giải nhì 100 triệu đồng, giải ba 70 triệu đồng, cùng giấy chứng nhận của Ban Tổ chức. Top 3 chung cuộc sẽ được tài trợ một tuần du lịch tại Việt Nam, tham gia giao lưu văn hoá quốc tế và triển lãm sản phẩm của mình tại sự kiện FPT Tech Day 2023 được tổ chức vào tháng 10.2023 tại Hà Nội. Ngoài ra, các đội thi sẽ có cơ hội tham gia những buổi tham luận cùng với các chuyên gia và kỹ sư đầu ngành; nhận được cơ hội việc làm với mức lương hấp dẫn tại công ty công nghệ hàng đầu Việt Nam.
Thông tin chi tiết của cuộc thi được đăng tải trên website: https://hackathon.quynhon.ai/","Em xin phép admin được chia sẻ tới các thành viên group mình cuộc thi về trí tuệ nhân tạo do FPT tổ chức. CUỘC THI ỨNG DỤNG TRÍ TUỆ NHÂN TẠO VỚI TỔNG GIÁ TRỊ GIẢI THƯỞNG 370 TRIỆU ĐỒNG FPT AI Challenge 2023 - cuộc thi về trí tuệ nhân tạo lớn nhất do Tập đoàn FPT tổ chức. Với tổng giải thưởng 370 triệu đồng, cuộc thi tìm kiếm những ý tưởng, sản phẩm và giải pháp ứng dụng trí tuệ nhân tạo giúp giải quyết các bài toán thực tế trong đời sống. Đây là sân chơi kỹ thuật dành cho những người có niềm đam mê với AI trên toàn thế giới. Thời gian đăng ký: từ 12h00 ngày 09.09 - 10.10.2023 Các thí sinh tham gia cuộc thi theo hình thức cá nhân hoặc nhóm với tối đa 3 thành viên và đăng ký tại trang web https://hackathon.quynhon.ai/registration. Ở Vòng loại, các đội thi sẽ chọn cho mình chủ đề phù hợp và tiến hành xây dựng những sản phẩm/giải pháp giúp giải quyết bài toán mà Ban Tổ chức đưa ra. 06 đội thi có điểm số cao nhất sẽ vào Vòng Chung kết. Ba chủ đề của cuộc thi FPT AI Challenge 2023 bao gồm: - AI-enable optimization for Binh Dinh province - Leveraging large language models (LLMs) for sustainable business recommendations: - AI techniques for computer vision applications Tại Vòng Chung kết, top 06 ý tưởng sáng tạo nhất sẽ trình bày ý tưởng về sản phẩm và giải pháp của mình với Ban Giám khảo qua hình thức online. Top 03 chung cuộc sẽ được tham dự Lễ trao giải và trưng bày sản phẩm trực tiếp tại sự kiện triển lãm công nghệ FPT Tech Day 2023. Đội chiến thắng sẽ nhận được giải thưởng tiền mặt với giải nhất trị giá 200 triệu đồng, giải nhì 100 triệu đồng, giải ba 70 triệu đồng, cùng giấy chứng nhận của Ban Tổ chức. Top 3 chung cuộc sẽ được tài trợ một tuần du lịch tại Việt Nam, tham gia giao lưu văn hoá quốc tế và triển lãm sản phẩm của mình tại sự kiện FPT Tech Day 2023 được tổ chức vào tháng 10.2023 tại Hà Nội. Ngoài ra, các đội thi sẽ có cơ hội tham gia những buổi tham luận cùng với các chuyên gia và kỹ sư đầu ngành; nhận được cơ hội việc làm với mức lương hấp dẫn tại công ty công nghệ hàng đầu Việt Nam. Thông tin chi tiết của cuộc thi được đăng tải trên website: https://hackathon.quynhon.ai/",,,#sharing,,
MACHINE LEARNING PAPERs EXPLAINED,MACHINE LEARNING PAPERs EXPLAINED,,,,,
"Em xin chào mọi người. Em đang thử train model text recognition sử dụng mô hình CRNN-CTC với tool từ PaddleOCR. Em train thử 100 epochs trên bộ dữ liệu ICDAR2015, với pretrained model từ PaddleOCR (https://paddleocr.bj.bcebos.com/dygraph_v2.0/en/rec_mv3_none_bilstm_ctc_v2.0_train.tar). Kết quả model bị overfit mạnh trên tập ICDAR2015 ạ, training accurracy tăng rất nhanh (khúc sau tới gần 100%), nhưng kết quả test lại rất thấp chỉ 51.4% (em có đính kèm hình bên dưới).
Em muốn xin hỏi làm sao để khắc phục vấn đề này ạ. Em có tìm hiểu trên mạng thì có 1 chỗ ghi là do bộ ICDAR2015 này ít ảnh (cỡ 4 ngàn mấy) nên bị overfit. Người ta train thì dùng các bộ dataset lớn như SynthText (9 triệu ảnh). Tuy nhiên em dùng Google Colab nên không đủ bộ nhớ để train những tập lớn như vậy (với tập ICDAR2015 mà khi train bộ nhớ CPU GPU colab đã gần bị overflow rồi). Không biết mọi người có link về các bộ dataset nào có kích thước vừa phải có thể dùng để train thử nghiệm trên Google Colab cho bài toán scene text recognition không ạ?
Em xin cảm ơn mọi người nhiều ạ.","Em xin chào mọi người. Em đang thử train model text recognition sử dụng mô hình CRNN-CTC với tool từ PaddleOCR. Em train thử 100 epochs trên bộ dữ liệu ICDAR2015, với pretrained model từ PaddleOCR (https://paddleocr.bj.bcebos.com/dygraph_v2.0/en/rec_mv3_none_bilstm_ctc_v2.0_train.tar). Kết quả model bị overfit mạnh trên tập ICDAR2015 ạ, training accurracy tăng rất nhanh (khúc sau tới gần 100%), nhưng kết quả test lại rất thấp chỉ 51.4% (em có đính kèm hình bên dưới). Em muốn xin hỏi làm sao để khắc phục vấn đề này ạ. Em có tìm hiểu trên mạng thì có 1 chỗ ghi là do bộ ICDAR2015 này ít ảnh (cỡ 4 ngàn mấy) nên bị overfit. Người ta train thì dùng các bộ dataset lớn như SynthText (9 triệu ảnh). Tuy nhiên em dùng Google Colab nên không đủ bộ nhớ để train những tập lớn như vậy (với tập ICDAR2015 mà khi train bộ nhớ CPU GPU colab đã gần bị overflow rồi). Không biết mọi người có link về các bộ dataset nào có kích thước vừa phải có thể dùng để train thử nghiệm trên Google Colab cho bài toán scene text recognition không ạ? Em xin cảm ơn mọi người nhiều ạ.",,,"#Q&A, #data, #cv",,
"Giáo sư Dunhui Deng, Bộ môn KHMT của Đại học Thanh Hoa (Tsinghua University) bên TQ có giới thiệu cuốn sách về Algorithims viết cho cho các ngôn ngữ C+(+, Python, Go, JavaScript, TypeScript, C, C#, Swift, Zig, Rust, and Dart.
Rất tiếc sách viết bằng tiếng tàu, tuy nhiên, mình đã thử dụng một số công cụ dịch như Google Translate, Bard, Claude-2 và ChatGPT-3.5 thì thấy chất lượng dịch ra tiếng Anh và tiếng Việt khá ổn.
Vì vậy, mình xin giới thiệu source code của cuốn sách này tại đây https://github.com/krahets/hello-algo

Và có bạn đã tìm thấy branch bản dịch tiếng Anh của cuốn sách và source code tại đây https://github.com/yuelinxin/hello-algo-en?fbclid=IwAR310jM6QwOOYgEtgAdCVJQOdOWnKmpiEcFN0F00rhq_5dNIUN04VIP0nck
Cảm ơn bạn Nghia Be 

Chúc các bạn có ngày cuối tuần vui vẻ!","Giáo sư Dunhui Deng, Bộ môn KHMT của Đại học Thanh Hoa (Tsinghua University) bên TQ có giới thiệu cuốn sách về Algorithims viết cho cho các ngôn ngữ C+(+, Python, Go, JavaScript, TypeScript, C, C#, Swift, Zig, Rust, and Dart. Rất tiếc sách viết bằng tiếng tàu, tuy nhiên, mình đã thử dụng một số công cụ dịch như Google Translate, Bard, Claude-2 và ChatGPT-3.5 thì thấy chất lượng dịch ra tiếng Anh và tiếng Việt khá ổn. Vì vậy, mình xin giới thiệu source code của cuốn sách này tại đây https://github.com/krahets/hello-algo Và có bạn đã tìm thấy branch bản dịch tiếng Anh của cuốn sách và source code tại đây https://github.com/yuelinxin/hello-algo-en?fbclid=IwAR310jM6QwOOYgEtgAdCVJQOdOWnKmpiEcFN0F00rhq_5dNIUN04VIP0nck Cảm ơn bạn Nghia Be Chúc các bạn có ngày cuối tuần vui vẻ!",,,#sharing,,
"Chào mọi người,
Hiện tại em đang tìm hiểu một số bài toán về speech. Em tìm hiểu và có biết bộ dataset về giới tính và vùng miền Voice Gender của Zalo AI Challenger 2018. Em đã cố gắng tìm các nguồn public trên mạng nhưng vẫn chưa tìm được dữ liệu này. Trang chủ cuộc thi hiện nay là năm 2022 và không xem lại được dữ liệu năm trước.
Mọi người trong nhóm ai còn lưu trữ bộ dataset này cho em xin với ạ. Cảm ơn mọi người đã đọc bài.","Chào mọi người, Hiện tại em đang tìm hiểu một số bài toán về speech. Em tìm hiểu và có biết bộ dataset về giới tính và vùng miền Voice Gender của Zalo AI Challenger 2018. Em đã cố gắng tìm các nguồn public trên mạng nhưng vẫn chưa tìm được dữ liệu này. Trang chủ cuộc thi hiện nay là năm 2022 và không xem lại được dữ liệu năm trước. Mọi người trong nhóm ai còn lưu trữ bộ dataset này cho em xin với ạ. Cảm ơn mọi người đã đọc bài.",,,"#Q&A, #data",,
"🇻🇳 Cộng đồng LLMs Việt Nam!

Hiện tại, mình đang xây dựng một mô hình ngôn ngữ thuần Việt (#Vietnamese_LLM). Dự án của mình dựa trên sự phát triển của các mô hình ngôn ngữ  dựa trên Open-source LLM như #BLOOMZ, #Open_LlaMA, và nhiều mô hình #khác.
Thông tin sơ lược về dự án trình bày trong file đính kèm:
#Kế hoạch của mình bao gồm việc phát triển: #1. Tạo bộ dữ liệu Vietnamese (self-instruct dataset) và #2. Finetuning  và Training các Open-source LLMs trên bộ dữ liệu tiếng Việt.
#1. Phát triển bộ dữ liệu tiếngViệt (Self-Instruct Vietnamse):
+ Dựa trên các bộ dữ liệu tiếng Anh hiện có như #Alpaca, #Dolly, #OpenAssistant, #ShareGPT và các nhiều bộ dữ liệu khác. Mình đang sử dụng API Azure OpenAI: GPT3, GPT 3.5 và GPT-4 để dịch các bộ dữ liệu này sang tiếng Việt.
+ Tạo thêm 100.000 (có thể nhiều hơn) câu hướng dẫn tự học giống như dự án Alpaca hoặc các mô hình tạo dữ liệu khác.
+ Tìm hiểu thêm về các nguồn dữ liệu tiếng Việt khác tập trung cho các lĩnh vực khác nhau. như báo chí, điện ảnh, y học... (Mong nhận được ý kiến góp ý từ mọi người).
#2. Để huấn luyện (#Finetuning & Traning ) các mô hình ngôn ngữ LLM, mình sẽ sử dụng kỹ thuật #LoRA và #QLoRA trên máy chủ Azure server có 8 GPUs Nvidia #A100 80GB. Điều này sẽ giúp mình Finetune và Train các mô hình ngôn ngữ (LLM) từ 7B tỷ đến 65B tỷ hoặc nhiều hơn.
Mình rất mong sự kết nối với các bạn có mong muốn thực hiện dự án này và hoan nghênh sự đóng góp từ cộng đồng.
Nếu bạn có sự góp ý về dữ liệu hoặc , xin vui lòng chia sẻ.
Cùng nhau, chúng ta có thể phát triển mô hình ngôn ngữ tiếng Việt chất lượng cao. Rất mong nhận được sự quan tâm và hỗ trợ của mọi người!
Form đăng ký tham gia: https://docs.google.com/forms/d/e/1FAIpQLSfoc4tnV6R0RJvVPmsH4cyfgnkKdUkASgYFA-sTuL1hfDE9sA/viewform?usp=pp_url

Mình là Nhiệm, một nghiên cứu sinh tiến sĩ đang công tác tại National Central University, Researcher Foxconn AI, Taiwan.
Trần Nhiệm. Email. tvnhiemhmus@g.ncu.edu.tw
zalo: +886 934 311 751.
linkedIn: https://www.linkedin.com/in/tran-nhiem-ab1851125/
#VietnameseLLMs
File đính kèm:
https://drive.google.com/file/d/182T0ExiJFKfIUvK1Vm3WQqeju8kw9J6J/view?usp=sharing ","Cộng đồng LLMs Việt Nam! Hiện tại, mình đang xây dựng một mô hình ngôn ngữ thuần Việt (#Vietnamese_LLM). Dự án của mình dựa trên sự phát triển của các mô hình ngôn ngữ dựa trên Open-source LLM như và nhiều mô hình Thông tin sơ lược về dự án trình bày trong file đính kèm: hoạch của mình bao gồm việc phát triển: Tạo bộ dữ liệu Vietnamese (self-instruct dataset) và Finetuning và Training các Open-source LLMs trên bộ dữ liệu tiếng Việt. Phát triển bộ dữ liệu tiếngViệt (Self-Instruct Vietnamse): + Dựa trên các bộ dữ liệu tiếng Anh hiện có như và các nhiều bộ dữ liệu khác. Mình đang sử dụng API Azure OpenAI: GPT3, GPT 3.5 và GPT-4 để dịch các bộ dữ liệu này sang tiếng Việt. + Tạo thêm 100.000 (có thể nhiều hơn) câu hướng dẫn tự học giống như dự án Alpaca hoặc các mô hình tạo dữ liệu khác. + Tìm hiểu thêm về các nguồn dữ liệu tiếng Việt khác tập trung cho các lĩnh vực khác nhau. như báo chí, điện ảnh, y học... (Mong nhận được ý kiến góp ý từ mọi người). Để huấn luyện (#Finetuning & Traning ) các mô hình ngôn ngữ LLM, mình sẽ sử dụng kỹ thuật và trên máy chủ Azure server có 8 GPUs Nvidia 80GB. Điều này sẽ giúp mình Finetune và Train các mô hình ngôn ngữ (LLM) từ 7B tỷ đến 65B tỷ hoặc nhiều hơn. Mình rất mong sự kết nối với các bạn có mong muốn thực hiện dự án này và hoan nghênh sự đóng góp từ cộng đồng. Nếu bạn có sự góp ý về dữ liệu hoặc , xin vui lòng chia sẻ. Cùng nhau, chúng ta có thể phát triển mô hình ngôn ngữ tiếng Việt chất lượng cao. Rất mong nhận được sự quan tâm và hỗ trợ của mọi người! Form đăng ký tham gia: https://docs.google.com/forms/d/e/1FAIpQLSfoc4tnV6R0RJvVPmsH4cyfgnkKdUkASgYFA-sTuL1hfDE9sA/viewform?usp=pp_url Mình là Nhiệm, một nghiên cứu sinh tiến sĩ đang công tác tại National Central University, Researcher Foxconn AI, Taiwan. Trần Nhiệm. Email. tvnhiemhmus@g.ncu.edu.tw zalo: +886 934 311 751. linkedIn: https://www.linkedin.com/in/tran-nhiem-ab1851125/ File đính kèm: https://drive.google.com/file/d/182T0ExiJFKfIUvK1Vm3WQqeju8kw9J6J/view?usp=sharing","#BLOOMZ,	#Open_LlaMA,	#khác.	#Kế	#1.	#2.	#1.	#Alpaca,	#Dolly,	#OpenAssistant,	#ShareGPT	#2.	#LoRA	#QLoRA	#A100	#VietnameseLLMs",,"#Q&A, #nlp, #data",,
"Chào mọi người, Hiện tại, mình đang tiến hành điều chỉnh (fine-tuning) mô hình BartPho-word-base để thực hiện tác vụ tóm tắt văn bản. Tuy nhiên, khi mình dự đoán kết quả, lại gặp phải tình trạng kết quả dự đoán chứa những ký tự không xác định (<unk>). Sau khi tìm hiểu, nhận ra rằng nguyên nhân có thể là do dữ liệu huấn luyện chứa nhiễu hoặc từ vựng của mô hình BartPho chưa đủ phong phú để xử lý tốt tình huống này.
Mình đã thử nhiều cách nhưng chưa xử lí được vấn đề này, vì thế mình lên đây tìm kiếm gợi ý và kinh nghiệm từ mọi người. Hi vọng mọi người giúp đỡ.","Chào mọi người, Hiện tại, mình đang tiến hành điều chỉnh (fine-tuning) mô hình BartPho-word-base để thực hiện tác vụ tóm tắt văn bản. Tuy nhiên, khi mình dự đoán kết quả, lại gặp phải tình trạng kết quả dự đoán chứa những ký tự không xác định (<unk>). Sau khi tìm hiểu, nhận ra rằng nguyên nhân có thể là do dữ liệu huấn luyện chứa nhiễu hoặc từ vựng của mô hình BartPho chưa đủ phong phú để xử lý tốt tình huống này. Mình đã thử nhiều cách nhưng chưa xử lí được vấn đề này, vì thế mình lên đây tìm kiếm gợi ý và kinh nghiệm từ mọi người. Hi vọng mọi người giúp đỡ.",,,"#Q&A, #deep_learing, #nlp",,
"Closed.
Cảm ơn cả nhà đã đưa ra nhiều gợi ý giúp mình hiểu vấn đề hơn và đã giải quyết xong case này. 
Phương án giải quyết là chỉ đưa vào hàm map với tf.numpy_function thao tác đọc tên ảnh để map sang feature. Còn việc map caps thành vector sẽ dùng hàm map khác với auto graph. Lý do là map với numpy_function rất khó control và thường hay mất shape nên gây lỗi đầu vào với TextVectorization (hoặc có thể layer nào khác cũng sẽ bị lỗi tương tự).
-----------------------------------------------------
Chào cả nhà. Mình có 1 project CV dùng Tensorflow. Tuy nhiên đang bị mắc kẹt trong việc tạo TF dataset như sau:
Mình tạo tf dataset: tên ảnh, caps
map bằng tf.numpy_function để có dataset: feature, caps.
=> cho vào train.
Nếu chạy GPU Colab với num_parallel_calls trong khi map feature thì sẽ lỗi runtime ở 1 chỗ nào đó khi train mô hình. Với 1 tập dữ liệu cố định, mỗi lần chạy có thể lỗi ở các ảnh khác nhau. Nếu thử với 1 tập dữ liệu nhỏ thì có thể may mắn chạy được 1 vài epoch mới lỗi (thậm chí có lần chạy được hết tất cả các epoch).
Tuy nhiên nếu chạy CPU hoặc chạy GPU mà không dùng num_parallel_calls thì không lỗi, chạy bình thường.
Chi tiết mình đăng trên Stackoverflow mà chưa có ai trả lời.
https://stackoverflow.com/questions/77007077/runtime-error-when-use-dataset-map-with-tf-numpy-function-and-num-parallel-cal
Các cao nhân đi qua xin vui lòng chỉ giáo. 
Cảm ơn rất nhiều.","Closed. Cảm ơn cả nhà đã đưa ra nhiều gợi ý giúp mình hiểu vấn đề hơn và đã giải quyết xong case này. Phương án giải quyết là chỉ đưa vào hàm map với tf.numpy_function thao tác đọc tên ảnh để map sang feature. Còn việc map caps thành vector sẽ dùng hàm map khác với auto graph. Lý do là map với numpy_function rất khó control và thường hay mất shape nên gây lỗi đầu vào với TextVectorization (hoặc có thể layer nào khác cũng sẽ bị lỗi tương tự). ----------------------------------------------------- Chào cả nhà. Mình có 1 project CV dùng Tensorflow. Tuy nhiên đang bị mắc kẹt trong việc tạo TF dataset như sau: Mình tạo tf dataset: tên ảnh, caps map bằng tf.numpy_function để có dataset: feature, caps. => cho vào train. Nếu chạy GPU Colab với num_parallel_calls trong khi map feature thì sẽ lỗi runtime ở 1 chỗ nào đó khi train mô hình. Với 1 tập dữ liệu cố định, mỗi lần chạy có thể lỗi ở các ảnh khác nhau. Nếu thử với 1 tập dữ liệu nhỏ thì có thể may mắn chạy được 1 vài epoch mới lỗi (thậm chí có lần chạy được hết tất cả các epoch). Tuy nhiên nếu chạy CPU hoặc chạy GPU mà không dùng num_parallel_calls thì không lỗi, chạy bình thường. Chi tiết mình đăng trên Stackoverflow mà chưa có ai trả lời. https://stackoverflow.com/questions/77007077/runtime-error-when-use-dataset-map-with-tf-numpy-function-and-num-parallel-cal Các cao nhân đi qua xin vui lòng chỉ giáo. Cảm ơn rất nhiều.",,,"#Q&A, #python",,
"VinAI Seminar - ""Rumour and Disinformation Detection in Online Conversations""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Jey Han Lau, University of Melbourne
Time: 10:00 am - 11:00 am (GMT+7), Sep 14, 2023","VinAI Seminar - ""Rumour and Disinformation Detection in Online Conversations"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Jey Han Lau, University of Melbourne Time: 10:00 am - 11:00 am (GMT+7), Sep 14, 2023",,,,,
Tool Amazon Machine Learning: Developer Guide,Tool Amazon Machine Learning: Developer Guide,,,,,
"Mình đang cần tìm cuốn sách này. Ensemble Learning Algorithms With Python: Make Better Predictions with Bagging, Boosting and Stacking của tác giả Jason Brownlee. Link https://machinelearningmastery.com/ensemble-learning.../ Bạn nào có cho mình xin với. Cảm ơn nhiều nhé","Mình đang cần tìm cuốn sách này. Ensemble Learning Algorithms With Python: Make Better Predictions with Bagging, Boosting and Stacking của tác giả Jason Brownlee. Link https://machinelearningmastery.com/ensemble-learning.../ Bạn nào có cho mình xin với. Cảm ơn nhiều nhé",,,#Q&A,,
"Xin chào mọi người, em đang có ý định làm một prj NLP nho nhỏ để đánh giá CV, trong đó ý tưởng chính của em là người dùng nhập vào một đoạn miêu tả CV của họ (gồm nghề nghiệp, kinh nghiệm làm việc, kỹ năng,...) và bên tuyển dụng nhập vào JD rồi so sánh độ matching giữa 2 yếu tố đấy. Tuy nhiên em đang gặp khó khăn ở ngay bước đầu tiên khi cần biểu diễn đoạn văn bản input thành các vector có cùng độ dài để tiện đối chiếu. E cảm thấy thuật word2vec bình thường không hiệu quả với dữ liệu bé, ví dụ nó sẽ không thể phân biệt được ""Tôi là fresher IT"" và ""Tôi là senior IT"" vì có cùng context
Mọi người cho em gợi ý để giải quyết vấn đề này với ạ, liệu có cách word embedding nào khác không ạ? Và e nên dùng thuật gì để đo độ matching giữa CV và JD ạ?","Xin chào mọi người, em đang có ý định làm một prj NLP nho nhỏ để đánh giá CV, trong đó ý tưởng chính của em là người dùng nhập vào một đoạn miêu tả CV của họ (gồm nghề nghiệp, kinh nghiệm làm việc, kỹ năng,...) và bên tuyển dụng nhập vào JD rồi so sánh độ matching giữa 2 yếu tố đấy. Tuy nhiên em đang gặp khó khăn ở ngay bước đầu tiên khi cần biểu diễn đoạn văn bản input thành các vector có cùng độ dài để tiện đối chiếu. E cảm thấy thuật word2vec bình thường không hiệu quả với dữ liệu bé, ví dụ nó sẽ không thể phân biệt được ""Tôi là fresher IT"" và ""Tôi là senior IT"" vì có cùng context Mọi người cho em gợi ý để giải quyết vấn đề này với ạ, liệu có cách word embedding nào khác không ạ? Và e nên dùng thuật gì để đo độ matching giữa CV và JD ạ?",,,"#Q&A, #nlp",,
"Em chào mọi người ạ, em đang eval cho model của mình trên coco2015 test dev để tiện so sánh với các paper khác, cơ mà sever submit đã bị disable và trên web mới của codalab thì hình như không còn support submit cho coco nữa thì phải, annotations thì họ cũng không công bố và em tìm khắp nơi trên internet cũng không thấy có leak. Còn cách nào để eval trên dataset này không ạ. Em cảm ơn mọi người rất nhiều ạ.","Em chào mọi người ạ, em đang eval cho model của mình trên coco2015 test dev để tiện so sánh với các paper khác, cơ mà sever submit đã bị disable và trên web mới của codalab thì hình như không còn support submit cho coco nữa thì phải, annotations thì họ cũng không công bố và em tìm khắp nơi trên internet cũng không thấy có leak. Còn cách nào để eval trên dataset này không ạ. Em cảm ơn mọi người rất nhiều ạ.",,,"#Q&A, #data",,
"Chào các bác, hiện tại em đang tìm hiểu về Sentiment Analysis. Các bác đi trước có tài liệu hay nguồn học nào hay cho em xin với ạ!
Em cám mọi người nhiều!","Chào các bác, hiện tại em đang tìm hiểu về Sentiment Analysis. Các bác đi trước có tài liệu hay nguồn học nào hay cho em xin với ạ! Em cám mọi người nhiều!",,,"#Q&A, #nlp",,
Bài báo mới đến từ Google có tên: TSMixer: An all-MLP architecture for time series forecasting (https://arxiv.org/pdf/2303.06053),Bài báo mới đến từ Google có tên: TSMixer: An all-MLP architecture for time series forecasting (https://arxiv.org/pdf/2303.06053),,,#sharing,,
"Em chào mọi người, hiện em đang cần deploy chatbot lên facebook message nhưng cần xác minh doanh nghiệp cho quyền pages_messing. Mọi người có ai đang làm hay quan tâm vấn đề này cho em xin ý kiến tham khảo với ạ. Em dùng tài khoản đã xác minh doanh nghiệp nhưng vẫn không xin được quyền từ facebook.","Em chào mọi người, hiện em đang cần deploy chatbot lên facebook message nhưng cần xác minh doanh nghiệp cho quyền pages_messing. Mọi người có ai đang làm hay quan tâm vấn đề này cho em xin ý kiến tham khảo với ạ. Em dùng tài khoản đã xác minh doanh nghiệp nhưng vẫn không xin được quyền từ facebook.",,,#Q&A,,
"Em mới học về AI đang muốn train model phân loại nội dung video theo mức độ phù hợp của từng lứa tuổi, ai có ý tưởng gì có thể sp em đc không ạ :V","Em mới học về AI đang muốn train model phân loại nội dung video theo mức độ phù hợp của từng lứa tuổi, ai có ý tưởng gì có thể sp em đc không ạ :V",,,"#Q&A, #cv",,
"Explain Paper - The Fastest Way to Read Research Papers
Explain Paper là một công cụ giúp quá trình đọc paper trở nên đơn giản hơn. Công cụ này rất hữu ích với các bạn mới bắt đầu đọc paper.
Các bạn chỉ cần tải paper lên, highlight text mà bạn không hiểu. Sau đó, AI sẽ giúp bạn giải thích phần highlight đó. Bên cạnh các thuật ngữ, AI cũng có thể giải thích cả một đoạn text.
Website: https://www.explainpaper.com/","Explain Paper - The Fastest Way to Read Research Papers Explain Paper là một công cụ giúp quá trình đọc paper trở nên đơn giản hơn. Công cụ này rất hữu ích với các bạn mới bắt đầu đọc paper. Các bạn chỉ cần tải paper lên, highlight text mà bạn không hiểu. Sau đó, AI sẽ giúp bạn giải thích phần highlight đó. Bên cạnh các thuật ngữ, AI cũng có thể giải thích cả một đoạn text. Website: https://www.explainpaper.com/",,,#sharing,,
"Chắc mọi người biết tới ứng dụng Code Interpreter của OpenAI và Code LLaMA của Meta, nay mình thấy có người viết API có thể kết nối cả 2 backends này để chạy trong terminal, có tên là Open Interpreter. Nếu với Code Interpreter thì sẽ chạy inference online dựa trên GPT-3.5, còn Code LLaMA sẽ chạy inference offline dựa trên cấu hình máy cá nhân. Các bạn có thể tham khảo source code tại đây","Chắc mọi người biết tới ứng dụng Code Interpreter của OpenAI và Code LLaMA của Meta, nay mình thấy có người viết API có thể kết nối cả 2 backends này để chạy trong terminal, có tên là Open Interpreter. Nếu với Code Interpreter thì sẽ chạy inference online dựa trên GPT-3.5, còn Code LLaMA sẽ chạy inference offline dựa trên cấu hình máy cá nhân. Các bạn có thể tham khảo source code tại đây",,,"#sharing, #nlp",,
"Hey everyone - sharing these 2 initiatives for Vietnamese startups that are interested in Gen AI:
(1) AWS Gen AI Day in Vietnam (Offline + Online)
Join us on September 15 in HCMC for AWS Gen AI Day! Experience insightful sessions with AI experts, Gen AI startup demos, and network with fellow startups, VCs, SMEs, and Enterprises. Content will be broadcasted to online participants. RSVP here: https://lu.ma/BuildingGenerativeAIonAWS
(2) AWS ASEAN Gen AI Accelerator Program (Sep 26 - Oct 26) Deadline: 16 September
Apply for the AWS x Accelerating Asia ASEAN Gen AI Reactor program (http://bit.ly/aseangenai). Get 5 weeks of intensive support, mentorship from AI experts and thought leaders, up to $200K in AWS credits and demo day with VCs. This is open to early stage startups in ASEAN with existing or in-progress solutions in Gen AI.","Hey everyone - sharing these 2 initiatives for Vietnamese startups that are interested in Gen AI: (1) AWS Gen AI Day in Vietnam (Offline + Online) Join us on September 15 in HCMC for AWS Gen AI Day! Experience insightful sessions with AI experts, Gen AI startup demos, and network with fellow startups, VCs, SMEs, and Enterprises. Content will be broadcasted to online participants. RSVP here: https://lu.ma/BuildingGenerativeAIonAWS (2) AWS ASEAN Gen AI Accelerator Program (Sep 26 - Oct 26) Deadline: 16 September Apply for the AWS x Accelerating Asia ASEAN Gen AI Reactor program (http://bit.ly/aseangenai). Get 5 weeks of intensive support, mentorship from AI experts and thought leaders, up to $200K in AWS credits and demo day with VCs. This is open to early stage startups in ASEAN with existing or in-progress solutions in Gen AI.",,,,,
"Em chào mọi người ạ:v
Em đang tìm hiểu về LLMs và vừa mới fine-tune được 1 model làm chatbot và em có 3 câu hỏi mong mọi người có thể giải đáp:
1. Em muốn deploy model lên web để có thể nhận feedback từ người dùng qua Internet thì có những cách phổ biển nào hay dùng ạ?
Em có tra thì thấy có 1 bên chạy trên được huggingface
https://huggingface.co/spaces/project-baize/chat-with-baize như này. Nhưng em tìm hiểu thì vẫn chưa biết đưa lên kiểu gì:<
2. Chatbot của em thì mới có thể trả lời được câu hỏi 1-1. Chưa ghi nhớ được đoạn hội thoại cũ để đưa ra câu trả lời mới (kiểu chatgpt). Vậy có cách nào để làm được như vậy, hoặc mọi người có thể đưa em từ khóa để em search ạ.
3. Với những chatbot được xây dựng từ LLMs thì mình có thể xây dựng kịch bản cho nó không ạ? Ví dụ như chatbot để trả lời CSKH, thỉnh thoảng sẽ tự động ra 1 câu hỏi khi khách hàng chưa hỏi câu hỏi đấy.
Em cảm ơn mn ạ.","Em chào mọi người ạ:v Em đang tìm hiểu về LLMs và vừa mới fine-tune được 1 model làm chatbot và em có 3 câu hỏi mong mọi người có thể giải đáp: 1. Em muốn deploy model lên web để có thể nhận feedback từ người dùng qua Internet thì có những cách phổ biển nào hay dùng ạ? Em có tra thì thấy có 1 bên chạy trên được huggingface https://huggingface.co/spaces/project-baize/chat-with-baize như này. Nhưng em tìm hiểu thì vẫn chưa biết đưa lên kiểu gì:< 2. Chatbot của em thì mới có thể trả lời được câu hỏi 1-1. Chưa ghi nhớ được đoạn hội thoại cũ để đưa ra câu trả lời mới (kiểu chatgpt). Vậy có cách nào để làm được như vậy, hoặc mọi người có thể đưa em từ khóa để em search ạ. 3. Với những chatbot được xây dựng từ LLMs thì mình có thể xây dựng kịch bản cho nó không ạ? Ví dụ như chatbot để trả lời CSKH, thỉnh thoảng sẽ tự động ra 1 câu hỏi khi khách hàng chưa hỏi câu hỏi đấy. Em cảm ơn mn ạ.",,,"#Q&A, #nlp",,
"Xin chào anh chị trong group, em vừa tìm hiểu về phương pháp word2vec trong NLP (cụ thể là skip-gram). Dù đã đọc qua vài bài (gồm cả của anh Tiệp) và xem vid trên Youtube nhưng vẫn có một vài đoạn em chưa hiểu lắm nên mong anh chị giải thích thêm ạ:

Thứ nhất, theo em hiểu thì với mỗi target word ta sẽ tìm các weight matrix (U và V) để xác suất của context word là cao nhất, bằng phương pháp backprobagation; sau đó lặp lại quá trình với C context words (với C là context window); sau đó lại lặp lại quá trình đấy với N từ trong từ điển. Như vậy số lần phải làm backprobagation là C * N. Nhưng khi học về MLP thì em được biết backprobagation là quá trình khá tốn kém thời gian (không biết liệu việc chỉ có 1 hidden layer và no activation function có làm giảm thời gian không) nên ta cần chọn C, N như nào để đảm bảo cả thời gian và accuracy ạ? 
Mỗi từ đầu vào biểu diễn dưới dạng one-hot coding, tuy nhiên khi trực quan hóa nó dưới dạng hình ảnh (như ở đây: https://projector.tensorflow.org/) thì em lại thấy người ta tìm context dựa trên từ ""ở gần"" với target word nhất (không biết là gần theo Euclid distance hay như nào) và coi mỗi từ trong từ điển là một vector có độ dài bằng nhau trong không gian. Vậy các vector đấy lấy ở đâu ạ? Chắc không phải là vector biểu diễn one-hot encoding mà là embedding vector trong ma trận U, V mà ta đã tìm ở trên ạ? Nếu hiểu theo nghĩa đấy thì kích thước mỗi vector chính là số neuron trong hidden vector đúng không ạ? 
Tại sao không dùng nhiều hidden layer cho thuật toán như MLP thế ạ? 
Ngoài ra nếu khi trực quan hóa ta dùng vector trong weight matrix tìm ra thì dùng vector trong target matrix U hay context matrix V ạ?
Cơ sở toán học nào đằng sau việc mô hình hóa này ạ? Ý em là, làm sao khoảng cách euclid giữa các vector của weight matrix trong không gian lại có liên quan đến mô hình xác suất mà ta xây dựng ở thuật toán ạ? 


 ","Xin chào anh chị trong group, em vừa tìm hiểu về phương pháp word2vec trong NLP (cụ thể là skip-gram). Dù đã đọc qua vài bài (gồm cả của anh Tiệp) và xem vid trên Youtube nhưng vẫn có một vài đoạn em chưa hiểu lắm nên mong anh chị giải thích thêm ạ: Thứ nhất, theo em hiểu thì với mỗi target word ta sẽ tìm các weight matrix (U và V) để xác suất của context word là cao nhất, bằng phương pháp backprobagation; sau đó lặp lại quá trình với C context words (với C là context window); sau đó lại lặp lại quá trình đấy với N từ trong từ điển. Như vậy số lần phải làm backprobagation là C * N. Nhưng khi học về MLP thì em được biết backprobagation là quá trình khá tốn kém thời gian (không biết liệu việc chỉ có 1 hidden layer và no activation function có làm giảm thời gian không) nên ta cần chọn C, N như nào để đảm bảo cả thời gian và accuracy ạ? Mỗi từ đầu vào biểu diễn dưới dạng one-hot coding, tuy nhiên khi trực quan hóa nó dưới dạng hình ảnh (như ở đây: https://projector.tensorflow.org/) thì em lại thấy người ta tìm context dựa trên từ ""ở gần"" với target word nhất (không biết là gần theo Euclid distance hay như nào) và coi mỗi từ trong từ điển là một vector có độ dài bằng nhau trong không gian. Vậy các vector đấy lấy ở đâu ạ? Chắc không phải là vector biểu diễn one-hot encoding mà là embedding vector trong ma trận U, V mà ta đã tìm ở trên ạ? Nếu hiểu theo nghĩa đấy thì kích thước mỗi vector chính là số neuron trong hidden vector đúng không ạ? Tại sao không dùng nhiều hidden layer cho thuật toán như MLP thế ạ? Ngoài ra nếu khi trực quan hóa ta dùng vector trong weight matrix tìm ra thì dùng vector trong target matrix U hay context matrix V ạ? Cơ sở toán học nào đằng sau việc mô hình hóa này ạ? Ý em là, làm sao khoảng cách euclid giữa các vector của weight matrix trong không gian lại có liên quan đến mô hình xác suất mà ta xây dựng ở thuật toán ạ?",,,"#Q&A, #nlp",,
"Mình có tìm hiểu về Image generation thì thấy LoRA khá phổ biến khi finetuning Stable Diffusion. Tuy nhiên thì lại thấy khá ít chỗ nói về cơ chế hoạt động đối với SD, chủ yếu chỉ thấy mention trong PEFT của NLP. Hy vọng bài viết có thể giúp mọi người hiểu hơn về cách LoRA giảm computation cost và kích cỡ file lưu trữ khi finetune SD nhé!","Mình có tìm hiểu về Image generation thì thấy LoRA khá phổ biến khi finetuning Stable Diffusion. Tuy nhiên thì lại thấy khá ít chỗ nói về cơ chế hoạt động đối với SD, chủ yếu chỉ thấy mention trong PEFT của NLP. Hy vọng bài viết có thể giúp mọi người hiểu hơn về cách LoRA giảm computation cost và kích cỡ file lưu trữ khi finetune SD nhé!",,,"#sharing, #cv",,
Có người đã tổng hợp tới 14 triệu ảnh kèm prompt sinh ra từ các mô hình diffusions tại đây,Có người đã tổng hợp tới 14 triệu ảnh kèm prompt sinh ra từ các mô hình diffusions tại đây,,,#sharing,,
"Nhân dịp mình có Pull Request (GLIGEN model Pipeline) được merge vào thư viện Huggingface/Diffuser mình xin phép chia sẻ đến mọi người. Hiện nay các phương pháp finetune personal diffusion như Textual Inversion, Dreambooth hay LoRA sẽ giúp ta thêm object hoặc style bất kì vào ảnh mà ta muốn sinh. Nhưng các phương pháp này đều yêu cầu phải có một lượng data và finetuneing. GLIGEN Pipeline hỗ trợ việc thêm object hoặc style mà không cần finetune (Zero-shot), ta chỉ cần truyền một bức ảnh chứa object hoặc style và mô hình có thể sinh ra ảnh dựa trên đó. Chi tiết hơn có thể tham khảo ở paper: https://arxiv.org/abs/2301.07093. Ngoài ra GLIGEN Pipeline còn có thể cho bạn xác định vị trí đặt vật thể trong ảnh sinh ra bằng việc cung cấp tọa độ box. Hiện tại mình support 2 model là Generation và Inpainting. Mọi người hứng thú có thể thử, code example được mình đính kèm ở model card.
Generation: https://huggingface.co/anhnct/Gligen_Text_Image
Inpainting: https://huggingface.co/anhnct/Gligen_Inpainting_Text_Image","Nhân dịp mình có Pull Request (GLIGEN model Pipeline) được merge vào thư viện Huggingface/Diffuser mình xin phép chia sẻ đến mọi người. Hiện nay các phương pháp finetune personal diffusion như Textual Inversion, Dreambooth hay LoRA sẽ giúp ta thêm object hoặc style bất kì vào ảnh mà ta muốn sinh. Nhưng các phương pháp này đều yêu cầu phải có một lượng data và finetuneing. GLIGEN Pipeline hỗ trợ việc thêm object hoặc style mà không cần finetune (Zero-shot), ta chỉ cần truyền một bức ảnh chứa object hoặc style và mô hình có thể sinh ra ảnh dựa trên đó. Chi tiết hơn có thể tham khảo ở paper: https://arxiv.org/abs/2301.07093. Ngoài ra GLIGEN Pipeline còn có thể cho bạn xác định vị trí đặt vật thể trong ảnh sinh ra bằng việc cung cấp tọa độ box. Hiện tại mình support 2 model là Generation và Inpainting. Mọi người hứng thú có thể thử, code example được mình đính kèm ở model card. Generation: https://huggingface.co/anhnct/Gligen_Text_Image Inpainting: https://huggingface.co/anhnct/Gligen_Inpainting_Text_Image",,,"#sharing, #deep_learning",,
"Gần đây mình quan sát thấy hiện tượng hay xu thế chuyển đổi code từ Python qua C/C++ cho các mô hình ngôn ngữ lớn. Nhưng chưa thấy ai viết lại models và train nó từ chính C/C++. Tuy nhiên, trong ngôn Rust thì xu thế viết lại models và training loop đang được cộng đồng làm khá mạnh, trong đó có Huggingface. Trước đó, Huggingface đã viết một số thư viện bằng Rust như datasets, safetensors,... và gần đây họ bắt đầu viết lại cả models và training loop cho chúng bằng ngôn ngữ Rust. https://github.com/huggingface/candle;
Thậm chí, có người khác còn port cả pytorch sang Rust như ở đây https://github.com/burn-rs/burn/tree/main và có 1 số examples về build và train models bằng Rust.
Hi vọng, với thông tin này sẽ gợi ý cho các bạn thêm phương án trong lộ trình học tập và làm việc.
Ps. Cách đây vài tháng mình có biết Elon Musk còn có 1 post về Rust nữa cơ. Có lẽ tiềm năng của Rust sẽ rất lớn trong tương lai gần.","Gần đây mình quan sát thấy hiện tượng hay xu thế chuyển đổi code từ Python qua C/C++ cho các mô hình ngôn ngữ lớn. Nhưng chưa thấy ai viết lại models và train nó từ chính C/C++. Tuy nhiên, trong ngôn Rust thì xu thế viết lại models và training loop đang được cộng đồng làm khá mạnh, trong đó có Huggingface. Trước đó, Huggingface đã viết một số thư viện bằng Rust như datasets, safetensors,... và gần đây họ bắt đầu viết lại cả models và training loop cho chúng bằng ngôn ngữ Rust. https://github.com/huggingface/candle; Thậm chí, có người khác còn port cả pytorch sang Rust như ở đây https://github.com/burn-rs/burn/tree/main và có 1 số examples về build và train models bằng Rust. Hi vọng, với thông tin này sẽ gợi ý cho các bạn thêm phương án trong lộ trình học tập và làm việc. Ps. Cách đây vài tháng mình có biết Elon Musk còn có 1 post về Rust nữa cơ. Có lẽ tiềm năng của Rust sẽ rất lớn trong tương lai gần.",,,#sharing,,
"Mình đã nhìn thấy post này (dưới comment) khá lâu trong group, có nhiều likes và shares nên thấy có trách nhiệm phản hồi kẻo nhiều bạn hiểu sai vấn đề.
Bạn chủ post nói đúng ở chỗ không nên nhắm mắt điền giá trị thiếu bằng 0 mà cần hiểu kỹ phân phối của dữ liệu.
Tuy nhiên, nói “TUYỆT ĐỐI KHÔNG” là rất hồ đồ. Điều bạn quan sát được chỉ đúng trong các trường hợp bạn thấy, đừng vội generalize ra toàn bộ kẻo bị overfitting với những gì mình thấy.
Những gì bạn phân tích chỉ áp dụng trong trường hợp toàn bộ các biến ở dạng số thực. Với dữ liệu hạng mục thì không có khái niệm KNN!
Kể cả với dữ liệu dạng số thực thì scale của các biến ảnh hưởng rất nhiều đến khoảng cách giữa các điểm. Nên bạn nói là KNN sẽ cho lựa chọn chính xác là chưa đúng.
Về dữ liệu bị khuyết, cần có domain knowledge để hiểu dữ liệu đó là khuyết ngẫu nhiên hay không và dùng các phương pháp tương ứng (xem [1]) cho phù hợp.
Nếu không chắc cách impute missing data như thế nào thì bắt đầu với cách dễ nhất rồi xem xét metrics và thử với nhiều cách khác nhau. Đừng tự đóng mình với một cái gì “TUYỆT ĐỐI KHÔNG”.
Tham khảo:
[1]
https://www.ncbi.nlm.nih.gov/books/NBK493614/#:~:text=Missing%20at%20random%20(MAR).,but%20not%20the%20unobserved%20data.
[2]
https://en.wikipedia.org/wiki/Missing_data","Mình đã nhìn thấy post này (dưới comment) khá lâu trong group, có nhiều likes và shares nên thấy có trách nhiệm phản hồi kẻo nhiều bạn hiểu sai vấn đề. Bạn chủ post nói đúng ở chỗ không nên nhắm mắt điền giá trị thiếu bằng 0 mà cần hiểu kỹ phân phối của dữ liệu. Tuy nhiên, nói “TUYỆT ĐỐI KHÔNG” là rất hồ đồ. Điều bạn quan sát được chỉ đúng trong các trường hợp bạn thấy, đừng vội generalize ra toàn bộ kẻo bị overfitting với những gì mình thấy. Những gì bạn phân tích chỉ áp dụng trong trường hợp toàn bộ các biến ở dạng số thực. Với dữ liệu hạng mục thì không có khái niệm KNN! Kể cả với dữ liệu dạng số thực thì scale của các biến ảnh hưởng rất nhiều đến khoảng cách giữa các điểm. Nên bạn nói là KNN sẽ cho lựa chọn chính xác là chưa đúng. Về dữ liệu bị khuyết, cần có domain knowledge để hiểu dữ liệu đó là khuyết ngẫu nhiên hay không và dùng các phương pháp tương ứng (xem [1]) cho phù hợp. Nếu không chắc cách impute missing data như thế nào thì bắt đầu với cách dễ nhất rồi xem xét metrics và thử với nhiều cách khác nhau. Đừng tự đóng mình với một cái gì “TUYỆT ĐỐI KHÔNG”. Tham khảo: [1] https://www.ncbi.nlm.nih.gov/books/NBK493614/#:~:text=Missing%20at%20random%20(MAR).,but%20not%20the%20unobserved%20data. [2] https://en.wikipedia.org/wiki/Missing_data",,,"#sharing, #data",,
"TUYỆT ĐỐI KHÔNG bao giờ điền giá trị thiếu bằng Mean (hoặc zero).
Đây là những gì xảy ra khi chúng ta thực hiện fill value bởi Mean/0:
.
.
Thay thế (điền) giá trị thiếu bằng trung bình hoặc zero hoặc bất kỳ giá trị cố định nào khác:
- Làm thay đổi các thống kê tóm tắt
- Làm thay đổi phân phối
- Làm tăng sự hiện diện của một giá trị cụ thể
Điều này có thể dẫn đến:
- Làm mô hình không chính xác
- Khiến kết luận sai lầm, và nhiều hơn nữa.
- Thay vào đó, luôn cố gắng điền giá trị thiếu với độ chính xác cao hơn.
Những trường hợp như vậy thì KNN imputer thường là một lựa chọn ưu tiên hơn
=> Nó điền giá trị thiếu bằng cách sử dụng k-Nearest Neighbors.
Các giá trị trống sẽ được điền bằng cách chạy kNN trên các giá trị còn lại.
Hiệu quả của nó so với Mean / Zero imputation thì hoàn toàn được minh họa như hình bên dưới:
- Mean / Zero thay đổi thống kê tóm tắt và phân phối
- KNN imputer giúp giữ nguyên
share by: learning and sharing for machine learning&ai
download tài liệu mình chia sẻ tại: https://bit.ly/drive-tailieu-ebook","TUYỆT ĐỐI KHÔNG bao giờ điền giá trị thiếu bằng Mean (hoặc zero). Đây là những gì xảy ra khi chúng ta thực hiện fill value bởi Mean/0: . . Thay thế (điền) giá trị thiếu bằng trung bình hoặc zero hoặc bất kỳ giá trị cố định nào khác: - Làm thay đổi các thống kê tóm tắt - Làm thay đổi phân phối - Làm tăng sự hiện diện của một giá trị cụ thể Điều này có thể dẫn đến: - Làm mô hình không chính xác - Khiến kết luận sai lầm, và nhiều hơn nữa. - Thay vào đó, luôn cố gắng điền giá trị thiếu với độ chính xác cao hơn. Những trường hợp như vậy thì KNN imputer thường là một lựa chọn ưu tiên hơn => Nó điền giá trị thiếu bằng cách sử dụng k-Nearest Neighbors. Các giá trị trống sẽ được điền bằng cách chạy kNN trên các giá trị còn lại. Hiệu quả của nó so với Mean / Zero imputation thì hoàn toàn được minh họa như hình bên dưới: - Mean / Zero thay đổi thống kê tóm tắt và phân phối - KNN imputer giúp giữ nguyên share by: learning and sharing for machine learning&ai download tài liệu mình chia sẻ tại: https://bit.ly/drive-tailieu-ebook",,,"#sharing , #data",,
"Mọi người cho em hỏi sự khác biệt giữa normalization và standardization là gì vậy ạ? Khi nào thì nên dùng cái nào ạ? Em đọc tài liệu mà thấy chỉ hướng dẫn cách tính toán nên hơi hoang mang không biết nên dùng cái nào (ví dụ KNN, Logistic Regression)
Em cảm ơn mọi người","Mọi người cho em hỏi sự khác biệt giữa normalization và standardization là gì vậy ạ? Khi nào thì nên dùng cái nào ạ? Em đọc tài liệu mà thấy chỉ hướng dẫn cách tính toán nên hơi hoang mang không biết nên dùng cái nào (ví dụ KNN, Logistic Regression) Em cảm ơn mọi người",,,"#Q&A, #machine_learning",,
"Xin chào anh chị ạ, anh chị cho em hỏi kỹ năng/kiến thức chuyên môn giữa AI Engineer với AI Researcher/Scientist có khả năng bổ trợ lẫn nhau không ạ? Em muốn định hướng theo hướng AI Engineer nhưng thắc mắc liệu việc tham gia các Lab/đề tài nghiên cứu có giúp mình trong việc làm kỹ sư không ạ (như hiểu rõ các model/hiểu thêm các công nghệ/ý tưởng mới…)?","Xin chào anh chị ạ, anh chị cho em hỏi kỹ năng/kiến thức chuyên môn giữa AI Engineer với AI Researcher/Scientist có khả năng bổ trợ lẫn nhau không ạ? Em muốn định hướng theo hướng AI Engineer nhưng thắc mắc liệu việc tham gia các Lab/đề tài nghiên cứu có giúp mình trong việc làm kỹ sư không ạ (như hiểu rõ các model/hiểu thêm các công nghệ/ý tưởng mới…)?",,,#Q&A,,
"Chào mọi người, em có một tập dữ liệu gồm các chuỗi thông số ví dụ 34,3,38,48,55,... và nhãn của mỗi chuỗi này là 0 hoặc 1( bất thường/không bất thường ).
Em nên dùng LSTM hay model nào ok hơn ạ
Em cảm ơn","Chào mọi người, em có một tập dữ liệu gồm các chuỗi thông số ví dụ 34,3,38,48,55,... và nhãn của mỗi chuỗi này là 0 hoặc 1( bất thường/không bất thường ). Em nên dùng LSTM hay model nào ok hơn ạ Em cảm ơn",,,"#Q&A, #deep_learning",,
"Tuy Claude-2, cạnh tranh với ChatGPT và Bard, chưa được triển tại Việt Nam, nhưng mình đã dùng được 1 thời gian và có ấn tượng tốt với chat bot này. Ưu điểm của Claude-2 là có khả năng ""suy luận"" với long content/text. Vậy câu hỏi rằng làm sao có thể dùng được nó mà không có VNP, giải pháp đơn giản nhất là dùng Opera Browser (trước mình dùng Brave, nhưng Opera cũng có khả năng chặn quảng cáo rất tốt). Sau đây, Anthropic là công ti mẹ của Claude-2 giới thiệu Cook Book cách dùng với long context tại đây","Tuy Claude-2, cạnh tranh với ChatGPT và Bard, chưa được triển tại Việt Nam, nhưng mình đã dùng được 1 thời gian và có ấn tượng tốt với chat bot này. Ưu điểm của Claude-2 là có khả năng ""suy luận"" với long content/text. Vậy câu hỏi rằng làm sao có thể dùng được nó mà không có VNP, giải pháp đơn giản nhất là dùng Opera Browser (trước mình dùng Brave, nhưng Opera cũng có khả năng chặn quảng cáo rất tốt). Sau đây, Anthropic là công ti mẹ của Claude-2 giới thiệu Cook Book cách dùng với long context tại đây",,,"#sharing, #nlp",,
"Em xin chào các anh chị ạ. Em đọc về mạng RNN dạng Vec2seq, trong sách có đoạn như hình bên dưới ạ. Anh chị nào hiểu được cái phương trình 15.1 đó có ý nghĩa như nào không ạ hay cho em xin cách đọc nó ạ. Em xin cảm ơn rất nhiều ạ. Chúc mọi người một ngày vui vẻ bình an.","Em xin chào các anh chị ạ. Em đọc về mạng RNN dạng Vec2seq, trong sách có đoạn như hình bên dưới ạ. Anh chị nào hiểu được cái phương trình 15.1 đó có ý nghĩa như nào không ạ hay cho em xin cách đọc nó ạ. Em xin cảm ơn rất nhiều ạ. Chúc mọi người một ngày vui vẻ bình an.",,,"#Q&A, #deep_learning, #math",,
"E chào cả nhà, e có thắc mắc về cách fune-tune YOLOV8 để model có performance tốt hơn ạ. E đang train yolov8 bằng default settings, và có một số vấn đề e đang gặp phải ạ:
- Hàm loss đang ở mức rất cao, cả trên tập training và val, e đang không biết tại sao lại như vậy ạ, và có cách nào để giảm nó xuống không ạ??
-Dataset là về fabric defect, và có distribution e như trong hình ạ, và e không thể thu thập thêm data được nữa, thì e sử dụng một số mạng GAN để sinh thêm dữ liệu thì có hiệu quả trong bài toán này không ạ?? Hoặc áp dụng thêm các kỹ thuật augmentation nào khác ngoài default setting của Yolov8 để có performance tốt hơn ạ?
- Mọi người gợi ý giúp em cách fine-tune các hyperparameter để có kết quả tốt hơn được không ạ!!
E đang train với card 2080ti 11gb, default settings của YOLOv8 ạ:
task: detect
mode: train
model: yolov8s.pt
data: /root/data/andy/yolo8/Fabricv6/data.yaml
epochs: 400
patience: 50
batch: 16
imgsz: 640
save: true
save_period: -1
cache: false
device: null
workers: 8
project: null
name: null
exist_ok: false
pretrained: false
optimizer: SGD
verbose: true
seed: 0
deterministic: true
single_cls: false
rect: false
cos_lr: false
close_mosaic: 0
resume: false
amp: true
overlap_mask: true
mask_ratio: 4
dropout: 0.0
val: true
split: val
save_json: false
save_hybrid: false
conf: null
iou: 0.7
max_det: 300
half: false
dnn: false
plots: true
source: null
show: false
save_txt: false
save_conf: false
save_crop: false
show_labels: true
show_conf: true
vid_stride: 1
line_width: null
visualize: false
augment: false
agnostic_nms: false
classes: null
retina_masks: false
boxes: true
format: torchscript
keras: false
optimize: false
int8: false
dynamic: false
simplify: false
opset: null
workspace: 4
nms: false
lr0: 0.01
lrf: 0.01
momentum: 0.937
weight_decay: 0.0005
warmup_epochs: 3.0
warmup_momentum: 0.8
warmup_bias_lr: 0.1
box: 7.5
cls: 0.5
dfl: 1.5
pose: 12.0
kobj: 1.0
label_smoothing: 0.0
nbs: 64
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 0.0
translate: 0.1
scale: 0.5
shear: 0.0
perspective: 0.0
flipud: 0.0
fliplr: 0.5
mosaic: 1.0
mixup: 0.0
copy_paste: 0.0
cfg: null
v5loader: false
tracker: botsort.yaml
save_dir: runs/detect/train31
Em cám ơn cả nhà ạ!!","E chào cả nhà, e có thắc mắc về cách fune-tune YOLOV8 để model có performance tốt hơn ạ. E đang train yolov8 bằng default settings, và có một số vấn đề e đang gặp phải ạ: - Hàm loss đang ở mức rất cao, cả trên tập training và val, e đang không biết tại sao lại như vậy ạ, và có cách nào để giảm nó xuống không ạ?? -Dataset là về fabric defect, và có distribution e như trong hình ạ, và e không thể thu thập thêm data được nữa, thì e sử dụng một số mạng GAN để sinh thêm dữ liệu thì có hiệu quả trong bài toán này không ạ?? Hoặc áp dụng thêm các kỹ thuật augmentation nào khác ngoài default setting của Yolov8 để có performance tốt hơn ạ? - Mọi người gợi ý giúp em cách fine-tune các hyperparameter để có kết quả tốt hơn được không ạ!! E đang train với card 2080ti 11gb, default settings của YOLOv8 ạ: task: detect mode: train model: yolov8s.pt data: /root/data/andy/yolo8/Fabricv6/data.yaml epochs: 400 patience: 50 batch: 16 imgsz: 640 save: true save_period: -1 cache: false device: null workers: 8 project: null name: null exist_ok: false pretrained: false optimizer: SGD verbose: true seed: 0 deterministic: true single_cls: false rect: false cos_lr: false close_mosaic: 0 resume: false amp: true overlap_mask: true mask_ratio: 4 dropout: 0.0 val: true split: val save_json: false save_hybrid: false conf: null iou: 0.7 max_det: 300 half: false dnn: false plots: true source: null show: false save_txt: false save_conf: false save_crop: false show_labels: true show_conf: true vid_stride: 1 line_width: null visualize: false augment: false agnostic_nms: false classes: null retina_masks: false boxes: true format: torchscript keras: false optimize: false int8: false dynamic: false simplify: false opset: null workspace: 4 nms: false lr0: 0.01 lrf: 0.01 momentum: 0.937 weight_decay: 0.0005 warmup_epochs: 3.0 warmup_momentum: 0.8 warmup_bias_lr: 0.1 box: 7.5 cls: 0.5 dfl: 1.5 pose: 12.0 kobj: 1.0 label_smoothing: 0.0 nbs: 64 hsv_h: 0.015 hsv_s: 0.7 hsv_v: 0.4 degrees: 0.0 translate: 0.1 scale: 0.5 shear: 0.0 perspective: 0.0 flipud: 0.0 fliplr: 0.5 mosaic: 1.0 mixup: 0.0 copy_paste: 0.0 cfg: null v5loader: false tracker: botsort.yaml save_dir: runs/detect/train31 Em cám ơn cả nhà ạ!!",,,"#Q&A, #deep_learning, #cv",,
FREE R BOOKS FOR DATA SCIENCE,FREE R BOOKS FOR DATA SCIENCE,,,,,
"Technical review (đêm ngày 22 tháng 8 năm 2023, GMT+7):
1/ OpenAi cho phép người dùng có thể tự finetune model GPT-3.5 turbo (model GPT-4 sẽ được finetune vào mùa thu, mùa thu ở Bắc bán cầu thường tính vào ngày 22-9 tới 21-12 hàng năm) với giọng văn và các tính năng mà người finetune muốn. Các bạn có thể tham khảo tại đây 

2/ Meta giới thiệu SeamlessM4T, a Multimodal AI Model for Speech and Text Translations trong đó có hỗ trợ tiếng Việt nhé các bạn (xem chi tiết tại đây https://about.fb.com/news/2023/08/seamlessm4t-ai-translation-model/), và mở source tại đây https://github.com/facebookresearch/seamless_communication
3/ HuggingFace giới thiệu IDEFICS: an open reproduction of State-of-the-Art Visual Language model https://huggingface.co/blog/idefics","Technical review (đêm ngày 22 tháng 8 năm 2023, GMT+7): 1/ OpenAi cho phép người dùng có thể tự finetune model GPT-3.5 turbo (model GPT-4 sẽ được finetune vào mùa thu, mùa thu ở Bắc bán cầu thường tính vào ngày 22-9 tới 21-12 hàng năm) với giọng văn và các tính năng mà người finetune muốn. Các bạn có thể tham khảo tại đây 2/ Meta giới thiệu SeamlessM4T, a Multimodal AI Model for Speech and Text Translations trong đó có hỗ trợ tiếng Việt nhé các bạn (xem chi tiết tại đây https://about.fb.com/news/2023/08/seamlessm4t-ai-translation-model/), và mở source tại đây https://github.com/facebookresearch/seamless_communication 3/ HuggingFace giới thiệu IDEFICS: an open reproduction of State-of-the-Art Visual Language model https://huggingface.co/blog/idefics",,,#sharing,,
"VinAI Seminar - ""Principled Frameworks for Designing Deep Learning Models: Efficiency, Robustness, and Expressivity""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Tan Nguyen, NUS
Time: 10:00 am - 11:00 am (GMT+7), Aug 28, 2023","VinAI Seminar - ""Principled Frameworks for Designing Deep Learning Models: Efficiency, Robustness, and Expressivity"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Tan Nguyen, NUS Time: 10:00 am - 11:00 am (GMT+7), Aug 28, 2023",,,#webinar,,
"[GÓC NHỜ TƯ VẤN]: tôi muốn hỏi về việc prompt engineering cho ảnh tạo sinh qua Stable Diffusion.
1/ Có bạn nào chuẩn bị dữ liệu cả ảnh và text để finetune các models Stable Diffusion và ControlNet chưa? Nếu có, xin bạn hãy chia sẻ kinh nghiệm về việc này;
2/ Kinh nghiệm của bạn về việc prompt sao cho models sinh ra ảnh mà bạn mong muốn?
Mình xin cảm ơn những chia sẻ của các bạn trước nhé.
Trân trọng","[GÓC NHỜ TƯ VẤN]: tôi muốn hỏi về việc prompt engineering cho ảnh tạo sinh qua Stable Diffusion. 1/ Có bạn nào chuẩn bị dữ liệu cả ảnh và text để finetune các models Stable Diffusion và ControlNet chưa? Nếu có, xin bạn hãy chia sẻ kinh nghiệm về việc này; 2/ Kinh nghiệm của bạn về việc prompt sao cho models sinh ra ảnh mà bạn mong muốn? Mình xin cảm ơn những chia sẻ của các bạn trước nhé. Trân trọng",,,#Q&A,,
"Xin chào mn. Có thể nhiều người đã biết trang này nhưng mình thấy khá hữu ích nên vẫn muốn chia sẻ cùng mn. Nếu các bạn muốn tìm luận văn hoặc luận án của các trường dh trên thế giới có thể vào đây nhé. Ko đầy đủ hết các trường nhưng lĩnh vực khá đa dạng, tất nhiên bao gồm cả AI.
https://oatd.org
Hi vọng có thể giúp ích cho mn!","Xin chào mn. Có thể nhiều người đã biết trang này nhưng mình thấy khá hữu ích nên vẫn muốn chia sẻ cùng mn. Nếu các bạn muốn tìm luận văn hoặc luận án của các trường dh trên thế giới có thể vào đây nhé. Ko đầy đủ hết các trường nhưng lĩnh vực khá đa dạng, tất nhiên bao gồm cả AI. https://oatd.org Hi vọng có thể giúp ích cho mn!",,,#sharing,,
"Các a/c cho em hỏi một vấn đề nho nhỏ ạ? Em học về OCR và e đang clone paddleOCR bản release 2.6 về colab để train bài toán text recognition. Tuy nhiên khi clone về và chạy dòng lệnh như trên ảnh, thì gặp lỗi trong file setup.py. A/c có thể cho em biết là tại sao lỗi và các sửa như thế nào không ạ? Em xin chân thành cảm ơn ạ. (lỗi này không phải do pip và em cũng đã tìm hiểu nhiều mà mãi chưa sửa đc ạ TT)","Các a/c cho em hỏi một vấn đề nho nhỏ ạ? Em học về OCR và e đang clone paddleOCR bản release 2.6 về colab để train bài toán text recognition. Tuy nhiên khi clone về và chạy dòng lệnh như trên ảnh, thì gặp lỗi trong file setup.py. A/c có thể cho em biết là tại sao lỗi và các sửa như thế nào không ạ? Em xin chân thành cảm ơn ạ. (lỗi này không phải do pip và em cũng đã tìm hiểu nhiều mà mãi chưa sửa đc ạ TT)",,,"#Q&A, #cv",,
Em được giao phân loại ý định của 1 người trong 1 đoạn hội thoại ít nhất 2 người. Anh chị nào có kinh nghiệm hay biết có paper nào có thể cho em xin chút hướng dẫn không ạ. Em search gần như chỉ có phân loại ý định của tất cả mn. Em cảm ơn ạ.,Em được giao phân loại ý định của 1 người trong 1 đoạn hội thoại ít nhất 2 người. Anh chị nào có kinh nghiệm hay biết có paper nào có thể cho em xin chút hướng dẫn không ạ. Em search gần như chỉ có phân loại ý định của tất cả mn. Em cảm ơn ạ.,,,#Q&A,,
Mọi người cho em hỏi có ai có bộ test data của cuộc thi Phân loại sắc thái bình luận https://www.aivivn.com/contests/1 . Em cảm ơn ạ.,Mọi người cho em hỏi có ai có bộ test data của cuộc thi Phân loại sắc thái bình luận https://www.aivivn.com/contests/1 . Em cảm ơn ạ.,,,"#Q&A, #nlp",,
"Em đang có bài tập lớn dùng RNN hoặc LSTM để cân bằng kênh âm thanh, bro cũng mảng này không cho em xin ít kinh nghiệm với ạ","Em đang có bài tập lớn dùng RNN hoặc LSTM để cân bằng kênh âm thanh, bro cũng mảng này không cho em xin ít kinh nghiệm với ạ",,,"#Q&A, #deep_learning",,
"PROJECT DỰ BÁO ĐỘT QUỴ TIM (Heart Attack) Full
(vừa ý nghĩa vừa học tập)
---
Tài liệu hướng dẫn Từng step by step cho 1 Project ML:
- Tìm hiểu dữ liệu,
- Quan sát dữ liệu,
- Pre processsing : Gán nhãn, phân loại dữ liệu. Chia tệp Data thành các biến độc lập và phụ thuộc. Chia data thành Training & Test Dataset.
- Tạo ML Modeling function để áp dụng thuật toán phân loại
---
Link tải file pdf : https://drive.google.com/drive/u/0/folders/13s3qEAGXMaLELJSWe_MZyb9uITWK1ZU8
Link Dataset Kaggle: https://www.kaggle.com/.../heart-attack-analysis...
Share by: Learning and sharing for machine learning&Ai","PROJECT DỰ BÁO ĐỘT QUỴ TIM (Heart Attack) Full (vừa ý nghĩa vừa học tập) --- Tài liệu hướng dẫn Từng step by step cho 1 Project ML: - Tìm hiểu dữ liệu, - Quan sát dữ liệu, - Pre processsing : Gán nhãn, phân loại dữ liệu. Chia tệp Data thành các biến độc lập và phụ thuộc. Chia data thành Training & Test Dataset. - Tạo ML Modeling function để áp dụng thuật toán phân loại --- Link tải file pdf : https://drive.google.com/drive/u/0/folders/13s3qEAGXMaLELJSWe_MZyb9uITWK1ZU8 Link Dataset Kaggle: https://www.kaggle.com/.../heart-attack-analysis... Share by: Learning and sharing for machine learning&Ai",,,#sharing,,
"E chào mọi người ạ, e cài công cụ annotation CVAT trên windows qua git bash theo link hưỡng dẫn https://opencv.github.io/cvat/docs/administration/basics/installation/,
đến bước tạo superuser thì bị báo lỗi
: bash: sudo: command not found
và
$ docker exec -it cvat_server /bin/bash
the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'
mng cho e lời khuyên với ạ e cám ơn mng nhiều!!!","E chào mọi người ạ, e cài công cụ annotation CVAT trên windows qua git bash theo link hưỡng dẫn https://opencv.github.io/cvat/docs/administration/basics/installation/, đến bước tạo superuser thì bị báo lỗi : bash: sudo: command not found và $ docker exec -it cvat_server /bin/bash the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty' mng cho e lời khuyên với ạ e cám ơn mng nhiều!!!",,,#Q&A,,
"Chào mọi người, em đang thử sửa cách trích xuất đặc trưng từ ảnh và muốn hỏi như sau. 
Em có một bức ảnh và các bounding box phát hiện ra người từ bức ảnh đó. Ở mô hình đề xuất của 1 paper em đang đọc, tác giả có dùng tọa độ của các bounding box (kích thước khác nhau) để cắt ra ảnh của từng người trong khung hình. Sau đó, mỗi bức ảnh được resize về kích thước (224, 224) và đưa qua ResNet-34 để trích xuất ra 1 véc-tơ đặc trưng 512 chiều. 
Bây giờ em muốn dùng 1 mạng CNN để trích xuất đặc trưng toàn ảnh thành 1 feature map, rồi biến đổi width và height của bounding box về tương ứng với width và height của feature map, và sau đó trích xuất đặc trưng từ feature map và biến đổi về thành 1 véc-tơ 512 chiều thì có khả thi không ạ?.
Em có thử và biết rằng việc trích xuất từ các bounding box có kích thước khác nhau sẽ cho ra các Tensor có kích thước khác nhau và không reshape về cùng 1 kích thước được. Mọi người cho em xin gợi ý với ạ. ","Chào mọi người, em đang thử sửa cách trích xuất đặc trưng từ ảnh và muốn hỏi như sau. Em có một bức ảnh và các bounding box phát hiện ra người từ bức ảnh đó. Ở mô hình đề xuất của 1 paper em đang đọc, tác giả có dùng tọa độ của các bounding box (kích thước khác nhau) để cắt ra ảnh của từng người trong khung hình. Sau đó, mỗi bức ảnh được resize về kích thước (224, 224) và đưa qua ResNet-34 để trích xuất ra 1 véc-tơ đặc trưng 512 chiều. Bây giờ em muốn dùng 1 mạng CNN để trích xuất đặc trưng toàn ảnh thành 1 feature map, rồi biến đổi width và height của bounding box về tương ứng với width và height của feature map, và sau đó trích xuất đặc trưng từ feature map và biến đổi về thành 1 véc-tơ 512 chiều thì có khả thi không ạ?. Em có thử và biết rằng việc trích xuất từ các bounding box có kích thước khác nhau sẽ cho ra các Tensor có kích thước khác nhau và không reshape về cùng 1 kích thước được. Mọi người cho em xin gợi ý với ạ.",,,"#Q&A, #cv, #deep_learning",,
"Trí tuệ Nhân tạo (AI) đã len lỏi khắp nơi, đi vào mọi ngóc ngách của cuộc sống, công việc, giải trí,... Vậy có ai chợt đặt ra câu hỏi: AI là gì? Tại sao hiện nay nó đang nhận được sự quan tâm rất lớn từ khắp mọi ngành?
#AI #NVIDIA #NTC #TheGioiMayChu","Trí tuệ Nhân tạo (AI) đã len lỏi khắp nơi, đi vào mọi ngóc ngách của cuộc sống, công việc, giải trí,... Vậy có ai chợt đặt ra câu hỏi: AI là gì? Tại sao hiện nay nó đang nhận được sự quan tâm rất lớn từ khắp mọi ngành?",#AI	#NVIDIA	#NTC	#TheGioiMayChu,,#sharing,,
"Chào anh chị và các bạn trong nhóm ạ, hiện tại em đang làm 1 project về sinh giọng nói ạ, nhưng em lại không có nhiều kiến thức về mảng này nên kết quả ra không được như ý ạ.
Cho em hỏi ở Việt Nam của mình có cộng dồng nào xử lý âm thanh không ạ? Vì em muốn tham khảo 1 số kiến thức ạ.
Cảm ơn các bạn và anh chị trong nhóm.","Chào anh chị và các bạn trong nhóm ạ, hiện tại em đang làm 1 project về sinh giọng nói ạ, nhưng em lại không có nhiều kiến thức về mảng này nên kết quả ra không được như ý ạ. Cho em hỏi ở Việt Nam của mình có cộng dồng nào xử lý âm thanh không ạ? Vì em muốn tham khảo 1 số kiến thức ạ. Cảm ơn các bạn và anh chị trong nhóm.",,,#Q&A,,
"em chào mọi người ạ, em có tập dữ liệu là các bản tin trạng thái của thiết bị trong Smart Home gửi lên server.
Từ tập dữ liệu này, xử lý bài toán nào được nhỉ....
Em đang định phân tích hành vi người dùng, nhưng cũng chưa biết kĩ thuật sử dụng. Ai có định hướng và phương pháp nào không ạ giúp em với","em chào mọi người ạ, em có tập dữ liệu là các bản tin trạng thái của thiết bị trong Smart Home gửi lên server. Từ tập dữ liệu này, xử lý bài toán nào được nhỉ.... Em đang định phân tích hành vi người dùng, nhưng cũng chưa biết kĩ thuật sử dụng. Ai có định hướng và phương pháp nào không ạ giúp em với",,,#Q&A,,
"Có vẻ như các bạn khá thích thú với các tài liệu hướng dẫn, nhất là dưới dạng cookbook. Nay mình giới thiệu thêm (chắc chắn có nhiều bạn đã biết tới) về cookbook của OpenAI. Hiện nó đã có tới >46k sao, và được cập nhật thường xuyên. Mình chắc chắn sẽ nghiền ngẫm repository này","Có vẻ như các bạn khá thích thú với các tài liệu hướng dẫn, nhất là dưới dạng cookbook. Nay mình giới thiệu thêm (chắc chắn có nhiều bạn đã biết tới) về cookbook của OpenAI. Hiện nó đã có tới >46k sao, và được cập nhật thường xuyên. Mình chắc chắn sẽ nghiền ngẫm repository này",,,#sharing,,
Có anh chị nào đã từng dùng CRAFT để trích xuất thông tin thành công chưa ạ?,Có anh chị nào đã từng dùng CRAFT để trích xuất thông tin thành công chưa ạ?,,,#Q&A,,
Kaggle cung cấp các khóa học chất lượng cao về Khoa học Dữ liệu.,Kaggle cung cấp các khóa học chất lượng cao về Khoa học Dữ liệu.,,,"#sharing, #data",,
"Các hệ thống OVX tích hợp NVIDIA GPU mới được thiết kế để tăng tốc quy trình đào tạo và suy luận AI, các tải xử lý chuyên sâu về đồ họa. Một loạt các nhà cung cấp lớn như Dell Technologies, Hewlett Packard Enterprise, Lenovo, Supermicro,v.v... sắp cho ra mắt sản phẩm.
#NVIDIA #OVX #OvxServer #Omniverse","Các hệ thống OVX tích hợp NVIDIA GPU mới được thiết kế để tăng tốc quy trình đào tạo và suy luận AI, các tải xử lý chuyên sâu về đồ họa. Một loạt các nhà cung cấp lớn như Dell Technologies, Hewlett Packard Enterprise, Lenovo, Supermicro,v.v... sắp cho ra mắt sản phẩm.",#NVIDIA	#OVX	#OvxServer	#Omniverse,,#sharing,,
Xin chào mn. Mn cho mình hỏi hiện nay có mô hình hoặc phương pháp nào hiệu quả trong việc detect những đối tượng rất rất nhỏ (tạm gọi tiny object). Mong được mn chia sẻ.,Xin chào mn. Mn cho mình hỏi hiện nay có mô hình hoặc phương pháp nào hiệu quả trong việc detect những đối tượng rất rất nhỏ (tạm gọi tiny object). Mong được mn chia sẻ.,,,"#Q&A, #cv",,
"Padding trong Convolutional Neural Network (CNN) là quá trình thêm các giá trị 0 (hoặc giá trị khác tuỳ theo cách thiết lập) vào xung quanh các biên của ảnh hoặc đặc trưng trước khi thực hiện phép tích chập. Mục đích chính của việc thêm padding là tăng kích thước của đặc trưng hoặc ảnh ban đầu để đảm bảo rằng các biên của ảnh cũng được xử lý một cách hiệu quả.
Có hai loại padding chính:
Valid Padding (Zero Padding): Trong loại này, không có padding được thêm vào, và phép tích chập được thực hiện trực tiếp trên các vùng không gian. Điều này dẫn đến việc giảm kích thước của đặc trưng hoặc ảnh sau khi thực hiện phép tích chập.
Same Padding: Đây là loại padding phổ biến, trong đó padding được thêm vào sao cho kích thước của đặc trưng hoặc ảnh sau khi thực hiện phép tích chập vẫn giữ nguyên kích thước so với ban đầu. Thông thường, giá trị padding được tính dựa trên kích thước của ma trận bộ lọc và các bước của phép tích chập.
Padding có thể giúp duy trì thông tin ở biên của ảnh hoặc đặc trưng sau khi thực hiện phép tích chập và giúp tránh việc mất mát thông tin quá nhiều. Nó cũng có thể giúp kiểm soát việc giảm kích thước quá nhanh của đặc trưng, đặc biệt khi sử dụng nhiều lớp tích chập liên tiếp.","Padding trong Convolutional Neural Network (CNN) là quá trình thêm các giá trị 0 (hoặc giá trị khác tuỳ theo cách thiết lập) vào xung quanh các biên của ảnh hoặc đặc trưng trước khi thực hiện phép tích chập. Mục đích chính của việc thêm padding là tăng kích thước của đặc trưng hoặc ảnh ban đầu để đảm bảo rằng các biên của ảnh cũng được xử lý một cách hiệu quả. Có hai loại padding chính: Valid Padding (Zero Padding): Trong loại này, không có padding được thêm vào, và phép tích chập được thực hiện trực tiếp trên các vùng không gian. Điều này dẫn đến việc giảm kích thước của đặc trưng hoặc ảnh sau khi thực hiện phép tích chập. Same Padding: Đây là loại padding phổ biến, trong đó padding được thêm vào sao cho kích thước của đặc trưng hoặc ảnh sau khi thực hiện phép tích chập vẫn giữ nguyên kích thước so với ban đầu. Thông thường, giá trị padding được tính dựa trên kích thước của ma trận bộ lọc và các bước của phép tích chập. Padding có thể giúp duy trì thông tin ở biên của ảnh hoặc đặc trưng sau khi thực hiện phép tích chập và giúp tránh việc mất mát thông tin quá nhiều. Nó cũng có thể giúp kiểm soát việc giảm kích thước quá nhanh của đặc trưng, đặc biệt khi sử dụng nhiều lớp tích chập liên tiếp.",,,"#sharing, #deep_learning",,
"E chào mng ạ, e đang xử lí dataset cho bài toán defect detection bằng YOLO, em băn khoăn là ngoài các ảnh thuộc các classes lỗi, thì e có nên cho vào ảnh không có lỗi (Just as background without object), nếu thêm vào thì có cải thiện hiệu năng của model không ạ?? E cám ơn mng ạ!!!","E chào mng ạ, e đang xử lí dataset cho bài toán defect detection bằng YOLO, em băn khoăn là ngoài các ảnh thuộc các classes lỗi, thì e có nên cho vào ảnh không có lỗi (Just as background without object), nếu thêm vào thì có cải thiện hiệu năng của model không ạ?? E cám ơn mng ạ!!!",,,"#Q&A, #data, #cv",,
"NVIDIA AI Workbench là gì? Tại sao nó hữu ích cho việc phát triển các ứng dụng Generative AI và đào tạo, tùy biến các mô hình ngôn ngữ lớn nói chung?
#NVIDIA #AI #Workbench #GenerativeAI #LLM","NVIDIA AI Workbench là gì? Tại sao nó hữu ích cho việc phát triển các ứng dụng Generative AI và đào tạo, tùy biến các mô hình ngôn ngữ lớn nói chung?",#NVIDIA	#AI	#Workbench	#GenerativeAI	#LLM,,,,
"Hi mng em có một số thắc mắc ở giai đoạn tiền xử lý data khi mình làm về Object Detection ạ.
Lúc mình đã có một tập dữ liệu được label từ tool labelImg trên github thì em thấy thường thì sẽ phải cần tiền xử lý dữ liệu này rồi mới cho vào train. Em có đọc một pj trên Kaggle thì thấy ngta để mục đó là Data pipeline thì em không hiểu lắm về cụ thể các bước trong đây mình cần làm gì ạ? Mình có những giai đoạn nào những việc làm gì mình cần để ý tới và cần phải làm trong bước này (Data pipeline và pre-processing data).
Mong mọi người giúp em ạ. Nếu được thì mọi người có thể cho em một số trang hoặc sách có nói cụ thể về quá trình này được không ạ. Em cảm ơn mọi người.",Hi mng em có một số thắc mắc ở giai đoạn tiền xử lý data khi mình làm về Object Detection ạ. Lúc mình đã có một tập dữ liệu được label từ tool labelImg trên github thì em thấy thường thì sẽ phải cần tiền xử lý dữ liệu này rồi mới cho vào train. Em có đọc một pj trên Kaggle thì thấy ngta để mục đó là Data pipeline thì em không hiểu lắm về cụ thể các bước trong đây mình cần làm gì ạ? Mình có những giai đoạn nào những việc làm gì mình cần để ý tới và cần phải làm trong bước này (Data pipeline và pre-processing data). Mong mọi người giúp em ạ. Nếu được thì mọi người có thể cho em một số trang hoặc sách có nói cụ thể về quá trình này được không ạ. Em cảm ơn mọi người.,,,"#Q&A, #data, #cv",,
700-page PDF — [#Algorithms] for Decision-Making — download brilliant & comprehensive FREE eBook from MIT: http://bit.ly/3wyNcnQ,700-page PDF — [#Algorithms] for Decision-Making — download brilliant & comprehensive FREE eBook from MIT: http://bit.ly/3wyNcnQ,,,,,
"Chào các bạn,
Các bạn cho mình hỏi framework nào về reinforcement learning good nhất cho training một Agent ạ?","Chào các bạn, Các bạn cho mình hỏi framework nào về reinforcement learning good nhất cho training một Agent ạ?",,,"#Q&A, #machine_learning",,
"Hiệp hội OpenUSD (Alliance for OpenUSD) sẽ tiến đến đảm bảo khả năng tương thích hoàn toàn cho các nội dung và công cụ 3D nhằm triển khai số hóa giữa các ngành công nghiệp.
#OpenUSD #AOUSD #UniversalSceneDescription #NVIDIA #3dworld",Hiệp hội OpenUSD (Alliance for OpenUSD) sẽ tiến đến đảm bảo khả năng tương thích hoàn toàn cho các nội dung và công cụ 3D nhằm triển khai số hóa giữa các ngành công nghiệp.,#OpenUSD	#AOUSD	#UniversalSceneDescription	#NVIDIA	#3dworld,,#sharing,,
"Mạng nơ-ron tích chập (CNN) là một kiến trúc mạng nơ-ron đặc biệt dành cho việc xử lý dữ liệu hình ảnh và giúp máy tính hiểu và phân tích hình ảnh một cách tự động. Nguyên tắc hoạt động của CNN dựa trên ba khái niệm chính: tích chập, tổng hợp và kích hoạt.
Tích chập (Convolution): Lớp tích chập là lớp đầu tiên trong mạng CNN. Nó sử dụng các bộ lọc (hay còn gọi là kernel) để trượt qua ảnh đầu vào. Mỗi bộ lọc có thể nhận biết các đặc trưng cụ thể trong ảnh như cạnh, góc, hoặc hình dạng. Khi bộ lọc trượt qua ảnh, nó tạo ra các bản đồ đặc trưng (feature maps) bằng cách thực hiện phép tích chập giữa bộ lọc và vùng tương ứng trên ảnh. Các bản đồ đặc trưng này giúp mô hình nhận biết các đặc trưng quan trọng trong hình ảnh.
Tổng hợp (Pooling): Lớp tổng hợp thường đặt sau lớp tích chập. Nhiệm vụ của lớp này là giảm kích thước của các bản đồ đặc trưng bằng cách lấy giá trị lớn nhất hoặc trung bình từ các vùng nhỏ trên bản đồ đặc trưng. Việc này giúp giảm số lượng tham số và chi phí tính toán, đồng thời làm giảm nguy cơ overfitting.
Kích hoạt (Activation): Lớp kích hoạt áp dụng một hàm kích hoạt phi tuyến lên các giá trị trong các bản đồ đặc trưng. Phép kích hoạt này giúp mô hình học cách biểu diễn các đặc trưng phức tạp và tạo tính phi tuyến cho mạng.
Lớp kết nối đầy đủ (Fully Connected Layer): Sau khi thông qua các lớp tích chập, tổng hợp và kích hoạt, dữ liệu được đưa vào lớp kết nối đầy đủ để thực hiện các tác vụ như phân loại hoặc dự đoán. Trong lớp này, các nơ-ron kết nối với tất cả các nơ-ron trong lớp trước, giúp học cách tổ hợp các đặc trưng để đưa ra dự đoán cuối cùng.
Toàn bộ quá trình này, từ lớp tích chập cho đến lớp kết nối đầy đủ, là quá trình học tự động. Mạng nơ-ron tự điều chỉnh các trọng số của các lớp để tối ưu hóa hiệu suất cho nhiệm vụ cụ thể, chẳng hạn như phân loại ảnh.","Mạng nơ-ron tích chập (CNN) là một kiến trúc mạng nơ-ron đặc biệt dành cho việc xử lý dữ liệu hình ảnh và giúp máy tính hiểu và phân tích hình ảnh một cách tự động. Nguyên tắc hoạt động của CNN dựa trên ba khái niệm chính: tích chập, tổng hợp và kích hoạt. Tích chập (Convolution): Lớp tích chập là lớp đầu tiên trong mạng CNN. Nó sử dụng các bộ lọc (hay còn gọi là kernel) để trượt qua ảnh đầu vào. Mỗi bộ lọc có thể nhận biết các đặc trưng cụ thể trong ảnh như cạnh, góc, hoặc hình dạng. Khi bộ lọc trượt qua ảnh, nó tạo ra các bản đồ đặc trưng (feature maps) bằng cách thực hiện phép tích chập giữa bộ lọc và vùng tương ứng trên ảnh. Các bản đồ đặc trưng này giúp mô hình nhận biết các đặc trưng quan trọng trong hình ảnh. Tổng hợp (Pooling): Lớp tổng hợp thường đặt sau lớp tích chập. Nhiệm vụ của lớp này là giảm kích thước của các bản đồ đặc trưng bằng cách lấy giá trị lớn nhất hoặc trung bình từ các vùng nhỏ trên bản đồ đặc trưng. Việc này giúp giảm số lượng tham số và chi phí tính toán, đồng thời làm giảm nguy cơ overfitting. Kích hoạt (Activation): Lớp kích hoạt áp dụng một hàm kích hoạt phi tuyến lên các giá trị trong các bản đồ đặc trưng. Phép kích hoạt này giúp mô hình học cách biểu diễn các đặc trưng phức tạp và tạo tính phi tuyến cho mạng. Lớp kết nối đầy đủ (Fully Connected Layer): Sau khi thông qua các lớp tích chập, tổng hợp và kích hoạt, dữ liệu được đưa vào lớp kết nối đầy đủ để thực hiện các tác vụ như phân loại hoặc dự đoán. Trong lớp này, các nơ-ron kết nối với tất cả các nơ-ron trong lớp trước, giúp học cách tổ hợp các đặc trưng để đưa ra dự đoán cuối cùng. Toàn bộ quá trình này, từ lớp tích chập cho đến lớp kết nối đầy đủ, là quá trình học tự động. Mạng nơ-ron tự điều chỉnh các trọng số của các lớp để tối ưu hóa hiệu suất cho nhiệm vụ cụ thể, chẳng hạn như phân loại ảnh.",,,"#Q&A, #deep_learning",,
"Chào các a/chị của group, em có học và tập tành ứng dụng AI được trong khoảng hơn 1 năm thì có một vài thắc mắc mong a/chị có thể giải đáp giúp em ạ:
Em có thử hai mảng lớn là CV và NLP và cũng tìm tòi các paper đọc để hiểu idea rồi so sánh với các idea khác cũng như rèn luyện kỹ năng lập trình tương ứng cho các paper thì có thấy một điều là nghành này quá rộng để đào sâu cho từng chuyên môn nếu muốn đạt level expert. Cái này đặt ra cho em thắc mắc là liệu có cái gọi là ""AI Engineer"" hay không khi mà lượng kiến thức của một mảng (ví dụ: NLP) đã thực sự rất sâu rồi (theo em sâu là hiểu cặn kẽ, biết ưu nhược điểm và biết cách tìm ra solution tối ưu, chưa kể các vấn đề liên quan như data, deploy, etc.). Vậy theo cái title của vị trí kia thì đồng nghĩa họ phải hiểu sâu rất nhiều mảng phải không ạ ? (Nhiều khi còn cả mấy mảng mở rộng như kiểu Generative, Video, 3D, etc.)
Trong thực tế thì một flow làm việc thì mọi người thường chọn cách tiếp cận với mô hình như nào ạ ? Mọi người sẽ code lại paper từ đầu rồi tweak thủ công hay là sẽ bê nguyên wrapper như kiểu HuggingFace để sử dụng ạ ?
Các vấn đề chuyên sâu về MLOps thì một kỹ sư trong mảng này nên có trong khoảng bao năm kinh nghiệm ạ là tốt ạ ? Liệu nhân lực mảng này và mảng Software nói chung sẽ giao thoa về những skills gì ạ ? Theo em nghĩ là lập trình nói chung, Cloud, Design.
Em cảm ơn a/chị nhiều ạ.","Chào các a/chị của group, em có học và tập tành ứng dụng AI được trong khoảng hơn 1 năm thì có một vài thắc mắc mong a/chị có thể giải đáp giúp em ạ: Em có thử hai mảng lớn là CV và NLP và cũng tìm tòi các paper đọc để hiểu idea rồi so sánh với các idea khác cũng như rèn luyện kỹ năng lập trình tương ứng cho các paper thì có thấy một điều là nghành này quá rộng để đào sâu cho từng chuyên môn nếu muốn đạt level expert. Cái này đặt ra cho em thắc mắc là liệu có cái gọi là ""AI Engineer"" hay không khi mà lượng kiến thức của một mảng (ví dụ: NLP) đã thực sự rất sâu rồi (theo em sâu là hiểu cặn kẽ, biết ưu nhược điểm và biết cách tìm ra solution tối ưu, chưa kể các vấn đề liên quan như data, deploy, etc.). Vậy theo cái title của vị trí kia thì đồng nghĩa họ phải hiểu sâu rất nhiều mảng phải không ạ ? (Nhiều khi còn cả mấy mảng mở rộng như kiểu Generative, Video, 3D, etc.) Trong thực tế thì một flow làm việc thì mọi người thường chọn cách tiếp cận với mô hình như nào ạ ? Mọi người sẽ code lại paper từ đầu rồi tweak thủ công hay là sẽ bê nguyên wrapper như kiểu HuggingFace để sử dụng ạ ? Các vấn đề chuyên sâu về MLOps thì một kỹ sư trong mảng này nên có trong khoảng bao năm kinh nghiệm ạ là tốt ạ ? Liệu nhân lực mảng này và mảng Software nói chung sẽ giao thoa về những skills gì ạ ? Theo em nghĩ là lập trình nói chung, Cloud, Design. Em cảm ơn a/chị nhiều ạ.",,,#Q&A,,
"Chào cả nhà. Em là dân ngoại đạo (Y khoa) nhưng đang dịch một bài báo liên quan đến sự kết hợp giữa ML và Y học ạ.
Hiện tại em đã dịch xong rồi nhưng cần người có chuyên môn review vì em không phải người trong ngành nên sợ chưa dùng từ đúng chuẩn.
Xin cả nhà giúp đỡ ạ.",Chào cả nhà. Em là dân ngoại đạo (Y khoa) nhưng đang dịch một bài báo liên quan đến sự kết hợp giữa ML và Y học ạ. Hiện tại em đã dịch xong rồi nhưng cần người có chuyên môn review vì em không phải người trong ngành nên sợ chưa dùng từ đúng chuẩn. Xin cả nhà giúp đỡ ạ.,,,#Q&A,,
"Kubernetes (K8s) không còn là một công cụ chỉ để chạy các workload như ứng dụng web hay microservices, nó chính là nền tảng lý tưởng để hỗ trợ toàn bộ vòng đời của các workload lớn về Trí tuệ nhân tạo (AI) và Học máy (ML), chẳng hạn như các Mô hình ngôn ngữ lớn (LLMs).
#kubernetes #k8s #AI #LLM","Kubernetes (K8s) không còn là một công cụ chỉ để chạy các workload như ứng dụng web hay microservices, nó chính là nền tảng lý tưởng để hỗ trợ toàn bộ vòng đời của các workload lớn về Trí tuệ nhân tạo (AI) và Học máy (ML), chẳng hạn như các Mô hình ngôn ngữ lớn (LLMs).",#kubernetes	#k8s	#AI	#LLM,,#sharing,,
"Mình tin ở đây có nhiều bạn biết đến trang này https://github.com/TheAlgorithms/ do các bạn Ấn Độ viết để hướng dẫn về giải thuật bằng các ngôn ngữ khác nhau. Trong đó repo https://github.com/TheAlgorithms/Python có nhiều sao nhất, với hơn 164k. Hi vọng, mình chia sẻ ở đây, cho các bạn chưa biết tới nó, có thể là tài liệu tham khảo tốt cho mọi người","Mình tin ở đây có nhiều bạn biết đến trang này https://github.com/TheAlgorithms/ do các bạn Ấn Độ viết để hướng dẫn về giải thuật bằng các ngôn ngữ khác nhau. Trong đó repo https://github.com/TheAlgorithms/Python có nhiều sao nhất, với hơn 164k. Hi vọng, mình chia sẻ ở đây, cho các bạn chưa biết tới nó, có thể là tài liệu tham khảo tốt cho mọi người",,,"#sharing, #math",,
"E chào các anh chị a, e đang làm bài toán object detection, e có dataset và e muốn visualize cả bounding box lên cùng 1 ảnh để check bounding box và label đã chính xác hay chưa, E dùng công cụ online là roboflow nhưng nó chậm quá, anh chị có biết tool nào offline không ạ, em cám ơn ạ","E chào các anh chị a, e đang làm bài toán object detection, e có dataset và e muốn visualize cả bounding box lên cùng 1 ảnh để check bounding box và label đã chính xác hay chưa, E dùng công cụ online là roboflow nhưng nó chậm quá, anh chị có biết tool nào offline không ạ, em cám ơn ạ",,,"#Q&A, #cv",,
"Xin chào cac ban,
Minh đã tong hop một repo cung cấp các tài nguyên cho OOD detection, robustness, and generalization in Deep Learning. Repo nay bao gom các bài báo, bài nói chuyện, thư viện, papers, v.v. Repo này sẽ được duy trì và cập nhật với các nguồn chất lượng cao! Minh hy vọng nó sẽ trở thành mot noi lý tưởng cho bất kỳ thứ gì OOD trong bookmark của bạn. Hãy cho nó một ngôi sao de ung ho minh nếu bạn thấy nó hữu ích;) Cam on nhieu nha!","Xin chào cac ban, Minh đã tong hop một repo cung cấp các tài nguyên cho OOD detection, robustness, and generalization in Deep Learning. Repo nay bao gom các bài báo, bài nói chuyện, thư viện, papers, v.v. Repo này sẽ được duy trì và cập nhật với các nguồn chất lượng cao! Minh hy vọng nó sẽ trở thành mot noi lý tưởng cho bất kỳ thứ gì OOD trong bookmark của bạn. Hãy cho nó một ngôi sao de ung ho minh nếu bạn thấy nó hữu ích;) Cam on nhieu nha!",,,"#sharing, #deep_learning",,
Các nhà nghiên cứu từ các trường đại học của Anh đã phát triển một AI có khả năng dự đoán thao tác gõ phím của người dùng với độ chính xác 95% bằng cách lắng nghe âm thanh phát ra khi gõ trên bàn phím.,Các nhà nghiên cứu từ các trường đại học của Anh đã phát triển một AI có khả năng dự đoán thao tác gõ phím của người dùng với độ chính xác 95% bằng cách lắng nghe âm thanh phát ra khi gõ trên bàn phím.,,,#sharing,,
"Em chào các tiền bối
Nay em ngoi lên đây để xin các tiền bối xem có quyển sách vào về Reinforcement Learning hay không ạ, kiểu vừa có code implemented vừa có phần giải thích
Em xin chân thành cảm ơn ạ","Em chào các tiền bối Nay em ngoi lên đây để xin các tiền bối xem có quyển sách vào về Reinforcement Learning hay không ạ, kiểu vừa có code implemented vừa có phần giải thích Em xin chân thành cảm ơn ạ",,,#Q&A,,
"Hi mng, em đang làm một mô hình nó nhận diện được con lắc đơn. Em có chuẩn bị một tập dữ liệu bằng các hình ảnh con lắc đơn và dùng LabelIMG để label ảnh, sau đó cho nó vào một file csv.
Em có thử train model trên tập dữ liệu đó thì nó cho ra một kết quả như ảnh dưới đây. Thì em có thắc mắc là liệu đây có fai một trường hợp Overfitting và làm sao để có thể sửa nó ạ.
Em có một câu hỏi nựa là sau khi em train xong như vậy thì em sẽ làm như nào để model có thể đưa ra dự đoán vậy ạ?
Mong mọi người giải đáp ạ 🥹 Em là newbie nên có những chỗ em chưa hiểu ạ.","Hi mng, em đang làm một mô hình nó nhận diện được con lắc đơn. Em có chuẩn bị một tập dữ liệu bằng các hình ảnh con lắc đơn và dùng LabelIMG để label ảnh, sau đó cho nó vào một file csv. Em có thử train model trên tập dữ liệu đó thì nó cho ra một kết quả như ảnh dưới đây. Thì em có thắc mắc là liệu đây có fai một trường hợp Overfitting và làm sao để có thể sửa nó ạ. Em có một câu hỏi nựa là sau khi em train xong như vậy thì em sẽ làm như nào để model có thể đưa ra dự đoán vậy ạ? Mong mọi người giải đáp ạ Em là newbie nên có những chỗ em chưa hiểu ạ.",,,"#Q&A, #cv",,
"E chào các ace ạ, e đang làm đề tài Fabric defect detection trên Yolov8 với dataset được cung cấp sẵn, e dùng default model YoloV8(s) để training nhưng kết quả kém quá ạ, Precision và recall và mAP chỉ tầm 0.5 và hàm loss rất cao, có phải dataset của em đang có vấn đề không ạ (em nghĩ là chất có thể chất lượng annontation không đảm bảo và imbalanced ạ), hoặc có thể do nguyên nhân nào khác mà output của em kém vậy ạ??, mng cho e lời khuyên để cải thiện được không ạ, em cảm ơn rất nhiều ạ!!","E chào các ace ạ, e đang làm đề tài Fabric defect detection trên Yolov8 với dataset được cung cấp sẵn, e dùng default model YoloV8(s) để training nhưng kết quả kém quá ạ, Precision và recall và mAP chỉ tầm 0.5 và hàm loss rất cao, có phải dataset của em đang có vấn đề không ạ (em nghĩ là chất có thể chất lượng annontation không đảm bảo và imbalanced ạ), hoặc có thể do nguyên nhân nào khác mà output của em kém vậy ạ??, mng cho e lời khuyên để cải thiện được không ạ, em cảm ơn rất nhiều ạ!!",,,"#Q&A, #cv",,
"Hi mọi người , mình có học về Vision Transfomer và thấy trong kiến trúc đó có 1 thứ gọi là residual connection(kết nối dư) , nó sẽ lấy input của 1 lớp trong 1 khối(ví dụ như khối multi self-attention) nối với output của lớp cuối cùng trong khối đó, mình đọc thì họ nói là trong quá trình truyền ngược, 1 số hàm kích hoạt phi tuyến tính sẽ bị bỏ qua. Nhưng mình vẫn lấn cấn 1 số thứ như là : cụ thể mục đích và nguyên nhân họ bỏ qua 1 số lớp hay hàm kích hoạt là gì, hàm kích hoạt hay lớp kiểu j sẽ bị loại bỏ và tại soa họ lại nối input của 1 lớp này với output của lớp cuối cùng. Ai có kiến thức giải đáp giúp mình với ạ. Thank mọi người!","Hi mọi người , mình có học về Vision Transfomer và thấy trong kiến trúc đó có 1 thứ gọi là residual connection(kết nối dư) , nó sẽ lấy input của 1 lớp trong 1 khối(ví dụ như khối multi self-attention) nối với output của lớp cuối cùng trong khối đó, mình đọc thì họ nói là trong quá trình truyền ngược, 1 số hàm kích hoạt phi tuyến tính sẽ bị bỏ qua. Nhưng mình vẫn lấn cấn 1 số thứ như là : cụ thể mục đích và nguyên nhân họ bỏ qua 1 số lớp hay hàm kích hoạt là gì, hàm kích hoạt hay lớp kiểu j sẽ bị loại bỏ và tại soa họ lại nối input của 1 lớp này với output của lớp cuối cùng. Ai có kiến thức giải đáp giúp mình với ạ. Thank mọi người!",,,"#Q&A, #deep_learning",,
"Mọi người cho em hỏi là trong bài toán của computer vision như object detection, segmentation,.. thì thường các cty sẽ tận dụng các model đã có sẵn để ứng dụng trực tiếp vd như YOLO, Detectron,.. hay sẽ phần lớn là tự build lại ạ? Em cảm ơn!","Mọi người cho em hỏi là trong bài toán của computer vision như object detection, segmentation,.. thì thường các cty sẽ tận dụng các model đã có sẵn để ứng dụng trực tiếp vd như YOLO, Detectron,.. hay sẽ phần lớn là tự build lại ạ? Em cảm ơn!",,,"#Q&A, #cv",,
"Hi mọi người,
Hôm nay Vietcuna trình bản phiên bản thứ 3 của LLM Vietcuna 7B.
Đây là một bản big update từ một mô hình instruction only sang model chat. Ngoài ra khả năng code cũng cải thiện đáng kể.
Đặc biệt, Vietcuna sẽ cho phép bạn dùng trực tiếp trên web như ChatGPT, và sẽ luôn miễn phí.
Link: https://chat.vilm.org/
Ngoài ra, bạn có thể tự tải và host model, finetune theo use case cụ thể của mình từ HuggingFace (chú ý đọc kĩ Model Card). Từ phiên bản thứ 3 trở đi, Vietcuna chính thức miễn phí cho mọi mục đích sử dụng.
Link HuggingFace: https://huggingface.co/vilm/vietcuna-7b-v3
Chúc mọi người có một trải nghiệm thật tốt :)","Hi mọi người, Hôm nay Vietcuna trình bản phiên bản thứ 3 của LLM Vietcuna 7B. Đây là một bản big update từ một mô hình instruction only sang model chat. Ngoài ra khả năng code cũng cải thiện đáng kể. Đặc biệt, Vietcuna sẽ cho phép bạn dùng trực tiếp trên web như ChatGPT, và sẽ luôn miễn phí. Link: https://chat.vilm.org/ Ngoài ra, bạn có thể tự tải và host model, finetune theo use case cụ thể của mình từ HuggingFace (chú ý đọc kĩ Model Card). Từ phiên bản thứ 3 trở đi, Vietcuna chính thức miễn phí cho mọi mục đích sử dụng. Link HuggingFace: https://huggingface.co/vilm/vietcuna-7b-v3 Chúc mọi người có một trải nghiệm thật tốt :)",,,"#sharing, #nlp",,
,nan,,,,,
"Các bác ơi có bác nào đang dùng api AI làm nét ảnh giống snapedit hoặcredmini ko ạ, cho e xin gợi ý một số api với ạ, free càng tốt. E cám ơn các bác nhiều","Các bác ơi có bác nào đang dùng api AI làm nét ảnh giống snapedit hoặcredmini ko ạ, cho e xin gợi ý một số api với ạ, free càng tốt. E cám ơn các bác nhiều",,,#Q&A,,
Đừng để tiếng Anh mãi là rào cản với bạn!,Đừng để tiếng Anh mãi là rào cản với bạn!,,,#sharing,,
"Mô hình OncoNPC được nghiên cứu và phát triển tại MIT và Viện Ung thư Dana-Farber cho phép dự đoán và xác định nơi hình thành khối u trước di căn thông qua phân tích khoảng 400 gen thường bị đột biến trong ung thư , từ đó đưa ra các liệu trình điều trị phù hợp và hiệu quả hơn so với trước đây
Một bước tiến mới trong việc áp dụng AI vào Y học
Các bạn có thể đọc thêm qua bài viết của MIT bên dưới","Mô hình OncoNPC được nghiên cứu và phát triển tại MIT và Viện Ung thư Dana-Farber cho phép dự đoán và xác định nơi hình thành khối u trước di căn thông qua phân tích khoảng 400 gen thường bị đột biến trong ung thư , từ đó đưa ra các liệu trình điều trị phù hợp và hiệu quả hơn so với trước đây Một bước tiến mới trong việc áp dụng AI vào Y học Các bạn có thể đọc thêm qua bài viết của MIT bên dưới",,,"#sharing, #cv",,
"Mọi người ơi, em được biết là thường thì Loss Function sử dụng lúc train và lúc test không giống nhau. Mọi người cho em hỏi cách xây dựng Loss Function ở lúc train và lúc test khác nhau như thế nào ạ, ngoài phần khác nhau về regularization thì chúng còn khác nhau ở phần Loss trung bình.
Mọi người giúp em hiểu rõ hơn về vấn đề này với .
Em cảm ơn !!!
Nguồn: Machine Learning: Mì, súp và....","Mọi người ơi, em được biết là thường thì Loss Function sử dụng lúc train và lúc test không giống nhau. Mọi người cho em hỏi cách xây dựng Loss Function ở lúc train và lúc test khác nhau như thế nào ạ, ngoài phần khác nhau về regularization thì chúng còn khác nhau ở phần Loss trung bình. Mọi người giúp em hiểu rõ hơn về vấn đề này với . Em cảm ơn !!! Nguồn: Machine Learning: Mì, súp và....",,,"#Q&A, #math, #machine_learning",,
"Em chào mọi người, em có đọc qua các bài viết về XAI. Về mặt lý thuyết là nó không phải blackbox. Tuy vậy, em vẫn còn những câu hỏi:
Nhưng thế nào là blackbox?
Và các mô hình như CNN và Transformer có là các blackbox hay không?
Những tiêu chuẩn nào để đánh giá một mô hình là blackbox?
Em cảm ơn mọi người ạ","Em chào mọi người, em có đọc qua các bài viết về XAI. Về mặt lý thuyết là nó không phải blackbox. Tuy vậy, em vẫn còn những câu hỏi: Nhưng thế nào là blackbox? Và các mô hình như CNN và Transformer có là các blackbox hay không? Những tiêu chuẩn nào để đánh giá một mô hình là blackbox? Em cảm ơn mọi người ạ",,,"#Q&A, #deep_learning",,
"Dạ em chào các anh chị và các bạn trong group. Em là người mới bắt đầu học nên muốn tìm các sách về Machine learning bằng tiếng Việt ạ. Anh, chị, bạn nào có sách giấy đã học xong không dùng nữa có thể pass lại cho em không ạ. Em ở Sài Gòn ạ.","Dạ em chào các anh chị và các bạn trong group. Em là người mới bắt đầu học nên muốn tìm các sách về Machine learning bằng tiếng Việt ạ. Anh, chị, bạn nào có sách giấy đã học xong không dùng nữa có thể pass lại cho em không ạ. Em ở Sài Gòn ạ.",,,"#Q&A, #machine_learning",,
"Chào mọi người , em là người mới học ML , em đang làm 1 dự án phân loại hình ảnh ( 1 số loại hoa ) thông qua app , model của em chạy ở phía server . Chuyện là em dùng model mobilenet_v2 của tensorflow hub để transfer , sau đó lưu model bằng lệnh : model.save(os.path.join('/content/drive/MyDrive/models','flower.h5')) .
sau đó load lại và sử dụng model bằng cách gọi :
flower_model = tf.keras.models.load_model(
(""model/flower/model_flower.h5""),
custom_objects={'KerasLayer':hub.KerasLayer}
)
nó hoạt động bình thường nhưng sau đó 1 khoảng thời gian thì nó lại không chạy được . nó báo : Exception encountered: Trying to load a model of incompatible/unknown type.
'C:\Users\Lenovo\AppData\Local\Temp\tfhub_modules\145bb06ec3b59b08fb564ab752bd5aa222bfb50a' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.
về cơ bản thì em đã có thể fix được lỗi bằng cách xóa cái folder 145bb06ec3b59b08fb564ab752bd5aa222bfb50a đi để tiến hành tải lại cái mới . nó hoạt động bình thường nhưng đấy chỉ là cách trị ngọn thôi không trị được gốc vì sau 1 khoảng thời gian nó lại ko hoạt động .
Những gì em hiểu được là : TensorFlow sẽ tạo một thư mục tạm thời để giữ các mô hình đã tải; tuy nhiên, sau một vài ngày hoặc lâu hơn, nội dung của các foler (mô hình đã tải) sẽ bị xóa. Sau đó, khi muốn tải lại một mô hình, TensorFlow sẽ định tuyến đến thư mục tạm thời, nhưng mô hình sẽ bị xóa khỏi thư mục tạm thời. Tức là cái cần thiết để chạy pre model là một model( tải từ tensorflow hub ) đang được lưu trữ ở 1 thư mục tạm thời ( thư mục sẽ bị xóa sau một khoảng thời gian ) vì vậy nó sẽ gây ra lỗi ko tìm thấy file .pb hay .pbtxt để chạy model .
Em nghĩ rằng lỗi sẽ nằm ở phần tensorflow nó quản lý tự động file pb ( pbtxt) khiến cho mình ko thể chỉ định chính xác 2 file pb và pbtxt vào 1 đường dẫn cụ thể ( ko thể bị xóa tự động ) . Hoặc lỗi gây ra bởi quá trình load model .
Mọi người ai đã từng gặp lỗi tương tụ như vậy chưa cho em hướng dẫn fix với .
Đây là những gì trong folder 145bb06ec3b59b08fb564ab752bd5aa222bfb50a","Chào mọi người , em là người mới học ML , em đang làm 1 dự án phân loại hình ảnh ( 1 số loại hoa ) thông qua app , model của em chạy ở phía server . Chuyện là em dùng model mobilenet_v2 của tensorflow hub để transfer , sau đó lưu model bằng lệnh : model.save(os.path.join('/content/drive/MyDrive/models','flower.h5')) . sau đó load lại và sử dụng model bằng cách gọi : flower_model = tf.keras.models.load_model( (""model/flower/model_flower.h5""), custom_objects={'KerasLayer':hub.KerasLayer} ) nó hoạt động bình thường nhưng sau đó 1 khoảng thời gian thì nó lại không chạy được . nó báo : Exception encountered: Trying to load a model of incompatible/unknown type. 'C:\Users\Lenovo\AppData\Local\Temp\tfhub_modules\145bb06ec3b59b08fb564ab752bd5aa222bfb50a' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'. về cơ bản thì em đã có thể fix được lỗi bằng cách xóa cái folder 145bb06ec3b59b08fb564ab752bd5aa222bfb50a đi để tiến hành tải lại cái mới . nó hoạt động bình thường nhưng đấy chỉ là cách trị ngọn thôi không trị được gốc vì sau 1 khoảng thời gian nó lại ko hoạt động . Những gì em hiểu được là : TensorFlow sẽ tạo một thư mục tạm thời để giữ các mô hình đã tải; tuy nhiên, sau một vài ngày hoặc lâu hơn, nội dung của các foler (mô hình đã tải) sẽ bị xóa. Sau đó, khi muốn tải lại một mô hình, TensorFlow sẽ định tuyến đến thư mục tạm thời, nhưng mô hình sẽ bị xóa khỏi thư mục tạm thời. Tức là cái cần thiết để chạy pre model là một model( tải từ tensorflow hub ) đang được lưu trữ ở 1 thư mục tạm thời ( thư mục sẽ bị xóa sau một khoảng thời gian ) vì vậy nó sẽ gây ra lỗi ko tìm thấy file .pb hay .pbtxt để chạy model . Em nghĩ rằng lỗi sẽ nằm ở phần tensorflow nó quản lý tự động file pb ( pbtxt) khiến cho mình ko thể chỉ định chính xác 2 file pb và pbtxt vào 1 đường dẫn cụ thể ( ko thể bị xóa tự động ) . Hoặc lỗi gây ra bởi quá trình load model . Mọi người ai đã từng gặp lỗi tương tụ như vậy chưa cho em hướng dẫn fix với . Đây là những gì trong folder 145bb06ec3b59b08fb564ab752bd5aa222bfb50a",,,"#Q&A, #python, #cv",,
9 FREE [#GenerativeAI] Courses from Google,9 FREE [#GenerativeAI] Courses from Google,,,,,
"Full Project: Airline Passenger Booking Analyze and Forecast using ML
---
Tài liệu hướng dẫn Từng step by step cho 1 Project ML:
- Tìm hiểu dữ liệu,
- Quan sát dữ liệu,
- Pre processsing : Gán nhãn, phân loại dữ liệu. Chia tệp Data thành các biến độc lập và phụ thuộc. Chia data thành Training & Test Dataset.
- Tạo ML Modeling function để áp dụng thuật toán phân loại
- Link download: https://bit.ly/Drive-MachineLearning
----------------
Tài liệu được chia sẻ bởi: Learning&sharing for machine learning & Ai","Full Project: Airline Passenger Booking Analyze and Forecast using ML --- Tài liệu hướng dẫn Từng step by step cho 1 Project ML: - Tìm hiểu dữ liệu, - Quan sát dữ liệu, - Pre processsing : Gán nhãn, phân loại dữ liệu. Chia tệp Data thành các biến độc lập và phụ thuộc. Chia data thành Training & Test Dataset. - Tạo ML Modeling function để áp dụng thuật toán phân loại - Link download: https://bit.ly/Drive-MachineLearning ---------------- Tài liệu được chia sẻ bởi: Learning&sharing for machine learning & Ai",,,"#sharing, #machine_learning, #data",,
"CUỐN SÁCH KHÔNG THỂ BỎ QUA VỀ AUTOMATED MACHINE LEARNING
Cuốn sách này giúp bạn hiểu rõ về AutoML - tự động hóa quá trình học máy.
✅ Hiểu về AutoML và vai trò quan trọng của AutoML trong việc đơn giản hóa quá trình học máy.
✅ Khám phá các phương pháp và kỹ thuật sử dụng trong AutoML, từ việc chọn đặc trưng, lựa chọn mô hình, điều chỉnh siêu tham số đến đánh giá mô hình.
✅ Tìm hiểu về các hệ thống và công cụ AutoML hiện có trên thị trường, qua các ví dụ và nghiên cứu thực tế.
✅ Đối mặt và vượt qua các thách thức, như khả năng mở rộng, khả năng giải thích và xử lý dữ liệu phức tạp.
✅ Độ tin cậy trong các hệ thống học máy tự động.
📚 Cuốn sách cung cấp các ứng dụng thực tế của AutoML trong các lĩnh vực khác nhau như y tế, tài chính và marketing, giúp bạn thấy rõ lợi ích của AutoML trong việc xây dựng mô hình học máy chính xác một cách nhanh chóng.
Link download qua drive dưới comment.
---------------
Tài liệu được chia sẻ bởi: Learning and Sharing for Machine Learning & AI","CUỐN SÁCH KHÔNG THỂ BỎ QUA VỀ AUTOMATED MACHINE LEARNING Cuốn sách này giúp bạn hiểu rõ về AutoML - tự động hóa quá trình học máy. Hiểu về AutoML và vai trò quan trọng của AutoML trong việc đơn giản hóa quá trình học máy. Khám phá các phương pháp và kỹ thuật sử dụng trong AutoML, từ việc chọn đặc trưng, lựa chọn mô hình, điều chỉnh siêu tham số đến đánh giá mô hình. Tìm hiểu về các hệ thống và công cụ AutoML hiện có trên thị trường, qua các ví dụ và nghiên cứu thực tế. Đối mặt và vượt qua các thách thức, như khả năng mở rộng, khả năng giải thích và xử lý dữ liệu phức tạp. Độ tin cậy trong các hệ thống học máy tự động. Cuốn sách cung cấp các ứng dụng thực tế của AutoML trong các lĩnh vực khác nhau như y tế, tài chính và marketing, giúp bạn thấy rõ lợi ích của AutoML trong việc xây dựng mô hình học máy chính xác một cách nhanh chóng. Link download qua drive dưới comment. --------------- Tài liệu được chia sẻ bởi: Learning and Sharing for Machine Learning & AI",,,"#sharing, #machine_learning",,
,nan,,,,,
"Mạng nơ-ron hồi quy đa lớp phân tán (Distributed Multilayer Recurrent Neural Network - DM-RNN) là một kiểu mạng nơ-ron hồi quy (RNN) được thiết kế để xử lý thông tin từ nhiều nguồn đồng thời và đa lớp. Nó là một biến thể của mạng nơ-ron hồi quy đa lớp (ML-RNN), trong đó các đơn vị nơ-ron trong mạng được phân chia thành nhiều nhóm, và mỗi nhóm chỉ xử lý một phần của dữ liệu đầu vào.
Mô hình DM-RNN giúp cải thiện hiệu suất của mạng hồi quy khi xử lý dữ liệu phức tạp và đa nguồn, như trong các ứng dụng như xử lý ngôn ngữ tự nhiên, nhận dạng giọng nói, dự đoán chuỗi thời gian và nhiều tác vụ học tập khác. Bằng cách phân tán các đơn vị nơ-ron, mạng DM-RNN có khả năng xử lý thông tin từ nhiều nguồn cùng một lúc và truyền tải thông tin giữa các lớp mạng một cách hiệu quả.
Các ưu điểm của mạng DM-RNN bao gồm:
Xử lý đa nguồn: Mạng DM-RNN cho phép xử lý thông tin từ nhiều nguồn đồng thời, giúp cải thiện khả năng mô hình hóa dữ liệu phức tạp và đa dạng.
Hiệu quả tính toán: Do các đơn vị nơ-ron được phân tán và chỉ xử lý một phần dữ liệu, mạng DM-RNN có khả năng tính toán hiệu quả và tránh các vấn đề về tính toán phức tạp.
Tính linh hoạt: Mạng DM-RNN có tính linh hoạt cao trong việc kết hợp các nguồn thông tin khác nhau và điều chỉnh kiến trúc mạng dễ dàng để phù hợp với các tác vụ xử lý dữ liệu cụ thể.
Tuy nhiên, mạng DM-RNN cũng có một số thách thức, bao gồm:
Đòi hỏi nhiều dữ liệu huấn luyện: Do mô hình có nhiều tham số và cấu trúc phức tạp, việc huấn luyện mạng DM-RNN có thể đòi hỏi một lượng lớn dữ liệu huấn luyện để đạt được hiệu suất tốt.
Điều chỉnh tham số phức tạp: Việc điều chỉnh các tham số và kiến trúc của mạng DM-RNN có thể phức tạp và đòi hỏi sự chuyên môn cao.
Một số ứng dụng của mạng DM-RNN bao gồm xử lý ngôn ngữ tự nhiên, nhận dạng giọng nói, dự đoán chuỗi thời gian và phân tích dữ liệu chuỗi. Mô hình này đã đem lại những kết quả đáng kể trong việc xử lý và phân tích dữ liệu phức tạp và đa nguồn.","Mạng nơ-ron hồi quy đa lớp phân tán (Distributed Multilayer Recurrent Neural Network - DM-RNN) là một kiểu mạng nơ-ron hồi quy (RNN) được thiết kế để xử lý thông tin từ nhiều nguồn đồng thời và đa lớp. Nó là một biến thể của mạng nơ-ron hồi quy đa lớp (ML-RNN), trong đó các đơn vị nơ-ron trong mạng được phân chia thành nhiều nhóm, và mỗi nhóm chỉ xử lý một phần của dữ liệu đầu vào. Mô hình DM-RNN giúp cải thiện hiệu suất của mạng hồi quy khi xử lý dữ liệu phức tạp và đa nguồn, như trong các ứng dụng như xử lý ngôn ngữ tự nhiên, nhận dạng giọng nói, dự đoán chuỗi thời gian và nhiều tác vụ học tập khác. Bằng cách phân tán các đơn vị nơ-ron, mạng DM-RNN có khả năng xử lý thông tin từ nhiều nguồn cùng một lúc và truyền tải thông tin giữa các lớp mạng một cách hiệu quả. Các ưu điểm của mạng DM-RNN bao gồm: Xử lý đa nguồn: Mạng DM-RNN cho phép xử lý thông tin từ nhiều nguồn đồng thời, giúp cải thiện khả năng mô hình hóa dữ liệu phức tạp và đa dạng. Hiệu quả tính toán: Do các đơn vị nơ-ron được phân tán và chỉ xử lý một phần dữ liệu, mạng DM-RNN có khả năng tính toán hiệu quả và tránh các vấn đề về tính toán phức tạp. Tính linh hoạt: Mạng DM-RNN có tính linh hoạt cao trong việc kết hợp các nguồn thông tin khác nhau và điều chỉnh kiến trúc mạng dễ dàng để phù hợp với các tác vụ xử lý dữ liệu cụ thể. Tuy nhiên, mạng DM-RNN cũng có một số thách thức, bao gồm: Đòi hỏi nhiều dữ liệu huấn luyện: Do mô hình có nhiều tham số và cấu trúc phức tạp, việc huấn luyện mạng DM-RNN có thể đòi hỏi một lượng lớn dữ liệu huấn luyện để đạt được hiệu suất tốt. Điều chỉnh tham số phức tạp: Việc điều chỉnh các tham số và kiến trúc của mạng DM-RNN có thể phức tạp và đòi hỏi sự chuyên môn cao. Một số ứng dụng của mạng DM-RNN bao gồm xử lý ngôn ngữ tự nhiên, nhận dạng giọng nói, dự đoán chuỗi thời gian và phân tích dữ liệu chuỗi. Mô hình này đã đem lại những kết quả đáng kể trong việc xử lý và phân tích dữ liệu phức tạp và đa nguồn.",,,"#deep_learning, #sharing",,
"Chào các anh chị, anh chị cho hỏi em mới bắt đầu học các khóa ML của bác Andrew Ng trên cousera thì thấy ở cuối thường có optional lab nhưng mà nó không giải thích ý nghĩa, cách dùng từng câu lệnh. Vậy em phải tự học thêm cách sử dụng mấy thư viện như numpy,... ở ngoài phải không ạ tại em cũng chưa biết gì về python lắm ạ. Em xin cảm ơn.","Chào các anh chị, anh chị cho hỏi em mới bắt đầu học các khóa ML của bác Andrew Ng trên cousera thì thấy ở cuối thường có optional lab nhưng mà nó không giải thích ý nghĩa, cách dùng từng câu lệnh. Vậy em phải tự học thêm cách sử dụng mấy thư viện như numpy,... ở ngoài phải không ạ tại em cũng chưa biết gì về python lắm ạ. Em xin cảm ơn.",,,"#machine_learning, #Q&A, #python",,
"Cho bạn nào chưa biết , cũng như vẫn đang đợi request từ Meta để access LLaMA 2
Hiện tại , Meta đã mở link mới để request access vào LLaMA 2 , các bạn có thể điền form lại , cũng như đồng bộ với repo của Meta trên Hugging Face ( email khi bạn điền form phải đúng với email tài khoản trên huggingface mới access được repo nhé )
Bạn nào muốn trải nghiệm model LLaMA 2 có thể đăng ký qua form bên dưới
https://ai.meta.com/.../models-and.../llama-downloads/","Cho bạn nào chưa biết , cũng như vẫn đang đợi request từ Meta để access LLaMA 2 Hiện tại , Meta đã mở link mới để request access vào LLaMA 2 , các bạn có thể điền form lại , cũng như đồng bộ với repo của Meta trên Hugging Face ( email khi bạn điền form phải đúng với email tài khoản trên huggingface mới access được repo nhé ) Bạn nào muốn trải nghiệm model LLaMA 2 có thể đăng ký qua form bên dưới https://ai.meta.com/.../models-and.../llama-downloads/",,,#sharing,,
"Chào mọi người ạ, cho em hỏi là em tập data với mỗi hình kích thước 5120x5120 thì có cách nào em giảm kích thước mà tối ưu nhất không ạ. Và việc em cắt thành 20 ảnh kích thước 256x256 liệu sau khi train có ảnh hưởng tới tính tổng quát của bức hình không ạ. Em cảm ơn ạ","Chào mọi người ạ, cho em hỏi là em tập data với mỗi hình kích thước 5120x5120 thì có cách nào em giảm kích thước mà tối ưu nhất không ạ. Và việc em cắt thành 20 ảnh kích thước 256x256 liệu sau khi train có ảnh hưởng tới tính tổng quát của bức hình không ạ. Em cảm ơn ạ",,,#Q&A. #data,,
"[Góc tìm developers]
Xin Update: vẩn cần team làm được
Tài chính 200 chịu quay đầu, hợp đồng công chứng đàng hoàng.
Xin chào anh em, mình có vài idea vầy có anh em nào có thể làm forex/stock/crypto/metal trading engine như sau không ạ? Xin chân thành tìm được người
Xin cám ơn cái devs","[Góc tìm developers] Xin Update: vẩn cần team làm được Tài chính 200 chịu quay đầu, hợp đồng công chứng đàng hoàng. Xin chào anh em, mình có vài idea vầy có anh em nào có thể làm forex/stock/crypto/metal trading engine như sau không ạ? Xin chân thành tìm được người Xin cám ơn cái devs",,,#sharing,,
"Tiếp theo series ngày hôm qua thì hôm nay thêm đoạn train LoRA để sinh ra các ảnh theo cách của riêng mình.
Hi vọng giúp được các bạn mới học trên con đường tìm hiểu về #stablediffusion. Phần link video để ở phần bình luận nhé (sorry vì ko hiểu sao ông Face bóp tương tác các post có link trong này, khó hiểu thật sự)
#miai","Tiếp theo series ngày hôm qua thì hôm nay thêm đoạn train LoRA để sinh ra các ảnh theo cách của riêng mình. Hi vọng giúp được các bạn mới học trên con đường tìm hiểu về Phần link video để ở phần bình luận nhé (sorry vì ko hiểu sao ông Face bóp tương tác các post có link trong này, khó hiểu thật sự)",#stablediffusion.	#miai,,"#sharing, #cv",,
"Mô hình học tập không đối xứng có nhiều ứng dụng hứa hẹn trong thực tế, đặc biệt là trong lĩnh vực trí tuệ nhân tạo và học máy. Dưới đây là một số ví dụ về các ứng dụng của mô hình học tập không đối xứng:

mô hình học tập không đối xứng có thể được sử dụng để nhận diện các đặc trưng quan trọng của các đối tượng trong hình ảnh một cách chính xác hơn.
Trong lĩnh vực xử lý ngôn ngữ tự nhiên, mô hình học tập không đối xứng có thể giúp tập trung vào việc học các mẫu ngôn ngữ phức tạp và quan trọng hơn trong dữ liệu văn bản.
Trong ứng dụng nhận dạng giọng nói, mô hình học tập không đối xứng có thể tập trung vào việc học các đặc điểm quan trọng của giọng nói, như cách luyến ái, nhịp điệu và ngữ điệu, giúp cải thiện độ chính xác của mô hình trong việc nhận dạng và phân tích giọng nói.
Trong lĩnh vực tự động lái xe, mô hình học tập không đối xứng có thể được sử dụng để tối ưu hóa các phản ứng và quyết định của hệ thống lái xe trên cơ sở các tình huống thực tế.

So với các mô hình phổ biến hiện nay như mạng nơ-ron tích chập (CNN) hoặc mạng nơ-ron hồi quy (RNN), mô hình học tập không đối xứng có đặc điểm và hiệu quả khác biệt trong việc tập trung vào việc học các mẫu quan trọng trong dữ liệu.Tuy nhiên, việc sử dụng mô hình học tập không đối xứng cũng có thể đòi hỏi nhiều dữ liệu huấn luyện và tính toán phức tạp hơn để điều chỉnh các trọng số của mạng.","Mô hình học tập không đối xứng có nhiều ứng dụng hứa hẹn trong thực tế, đặc biệt là trong lĩnh vực trí tuệ nhân tạo và học máy. Dưới đây là một số ví dụ về các ứng dụng của mô hình học tập không đối xứng: mô hình học tập không đối xứng có thể được sử dụng để nhận diện các đặc trưng quan trọng của các đối tượng trong hình ảnh một cách chính xác hơn. Trong lĩnh vực xử lý ngôn ngữ tự nhiên, mô hình học tập không đối xứng có thể giúp tập trung vào việc học các mẫu ngôn ngữ phức tạp và quan trọng hơn trong dữ liệu văn bản. Trong ứng dụng nhận dạng giọng nói, mô hình học tập không đối xứng có thể tập trung vào việc học các đặc điểm quan trọng của giọng nói, như cách luyến ái, nhịp điệu và ngữ điệu, giúp cải thiện độ chính xác của mô hình trong việc nhận dạng và phân tích giọng nói. Trong lĩnh vực tự động lái xe, mô hình học tập không đối xứng có thể được sử dụng để tối ưu hóa các phản ứng và quyết định của hệ thống lái xe trên cơ sở các tình huống thực tế. So với các mô hình phổ biến hiện nay như mạng nơ-ron tích chập (CNN) hoặc mạng nơ-ron hồi quy (RNN), mô hình học tập không đối xứng có đặc điểm và hiệu quả khác biệt trong việc tập trung vào việc học các mẫu quan trọng trong dữ liệu.Tuy nhiên, việc sử dụng mô hình học tập không đối xứng cũng có thể đòi hỏi nhiều dữ liệu huấn luyện và tính toán phức tạp hơn để điều chỉnh các trọng số của mạng.",,,"#sharing, #machine_learning",,
"Có bao giờ anh em thắc mắc làm sao mà mô hình như GPT4 có thể vừa xử lý ảnh lẫn prompt chưa? Dạo này mình tìm hiểu về Vision-Language models thì thấy thằng BLIP-2 có cách ứng dụng ViT + LLM mà không cần train lại 2 ông to đùng kia => Vừa tiết kiệm mà lại tận dụng được sức mạnh của pretrained ViT + LLM. Hi vọng bài viết sẽ giúp mọi người hiểu hơn về cách multimodal models hoạt động nhé! 
Link Viblo: https://viblo.asia/p/giai-quyet-bai-toan-vision-language-voi-blip-2-va-instructblip-W13VMeW7VY7",Có bao giờ anh em thắc mắc làm sao mà mô hình như GPT4 có thể vừa xử lý ảnh lẫn prompt chưa? Dạo này mình tìm hiểu về Vision-Language models thì thấy thằng BLIP-2 có cách ứng dụng ViT + LLM mà không cần train lại 2 ông to đùng kia => Vừa tiết kiệm mà lại tận dụng được sức mạnh của pretrained ViT + LLM. Hi vọng bài viết sẽ giúp mọi người hiểu hơn về cách multimodal models hoạt động nhé! Link Viblo: https://viblo.asia/p/giai-quyet-bai-toan-vision-language-voi-blip-2-va-instructblip-W13VMeW7VY7,,,"#sharing, #cv, #deep_learning",,
"Kính gửi các bác. Tranh thủ đang học về Stable Diffusion em mạnh dạn làm clip chia sẻ cùng cả nhà. Clip chỉ với mục đích tìm hiểu siêu cơ bản về SD và hướng tới các bạn mới học.
Hi vọng giúp được các bạn!",Kính gửi các bác. Tranh thủ đang học về Stable Diffusion em mạnh dạn làm clip chia sẻ cùng cả nhà. Clip chỉ với mục đích tìm hiểu siêu cơ bản về SD và hướng tới các bạn mới học. Hi vọng giúp được các bạn!,,,"#sharing, #deep_learning",,
"Mô hình học tập không đối xứng (Asymmetric Learning Model) là một loại mô hình học máy hoặc mô hình học tập trong đó quá trình học và điều chỉnh các trọng số của mạng neuron được thực hiện không đối xứng giữa các kết nối nơ-ron.
Trong mô hình học tập đối xứng, các trọng số của mạng neuron được điều chỉnh cùng một lượng cho tất cả các kết nối nơ-ron. Tức là, khi mạng nhận được một tín hiệu đầu vào và tạo ra đầu ra tương ứng, tất cả các trọng số sẽ được điều chỉnh theo cùng một cách để giảm thiểu sai số giữa đầu ra thực tế và đầu ra mong đợi.
Tuy nhiên, trong mô hình học tập không đối xứng, quá trình điều chỉnh các trọng số có thể không được thực hiện cùng một cách cho tất cả các kết nối nơ-ron. Thay vào đó, điều chỉnh trọng số có thể được tùy chỉnh theo cách không đối xứng dựa trên đặc điểm của dữ liệu đầu vào và đầu ra. Điều này cho phép mô hình học tập chủ động tập trung vào việc học các mẫu phức tạp hoặc quan trọng hơn trong dữ liệu, trong khi loại bỏ những mẫu ít quan trọng hoặc nhiễu.
Mô hình học tập không đối xứng có thể có ứng dụng trong nhiều lĩnh vực của học máy và trí tuệ nhân tạo, và nó là một trong những phương pháp mở rộng và tối ưu hóa học tập truyền thống để đạt được hiệu suất cao hơn và khả năng tương thích với các tác vụ phức tạp hơn.","Mô hình học tập không đối xứng (Asymmetric Learning Model) là một loại mô hình học máy hoặc mô hình học tập trong đó quá trình học và điều chỉnh các trọng số của mạng neuron được thực hiện không đối xứng giữa các kết nối nơ-ron. Trong mô hình học tập đối xứng, các trọng số của mạng neuron được điều chỉnh cùng một lượng cho tất cả các kết nối nơ-ron. Tức là, khi mạng nhận được một tín hiệu đầu vào và tạo ra đầu ra tương ứng, tất cả các trọng số sẽ được điều chỉnh theo cùng một cách để giảm thiểu sai số giữa đầu ra thực tế và đầu ra mong đợi. Tuy nhiên, trong mô hình học tập không đối xứng, quá trình điều chỉnh các trọng số có thể không được thực hiện cùng một cách cho tất cả các kết nối nơ-ron. Thay vào đó, điều chỉnh trọng số có thể được tùy chỉnh theo cách không đối xứng dựa trên đặc điểm của dữ liệu đầu vào và đầu ra. Điều này cho phép mô hình học tập chủ động tập trung vào việc học các mẫu phức tạp hoặc quan trọng hơn trong dữ liệu, trong khi loại bỏ những mẫu ít quan trọng hoặc nhiễu. Mô hình học tập không đối xứng có thể có ứng dụng trong nhiều lĩnh vực của học máy và trí tuệ nhân tạo, và nó là một trong những phương pháp mở rộng và tối ưu hóa học tập truyền thống để đạt được hiệu suất cao hơn và khả năng tương thích với các tác vụ phức tạp hơn.",,,"#sharing, #machine_learning",,
"Meta AI is open-sourcing AudioCraft, a multi-purpose framework for generating music and sounds and enabling compression capabilities.","Meta AI is open-sourcing AudioCraft, a multi-purpose framework for generating music and sounds and enabling compression capabilities.",,,,,
"Chào mọi người, trong nhóm mình ai có code về faster r cnn thì cho mình tham khảo với ạ, nếu được là triển khai từ đầu toàn bộ không dùng model có sẵn nhé mọi người, mình muốn hiểu kĩ hơn và tiếp cận nó tốt hơn. Cảm ơn mọi người!","Chào mọi người, trong nhóm mình ai có code về faster r cnn thì cho mình tham khảo với ạ, nếu được là triển khai từ đầu toàn bộ không dùng model có sẵn nhé mọi người, mình muốn hiểu kĩ hơn và tiếp cận nó tốt hơn. Cảm ơn mọi người!",,,"#Q&A, #deep_learning",,
"Chào mọi người, mình là Khôi Nguyễn. Một năm vừa qua, mình có thời gian đọc và thảo luận nội dung của cuốn sách ""Probabilistic Machine Learning"" của tác giả Kevin Murphy, đây là xuất bản mới của cuốn sách nổi tiếng ""Machine Learning: A Probabilistic Perspective"", xuất bản năm 2012. Đây là một trong những cuốn sách hay nhất về Machine Learning (ML), cung cấp những kiến thức nền tảng và mở rộng của lĩnh vực này. Sau 10 năm, nhận thấy kiến thức về ML tiếp tục thay đổi và cập nhật thì tác giả có cập nhật và mở rộng cuốn sách của mình, cũng như sắp xếp lại nội dung cho phù hợp hơn. Các kiến thức mới được cập nhật bao gồm cả về Machine Learning và Deep Learning, cover cả về Generative Model, Few-shot Learning, Continual Learning, Graph Neural Network, Transformer,... Cuốn sách mới được chia làm 2 cuốn con, đã và đang được xuất bản trong năm 2022 và 2023 này.
Nhận thấy nội dung cuốn sách rất hữu ích nên mình có ý định chia sẽ hướng dẫn đọc cuốn sách này. Lý do chính là nội dung khá là nhiều, xem như là Bible về Machine Learning, nếu có hướng dẫn thì việc đọc sẽ thuận lợi hơn phần nào. Hy vọng rằng, sau khi đọc cuốn sách này (và có thể nhiều lần đọc nữa) thì các bạn có thể tích luỹ được những kiến thức cần thiết phục vụ cho công việc của mình sau này, đặc biệt là các công việc nghiên cứu về ML/AI. Cảm ơn mọi người nhiều đã dành thời gian đọc.
Link của tài liệu hướng dẫn tại đây: https://tinyurl.com/ProbML","Chào mọi người, mình là Khôi Nguyễn. Một năm vừa qua, mình có thời gian đọc và thảo luận nội dung của cuốn sách ""Probabilistic Machine Learning"" của tác giả Kevin Murphy, đây là xuất bản mới của cuốn sách nổi tiếng ""Machine Learning: A Probabilistic Perspective"", xuất bản năm 2012. Đây là một trong những cuốn sách hay nhất về Machine Learning (ML), cung cấp những kiến thức nền tảng và mở rộng của lĩnh vực này. Sau 10 năm, nhận thấy kiến thức về ML tiếp tục thay đổi và cập nhật thì tác giả có cập nhật và mở rộng cuốn sách của mình, cũng như sắp xếp lại nội dung cho phù hợp hơn. Các kiến thức mới được cập nhật bao gồm cả về Machine Learning và Deep Learning, cover cả về Generative Model, Few-shot Learning, Continual Learning, Graph Neural Network, Transformer,... Cuốn sách mới được chia làm 2 cuốn con, đã và đang được xuất bản trong năm 2022 và 2023 này. Nhận thấy nội dung cuốn sách rất hữu ích nên mình có ý định chia sẽ hướng dẫn đọc cuốn sách này. Lý do chính là nội dung khá là nhiều, xem như là Bible về Machine Learning, nếu có hướng dẫn thì việc đọc sẽ thuận lợi hơn phần nào. Hy vọng rằng, sau khi đọc cuốn sách này (và có thể nhiều lần đọc nữa) thì các bạn có thể tích luỹ được những kiến thức cần thiết phục vụ cho công việc của mình sau này, đặc biệt là các công việc nghiên cứu về ML/AI. Cảm ơn mọi người nhiều đã dành thời gian đọc. Link của tài liệu hướng dẫn tại đây: https://tinyurl.com/ProbML",,,"#sharing, #machine_learning",,
"120 Python Project có Source Code phù hợp cho tất cả các bạn Newbie, Beginers, Intermediate, Advanced.
Link: https://s.net.vn/VFws","120 Python Project có Source Code phù hợp cho tất cả các bạn Newbie, Beginers, Intermediate, Advanced. Link: https://s.net.vn/VFws",,,"#sharing, #python",,
Kiến trúc và cách hoạt động Stable Diffusion.,Kiến trúc và cách hoạt động Stable Diffusion.,,,"#sharing, #deep_learning",,
Tài liệu khóa học NLP chất lượng cao CS224n của ĐH Stanford 😘,Tài liệu khóa học NLP chất lượng cao CS224n của ĐH Stanford,,,"#sharing, #nlp",,
"Chào mọi người
Sắp tới em có buổi phỏng vấn vị trí Intern AI, anh chị có thể cho em biết các câu hỏi phỏng vấn thường gặp hay tắc tình huống khi đi phỏng vấn này được không ạ. Em cảm ơn ạ.","Chào mọi người Sắp tới em có buổi phỏng vấn vị trí Intern AI, anh chị có thể cho em biết các câu hỏi phỏng vấn thường gặp hay tắc tình huống khi đi phỏng vấn này được không ạ. Em cảm ơn ạ.",,,#Q&A,,
"Google DeepMind has unveiled Robotic Transformer 2 (RT-2), a pioneering vision-language-action model that equips robots with exceptional capabilities. By translating web and robotics data into general instructions, RT-2 offers groundbreaking improvements in generalization and semantic understanding, setting a new milestone for robotic intelligence","Google DeepMind has unveiled Robotic Transformer 2 (RT-2), a pioneering vision-language-action model that equips robots with exceptional capabilities. By translating web and robotics data into general instructions, RT-2 offers groundbreaking improvements in generalization and semantic understanding, setting a new milestone for robotic intelligence",,,,,
"120 Python Project có Source Code phù hợp cho tất cả các bạn Newbie, Beginers, Intermediate, Advanced.","120 Python Project có Source Code phù hợp cho tất cả các bạn Newbie, Beginers, Intermediate, Advanced.",,,"#sharing, #python",,
"Một sinh viên Harvard, Maya Bodnick, đã kiểm tra khả năng của GPT-4 trong việc viết các bài luận khoa học xã hội và nhân văn năm thứ nhất. Kết quả là GPT-4 đạt điểm trung bình 3,57 ""đáng nể""!","Một sinh viên Harvard, Maya Bodnick, đã kiểm tra khả năng của GPT-4 trong việc viết các bài luận khoa học xã hội và nhân văn năm thứ nhất. Kết quả là GPT-4 đạt điểm trung bình 3,57 ""đáng nể""!",,,"#sharing, #nlp",,
FLASK: a new evaluation protocol designed to provide a comprehensive analysis of LLMs performance based on 12 specific skills.,FLASK: a new evaluation protocol designed to provide a comprehensive analysis of LLMs performance based on 12 specific skills.,,,,,
1/. Universal Adversarial LLM Attacks - finds universal and transferable adversarial attacks that cause aligned models like ChatGPT and Bard to generate objectionable behaviors; the approach automatically produces adversarial suffixes using greedy and gradient search.,1/. Universal Adversarial LLM Attacks - finds universal and transferable adversarial attacks that cause aligned models like ChatGPT and Bard to generate objectionable behaviors; the approach automatically produces adversarial suffixes using greedy and gradient search.,,,,,
"PYTHON VÀ TÀI CHÍNH: DỰ ĐOÁN CHỈ SỐ CHỨNG KHOÁN VNINDEX BẰNG MODEL LOGISTIC REGRESSION
Chứng khoán VNINDEX là chỉ số chung của thị trường chứng khoán Việt Nam và đóng vai trò quan trọng trong việc đo lường sự biến động và phản ánh xu hướng của thị trường. Dự đoán biến động của VNINDEX có thể hỗ trợ nhà đầu tư và các chuyên gia tài chính trong việc đưa ra quyết định thông minh về đầu tư và quản lý rủi ro. Trong bài viết này, hãy cùng ICLS Tech tìm hiểu cách sử dụng mô hình Logistic Regression để dự đoán chỉ số chứng khoán VNINDEX và đạt tỷ lệ đúng dự đoán trên 50% nhé!
#ICLSTech #ThuvienPython #PythonLibraries #AlgoTrading #FinTech #QuantitativeFinance #DataScience
Nguồn: ICLS Tech","PYTHON VÀ TÀI CHÍNH: DỰ ĐOÁN CHỈ SỐ CHỨNG KHOÁN VNINDEX BẰNG MODEL LOGISTIC REGRESSION Chứng khoán VNINDEX là chỉ số chung của thị trường chứng khoán Việt Nam và đóng vai trò quan trọng trong việc đo lường sự biến động và phản ánh xu hướng của thị trường. Dự đoán biến động của VNINDEX có thể hỗ trợ nhà đầu tư và các chuyên gia tài chính trong việc đưa ra quyết định thông minh về đầu tư và quản lý rủi ro. Trong bài viết này, hãy cùng ICLS Tech tìm hiểu cách sử dụng mô hình Logistic Regression để dự đoán chỉ số chứng khoán VNINDEX và đạt tỷ lệ đúng dự đoán trên 50% nhé! Nguồn: ICLS Tech",#ICLSTech	#ThuvienPython	#PythonLibraries	#AlgoTrading	#FinTech	#QuantitativeFinance	#DataScience,,"#machine_learning, #python, #sharing",,
"[Góc tư vấn]
Hi m.n, cho mình hỏi rằng liệu laptop Macbook pro M2 có phù hợp việc học cho lập trình machine learning không? Và việc cài đặt thư viện cho machine learning trên Macbook M2 có tương thích hay là bị xung đột nhiều ko ạ?
Mình định mua laptop Macbook pro M2 để học ML thôi? Chưa cần phải làm việc vs khối lượng lớn dữ liệu như khi đi làm đâu ạ?
Thanks m.n !!!","[Góc tư vấn] Hi m.n, cho mình hỏi rằng liệu laptop Macbook pro M2 có phù hợp việc học cho lập trình machine learning không? Và việc cài đặt thư viện cho machine learning trên Macbook M2 có tương thích hay là bị xung đột nhiều ko ạ? Mình định mua laptop Macbook pro M2 để học ML thôi? Chưa cần phải làm việc vs khối lượng lớn dữ liệu như khi đi làm đâu ạ? Thanks m.n !!!",,,"#Q&A, #machine_learning",,
"Stability AI released Stable Diffusion XL (SDXL) 1.0 🚀
💡 SDXL 1.0 is an open access image model with a large parameter count. It has a 3.5 billion parameter base model and a 6.6 billion parameter model ensemble pipeline. A refiner component improves the base model's output with better color, contrast, and details.","Stability AI released Stable Diffusion XL (SDXL) 1.0 SDXL 1.0 is an open access image model with a large parameter count. It has a 3.5 billion parameter base model and a 6.6 billion parameter model ensemble pipeline. A refiner component improves the base model's output with better color, contrast, and details.",,,,,
"Chào mọi người, e dự định học thạc sĩ và nghiên cứu vào ""machine learning"". E mới bắt đầu học nên không biết rõ 1 số môn nên học trước hay sau.
Cụ thể trong trường e có 1 số môn tự chọn, và e tính là sẽ học ""machine learning"" trước sau đó sẽ là ""deep learning"". Nhưng có 1 môn là ""Data Warehousing and Big Data"" thì e không biết là môn này có liên quan mật thiết với việc học machine learning không và nên học nó trước hay sau ""machine learning và deep learning""? Tương tự với môn ""Cloud Computing"" ạ ?
e cảm ơn","Chào mọi người, e dự định học thạc sĩ và nghiên cứu vào ""machine learning"". E mới bắt đầu học nên không biết rõ 1 số môn nên học trước hay sau. Cụ thể trong trường e có 1 số môn tự chọn, và e tính là sẽ học ""machine learning"" trước sau đó sẽ là ""deep learning"". Nhưng có 1 môn là ""Data Warehousing and Big Data"" thì e không biết là môn này có liên quan mật thiết với việc học machine learning không và nên học nó trước hay sau ""machine learning và deep learning""? Tương tự với môn ""Cloud Computing"" ạ ? e cảm ơn",,,"#Q&A, #machine_learning",,
"Xin chào mn ạ, mình là người ko có kiến thức sâu về AI, NLP, NLU,... muốn xây dựng một model theo các data có sẵn mình lụm được trên mạng như này, nhờ mng chỉ cách mình train để tạo thành model với ạ
https://github.com/lab914hust/SmartHomeNLU. git","Xin chào mn ạ, mình là người ko có kiến thức sâu về AI, NLP, NLU,... muốn xây dựng một model theo các data có sẵn mình lụm được trên mạng như này, nhờ mng chỉ cách mình train để tạo thành model với ạ https://github.com/lab914hust/SmartHomeNLU. git",,,"#Q&A, #machine_learning",,
"Có thể nhiều người biết rồi, nhưng mình thấy Channel này trên youtube khá hay:
https://www.youtube.com/@statquest
Trong đó có nhiều playlists cho Statistics, ML, DeepLearning, ...","Có thể nhiều người biết rồi, nhưng mình thấy Channel này trên youtube khá hay: https://www.youtube.com/@statquest Trong đó có nhiều playlists cho Statistics, ML, DeepLearning, ...",,,"#sharing, #machine_learning",,
"Chào mọi người,
Hiện tại nhóm em/mình đang thực hiện một khóa luận tốt nghiệp về thử đồ ảo và gợi ý trang phục (virtual try-on & outfit recommendation). Nhóm có xây dựng một website - 👕KiseKloset hỗ trợ 2 tính năng này với mục đích giúp tăng trải nghiệm khi mua sắm quần áo online. Hệ thống sẽ cho phép thử đồ ảo từ ảnh người mẫu và ảnh áo (hiện nay chỉ mới chỉ hỗ trợ thử áo) sau đó đề xuất thêm các trang phục dựa vào mẫu áo bạn sử dụng và các thông tin cung cấp thêm dưới dạng text.
📎Mọi người có thể đọc thêm hướng dẫn trong trang web để biết thêm chi tiết cách sử dụng.
Rất mong mọi người dành chút thời gian để trải nghiệm thử ứng dụng và đánh giá. Mọi ý kiến của mọi người đều rất giá trị để nhóm đánh giá và cải tiến ứng dụng 🥰
📌Link website: http://selab.edu.vn:20440 or thevncore-lab.mooo(dot)com:20440
📌Mong mọi người cho ý kiến qua khảo sát: https://forms.gle/NNpqY7XiLkdiUh7D9
Em/mình xin cảm ơn.","Chào mọi người, Hiện tại nhóm em/mình đang thực hiện một khóa luận tốt nghiệp về thử đồ ảo và gợi ý trang phục (virtual try-on & outfit recommendation). Nhóm có xây dựng một website - KiseKloset hỗ trợ 2 tính năng này với mục đích giúp tăng trải nghiệm khi mua sắm quần áo online. Hệ thống sẽ cho phép thử đồ ảo từ ảnh người mẫu và ảnh áo (hiện nay chỉ mới chỉ hỗ trợ thử áo) sau đó đề xuất thêm các trang phục dựa vào mẫu áo bạn sử dụng và các thông tin cung cấp thêm dưới dạng text. Mọi người có thể đọc thêm hướng dẫn trong trang web để biết thêm chi tiết cách sử dụng. Rất mong mọi người dành chút thời gian để trải nghiệm thử ứng dụng và đánh giá. Mọi ý kiến của mọi người đều rất giá trị để nhóm đánh giá và cải tiến ứng dụng Link website: http://selab.edu.vn:20440 or thevncore-lab.mooo(dot)com:20440 Mong mọi người cho ý kiến qua khảo sát: https://forms.gle/NNpqY7XiLkdiUh7D9 Em/mình xin cảm ơn.",,,"#sharing, #machine_learning",,
"TRỰC QUAN HÓA MẠNG NEURAL NETWORK.
Trang web http://playground.tensorflow.org/ là một công cụ trực quan và tương tác cho phép người dùng thử nghiệm và học tập về các mô hình mạng nơ-ron (neural network) đơn giản.",TRỰC QUAN HÓA MẠNG NEURAL NETWORK. Trang web http://playground.tensorflow.org/ là một công cụ trực quan và tương tác cho phép người dùng thử nghiệm và học tập về các mô hình mạng nơ-ron (neural network) đơn giản.,,,"#sharing, #deep_learning",,
Các anh chị em cho mình hỏi ở sg chỗ nào đào tạo ngoài giờ data sientist ổn vậy ạ? Mình cảm ơn mọi người!!,Các anh chị em cho mình hỏi ở sg chỗ nào đào tạo ngoài giờ data sientist ổn vậy ạ? Mình cảm ơn mọi người!!,,,"#Q&A, #data",,
"Mọi người cho em xin chỗ hay tài liệu dạy về YOLO v7 và cách training nó với ạ, trên mạng ít tài liệu về nó thật sự.","Mọi người cho em xin chỗ hay tài liệu dạy về YOLO v7 và cách training nó với ạ, trên mạng ít tài liệu về nó thật sự.",,,"#Q&A, #deep_learning",,
"⚙ Data Platforms Architecture ⚙
Các thành phần chính của một Data Platform hoàn chỉnh:
1️⃣ Data Governance
2️⃣ Data Catalog
3️⃣ Metadata Management
4️⃣ Data Security
5️⃣ Data Privacy and Compliance
6️⃣ Data Monitoring and Auditing
7️⃣ Disaster Recovery and Business Continuity
8️⃣ Performance Optimization
9️⃣ Scalability and Elasticity
🔟 Continuous Improvement",Data Platforms Architecture Các thành phần chính của một Data Platform hoàn chỉnh: 1⃣ Data Governance 2⃣ Data Catalog 3⃣ Metadata Management 4⃣ Data Security 5⃣ Data Privacy and Compliance 6⃣ Data Monitoring and Auditing 7⃣ Disaster Recovery and Business Continuity 8⃣ Performance Optimization 9⃣ Scalability and Elasticity Continuous Improvement,,,,,
"Chào mọi người, em là sinh viên năm tư ngành Khoa học máy tính. Hiện tại e và nhóm đang phát triển một ứng dụng quét đơn thuốc tự động. Ứng dụng sẽ quét ảnh chụp đơn thuốc giúp người dùng tra cứu thông tin về các loại thuốc mình đang sử dụng, đồng thời có thể đặt lịch uống thuốc để tránh trường hơp sử dụng thuốc không đúng cách. Ứng dụng của nhóm đã có thể tải xuống trên của hàng Play store (do không đủ kinh phí nên chưa thể đăng lên Appstore ạ). Rất mong mọi người dành ít thời gian trải nghiệm ứng dụng. Mọi phản hồi của mọi người đều rất giá trị với nhóm chúng em. Em xin chân thành cảm ơn.
Link tải App: https://bit.ly/3q3VGWz
Ps: Em có đính kèm QR với ảnh demo test app bên dưới, mn có thể tải và sử dụng nhé.","Chào mọi người, em là sinh viên năm tư ngành Khoa học máy tính. Hiện tại e và nhóm đang phát triển một ứng dụng quét đơn thuốc tự động. Ứng dụng sẽ quét ảnh chụp đơn thuốc giúp người dùng tra cứu thông tin về các loại thuốc mình đang sử dụng, đồng thời có thể đặt lịch uống thuốc để tránh trường hơp sử dụng thuốc không đúng cách. Ứng dụng của nhóm đã có thể tải xuống trên của hàng Play store (do không đủ kinh phí nên chưa thể đăng lên Appstore ạ). Rất mong mọi người dành ít thời gian trải nghiệm ứng dụng. Mọi phản hồi của mọi người đều rất giá trị với nhóm chúng em. Em xin chân thành cảm ơn. Link tải App: https://bit.ly/3q3VGWz Ps: Em có đính kèm QR với ảnh demo test app bên dưới, mn có thể tải và sử dụng nhé.",,,"#sharing, #cv",,
"VinAI Seminar - ""Domain Adaptation on Wheels: Closing the Gap to the Open-world""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Tuan-Hung Vu, Valeo.ai
Time: 03:00 pm - 04:00 pm (GMT+7), Jul 25, 2023","VinAI Seminar - ""Domain Adaptation on Wheels: Closing the Gap to the Open-world"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Tuan-Hung Vu, Valeo.ai Time: 03:00 pm - 04:00 pm (GMT+7), Jul 25, 2023",,,,,
Mình xin phép chia sẻ cho mọi người bài viết giải thích chi tiết về paper LLaMa-2 - một LLM đình đám của Meta mới ra mắt đình đám trong tuần vừa qua. Hi vọng rằng nó sẽ làm thay đổi nhiều thứ trong tương lai của LLM,Mình xin phép chia sẻ cho mọi người bài viết giải thích chi tiết về paper LLaMa-2 - một LLM đình đám của Meta mới ra mắt đình đám trong tuần vừa qua. Hi vọng rằng nó sẽ làm thay đổi nhiều thứ trong tương lai của LLM,,,"#sharing, #deep_learning",,
"Hello everyone, I wish you have a happy weekend.
Recently, Meta introduced LLaMA v2, which is an upgrade version of LLaMA with far and wide improvements in terms of excellent performance, length of context, and amount of training tokens.
Let's shed light on the application of this model.","Hello everyone, I wish you have a happy weekend. Recently, Meta introduced LLaMA v2, which is an upgrade version of LLaMA with far and wide improvements in terms of excellent performance, length of context, and amount of training tokens. Let's shed light on the application of this model.",,,,,
"[About linear regression/linear algebra] I would like to ask the following question in English since it will be more convenient.
We are given a input feature matrix X of shape nxd, where n>d and the associated label vector y of size nx1. We know that the solution of the least square min_{beta}|| X beta -y||^2_2 is given by: beta = (X^T X)^{-1}Xy.
Now consider an arbitrary binary matrix B of shape nxd, B_{i,j} in {0,1} and ||B||_0 = (n-1) * d, i.e the number of zeroes in B is d. Using least squares with data matrix X1 = X*B, and the label y, we obtain beta1 = (X1^TX)^{-1}X1y. Here * denotes the elementwise product.
Find B so that ||B||_0 = d and beta_1 = beta ?
Note: It is easy to see one such solution is when exactly one row of B is zeros, and the rest of other entries are 1. Does that exists other solutions ?","[About linear regression/linear algebra] I would like to ask the following question in English since it will be more convenient. We are given a input feature matrix X of shape nxd, where n>d and the associated label vector y of size nx1. We know that the solution of the least square min_{beta}|| X beta -y||^2_2 is given by: beta = (X^T X)^{-1}Xy. Now consider an arbitrary binary matrix B of shape nxd, B_{i,j} in {0,1} and ||B||_0 = (n-1) * d, i.e the number of zeroes in B is d. Using least squares with data matrix X1 = X*B, and the label y, we obtain beta1 = (X1^TX)^{-1}X1y. Here * denotes the elementwise product. Find B so that ||B||_0 = d and beta_1 = beta ? Note: It is easy to see one such solution is when exactly one row of B is zeros, and the rest of other entries are 1. Does that exists other solutions ?",,,,,
Em được giao bài toán phân loại văn bản nhưng trước tiên phải phát hiện slot trước rồi sau đó phân loại văn bản sẽ dựa vào những slot đó kết hợp ngữ nghĩa câu để phân loại. Em có tìm những paper về joint intent and slot filling hay model diet đều theo kiểu phân loại trước rồi từ phân loại văn bản mới tìm slot nên bị ngược với bài toán của em. Em có thử lấy các model ấy test thì kết quả còn tệ hơn nếu mình chỉ dùng mỗi model như bert để phân loại. Không biết anh chị có link paper nào có thể share cho em với được không nhỉ. Em kiếm mãi không tìm thấy paper nào làm như vậy . em cảm ơn,Em được giao bài toán phân loại văn bản nhưng trước tiên phải phát hiện slot trước rồi sau đó phân loại văn bản sẽ dựa vào những slot đó kết hợp ngữ nghĩa câu để phân loại. Em có tìm những paper về joint intent and slot filling hay model diet đều theo kiểu phân loại trước rồi từ phân loại văn bản mới tìm slot nên bị ngược với bài toán của em. Em có thử lấy các model ấy test thì kết quả còn tệ hơn nếu mình chỉ dùng mỗi model như bert để phân loại. Không biết anh chị có link paper nào có thể share cho em với được không nhỉ. Em kiếm mãi không tìm thấy paper nào làm như vậy . em cảm ơn,,,"#Q&A, #nlp",,
AI sẽ thay đổi toàn bộ ngành nông nghiệp!,AI sẽ thay đổi toàn bộ ngành nông nghiệp!,,,,,
"Mình có 1 bài toán như sau. Mình có input là tên 1 vật (dạng text), và mình muốn ra output là mô tả của vật đó (dạng text). Mô tả của vật thì có 1 format nhất định, vdu nó là động vật hay thực vật, rồi thuộc họ j, có mấy chân, vvv. Mình ko làm về NLP nên ko rõ hiện tại có kĩ thuật j có thể làm đc? Hoặc các bạn có thể gợi ý keywords cũng đc. Mình cảm ơn nhiều.","Mình có 1 bài toán như sau. Mình có input là tên 1 vật (dạng text), và mình muốn ra output là mô tả của vật đó (dạng text). Mô tả của vật thì có 1 format nhất định, vdu nó là động vật hay thực vật, rồi thuộc họ j, có mấy chân, vvv. Mình ko làm về NLP nên ko rõ hiện tại có kĩ thuật j có thể làm đc? Hoặc các bạn có thể gợi ý keywords cũng đc. Mình cảm ơn nhiều.",,,"#Q&A, #nlp",,
"🔥Meta và Microsoft hợp tác công bố Llama 2: Có phiên bản Chat, cho phép thương mại hóa💥
📌MỘT SỐ THÔNG TIN QUAN TRỌNG:
✅Dữ liệu: 2 nghìn tỉ tokens. Nhiều hơn 40% so với Llama 1.
✅Context length: 4096.
✅Kích cỡ: 7B, 13B, 70B tham số (phiên bản 34B chưa được công bố vì chưa đạt tiêu chí an toàn).
✅Hai phiên bản: Llama-2 và Llama-2-chat (được supervised fine-tune trên hơn 100,000 samples và huấn luyên với RLHF trên hơn 1 triệu samples).
Nguồn ảnh: real.ml.memes
-------------------------------
Nguồn: VietAI","Meta và Microsoft hợp tác công bố Llama 2: Có phiên bản Chat, cho phép thương mại hóa MỘT SỐ THÔNG TIN QUAN TRỌNG: Dữ liệu: 2 nghìn tỉ tokens. Nhiều hơn 40% so với Llama 1. Context length: 4096. Kích cỡ: 7B, 13B, 70B tham số (phiên bản 34B chưa được công bố vì chưa đạt tiêu chí an toàn). Hai phiên bản: Llama-2 và Llama-2-chat (được supervised fine-tune trên hơn 100,000 samples và huấn luyên với RLHF trên hơn 1 triệu samples). Nguồn ảnh: real.ml.memes ------------------------------- Nguồn: VietAI",,,"#sharing, #deep_learning",,
"Chào mọi người, mình đang có model classification có input 2 ảnh và đâu ra là label 0 hoặc 1 (Hình bên dưới). Tuy nhiên phần data preparation em không biết chuẩn bị thế nào. Mặc dù có nghiên cứu nhiều blog trên mạng tuy nhiên gặp khá nhiều lỗi (Blog: https://github.com/keras-team/keras/issues/8130). Bên dưới là code data preparation với ImageDataGenerator. Mong hướng dẫn phần data preparation cho model 2 input ảnh. Mình cảm ơn.","Chào mọi người, mình đang có model classification có input 2 ảnh và đâu ra là label 0 hoặc 1 (Hình bên dưới). Tuy nhiên phần data preparation em không biết chuẩn bị thế nào. Mặc dù có nghiên cứu nhiều blog trên mạng tuy nhiên gặp khá nhiều lỗi (Blog: https://github.com/keras-team/keras/issues/8130). Bên dưới là code data preparation với ImageDataGenerator. Mong hướng dẫn phần data preparation cho model 2 input ảnh. Mình cảm ơn.",,,"#Q&A, #cv, #data",,
"Chào mọi người, bên em đang làm 1 dự án trading forex và hàng hóa. Mọi người cho em hỏi là model machine learning nào phù hợp nhất dùng cho trading ạ. Bộ data thì bên em đang lấy tạm trên tradingview xuống. Em cám ơn","Chào mọi người, bên em đang làm 1 dự án trading forex và hàng hóa. Mọi người cho em hỏi là model machine learning nào phù hợp nhất dùng cho trading ạ. Bộ data thì bên em đang lấy tạm trên tradingview xuống. Em cám ơn",,,"#Q&A, #machine_learning",,
"Chào các bác, Em có một vấn đề chưa tìm ra cách giải quyết khi sử dụng model yolov8 detection được chuyển đổi sang định dạng ONNX.
Có thể thấy trên hình dưới
Đầu vào
```
name: images
type: float32[1,3,640,640]
```
Đầu ra
```
name: output0
type: float32[1,6,8400]
``` 
Ở đây theo em hiểu thì
Đầu vào là một array float màu RGB kích cỡ 640x640 chiều từ trái qua phải, từ trên xuống dưới.
Đầu ra có 6 object array float mỗi 1  object có 8400 giá trị float. Dưới đây là mẫu:
```
{6.489357,14.580679,30.33416,36.84166,37.909622,42.389004,48.190636,55.96879,65.08879,72.705215,80.317604,87.94593,94.21246,100.19607,102.2921,108.48764,117.27652,149.78645,162.2814,167.18387,170.57477,173.82574,180.09859.....},
...
{22.857464,11.46661,7.262705,8.254338,8.966676,9.688668,10.24382,9.745479,9.374667,10.221896,10.54862,10.452046,10.352717,9.953964,10.596357,10.261409,7.7730684,6.377458,7.735608,9.637968,11.539716,13.111305,13.758495,13.440787,10.037614,8.329343,9.301963,8.501963,6.6712275,6.865048.....},
```
Em đang không rõ dãy này được đọc như thế nào, các bác đã từng làm qua cho em xin hướng tiếp cận để đọc output ạ.
Em cảm ơn trước, chúc các bác một ngày vui vẻ.","Chào các bác, Em có một vấn đề chưa tìm ra cách giải quyết khi sử dụng model yolov8 detection được chuyển đổi sang định dạng ONNX. Có thể thấy trên hình dưới Đầu vào ``` name: images type: float32[1,3,640,640] ``` Đầu ra ``` name: output0 type: float32[1,6,8400] ``` Ở đây theo em hiểu thì Đầu vào là một array float màu RGB kích cỡ 640x640 chiều từ trái qua phải, từ trên xuống dưới. Đầu ra có 6 object array float mỗi 1 object có 8400 giá trị float. Dưới đây là mẫu: ``` {6.489357,14.580679,30.33416,36.84166,37.909622,42.389004,48.190636,55.96879,65.08879,72.705215,80.317604,87.94593,94.21246,100.19607,102.2921,108.48764,117.27652,149.78645,162.2814,167.18387,170.57477,173.82574,180.09859.....}, ... {22.857464,11.46661,7.262705,8.254338,8.966676,9.688668,10.24382,9.745479,9.374667,10.221896,10.54862,10.452046,10.352717,9.953964,10.596357,10.261409,7.7730684,6.377458,7.735608,9.637968,11.539716,13.111305,13.758495,13.440787,10.037614,8.329343,9.301963,8.501963,6.6712275,6.865048.....}, ``` Em đang không rõ dãy này được đọc như thế nào, các bác đã từng làm qua cho em xin hướng tiếp cận để đọc output ạ. Em cảm ơn trước, chúc các bác một ngày vui vẻ.",,,,,
"Nhờ các anh giúp em xác định output và input !
Em dùng Machine Learning trong Visual studio Sử dụng tính năng object detection ra được file onnx như thế này
Nhưng khi mình điền input và ouput vào pipeline dùng thư viện ML.net thì báo sai input và output :( !
Không biết tại sao :(
[ColumnName(""input"")]
[VectorType(1, 3, 600, 800)]
public float[] Input { get; set; }
[ColumnName(""boxes"")]
[VectorType(1, 4)]
public float[] Boxes { get; set; }
[ColumnName(""labels"")]
[VectorType(1)]
public long[] Labels { get; set; }
[ColumnName(""scores"")]
[VectorType(1)]
public float[] Scores { get; set;
var pipeline =context.Transforms.ResizeImages(resizing: Microsoft.ML.Transforms.Image.ImageResizingEstimator.ResizingKind.Fill,
outputColumnName: ""resize"",
imageWidth: 800,
imageHeight: 600,
inputColumnName: nameof(RTFInput.ImageSource)
)
.Append(
context.Transforms.ExtractPixels(
offsetImage: 127f,
scaleImage: 1 / 128f,
inputColumnName: ""resize"",
outputColumnName: ""input"")
).Append(
context.Transforms.ApplyOnnxModel(
modelFile: modelFile,
inputColumnNames: new string[] { ""input"" },
outputColumnNames: new string[] { ""scores"", ""boxes"", ""labels"" }));","Nhờ các anh giúp em xác định output và input ! Em dùng Machine Learning trong Visual studio Sử dụng tính năng object detection ra được file onnx như thế này Nhưng khi mình điền input và ouput vào pipeline dùng thư viện ML.net thì báo sai input và output :( ! Không biết tại sao :( [ColumnName(""input"")] [VectorType(1, 3, 600, 800)] public float[] Input { get; set; } [ColumnName(""boxes"")] [VectorType(1, 4)] public float[] Boxes { get; set; } [ColumnName(""labels"")] [VectorType(1)] public long[] Labels { get; set; } [ColumnName(""scores"")] [VectorType(1)] public float[] Scores { get; set; var pipeline =context.Transforms.ResizeImages(resizing: Microsoft.ML.Transforms.Image.ImageResizingEstimator.ResizingKind.Fill, outputColumnName: ""resize"", imageWidth: 800, imageHeight: 600, inputColumnName: nameof(RTFInput.ImageSource) ) .Append( context.Transforms.ExtractPixels( offsetImage: 127f, scaleImage: 1 / 128f, inputColumnName: ""resize"", outputColumnName: ""input"") ).Append( context.Transforms.ApplyOnnxModel( modelFile: modelFile, inputColumnNames: new string[] { ""input"" }, outputColumnNames: new string[] { ""scores"", ""boxes"", ""labels"" }));",,,"#Q&A, #deep_learning",,
"#hoidap
Mình mới cà được tầm 6000 comment trên trang tripnow .Mình định làm sentiment analysis. Mỗi comment có score nên mình lấy nó để đánh giá comment là tích cực hay tiêu cực.Nhưng trước khi đi vào model thì bước preprocessing mình có chút thắc mắc đó là trong tiếng việt khi ""Tokenizer"" thì mọi người hay làm thế nào. Dùng 1-gram hay Bigram.
p/s :Do lần đầu áp dụng trong tiếng việt mong mọi người giúp đỡ.","Mình mới cà được tầm 6000 comment trên trang tripnow .Mình định làm sentiment analysis. Mỗi comment có score nên mình lấy nó để đánh giá comment là tích cực hay tiêu cực.Nhưng trước khi đi vào model thì bước preprocessing mình có chút thắc mắc đó là trong tiếng việt khi ""Tokenizer"" thì mọi người hay làm thế nào. Dùng 1-gram hay Bigram. p/s :Do lần đầu áp dụng trong tiếng việt mong mọi người giúp đỡ.",#hoidap,,"#Q&A, #nlp",,
"Hi mng nha, em là SV bên ngành tự động hoá trường BK HCM, đợt làm luận văn vừa rồi em có làm đề tài về “3D Object detection và 6D Pose estimation application for robotics bin picking system”, thì khá thích và đam mê về mảng Computer vision/Machine vision muốn theo sau khi tốt nghiệp, nên sau khi vừa hoàn tất chương trình học đợt tháng 6 vừa rồi thì em có tự học kha khá các thuật toán của machine learning, thì có tham khảo các bên tuyển dụng hiện có thì phần lớn các bên tuyển dụng sẽ hướng đến AI engineer luôn và sẽ đòi hỏi biết cả computer vision và NLP, thì em phân vân bây giờ có cần học luôn kiến thức cả NLP và CV luôn không hay tập trung đẩy mạnh mảng CV. Tại em cũng muốn rút ngắn thời gian tự học để có thể xin đi intern tích thêm kinh nghiệm thay vì chờ học xong full tất cả thì quá lâu. Hi vọng mng có thể cho em tí lời khuyên ạ. Em cảm ơn!","Hi mng nha, em là SV bên ngành tự động hoá trường BK HCM, đợt làm luận văn vừa rồi em có làm đề tài về “3D Object detection và 6D Pose estimation application for robotics bin picking system”, thì khá thích và đam mê về mảng Computer vision/Machine vision muốn theo sau khi tốt nghiệp, nên sau khi vừa hoàn tất chương trình học đợt tháng 6 vừa rồi thì em có tự học kha khá các thuật toán của machine learning, thì có tham khảo các bên tuyển dụng hiện có thì phần lớn các bên tuyển dụng sẽ hướng đến AI engineer luôn và sẽ đòi hỏi biết cả computer vision và NLP, thì em phân vân bây giờ có cần học luôn kiến thức cả NLP và CV luôn không hay tập trung đẩy mạnh mảng CV. Tại em cũng muốn rút ngắn thời gian tự học để có thể xin đi intern tích thêm kinh nghiệm thay vì chờ học xong full tất cả thì quá lâu. Hi vọng mng có thể cho em tí lời khuyên ạ. Em cảm ơn!",,,"#Q&A, #cv",,
"Mọi người cho em hỏi ạ: Em có 1 bộ dữ liệu khá lớn, nếu mà train cả 1 epoch thì sẽ quá thời gian chạy tối đa trên Kaggle. Nên em định chia đôi thành 2 datasets và làm theo 1 trong 2 cách sau được không hay là nên làm theo cách khác tốt hơn ạ:
1. Train luân phiên 1 epoch với dataset 1 rồi đến 1 epoch với dataset 2
2. Train xong với dataset 1 rồi train tiếp đến dataset 2
Em cảm ơn ạ","Mọi người cho em hỏi ạ: Em có 1 bộ dữ liệu khá lớn, nếu mà train cả 1 epoch thì sẽ quá thời gian chạy tối đa trên Kaggle. Nên em định chia đôi thành 2 datasets và làm theo 1 trong 2 cách sau được không hay là nên làm theo cách khác tốt hơn ạ: 1. Train luân phiên 1 epoch với dataset 1 rồi đến 1 epoch với dataset 2 2. Train xong với dataset 1 rồi train tiếp đến dataset 2 Em cảm ơn ạ",,,"#Q&A, #machine_learning, #data",,
"Chào các anh chị trong group, em mới tìm hiểu về machine learning, đọc đếm đoạn này thì e có 2 câu hỏi:
+) −∇f0(x0) là gì? Vì trước h em hiểu đó là vector gradient nhưng trong bài viết lại ghi là vector pháp tuyến
+) mặt phẳng supporting hyperlane trông ntn trong kgian 3 chiều,
Mong các anh chị giải đáp giúp e ạ, em cảm ơn ạ","Chào các anh chị trong group, em mới tìm hiểu về machine learning, đọc đếm đoạn này thì e có 2 câu hỏi: +) −∇f0(x0) là gì? Vì trước h em hiểu đó là vector gradient nhưng trong bài viết lại ghi là vector pháp tuyến +) mặt phẳng supporting hyperlane trông ntn trong kgian 3 chiều, Mong các anh chị giải đáp giúp e ạ, em cảm ơn ạ",,,"#Q&A, #math",,
"Mọi người cho em hỏi: Với Text to speech, để đạt được chất lượng thương mại như của Vbee, Vietel, FPT thì họ cần khoảng bao nhiêu giờ dữ liệu vậy? (Bỏ qua vấn đề kỹ thuật)","Mọi người cho em hỏi: Với Text to speech, để đạt được chất lượng thương mại như của Vbee, Vietel, FPT thì họ cần khoảng bao nhiêu giờ dữ liệu vậy? (Bỏ qua vấn đề kỹ thuật)",,,"#Q&A, #nlp",,
"Hi mọi người , em đang tìm hiểu và sử dụng Yolov7 Pose : https://github.com/WongKinYiu/yolov7/tree/pose . Em thấy tác giả đề cập đến model này dựa theo bài báo : https://arxiv.org/ftp/arxiv/papers/2204/2204.06806.pdf . Em là newbie trong mảng này nên chưa biết model hoạt động như thế nào , làm thế nào để lấy được Keypoint ... Mong mọi người chỉ giáo ạ . Mọi người có resources nào thì cho em tham khảo với ạ . Em xin chân thành cảm ơn !!!","Hi mọi người , em đang tìm hiểu và sử dụng Yolov7 Pose : https://github.com/WongKinYiu/yolov7/tree/pose . Em thấy tác giả đề cập đến model này dựa theo bài báo : https://arxiv.org/ftp/arxiv/papers/2204/2204.06806.pdf . Em là newbie trong mảng này nên chưa biết model hoạt động như thế nào , làm thế nào để lấy được Keypoint ... Mong mọi người chỉ giáo ạ . Mọi người có resources nào thì cho em tham khảo với ạ . Em xin chân thành cảm ơn !!!",,,"#Q&A, #deep_learning",,
"Em chào mọi người ạ,em đang làm bài toán sử dụng local Naive Bayes Nearest Neighbour phân loại ảnh thú cưng :chuột ,mèo (tập train của mỗi class sẽ có 20 ảnh ,các ảnh đều lấy từ trên mạng)Em có sử dụng SIFT để rút trích đặc trưng ,nhưng kết quả ko đc như mong muốn,em xin hỏi liệu có phương pháp đê cải thiện bài toán không ạ? Em cảm ơn ạ","Em chào mọi người ạ,em đang làm bài toán sử dụng local Naive Bayes Nearest Neighbour phân loại ảnh thú cưng :chuột ,mèo (tập train của mỗi class sẽ có 20 ảnh ,các ảnh đều lấy từ trên mạng)Em có sử dụng SIFT để rút trích đặc trưng ,nhưng kết quả ko đc như mong muốn,em xin hỏi liệu có phương pháp đê cải thiện bài toán không ạ? Em cảm ơn ạ",,,"#Q&A, #cv, #machine_learning",,
Machine Learning Operations (MLOps) - End-to-End Process,Machine Learning Operations (MLOps) - End-to-End Process,,,,,
"Chào các bạn, nguyên văn của chuyên gia computer vision FPT có đoạn viết cho bài toán chứng minh thư: :D
""1. Cropper
Tác vụ này xác định 4 góc của thẻ CMND và sau đó cắt về dạng ảnh chữ nhật. Ý nghĩa chính của tác vụ là phục vụ cho việc Detector liền kề sau đó dễ dàng hơn.
Các mô hình phát hiện đối tượng (object detection) phổ biến hiện nay chỉ trả về 2 góc (trái trên phải dưới, hoặc tâm box kèm giá trị chiều ngang dọc) giúp ta định hình một box hình chữ nhật. Chúng tôi sử dụng một mẹo nhỏ bằng cách coi mỗi góc của CMND là một đối tượng và sau đó phát hiện 4 góc này. Tiếp theo đó bằng cách áp dụng một vài phép biến đổi hình học cơ bản để cắt về dạng ảnh chữ nhật.
Mô hình phát hiện mà nhóm nghiên cứu đang sử dụng là bộ phát hiện đơn pha: SSD (SSD: Single Shot MultiBox Detector), với bộ trích xuất đặc trưng là MobileNet v2 (MobileNetV2: Inverted Residuals và Linear Bottlenecks).
SSD cung cấp cho nhóm nghiên cứu tốc độ truy xuất nhanh, trong khi MobileNet v2 giảm số lượng tính toán và bộ nhớ sử dụng nhưng vẫn duy trì được độ chính xác tốt.""
Mình nghĩ là họ muốn detect chính xác 4 điểm giống như regression, tuy vậy mình đang không hiểu cái mẹo nhỏ của họ ở đây cụ thể là gì? Họ coi mỗi góc là một object??? Nghĩa là thế nào? SSD dùng ở đây là để detect vùng chứng minh thư theo hộp chữ nhật chứ làm sao detect 4 điểm?
Các bạn có ý kiến gì không?","Chào các bạn, nguyên văn của chuyên gia computer vision FPT có đoạn viết cho bài toán chứng minh thư: :D ""1. Cropper Tác vụ này xác định 4 góc của thẻ CMND và sau đó cắt về dạng ảnh chữ nhật. Ý nghĩa chính của tác vụ là phục vụ cho việc Detector liền kề sau đó dễ dàng hơn. Các mô hình phát hiện đối tượng (object detection) phổ biến hiện nay chỉ trả về 2 góc (trái trên phải dưới, hoặc tâm box kèm giá trị chiều ngang dọc) giúp ta định hình một box hình chữ nhật. Chúng tôi sử dụng một mẹo nhỏ bằng cách coi mỗi góc của CMND là một đối tượng và sau đó phát hiện 4 góc này. Tiếp theo đó bằng cách áp dụng một vài phép biến đổi hình học cơ bản để cắt về dạng ảnh chữ nhật. Mô hình phát hiện mà nhóm nghiên cứu đang sử dụng là bộ phát hiện đơn pha: SSD (SSD: Single Shot MultiBox Detector), với bộ trích xuất đặc trưng là MobileNet v2 (MobileNetV2: Inverted Residuals và Linear Bottlenecks). SSD cung cấp cho nhóm nghiên cứu tốc độ truy xuất nhanh, trong khi MobileNet v2 giảm số lượng tính toán và bộ nhớ sử dụng nhưng vẫn duy trì được độ chính xác tốt."" Mình nghĩ là họ muốn detect chính xác 4 điểm giống như regression, tuy vậy mình đang không hiểu cái mẹo nhỏ của họ ở đây cụ thể là gì? Họ coi mỗi góc là một object??? Nghĩa là thế nào? SSD dùng ở đây là để detect vùng chứng minh thư theo hộp chữ nhật chứ làm sao detect 4 điểm? Các bạn có ý kiến gì không?",,,"#Q&A, #cv",,
"Mọi người cho mình hỏi , giờ mình có 1 ảnh chứa 1 số lá bài . Vậy thì có cách nào để đọc được từ hình ảnh rồi ra text gồm giá trị lá bài + Chất của lá bài đó không ! Cảm ơn","Mọi người cho mình hỏi , giờ mình có 1 ảnh chứa 1 số lá bài . Vậy thì có cách nào để đọc được từ hình ảnh rồi ra text gồm giá trị lá bài + Chất của lá bài đó không ! Cảm ơn",,,"#Q&A, #cv",,
"Dạ mọi người có thể cho em xin thông tin về các trại hè, trường hè, workshop, conference, Seminar (offline năm nay) liên quan đến AI/Machine Learning ở Việt Nam không ạ? Em hi vọng post này có thể tổng hợp được thông tin về các sự kiện như thế để mọi người cùng sở thích có thể gặp gỡ và trao đổi ạ ^^.","Dạ mọi người có thể cho em xin thông tin về các trại hè, trường hè, workshop, conference, Seminar (offline năm nay) liên quan đến AI/Machine Learning ở Việt Nam không ạ? Em hi vọng post này có thể tổng hợp được thông tin về các sự kiện như thế để mọi người cùng sở thích có thể gặp gỡ và trao đổi ạ ^^.",,,#webinar,,
Bard của Google đã hỗ trợ thêm các ngôn ngữ mới trong đó có tiếng Việt. Cùng một số tính năng đáng chú ý. Chi tiết tại link sau:,Bard của Google đã hỗ trợ thêm các ngôn ngữ mới trong đó có tiếng Việt. Cùng một số tính năng đáng chú ý. Chi tiết tại link sau:,,,"#sharing, #nlp",,
"Cho mình hỏi,
Mình đang xử lý bài toán xếp thời khoá biểu, qua tìm hiểu thì dùng thuật toá Genetic Algorithm (GA) vẫn là phổ biến nhất.
Có ai đã dùng nhiều và chỉ giúp thư viện nào tốt nhất để ứng dụng GA này cho nền .Net không?
Cảm ơn!","Cho mình hỏi, Mình đang xử lý bài toán xếp thời khoá biểu, qua tìm hiểu thì dùng thuật toá Genetic Algorithm (GA) vẫn là phổ biến nhất. Có ai đã dùng nhiều và chỉ giúp thư viện nào tốt nhất để ứng dụng GA này cho nền .Net không? Cảm ơn!",,,"#Q&A, #machine_learning",,
"Các bạn vui lòng đăng thông tin tuyển sinh, tuyển dụng, và sự kiện tháng 7/2023 vào post này.
Chúc các bạn một mùa hè vui vẻ.","Các bạn vui lòng đăng thông tin tuyển sinh, tuyển dụng, và sự kiện tháng 7/2023 vào post này. Chúc các bạn một mùa hè vui vẻ.",,,#sharing,,
"Em chào tất cả các anh chị có trong đây ạ. Em là sinh viên và em có đang tìm hiểu về transformer. Cụ thể là bài toán text summarization với mô hình PEGASUS . Db là tiếng việt nhưng em đang gặp 1 số vấn đề kiểu khi em Train ra đoạn text summarization nó bị tình trạng mất dấu, mất chữ cái tiếng việt. Anh chị có thể giúp em tìm lý do và khắc phục được không ạ. Em xin cảm ơn","Em chào tất cả các anh chị có trong đây ạ. Em là sinh viên và em có đang tìm hiểu về transformer. Cụ thể là bài toán text summarization với mô hình PEGASUS . Db là tiếng việt nhưng em đang gặp 1 số vấn đề kiểu khi em Train ra đoạn text summarization nó bị tình trạng mất dấu, mất chữ cái tiếng việt. Anh chị có thể giúp em tìm lý do và khắc phục được không ạ. Em xin cảm ơn",,,"#Q&A, #nlp, #deep_learning",,
"Em chào mọi người ạ.Em là newbie mới nhập môn của ngành.Chuyện là trong quá trình xử lý data em có 1 số thắc mắc mong đc mn giải đáp ạ:
1.Tại sao lại fit trên train và transform trên test? (Cái này em có đọc qua và hiểu sơ sơ nhưng mong đc mọi người giải đáp rõ ràng hơn ạ) Và nếu như vậy thì có bị xem là data leakage khi các tham số sẽ tính trong quá trình fit trên train lại đc áp dụng trên test ?
Hơn nữa theo như em thấy thông thường khi normalize hay encode các thứ thì sẽ fit(fit_transform) trên train và transform trên test vậy với các kĩ thuật khác như xử lý outlier hay missing value thì mình cũng làm như vậy hay là làm với tập nào thì mình fit_transform trên tập đấy luôn ạ?
2.Về data leakage ạ(cũng như cái trên thì cái này em cũng có đọc qua)nhưng vì muốn hiểu cặn kẽ hơn nên em mong được mn giải thích và cách khắc phục,hạn chế hay những điều k đc phép làm để tránh hiện tượng này ạ!
Em xin chân thành cảm ơn mọi người ạ !","Em chào mọi người ạ.Em là newbie mới nhập môn của ngành.Chuyện là trong quá trình xử lý data em có 1 số thắc mắc mong đc mn giải đáp ạ: 1.Tại sao lại fit trên train và transform trên test? (Cái này em có đọc qua và hiểu sơ sơ nhưng mong đc mọi người giải đáp rõ ràng hơn ạ) Và nếu như vậy thì có bị xem là data leakage khi các tham số sẽ tính trong quá trình fit trên train lại đc áp dụng trên test ? Hơn nữa theo như em thấy thông thường khi normalize hay encode các thứ thì sẽ fit(fit_transform) trên train và transform trên test vậy với các kĩ thuật khác như xử lý outlier hay missing value thì mình cũng làm như vậy hay là làm với tập nào thì mình fit_transform trên tập đấy luôn ạ? 2.Về data leakage ạ(cũng như cái trên thì cái này em cũng có đọc qua)nhưng vì muốn hiểu cặn kẽ hơn nên em mong được mn giải thích và cách khắc phục,hạn chế hay những điều k đc phép làm để tránh hiện tượng này ạ! Em xin chân thành cảm ơn mọi người ạ !",,,"#Q&A, #data",,
"Chào anh chị ạ , em hiện tại đã kết thúc năm 1 cntt ,chuẩn bị sang năm 2 em muốn theo AI thì mọi người có thể cho em một số tài liệu , YouTube, hay khoá học nào trên mạng ,tiếng anh hoặc trả phí đều được ạ em cảm ơn mọi người","Chào anh chị ạ , em hiện tại đã kết thúc năm 1 cntt ,chuẩn bị sang năm 2 em muốn theo AI thì mọi người có thể cho em một số tài liệu , YouTube, hay khoá học nào trên mạng ,tiếng anh hoặc trả phí đều được ạ em cảm ơn mọi người",,,"#Q&A, #machine_learning",,
cho mình hỏi thư viện VietOCR của a Quốc có cấu tạo là vgg19 và transformer đúng không ạ,cho mình hỏi thư viện VietOCR của a Quốc có cấu tạo là vgg19 và transformer đúng không ạ,,,"#Q&A, #deep_learning",,
"🔥KERAS CORE RA MẮT: HỖ TRỢ ĐỒNG THỜI PYTORCH, TENSORFLOW VÀ JAX🔥
✅Giới thiệu Keras Core, tiền thân của Keras 3.0, cho phép sử dụng PyTorch, TensorFlow và JAX trong một đoạn code duy nhất.
👨‍💻""We're excited to share with you a new library called Keras Core, a preview version of the future of Keras. In Fall 2023, this library will become Keras 3.0. Keras Core is a full rewrite of the Keras codebase that rebases it on top of a modular backend architecture. It makes it possible to run Keras workflows on top of arbitrary frameworks — starting with TensorFlow, JAX, and PyTorch."" - Keras Team.
Nguồn: VietAI","KERAS CORE RA MẮT: HỖ TRỢ ĐỒNG THỜI PYTORCH, TENSORFLOW VÀ JAX Giới thiệu Keras Core, tiền thân của Keras 3.0, cho phép sử dụng PyTorch, TensorFlow và JAX trong một đoạn code duy nhất. ""We're excited to share with you a new library called Keras Core, a preview version of the future of Keras. In Fall 2023, this library will become Keras 3.0. Keras Core is a full rewrite of the Keras codebase that rebases it on top of a modular backend architecture. It makes it possible to run Keras workflows on top of arbitrary frameworks — starting with TensorFlow, JAX, and PyTorch."" - Keras Team. Nguồn: VietAI",,,"#sharing, #python",,
"Kính chào các bác. Nhân dịp đang tìm hiểu về Kafka và thấy có áp dụng được cho ML nên xin mạnh dạn chia sẻ cùng cả nhà. Hi vọng giúp được các bạn mới học.
Link video: https://www.youtube.com/watch?v=JaBXLUdHEDU",Kính chào các bác. Nhân dịp đang tìm hiểu về Kafka và thấy có áp dụng được cho ML nên xin mạnh dạn chia sẻ cùng cả nhà. Hi vọng giúp được các bạn mới học. Link video: https://www.youtube.com/watch?v=JaBXLUdHEDU,,,"#sharing, #data",,
"VinAI Seminar - ""Innovations in Text-Guided Visual Content Generation""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Wang Hao, Nanyang Technological University
Time: 11:00 am - 12:00 pm (GMT+7), Mon, Jul 17, 2023","VinAI Seminar - ""Innovations in Text-Guided Visual Content Generation"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Wang Hao, Nanyang Technological University Time: 11:00 am - 12:00 pm (GMT+7), Mon, Jul 17, 2023",,,,,
"Em chào mọi người. Em có một thắc mắc. Em hiện tại tìm về ML được một thời gian rồi, em cũng có một vài project cá nhân. Hiện tại em muốn tìm một vị trí thực tập. Tuy nhiên khi em tìm hiểu thì em nhận thấy rằng có khá ít công ty tuyển vị trí từ thực tập sinh đến Junior, Middle- (Chủ yếu là em thấy có ở Viettel, Vin và FPT). Em cảm thấy khá hoang mang về tương lai của mình khi em nhận thấy có rất nhiều người đi theo mảng này nhưng có rất ít chỗ tuyển dụng (Em chưa có ý định học lên và nghiên cứu do điều kiện tài chính không cho phép). Mọi người có thể cho em một vài lời khuyên cũng như giới thiệu giúp em một vài nơi để em có thể xin thực tập cũng như phát triển tương lai được không ạ. Em xin chân thành cảm ơn!","Em chào mọi người. Em có một thắc mắc. Em hiện tại tìm về ML được một thời gian rồi, em cũng có một vài project cá nhân. Hiện tại em muốn tìm một vị trí thực tập. Tuy nhiên khi em tìm hiểu thì em nhận thấy rằng có khá ít công ty tuyển vị trí từ thực tập sinh đến Junior, Middle- (Chủ yếu là em thấy có ở Viettel, Vin và FPT). Em cảm thấy khá hoang mang về tương lai của mình khi em nhận thấy có rất nhiều người đi theo mảng này nhưng có rất ít chỗ tuyển dụng (Em chưa có ý định học lên và nghiên cứu do điều kiện tài chính không cho phép). Mọi người có thể cho em một vài lời khuyên cũng như giới thiệu giúp em một vài nơi để em có thể xin thực tập cũng như phát triển tương lai được không ạ. Em xin chân thành cảm ơn!",,,"#Q&A, #machine_learning",,
"#ask #Transformer #attention
Chào mọi người, mình đang tìm hiểu về Transformer architecture. Paper ""Attention is all you need"" không mô tả chi tiết về query, key, value để xác định dependency. Theo mình hiểu thì 3 vectors này được tạo ra bằng multiplication between embedding vectors and randomized (?) Q/ K/ V matrics, rồi dùng dot-product attention.
1. Tuy nhiên underlying idea/ intuition của việc sử dụng Q, K, V này là gì? Paper chỉ đề cập how they did it, but not why.
2. Vai trò cụ thể của query, key, value là gì? Vì trong encoder-decoder attention layers, queries come from the decoder layer, keys and values come from the output of the decoder; while for encoder self-attention layers/ decoder self-attention layers, keys, values and queries come from the same place.
Thanks!","Chào mọi người, mình đang tìm hiểu về Transformer architecture. Paper ""Attention is all you need"" không mô tả chi tiết về query, key, value để xác định dependency. Theo mình hiểu thì 3 vectors này được tạo ra bằng multiplication between embedding vectors and randomized (?) Q/ K/ V matrics, rồi dùng dot-product attention. 1. Tuy nhiên underlying idea/ intuition của việc sử dụng Q, K, V này là gì? Paper chỉ đề cập how they did it, but not why. 2. Vai trò cụ thể của query, key, value là gì? Vì trong encoder-decoder attention layers, queries come from the decoder layer, keys and values come from the output of the decoder; while for encoder self-attention layers/ decoder self-attention layers, keys, values and queries come from the same place. Thanks!",#ask	#Transformer	#attention,,"#Q&A, #deep_learning",,
"Hi mọi người,
Hôm nay mình xin phép chia sẻ blog về tổng quan mô hình Transformer, một mô hình khá nổi tiếng trong deep learning, mô hình này cũng là cơ sở của các mô hình BERT khác (và nhiều thứ khác). Do đó, để hiểu được BERTs các bạn cần nắm rất rõ về mô hình Transformer này. 

Ở blog này mình giới thiệu từ tổng quan đến cực kì chi tiết kiến trúc mô hình, giải thích tại sao các đề xuất về cơ thế positional encoding và self attention lại hợp lý. Đồng thời các điểm lưu ý để huấn luyện mô hình. 

Đồng thời mình cũng cung cấp source code để huấn luyện mô hình Transformer trên tập song ngữ TED gồm 600k câu do mình tự thu thập và matching cho các bạn tham khảo. Với bộ dữ liệu hy vọng giảm bớt khó khăn khi đi xin dữ liệu :)

Blog các bạn có thể đọc tại đây nhé.
https://pbcquoc.github.io/transformer/
Source code notebook thì các bạn clone về tại đây.
https://github.com/pbcquoc/transformer
Notebook tìm tại đây: 
https://github.com/pbcquoc/transformer/blob/master/transformer.ipynb
Bộ dữ liệu song ngữ anh-việt tại đây. 
https://drive.google.com/file/d/141kAeLKRHxHHkWCru6t1QZS6PRo8i_vE/view?usp=sharing
Các bạn có thể thảo luận tại đây hoặc tại blog của mình nhé. ","Hi mọi người, Hôm nay mình xin phép chia sẻ blog về tổng quan mô hình Transformer, một mô hình khá nổi tiếng trong deep learning, mô hình này cũng là cơ sở của các mô hình BERT khác (và nhiều thứ khác). Do đó, để hiểu được BERTs các bạn cần nắm rất rõ về mô hình Transformer này. Ở blog này mình giới thiệu từ tổng quan đến cực kì chi tiết kiến trúc mô hình, giải thích tại sao các đề xuất về cơ thế positional encoding và self attention lại hợp lý. Đồng thời các điểm lưu ý để huấn luyện mô hình. Đồng thời mình cũng cung cấp source code để huấn luyện mô hình Transformer trên tập song ngữ TED gồm 600k câu do mình tự thu thập và matching cho các bạn tham khảo. Với bộ dữ liệu hy vọng giảm bớt khó khăn khi đi xin dữ liệu :) Blog các bạn có thể đọc tại đây nhé. https://pbcquoc.github.io/transformer/ Source code notebook thì các bạn clone về tại đây. https://github.com/pbcquoc/transformer Notebook tìm tại đây: https://github.com/pbcquoc/transformer/blob/master/transformer.ipynb Bộ dữ liệu song ngữ anh-việt tại đây. https://drive.google.com/file/d/141kAeLKRHxHHkWCru6t1QZS6PRo8i_vE/view?usp=sharing Các bạn có thể thảo luận tại đây hoặc tại blog của mình nhé.",,,"#sharing, #deep_learning",,
"Chào mọi người !!.Em hiện tại có mong muốn ngắn hạn là trở thành thực tập sinh AI ,mong anh chị có thể tư vấn giúp em cần phải làm như thế nào ,về lộ trình cần học, kỹ năng tìm kiếm việc,cũng như chứng minh đủ năng lực với nhà tuyển dụng, ,Em đang là sinh viên năm 3 công nghệ thông tin muốn theo đuổi đam mê là AI , em có nền tảng cơ bản là toán nhưng lại khá đuối tiếng anh chỉ có thể dịch cơ bản tiếng anh,em cũng đã thử sức với neural network,CNN,rnn, training ,framework tensorflow, pytorch, một số thuật toán machine learning, cũng như code có một số code cơ bản deeplearning về nhận diện ,tạo ảnh ,tạo văn bản.","Chào mọi người !!.Em hiện tại có mong muốn ngắn hạn là trở thành thực tập sinh AI ,mong anh chị có thể tư vấn giúp em cần phải làm như thế nào ,về lộ trình cần học, kỹ năng tìm kiếm việc,cũng như chứng minh đủ năng lực với nhà tuyển dụng, ,Em đang là sinh viên năm 3 công nghệ thông tin muốn theo đuổi đam mê là AI , em có nền tảng cơ bản là toán nhưng lại khá đuối tiếng anh chỉ có thể dịch cơ bản tiếng anh,em cũng đã thử sức với neural network,CNN,rnn, training ,framework tensorflow, pytorch, một số thuật toán machine learning, cũng như code có một số code cơ bản deeplearning về nhận diện ,tạo ảnh ,tạo văn bản.",,,"#Q&A, #machine_learning",,
Xin phép chia sẻ với các bác một lộ trình trở thành Data Scientist ạ. Lộ trình này được đút kết từ chính kinh nghiệm của bản thân nên xin các bác gạch đá nhẹ tay ạ 🥲.,Xin phép chia sẻ với các bác một lộ trình trở thành Data Scientist ạ. Lộ trình này được đút kết từ chính kinh nghiệm của bản thân nên xin các bác gạch đá nhẹ tay ạ .,,,"#Q&A, #data",,
"Chào anh chị, em muốn hỏi về tiền xử lý dữ liệu ạ.
Trong dataset movie có một số categorical feature có nhiều giá trị cho một record. Ví dụ như credits là danh sách các diễn viên, đạo diễn của phim. Mỗi record chứa một list gồm trung bình 15-30 giá trị, có khoảng 140000 giá trị khác nhau cho toàn bộ dataset.
Em thắc mắc là mình nên dùng phương pháp gì để chuyển về vector số cho vào model ML (Catboost, XGBoost,...) ạ? Bởi vì one-hot encoding hoặc tokenizer em thấy đều không khả thi.
Em cảm ơn ạ.","Chào anh chị, em muốn hỏi về tiền xử lý dữ liệu ạ. Trong dataset movie có một số categorical feature có nhiều giá trị cho một record. Ví dụ như credits là danh sách các diễn viên, đạo diễn của phim. Mỗi record chứa một list gồm trung bình 15-30 giá trị, có khoảng 140000 giá trị khác nhau cho toàn bộ dataset. Em thắc mắc là mình nên dùng phương pháp gì để chuyển về vector số cho vào model ML (Catboost, XGBoost,...) ạ? Bởi vì one-hot encoding hoặc tokenizer em thấy đều không khả thi. Em cảm ơn ạ.",,,"#Q&A, #data",,
"Chào mọi người ạ , mình mới học về object deetection và Faster R-CNN, minhd có tham khảo và chạy thử code https://github.com/wingedrasengan927/pytorch-tutorials/blob/master/Object%20Detection/utils.py

code gốc thì train bằng cpi, và mình muốn dùng gpu để train , nhưng lạ là mình đã to(device) cả mô hình và data trong hàm train:

def training_loop(model, learning_rate, train_dataloader, n_epochs,device):
model.to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
model.train()
loss_list = []
for i in tqdm(range(n_epochs)):
total_loss = 0
for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:
img_batch = img_batch.to(device)
gt_bboxes_batch = gt_bboxes_batch.to(device)
gt_classes_batch = gt_classes_batch.to(device)
# forward pass
loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)
# backpropagation
optimizer.zero_grad()
loss.backward()
optimizer.step()
total_loss += loss.item()
loss_list.append(total_loss)
return loss_list

nhưng vẫn gặp lỗi 
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-23-ba5d2a14be34> in <cell line: 4>()
      2 n_epochs = 1000
      3 
----> 4 loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs,device)
8 frames
<ipython-input-22-14c287b01571> in training_loop(model, learning_rate, train_dataloader, n_epochs, device)
     16 
     17             # forward pass
---> 18             loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)
     19 
     20             # backpropagation
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []
<ipython-input-3-e0dda12f5636> in forward(self, images, gt_bboxes, gt_classes)
    189 
    190         total_rpn_loss, feature_map, proposals, \
--> 191         positive_anc_ind_sep, GT_class_pos = self.rpn(images, gt_bboxes, gt_classes)
    192 
    193         # get separate proposals for each sample
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []
<ipython-input-3-e0dda12f5636> in forward(self, images, gt_bboxes, gt_classes)
     88         positive_anc_ind, negative_anc_ind, GT_conf_scores, \
     89         GT_offsets, GT_class_pos, positive_anc_coords, \
---> 90         negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)
     91 
     92         # pass through the proposal module
<ipython-input-2-3424462ffbf3> in get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh, neg_thresh)
    190     # get the iou matrix which contains iou of every anchor box
    191     # against all the groundtruth bboxes in an image
--> 192     iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)
    193 
    194     # for every groundtruth bbox in an image, find the iou
<ipython-input-2-3424462ffbf3> in get_iou_mat(batch_size, anc_boxes_all, gt_bboxes_all)
    149         gt_bboxes = gt_bboxes_all[i]
    150         anc_boxes = anc_boxes_flat[i]
--> 151         ious_mat[i, :] = ops.box_iou(anc_boxes, gt_bboxes)
    152 
    153     return ious_mat
/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py in box_iou(boxes1, boxes2)
    269     if not torch.jit.is_scripting() and not torch.jit.is_tracing():
    270         _log_api_usage_once(box_iou)
--> 271     inter, union = _box_inter_union(boxes1, boxes2)
    272     iou = inter / union
    273     return iou
/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py in _box_inter_union(boxes1, boxes2)
    242     area2 = box_area(boxes2)
    243 
--> 244     lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    245     rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]
    246
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
code của mình ở link dưới, ai có thể giúp mình sửa lỗi với ạ
https://colab.research.google.com/drive/1WBX5GwKH5_pWajqJe60vVhzRDotf-LI8?usp=sharing","Chào mọi người ạ , mình mới học về object deetection và Faster R-CNN, minhd có tham khảo và chạy thử code https://github.com/wingedrasengan927/pytorch-tutorials/blob/master/Object%20Detection/utils.py code gốc thì train bằng cpi, và mình muốn dùng gpu để train , nhưng lạ là mình đã to(device) cả mô hình và data trong hàm train: def training_loop(model, learning_rate, train_dataloader, n_epochs,device): model.to(device) optimizer = optim.Adam(model.parameters(), lr=learning_rate) model.train() loss_list = [] for i in tqdm(range(n_epochs)): total_loss = 0 for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader: img_batch = img_batch.to(device) gt_bboxes_batch = gt_bboxes_batch.to(device) gt_classes_batch = gt_classes_batch.to(device) # forward pass loss = model(img_batch, gt_bboxes_batch, gt_classes_batch) # backpropagation optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() loss_list.append(total_loss) return loss_list nhưng vẫn gặp lỗi --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-23-ba5d2a14be34> in <cell line: 4>() 2 n_epochs = 1000 3 ----> 4 loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs,device) 8 frames <ipython-input-22-14c287b01571> in training_loop(model, learning_rate, train_dataloader, n_epochs, device) 16 17 # forward pass ---> 18 loss = model(img_batch, gt_bboxes_batch, gt_classes_batch) 19 20 # backpropagation /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs) 1499 or _global_backward_pre_hooks or _global_backward_hooks 1500 or _global_forward_hooks or _global_forward_pre_hooks): -> 1501 return forward_call(*args, **kwargs) 1502 # Do not call functions when jit is used 1503 full_backward_hooks, non_full_backward_hooks = [], [] <ipython-input-3-e0dda12f5636> in forward(self, images, gt_bboxes, gt_classes) 189 190 total_rpn_loss, feature_map, proposals, \ --> 191 positive_anc_ind_sep, GT_class_pos = self.rpn(images, gt_bboxes, gt_classes) 192 193 # get separate proposals for each sample /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs) 1499 or _global_backward_pre_hooks or _global_backward_hooks 1500 or _global_forward_hooks or _global_forward_pre_hooks): -> 1501 return forward_call(*args, **kwargs) 1502 # Do not call functions when jit is used 1503 full_backward_hooks, non_full_backward_hooks = [], [] <ipython-input-3-e0dda12f5636> in forward(self, images, gt_bboxes, gt_classes) 88 positive_anc_ind, negative_anc_ind, GT_conf_scores, \ 89 GT_offsets, GT_class_pos, positive_anc_coords, \ ---> 90 negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes) 91 92 # pass through the proposal module <ipython-input-2-3424462ffbf3> in get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh, neg_thresh) 190 # get the iou matrix which contains iou of every anchor box 191 # against all the groundtruth bboxes in an image --> 192 iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all) 193 194 # for every groundtruth bbox in an image, find the iou <ipython-input-2-3424462ffbf3> in get_iou_mat(batch_size, anc_boxes_all, gt_bboxes_all) 149 gt_bboxes = gt_bboxes_all[i] 150 anc_boxes = anc_boxes_flat[i] --> 151 ious_mat[i, :] = ops.box_iou(anc_boxes, gt_bboxes) 152 153 return ious_mat /usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py in box_iou(boxes1, boxes2) 269 if not torch.jit.is_scripting() and not torch.jit.is_tracing(): 270 _log_api_usage_once(box_iou) --> 271 inter, union = _box_inter_union(boxes1, boxes2) 272 iou = inter / union 273 return iou /usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py in _box_inter_union(boxes1, boxes2) 242 area2 = box_area(boxes2) 243 --> 244 lt = torch.max(boxes1[:, None, :2], boxes2[:, :2]) # [N,M,2] 245 rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:]) # [N,M,2] 246 RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! code của mình ở link dưới, ai có thể giúp mình sửa lỗi với ạ https://colab.research.google.com/drive/1WBX5GwKH5_pWajqJe60vVhzRDotf-LI8?usp=sharing",,,"#Q&A, #deep_learning",,
"Chào mọi người, hiện tại em có đẩy một project python lên Pythonanywhere, mọi thứ đều rất ổn ngoại trừ cái Tesseract, em không rõ tại sao nó lại trở nên quá chậm
from tesserocr import PyTessBaseAPI
with PyTessBaseAPI(path=TESSDATA_DIR) as api:
for _img in gray_images:
pil_image = Image.fromarray(_img)
api.SetImage(pil_image)
start_time = time.time()
text = ''.join(re.findall(r'\d+', api.GetUTF8Text()))
print('get_cells_2-ocr: ', time.time() - start_time)
_results.append(text)

On local:
get_cells_2-ocr:  0.009741067886352539
get_cells_2-ocr:  0.007970333099365234
get_cells_2-ocr:  0.0022356510162353516
get_cells_2-ocr:  0.00189208984375
get_cells_2-ocr:  0.25850939750671387
get_cells_2-ocr:  0.16674566268920898
get_cells_2-ocr:  1.3970351219177246
get_cells_2-ocr:  0.06349921226501465
get_cells_2-ocr:  0.05964350700378418
get_cells_2-ocr:  0.05002236366271973
get_cells_2-ocr:  0.06497740745544434
get_cells_2-ocr:  0.018608570098876953
On Pythonanywhere:
2023-07-10 04:03:57 3.670355796813965
2023-07-10 04:03:57 
2023-07-10 04:04:01 get_cells_2-ocr: 
2023-07-10 04:04:01  
2023-07-10 04:04:01 3.228891134262085
2023-07-10 04:04:01 
2023-07-10 04:04:06 get_cells_2-ocr: 
2023-07-10 04:04:06  
2023-07-10 04:04:06 5.227738380432129
2023-07-10 04:04:06 
2023-07-10 04:04:13 get_cells_2-ocr: 
2023-07-10 04:04:13  
2023-07-10 04:04:13 7.223729372024536
2023-07-10 04:04:13 
2023-07-10 04:04:17 get_cells_2-ocr: 
2023-07-10 04:04:17  
2023-07-10 04:04:17 3.716923475265503
2023-07-10 04:04:17 
2023-07-10 04:04:22 get_cells_2-ocr: 
2023-07-10 04:04:22  
2023-07-10 04:04:22 4.74852442741394

Như mọi người thấy thì hiệu suất của tesseract trên pythonanywhere chênh lệch quá lớn so với trên máy local, có bạn nào từng gặp vấn đề tương tự chưa ạ, cho mình xin chút ý kiến với","Chào mọi người, hiện tại em có đẩy một project python lên Pythonanywhere, mọi thứ đều rất ổn ngoại trừ cái Tesseract, em không rõ tại sao nó lại trở nên quá chậm from tesserocr import PyTessBaseAPI with PyTessBaseAPI(path=TESSDATA_DIR) as api: for _img in gray_images: pil_image = Image.fromarray(_img) api.SetImage(pil_image) start_time = time.time() text = ''.join(re.findall(r'\d+', api.GetUTF8Text())) print('get_cells_2-ocr: ', time.time() - start_time) _results.append(text) On local: get_cells_2-ocr: 0.009741067886352539 get_cells_2-ocr: 0.007970333099365234 get_cells_2-ocr: 0.0022356510162353516 get_cells_2-ocr: 0.00189208984375 get_cells_2-ocr: 0.25850939750671387 get_cells_2-ocr: 0.16674566268920898 get_cells_2-ocr: 1.3970351219177246 get_cells_2-ocr: 0.06349921226501465 get_cells_2-ocr: 0.05964350700378418 get_cells_2-ocr: 0.05002236366271973 get_cells_2-ocr: 0.06497740745544434 get_cells_2-ocr: 0.018608570098876953 On Pythonanywhere: 2023-07-10 04:03:57 3.670355796813965 2023-07-10 04:03:57 2023-07-10 04:04:01 get_cells_2-ocr: 2023-07-10 04:04:01 2023-07-10 04:04:01 3.228891134262085 2023-07-10 04:04:01 2023-07-10 04:04:06 get_cells_2-ocr: 2023-07-10 04:04:06 2023-07-10 04:04:06 5.227738380432129 2023-07-10 04:04:06 2023-07-10 04:04:13 get_cells_2-ocr: 2023-07-10 04:04:13 2023-07-10 04:04:13 7.223729372024536 2023-07-10 04:04:13 2023-07-10 04:04:17 get_cells_2-ocr: 2023-07-10 04:04:17 2023-07-10 04:04:17 3.716923475265503 2023-07-10 04:04:17 2023-07-10 04:04:22 get_cells_2-ocr: 2023-07-10 04:04:22 2023-07-10 04:04:22 4.74852442741394 Như mọi người thấy thì hiệu suất của tesseract trên pythonanywhere chênh lệch quá lớn so với trên máy local, có bạn nào từng gặp vấn đề tương tự chưa ạ, cho mình xin chút ý kiến với",,,"#Q&A, #python",,
"Hôm nay nhân dịp Vietcuna 3B cán mốc 1.6k lượt tải, team mình phát hành bản alpha của Vietcuna 7B.
Dù đã được finetune instruction nhưng đây không phải phiên bản hoàn chỉnh nhất. Mục đích của phiên bản này là một base model cho cộng đồng tự finetune thêm. Phiên bản hoàn chỉnh sẽ chỉ được phát hành cho các doanh nghiệp (liên hệ riêng với mình) trong khoảng thời gian này.
Ngoài ra lần này mình cũng phát hành luôn phiên bản dịch tiếng Việt của Dataset Lima. Lưu ý đây là phiên bản được dịch từ phiên bản gốc với model en2vi của VinAI nên sẽ chỉ được dùng với mục đích nghiên cứu. Phiên bản augmented được làm bằng tay với cộng tác viên cũng sẽ chỉ được phát hành cho các doanh nhiệp.
HuggingFace Model: https://huggingface.co/vilm/vietcuna-7b-alpha
GitHub with Gradio UI: https://github.com/vilm-ai/vietcuna
Lima-vi Dataset: https://huggingface.co/datasets/vilm/lima-vi
Mong mọi người cùng nhau phát triển phiên bản alpha này để cộng đồng có các phiên bản mạnh mẽ hơn của Vietcuna :D","Hôm nay nhân dịp Vietcuna 3B cán mốc 1.6k lượt tải, team mình phát hành bản alpha của Vietcuna 7B. Dù đã được finetune instruction nhưng đây không phải phiên bản hoàn chỉnh nhất. Mục đích của phiên bản này là một base model cho cộng đồng tự finetune thêm. Phiên bản hoàn chỉnh sẽ chỉ được phát hành cho các doanh nghiệp (liên hệ riêng với mình) trong khoảng thời gian này. Ngoài ra lần này mình cũng phát hành luôn phiên bản dịch tiếng Việt của Dataset Lima. Lưu ý đây là phiên bản được dịch từ phiên bản gốc với model en2vi của VinAI nên sẽ chỉ được dùng với mục đích nghiên cứu. Phiên bản augmented được làm bằng tay với cộng tác viên cũng sẽ chỉ được phát hành cho các doanh nhiệp. HuggingFace Model: https://huggingface.co/vilm/vietcuna-7b-alpha GitHub with Gradio UI: https://github.com/vilm-ai/vietcuna Lima-vi Dataset: https://huggingface.co/datasets/vilm/lima-vi Mong mọi người cùng nhau phát triển phiên bản alpha này để cộng đồng có các phiên bản mạnh mẽ hơn của Vietcuna :D",,,"#sharing, #nlp",,
"Pydantic 2.0 vừa release đẩy một phần core qua xử lý bằng Rust giúp tối ưu tốc độ lên tới 5 lần, bác nào dùng FastAPI chú ý nhé ạ 🧐","Pydantic 2.0 vừa release đẩy một phần core qua xử lý bằng Rust giúp tối ưu tốc độ lên tới 5 lần, bác nào dùng FastAPI chú ý nhé ạ",,,#sharing,,
Em hiện là sinh viên năm 3 và muốn tham gia các cuộc thi để bổ sung thành tích vào CV thì em muốn hỏi mọi người bên mảng AI hay ML có cuộc thi nào trong nước không ạ?,Em hiện là sinh viên năm 3 và muốn tham gia các cuộc thi để bổ sung thành tích vào CV thì em muốn hỏi mọi người bên mảng AI hay ML có cuộc thi nào trong nước không ạ?,,,"#Q&A, #machine_learning",,
"❓BẠN ĐÃ BIẾT GÌ VỀ GOOGLE I/O EXTENDED CLOUD HANOI❓
📌 Đăng ký tham gia sự kiện ngay tại: https://bit.ly/3NdmYkJ
—-------------------
Google I/O Extended Cloud Hanoi là một trong những sự kiện thường niên nổi bật về công nghệ, đặc biệt là công nghệ Cloud tại Hà Nội. Được tổ chức lần đầu vào năm 2021 bởi Google Developer Group Cloud Hanoi(GDG Cloud Hanoi), sự kiện đã chinh phục trái tim của những bạn trẻ đam mê công nghệ.
✨ Hành trình đầy ý nghĩa này đã lan tỏa sứ mệnh kết nối, chia sẻ và truyền cảm hứng đến cộng đồng công nghệ tại Việt Nam và đặc biệt là tại Hà Nội.
Hãy cùng GDG Cloud Hanoi nhìn lại những dấu ấn đáng nhớ trên hành trình 3 năm qua của sự kiện:
🔻 Tổ chức thành công Google I/O Extended Cloud Hanoi lần đầu tiên vào năm 2021
🔻 Trải qua 2 mùa công nghệ đáng nhớ với sự tham gia nhiệt tình của cộng đồng
🔻 11 phiên thảo luận hấp dẫn với nhiều chủ đề được đánh giá cao
🔻 1000+ lượt tham dự cùng với sự hiện diện của 16 chuyên gia hàng đầu
🔻 Hợp tác với 10+ đối tác đa ngành
🎉 Tiếp nối hành trình với năm 2023, Google I/O Extended Cloud Hanoi hứa hẹn sẽ tiếp tục mang đến cho người tham dự những phiên thảo luận sôi nổi và bổ ích về các công nghệ mới nhất, được dẫn dắt bởi các chuyên gia hàng đầu đến từ trong và ngoài nước. Ngoài kiến thức, những phần quà tặng siêu ngầu và các hoạt động bên lề sẽ góp phần mang lại trải nghiệm không thể bỏ qua cho các dân chơi công nghệ thực thụ 😋.
😉 Bạn có kỉ niệm đáng nhớ nào với Google I/O Extended Cloud Hanoi không? Hãy cùng chia sẻ những kỷ niệm đó với GDG Cloud Hanoi trong phần bình luận bên dưới nhé.
💪 Đừng bỏ lỡ cơ hội cùng chúng tôi Unleashing the Power of Cloud trong sự kiện năm nay nha!!!
—-------------------
𝐆𝐎𝐎𝐆𝐋𝐄 𝐈/𝐎 𝐄𝐗𝐓𝐄𝐍𝐃𝐄𝐃 𝐂𝐋𝐎𝐔𝐃 𝐇𝐀𝐍𝐎𝐈 𝟐𝟎𝟐𝟑 - Sự kiện dành cho cộng đồng yêu công nghệ “cháy” nhất mùa hè năm nay tại Hà Nội
📌 Thời gian: 13:00 - 17:00 Thứ Bảy, ngày 29/7/2023
📌 Địa điểm tổ chức: Hội trường Tầng 8, FPT Tower, số 10 Phạm Văn Bạch, Dịch Vọng, Cầu Giấy, Hà Nội
📌 Link đăng ký (miễn phí): https://bit.ly/3NdmYkJ
Deadline: 25/07/2023, hoặc đóng sớm khi đủ số lượng
👉 Liên hệ tài trợ, đăng ký diễn giả, gian hàng: Ms. Hương Nguyễn - 0377 484 425
👉 Thông tin chương trình, báo chí: Mr. Huy Đặng - 0373 952 446
#GoogleIO #GDGCloudHanoi #GoogleIOExtendedCloudHanoi","BẠN ĐÃ BIẾT GÌ VỀ GOOGLE I/O EXTENDED CLOUD HANOI Đăng ký tham gia sự kiện ngay tại: https://bit.ly/3NdmYkJ —------------------- Google I/O Extended Cloud Hanoi là một trong những sự kiện thường niên nổi bật về công nghệ, đặc biệt là công nghệ Cloud tại Hà Nội. Được tổ chức lần đầu vào năm 2021 bởi Google Developer Group Cloud Hanoi(GDG Cloud Hanoi), sự kiện đã chinh phục trái tim của những bạn trẻ đam mê công nghệ. Hành trình đầy ý nghĩa này đã lan tỏa sứ mệnh kết nối, chia sẻ và truyền cảm hứng đến cộng đồng công nghệ tại Việt Nam và đặc biệt là tại Hà Nội. Hãy cùng GDG Cloud Hanoi nhìn lại những dấu ấn đáng nhớ trên hành trình 3 năm qua của sự kiện: Tổ chức thành công Google I/O Extended Cloud Hanoi lần đầu tiên vào năm 2021 Trải qua 2 mùa công nghệ đáng nhớ với sự tham gia nhiệt tình của cộng đồng 11 phiên thảo luận hấp dẫn với nhiều chủ đề được đánh giá cao 1000+ lượt tham dự cùng với sự hiện diện của 16 chuyên gia hàng đầu Hợp tác với 10+ đối tác đa ngành Tiếp nối hành trình với năm 2023, Google I/O Extended Cloud Hanoi hứa hẹn sẽ tiếp tục mang đến cho người tham dự những phiên thảo luận sôi nổi và bổ ích về các công nghệ mới nhất, được dẫn dắt bởi các chuyên gia hàng đầu đến từ trong và ngoài nước. Ngoài kiến thức, những phần quà tặng siêu ngầu và các hoạt động bên lề sẽ góp phần mang lại trải nghiệm không thể bỏ qua cho các dân chơi công nghệ thực thụ . Bạn có kỉ niệm đáng nhớ nào với Google I/O Extended Cloud Hanoi không? Hãy cùng chia sẻ những kỷ niệm đó với GDG Cloud Hanoi trong phần bình luận bên dưới nhé. Đừng bỏ lỡ cơ hội cùng chúng tôi Unleashing the Power of Cloud trong sự kiện năm nay nha!!! —------------------- / - Sự kiện dành cho cộng đồng yêu công nghệ “cháy” nhất mùa hè năm nay tại Hà Nội Thời gian: 13:00 - 17:00 Thứ Bảy, ngày 29/7/2023 Địa điểm tổ chức: Hội trường Tầng 8, FPT Tower, số 10 Phạm Văn Bạch, Dịch Vọng, Cầu Giấy, Hà Nội Link đăng ký (miễn phí): https://bit.ly/3NdmYkJ Deadline: 25/07/2023, hoặc đóng sớm khi đủ số lượng Liên hệ tài trợ, đăng ký diễn giả, gian hàng: Ms. Hương Nguyễn - 0377 484 425 Thông tin chương trình, báo chí: Mr. Huy Đặng - 0373 952 446",#GoogleIO	#GDGCloudHanoi	#GoogleIOExtendedCloudHanoi,,#webinar,,
Mn ơi cho e hỏi về kiến thức toán. Em cần học những kiến thức toán gì trước ạ và thứ tự học cái kiến thức đó ra sao để theo ML ạ,Mn ơi cho e hỏi về kiến thức toán. Em cần học những kiến thức toán gì trước ạ và thứ tự học cái kiến thức đó ra sao để theo ML ạ,,,"#Q&A, #math",,
"em chào mọi người ạ, hiện tại em có đang làm trích xuất thực thể trong văn bản tiếng việt và PoS dựa theo toolkit phonlp (embedding là phoBERT) và dataset là VLSP2016 thì có một số là cho ra thẻ đúng còn một số thì em có gặp vấn đề về thừa thẻ trong tác vụ PoS ạ ở đây em có ví dụ cụ thể luôn ạ. Mọi người có thể giải đáp thắc mắc là tại sao lại bị thừa thẻ ko ạ. Hiện tại nhóm em đang giả thuyết là do embedding của phoBERT và dataset. Em xin cảm ơn
(note: trong ví dụ là em bị thừa 2 thẻ đầu là Ny)","em chào mọi người ạ, hiện tại em có đang làm trích xuất thực thể trong văn bản tiếng việt và PoS dựa theo toolkit phonlp (embedding là phoBERT) và dataset là VLSP2016 thì có một số là cho ra thẻ đúng còn một số thì em có gặp vấn đề về thừa thẻ trong tác vụ PoS ạ ở đây em có ví dụ cụ thể luôn ạ. Mọi người có thể giải đáp thắc mắc là tại sao lại bị thừa thẻ ko ạ. Hiện tại nhóm em đang giả thuyết là do embedding của phoBERT và dataset. Em xin cảm ơn (note: trong ví dụ là em bị thừa 2 thẻ đầu là Ny)",,,"#Q&A, #deep_learning, #nlp",,
"Hi mọi người cho phép em hỏi về thuật toán Coordinate ascent variational inference (CAVI) ạ. Em đang đọc paper này (https://arxiv.org/pdf/1601.00670.pdf). Tuy nhiên em ko hiểu được chỗ Equation 19 tại sao có thể nói ELBO(q) maximize khi q_j(z_j) bằng q*_j(z_j) ạ.
Vì để có eq 18 thì em đang hiểu là phải công nhận kết quả của eq17. Mà eq17 là kết quả luôn được thừa nhận luôn rồi thì em thấy reasoning đang bị circular.
Có bác nào có cao kiến góp ý cho em với ạ.",Hi mọi người cho phép em hỏi về thuật toán Coordinate ascent variational inference (CAVI) ạ. Em đang đọc paper này (https://arxiv.org/pdf/1601.00670.pdf). Tuy nhiên em ko hiểu được chỗ Equation 19 tại sao có thể nói ELBO(q) maximize khi q_j(z_j) bằng q*_j(z_j) ạ. Vì để có eq 18 thì em đang hiểu là phải công nhận kết quả của eq17. Mà eq17 là kết quả luôn được thừa nhận luôn rồi thì em thấy reasoning đang bị circular. Có bác nào có cao kiến góp ý cho em với ạ.,,,"#Q&A, #machine_learning",,
"Machine Learning cho người ngoại đạo.
Chào mọi người, hiện mình đang là nghiên cứu về phân tích chuyển động của con người.
Phần nghiên cứu của mình có liên quan nhiều đến tính toán các thông số của chuyển động giống như bên ngành kỹ thuật tính toán chuyển động của cánh tay robot. Nên ML được ứng dụng rất nhiều để tính toán và đưa ra dự đoán.
Mình có một chút kiến thức về phân tích dữ liệu bằng R, tuy nhiên là do mình tự học nên nó không có hệ thống, cần phân tích cái gì thì lại dùng google tra cứu.
Cho nên hiện mình rất muốn học ML và Data Analysis một cách có hệ thống, nền tảng từ cơ bản đi lên.
Mọi người có tư vấn giúp mình một số thắc mắc?
- Kiến thức cơ bản về toán và tin cần những gì? (mình học y nên hồi đại học không học về toán- tin cao cấp).
- Kiến thức về cơ bản về ML cần học cái gì trước tiên?
Cảm ơn mọi người.","Machine Learning cho người ngoại đạo. Chào mọi người, hiện mình đang là nghiên cứu về phân tích chuyển động của con người. Phần nghiên cứu của mình có liên quan nhiều đến tính toán các thông số của chuyển động giống như bên ngành kỹ thuật tính toán chuyển động của cánh tay robot. Nên ML được ứng dụng rất nhiều để tính toán và đưa ra dự đoán. Mình có một chút kiến thức về phân tích dữ liệu bằng R, tuy nhiên là do mình tự học nên nó không có hệ thống, cần phân tích cái gì thì lại dùng google tra cứu. Cho nên hiện mình rất muốn học ML và Data Analysis một cách có hệ thống, nền tảng từ cơ bản đi lên. Mọi người có tư vấn giúp mình một số thắc mắc? - Kiến thức cơ bản về toán và tin cần những gì? (mình học y nên hồi đại học không học về toán- tin cao cấp). - Kiến thức về cơ bản về ML cần học cái gì trước tiên? Cảm ơn mọi người.",,,"#Q&A, #machine_learning",,
"em chào các thầy/cô, anh/chị và các bạn ạ,
Mọi người cho e nhờ chút ạ,
Em đang làm việc với bài toán Object detection, mục tiêu là áp dụng và cải tiến model YOLOv8 trên custom dataset được cung cấp, em đang tìm hiểu về cấu trúc mạng của model yolov8, nhưng nó còn quá mới và chưa có official paper ạ, Em muốn tìm hiểu và đọc thêm các latest techniques về Object detection để có thể mang về cải tiến model Yolov8 gốc để có thể viết paper ạ, em xin các anh chị recommend giúp e các trang/ journal hoặc nơi nào để đọc về các novelty techniques về bài toán Object detection ạ, hoặc anh chị có gợi ý cải thiện model thì tốt quá ạ, em cảm ơn mọi người rất nhiều ạ.","em chào các thầy/cô, anh/chị và các bạn ạ, Mọi người cho e nhờ chút ạ, Em đang làm việc với bài toán Object detection, mục tiêu là áp dụng và cải tiến model YOLOv8 trên custom dataset được cung cấp, em đang tìm hiểu về cấu trúc mạng của model yolov8, nhưng nó còn quá mới và chưa có official paper ạ, Em muốn tìm hiểu và đọc thêm các latest techniques về Object detection để có thể mang về cải tiến model Yolov8 gốc để có thể viết paper ạ, em xin các anh chị recommend giúp e các trang/ journal hoặc nơi nào để đọc về các novelty techniques về bài toán Object detection ạ, hoặc anh chị có gợi ý cải thiện model thì tốt quá ạ, em cảm ơn mọi người rất nhiều ạ.",,,"#Q&A, #cv, #deep_learning",,
"Em chào mọi người.
Hiện em đang muốn sử dụng mô hình VNCoreNLP, tuy nhiên, khi cài đặt trên vscode thì em gặp lỗi như trong ảnh. Ai biết cách khắc phục thì giúp em với.
Em cảm ơn mọi người.","Em chào mọi người. Hiện em đang muốn sử dụng mô hình VNCoreNLP, tuy nhiên, khi cài đặt trên vscode thì em gặp lỗi như trong ảnh. Ai biết cách khắc phục thì giúp em với. Em cảm ơn mọi người.",,,"#Q&A, #deep_learning",,
"Em chào mọi người ạ . Giờ em muốn sử dụng 1 mô hình để liên tục nhận và phân loại dữ liệu trong 24h thành 2 nhãn thì nên sử dụng phương pháp gì .
( em đã có sẵn 1 bộ dữ liệu đã phân loại 2 nhãn này trong quá khứ để train mô hình phân loại )",Em chào mọi người ạ . Giờ em muốn sử dụng 1 mô hình để liên tục nhận và phân loại dữ liệu trong 24h thành 2 nhãn thì nên sử dụng phương pháp gì . ( em đã có sẵn 1 bộ dữ liệu đã phân loại 2 nhãn này trong quá khứ để train mô hình phân loại ),,,"#Q&A, #machine_learning",,
"Em chào mọi người ạ. Em có bài tập là build 1 Recommendation System (RS), nhưng em hiện tại vẫn khá mù mờ vì chưa hiểu rõ phần train và phần test được thực hiện ntn.
Như em đã tìm hiểu thì trong machine learning, tập train sẽ xấp xỉ hàm dự đoán còn tập test sẽ để xđ xem hàm đó tốt đến đâu. Nhưng ở phần RS này, có 1 số thuật toán để có thể dự đoán được luôn đánh giá của người dùng (Collaborative-filtering user-based/item-based, matrix factorization,...) thì mình chỉ cần áp dụng thuật toán với dataset sau khi xử lí để có được các đánh giá xấp xỉ. Hơn nữa, em tìm thấy code mẫu trên mạng ( em để link bên dưới) thì thấy người ta vẫn thừa số hóa cả ma trận ratings, tuy nhiên chỉ cố gắng minimize loss của phần train - 1 subset của ma trận ratings trên và thả trôi phần test - phần còn lại của ma trận. Đến đây em kiểu bị @@, vì sao k thừa số hóa rồi minimize loss của cả ma trận nếu như test loss nhỏ là tốt. Từ đấy dẫn đến câu hỏi ban đầu của em: Tại sao lại chia train vs test ra ngay từ ban đầu?
Em xin trân thành cảm ơn mọi ý kiến đóng góp ạ. Link bài code đây ạ (cụ thể ở phần function build_model): https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&utm_campaign=colab-external&utm_medium=referral&utm_content=recommendation-systems#scrollTo=M9RxIX_Oo4tp
PS: Với lại cho em hỏi thêm là có phải phương pháp đánh giá loss chưa tối ưu, bởi theo nghiên cứu của tác giả Đỗ Thị Liên và cộng sự (https://portal.ptit.edu.vn/saudaihoc/wp-content/uploads/2020/02/LA_%C4%90%E1%BB%97-Th%E1%BB%8B-Li%C3%AAn.pdf?fbclid=IwAR1BU24ivQHfe6mg8dYbi0iwq5Plcmew3B2ODYf0SmZubAtOIscNChQQO_I#page23) tại trang 51, cần phải ẩn đi 1 số p đánh giá đã biết của người dùng trên tập test, sau đó so sánh giá trị dự đoán với p giá trị thực. Tuy nhiên trong các phần sau và phần kết quả, tác giả k đề cập đến việc đã lấy p chiếm bao nhiêu % nên em muốn hỏi mọi người là nên đánh giá hiệu năng ntn và nếu là cách của tác giả Liên thì lấy p là bao nhiêu % ạ.
1 lần nữa em xin chân thành cảm ơn nhiều ạ","Em chào mọi người ạ. Em có bài tập là build 1 Recommendation System (RS), nhưng em hiện tại vẫn khá mù mờ vì chưa hiểu rõ phần train và phần test được thực hiện ntn. Như em đã tìm hiểu thì trong machine learning, tập train sẽ xấp xỉ hàm dự đoán còn tập test sẽ để xđ xem hàm đó tốt đến đâu. Nhưng ở phần RS này, có 1 số thuật toán để có thể dự đoán được luôn đánh giá của người dùng (Collaborative-filtering user-based/item-based, matrix factorization,...) thì mình chỉ cần áp dụng thuật toán với dataset sau khi xử lí để có được các đánh giá xấp xỉ. Hơn nữa, em tìm thấy code mẫu trên mạng ( em để link bên dưới) thì thấy người ta vẫn thừa số hóa cả ma trận ratings, tuy nhiên chỉ cố gắng minimize loss của phần train - 1 subset của ma trận ratings trên và thả trôi phần test - phần còn lại của ma trận. Đến đây em kiểu bị @@, vì sao k thừa số hóa rồi minimize loss của cả ma trận nếu như test loss nhỏ là tốt. Từ đấy dẫn đến câu hỏi ban đầu của em: Tại sao lại chia train vs test ra ngay từ ban đầu? Em xin trân thành cảm ơn mọi ý kiến đóng góp ạ. Link bài code đây ạ (cụ thể ở phần function build_model): https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&utm_campaign=colab-external&utm_medium=referral&utm_content=recommendation-systems#scrollTo=M9RxIX_Oo4tp PS: Với lại cho em hỏi thêm là có phải phương pháp đánh giá loss chưa tối ưu, bởi theo nghiên cứu của tác giả Đỗ Thị Liên và cộng sự (https://portal.ptit.edu.vn/saudaihoc/wp-content/uploads/2020/02/LA_%C4%90%E1%BB%97-Th%E1%BB%8B-Li%C3%AAn.pdf?fbclid=IwAR1BU24ivQHfe6mg8dYbi0iwq5Plcmew3B2ODYf0SmZubAtOIscNChQQO_I#page23) tại trang 51, cần phải ẩn đi 1 số p đánh giá đã biết của người dùng trên tập test, sau đó so sánh giá trị dự đoán với p giá trị thực. Tuy nhiên trong các phần sau và phần kết quả, tác giả k đề cập đến việc đã lấy p chiếm bao nhiêu % nên em muốn hỏi mọi người là nên đánh giá hiệu năng ntn và nếu là cách của tác giả Liên thì lấy p là bao nhiêu % ạ. 1 lần nữa em xin chân thành cảm ơn nhiều ạ",,,"#Q&A, #machine_learning",,
"Mình làm hạ tầng phần cứng, có thể hỗ trợ share lab có sẵn các tài nguyên máy ảo, kết nối hadoop, có mạng Internet để tự cài thêm package. Bạn nào cần tài nguyên trải nghiệm, test có thể ib mình, lab tồn tại được 10 ngày mỗi lần khởi tạo. Mỗi máy ảo linux có 24GB RAM","Mình làm hạ tầng phần cứng, có thể hỗ trợ share lab có sẵn các tài nguyên máy ảo, kết nối hadoop, có mạng Internet để tự cài thêm package. Bạn nào cần tài nguyên trải nghiệm, test có thể ib mình, lab tồn tại được 10 ngày mỗi lần khởi tạo. Mỗi máy ảo linux có 24GB RAM",,,#sharing,,
"Anh/chị/bạn trong group mình có thể cho em xin các nguồn thông tin mà mọi người cập nhật thông tin về các công nghệ machine learning, các thuật toán, các dòng code hay ở đâu không ạ? Tại vì em là newbie nên không rõ diễn đàn nào hay và có độ thảo luận cao. Các diễn đàn bằng tiếng anh càng tốt ạ <3.
Cảm ơn mọi người đã đọc post, chúc mọi người làm việc thật năng suất!","Anh/chị/bạn trong group mình có thể cho em xin các nguồn thông tin mà mọi người cập nhật thông tin về các công nghệ machine learning, các thuật toán, các dòng code hay ở đâu không ạ? Tại vì em là newbie nên không rõ diễn đàn nào hay và có độ thảo luận cao. Các diễn đàn bằng tiếng anh càng tốt ạ <3. Cảm ơn mọi người đã đọc post, chúc mọi người làm việc thật năng suất!",,,"#Q&A, #machine_learning",,
"Một video thú vị về toán. Hi vọng nó giải thích cặn kẽ để ta hiểu tại sao lại có số Pi, số e,…","Một video thú vị về toán. Hi vọng nó giải thích cặn kẽ để ta hiểu tại sao lại có số Pi, số e,…",,,"#sharing, #math",,
"Chào mọi người, em là sinh viên đang theo học AI/ML, em có một thắc mắc về hướng đi trong ngành. Theo em biết thì có 2 hướng chính là Research và Engineer, anh/chị cho em hỏi những điểm khác nhau của 2 hướng này, và cơ hội của hướng nào sẽ là nhiều hơn ạ.","Chào mọi người, em là sinh viên đang theo học AI/ML, em có một thắc mắc về hướng đi trong ngành. Theo em biết thì có 2 hướng chính là Research và Engineer, anh/chị cho em hỏi những điểm khác nhau của 2 hướng này, và cơ hội của hướng nào sẽ là nhiều hơn ạ.",,,"#Q&A, #machine_learning",,
"Chào mọi người, không biết ở tp.HCM có nơi nào bán sách  Calculus Early Transcendentals Ninth Edition không, mặc dù có bản pdf nhưng mình làm việc với máy tính rất nhiều nên muốn đọc bản giấy cho đỡ đau mắt và giải trí trong thời gian rảnh một tí.","Chào mọi người, không biết ở tp.HCM có nơi nào bán sách Calculus Early Transcendentals Ninth Edition không, mặc dù có bản pdf nhưng mình làm việc với máy tính rất nhiều nên muốn đọc bản giấy cho đỡ đau mắt và giải trí trong thời gian rảnh một tí.",,,"#Q&A, #math",,
"Em chào mọi người ạ,em là newbie chập chững vào nghề đang gặp 1 vấn đề ở 1 bài toán mong được mn giải đáp ạ!
Bài toán:Em có áp dụng HOG để trích xuất đặc trưng ảnh,ảnh của em là 150*150.khi dùng HOG thì em cho orientations = 8,pixel per cell =16,cell per block =4 thì khi trích xuất xong 1 ảnh sẽ đc biểu diễn dưới 1 vector kích thước (4608,)
1.Em có thắc mắc con số 4608 này tạo ra ntn ạ?
2.Tại vì em thấy size ảnh k chia hết cho pixel per cell và nếu luận ngược lên 4608 / 8*4*4= 36 =6*6(với 8*4*4 là độ dài Vector HOG của mỗi block) hay nói cách khác trên mỗi phương sẽ khi đi từ trái qua phải hoặc trên xuống dưới ta sẽ phải tính HOG trên 6 block kết hợp vs việc 4 cell cho mỗi block thì ta sẽ có đc số cell theo từng phương là 9
Nma nếu tính như vậy thì số pixel theo mỗi phương sẽ là 9*16 < 150 như đúng size ảnh gốc vậy liệu có phải đám pixel còn lại bị thiếu sẽ bị cắt đi k dùng đến khi tính hay là sẽ được ném vào các cell trong 9 cell có sẵn đó ạ ?
Em cảm ơn mọi người nhiều ạ,mong sẽ được admin duyệt bài sớm và sớm nhận được giúp đỡ từ mn ạ!!!","Em chào mọi người ạ,em là newbie chập chững vào nghề đang gặp 1 vấn đề ở 1 bài toán mong được mn giải đáp ạ! Bài toán:Em có áp dụng HOG để trích xuất đặc trưng ảnh,ảnh của em là 150*150.khi dùng HOG thì em cho orientations = 8,pixel per cell =16,cell per block =4 thì khi trích xuất xong 1 ảnh sẽ đc biểu diễn dưới 1 vector kích thước (4608,) 1.Em có thắc mắc con số 4608 này tạo ra ntn ạ? 2.Tại vì em thấy size ảnh k chia hết cho pixel per cell và nếu luận ngược lên 4608 / 8*4*4= 36 =6*6(với 8*4*4 là độ dài Vector HOG của mỗi block) hay nói cách khác trên mỗi phương sẽ khi đi từ trái qua phải hoặc trên xuống dưới ta sẽ phải tính HOG trên 6 block kết hợp vs việc 4 cell cho mỗi block thì ta sẽ có đc số cell theo từng phương là 9 Nma nếu tính như vậy thì số pixel theo mỗi phương sẽ là 9*16 < 150 như đúng size ảnh gốc vậy liệu có phải đám pixel còn lại bị thiếu sẽ bị cắt đi k dùng đến khi tính hay là sẽ được ném vào các cell trong 9 cell có sẵn đó ạ ? Em cảm ơn mọi người nhiều ạ,mong sẽ được admin duyệt bài sớm và sớm nhận được giúp đỡ từ mn ạ!!!",,,"#Q&A, #cv",,
"Xin được share với các bác 1 seminar dành chung cho tất cả các anh em hứng thú với MLOps với chủ đề:
“Optimize ML service’s performance with continuous right-sizing containers in Kubernetes”",Xin được share với các bác 1 seminar dành chung cho tất cả các anh em hứng thú với MLOps với chủ đề: “Optimize ML service’s performance with continuous right-sizing containers in Kubernetes”,,,#webinar,,
"THUÊ SERVER TRÊN VAST.AI
Chào mọi người, trong group mình có ai đã từng thuê server trên vast.ai chưa ạ cho mình ít review. Thấy giá cũng rẻ mà nhỉ. Em định dùng để train model AI thay vì Kaggle hay Colab ko biết có nhanh hơn ko. Với lại nó cần credit card để nạp tiền vào mà giờ em không có credit card thì ko biết có giải pháp nào ko. Em xin cảm ơn.","THUÊ SERVER TRÊN VAST.AI Chào mọi người, trong group mình có ai đã từng thuê server trên vast.ai chưa ạ cho mình ít review. Thấy giá cũng rẻ mà nhỉ. Em định dùng để train model AI thay vì Kaggle hay Colab ko biết có nhanh hơn ko. Với lại nó cần credit card để nạp tiền vào mà giờ em không có credit card thì ko biết có giải pháp nào ko. Em xin cảm ơn.",,,#Q&A,,
Khoa học dữ liệu (Data Science) là một lĩnh vực đang phát triển nhanh chóng với nhiều cơ hội nghề nghiệp. Hãy cùng khám phá một số vai trò chính trong lĩnh vực Khoa học dữ liệu.,Khoa học dữ liệu (Data Science) là một lĩnh vực đang phát triển nhanh chóng với nhiều cơ hội nghề nghiệp. Hãy cùng khám phá một số vai trò chính trong lĩnh vực Khoa học dữ liệu.,,,"#sharing, #data",,
🔥 Vision Transformer (ViT) - Hiện tượng mới trong lĩnh vực thị giác máy tính! 🔥,Vision Transformer (ViT) - Hiện tượng mới trong lĩnh vực thị giác máy tính!,,,"#sharing, #cv, #deep_learning",,
"Mình đang cày project để apply xin việc. Mình đang tìm các competitions để luyện tập. Bao gồm EDA, Pre-Processing, Modeling và Predict. Hiện tại mình mới tham gia 2 cuộc thi nhưng kết quả của mình chỉ giao động từ 40-55% so với độ chính xác của cuộc thi do mình không biết optimize (hoặc optimize xong thì kernel die chẳng hạn). Mình muốn tìm bạn colab với mình, cùng nhau cày project kiếm medal. Chẳng hạn mình có thể Pre-Processing, EDA,... còn bạn làm phần còn lại, hoặc cả 2 cùng thảo luận về các cách làm... Không nhất thiết ngày nào cũng phải làm cùng nhau nhưng cố gắng tuần nào cũng trao đổi 🥰🥰 ad duyệt cho mình kiếm tý medal vớiiiii","Mình đang cày project để apply xin việc. Mình đang tìm các competitions để luyện tập. Bao gồm EDA, Pre-Processing, Modeling và Predict. Hiện tại mình mới tham gia 2 cuộc thi nhưng kết quả của mình chỉ giao động từ 40-55% so với độ chính xác của cuộc thi do mình không biết optimize (hoặc optimize xong thì kernel die chẳng hạn). Mình muốn tìm bạn colab với mình, cùng nhau cày project kiếm medal. Chẳng hạn mình có thể Pre-Processing, EDA,... còn bạn làm phần còn lại, hoặc cả 2 cùng thảo luận về các cách làm... Không nhất thiết ngày nào cũng phải làm cùng nhau nhưng cố gắng tuần nào cũng trao đổi ad duyệt cho mình kiếm tý medal vớiiiii",,,"#sharing, #machine_learning",,
"Anh chị có kinh nghiệm cho em hỏi chút với ạ. Em thấy trong các bài toán nhận dạng thực thể người ta hay kết hợp model transformer + CRF . thì cho em hỏi tại sao mình lại cho qua CRF mà ko phân loại luôn ạ. Bởi vì em thấy các từ lúc này đều có 1 ý nghĩa riêng rồi mà nhỉ, vì em có đọc tìm hiểu trên mạng nhưng thấy vẫn chưa được giải thích đúng lắm . Em cảm ơn những lời giải thích ạ","Anh chị có kinh nghiệm cho em hỏi chút với ạ. Em thấy trong các bài toán nhận dạng thực thể người ta hay kết hợp model transformer + CRF . thì cho em hỏi tại sao mình lại cho qua CRF mà ko phân loại luôn ạ. Bởi vì em thấy các từ lúc này đều có 1 ý nghĩa riêng rồi mà nhỉ, vì em có đọc tìm hiểu trên mạng nhưng thấy vẫn chưa được giải thích đúng lắm . Em cảm ơn những lời giải thích ạ",,,"#Q&A, #cv, #deep_learning",,
"Em xin chào mọi người, là em sinh viên ngành Khoa học Máy tính.
Vì quá mông lung trên con đường học để trở Computer Vision Engineer, nên em muốn hỏi mn
""Cách học CV như thế nào để sau này có thể tìm được việc ạ"".
Hiện tại thì em đã học ANN, CNN, RNN, LSTM (ở mức cơ bản thôi ạ), em định hướng là sẽ code theo các project trên mạng, trên sách để làm quen. Nhưng code xong em cảm thấy nó không phải là của mình, và cũng không thể tự code được cái gì?
Em không biết học như thế nào cho đúng? Mong mọi người giúp em với ạ. Em trầm cảm vô cùng.","Em xin chào mọi người, là em sinh viên ngành Khoa học Máy tính. Vì quá mông lung trên con đường học để trở Computer Vision Engineer, nên em muốn hỏi mn ""Cách học CV như thế nào để sau này có thể tìm được việc ạ"". Hiện tại thì em đã học ANN, CNN, RNN, LSTM (ở mức cơ bản thôi ạ), em định hướng là sẽ code theo các project trên mạng, trên sách để làm quen. Nhưng code xong em cảm thấy nó không phải là của mình, và cũng không thể tự code được cái gì? Em không biết học như thế nào cho đúng? Mong mọi người giúp em với ạ. Em trầm cảm vô cùng.",,,"#Q&A, #cv, #deep_learning",,
Anh/chị cho em hỏi muốn học ML thì nên bắt đầu từ đâu ạ?,Anh/chị cho em hỏi muốn học ML thì nên bắt đầu từ đâu ạ?,,,"#Q&A, #machine_learning",,
"Chào các bạn
Mình đang tìm host để deploy con services AI xử lý ảnh. Mọi ng thường triển khai ở đâu? Ưu tiên ngon bổ rẻ nhé ae
Cảm ơn anh em",Chào các bạn Mình đang tìm host để deploy con services AI xử lý ảnh. Mọi ng thường triển khai ở đâu? Ưu tiên ngon bổ rẻ nhé ae Cảm ơn anh em,,,#Q&A,,
"Em chào mọi người ạ,
Em đang tham khảo về một đề tài LVTN xoay quanh AI làm về một hệ thống social network kết nối cộng đồng người dùng và chuyên gia.
Cụ thể, có những nhóm công ty DN, cá nhân, có những bài toán đặc thù về AI. Nhưng không biết giải quyết như thế nào, thay vì phải học và phải tốn thời gian để học, họ muốn giải quyết được những bài toán này trong thời gian ngắn. Thì hệ thống này sẽ là hệ thống social network để kết nối những nhu cầu này. Hệ thống sẽ mang đặc thù kết nối, cộng tác giữa các bên có bài toán nhưng không biết làm gì, giữa các bên có kĩ năng. Tuy nhiên, không đơn giản chỉ tương tác. Mà phải duy trì, và quản trị kết nối đó (Ví dụ khi đang làm giữa chừng, mà bên làm họ rút không làm nữa thì phải xử lí như thế nào). Hệ thống cũng phải đảm bảo tự động, thông minh, kết nối các bên liên quan. Giả sử có nhu cầu về 10 bài toán AI khác nhau, thì hệ thống phải làm sao đề xuất được nguồn lực nào, hạ tầng nào phù hợp cho từng nhu cầu cụ thể (Không đơn thuần chỉ là up bài lên rồi người khác vào trả lời).
Mọi người ai đã từng làm qua hoặc biết có thể cho em xin một số tài liệu liên quan được không ạ. Và mn có thể cho em xin một số hệ thống giống như trên ở thực tế để có thể tham khảo không ạ. Vì em có search nhưng chưa tìm được ạ.
Cảm ơn mọi người, chúc mọi người cuối tuần vui vẻ ^^","Em chào mọi người ạ, Em đang tham khảo về một đề tài LVTN xoay quanh AI làm về một hệ thống social network kết nối cộng đồng người dùng và chuyên gia. Cụ thể, có những nhóm công ty DN, cá nhân, có những bài toán đặc thù về AI. Nhưng không biết giải quyết như thế nào, thay vì phải học và phải tốn thời gian để học, họ muốn giải quyết được những bài toán này trong thời gian ngắn. Thì hệ thống này sẽ là hệ thống social network để kết nối những nhu cầu này. Hệ thống sẽ mang đặc thù kết nối, cộng tác giữa các bên có bài toán nhưng không biết làm gì, giữa các bên có kĩ năng. Tuy nhiên, không đơn giản chỉ tương tác. Mà phải duy trì, và quản trị kết nối đó (Ví dụ khi đang làm giữa chừng, mà bên làm họ rút không làm nữa thì phải xử lí như thế nào). Hệ thống cũng phải đảm bảo tự động, thông minh, kết nối các bên liên quan. Giả sử có nhu cầu về 10 bài toán AI khác nhau, thì hệ thống phải làm sao đề xuất được nguồn lực nào, hạ tầng nào phù hợp cho từng nhu cầu cụ thể (Không đơn thuần chỉ là up bài lên rồi người khác vào trả lời). Mọi người ai đã từng làm qua hoặc biết có thể cho em xin một số tài liệu liên quan được không ạ. Và mn có thể cho em xin một số hệ thống giống như trên ở thực tế để có thể tham khảo không ạ. Vì em có search nhưng chưa tìm được ạ. Cảm ơn mọi người, chúc mọi người cuối tuần vui vẻ ^^",,,#Q&A,,
"Chào mọi người,
Phoenix Team xin chia sẻ với mọi người source code của 1st private-test solution, Zalo AI Challenge 2020 phần traffic sign detection.
Một số tricks mà team có sử dụng:

1. Training data preprocessing
- Chia ảnh thành các grid nhỏ hơn, sử dụng input size 160x160 và stried window 40x40.
- Loại bớt các ảnh background và các ảnh có dính 1 phần box quá nhỏ (< 10% diện tích box), chỉ để tỉ lệ 1:3.

2. Training pharse
- Augmentation: Mosaic (9) + mixup.
- Model: Yolov5x, SGD Optimizer + Ema.
- 5-Stratified CV (chia ảnh gốc, sau đó preprocess sau).

3. Inference 
- Chia ảnh thành các grid 384x384, scale-up lên input 608x608, stride 84x88. 
-Sau khi đi qua model, NMS được sử dụng giữa các ảnh nhỏ theo area của các box, iou 0.1.
- WBF (Weighted boxes fusion) được sử dụng giữa các fold.
- Best 5-fold fp16 model: 60.5
Các bạn có câu hỏi nào có thể viết dưới post này hoặc để lại ở phần issue của git. 

Các thành viên trong team: Hải Nam Nguyễn (xSeries - FUNiX), Thân Cường (AI Engineer - Asilla), Nguyen Hai Son (BKHN - AI Intern, Asilla).
Chúc các bạn có thể áp dụng một trong các tricks ở trên vào cuộc thi của VinBigData Sắp tới :)
 ","Chào mọi người, Phoenix Team xin chia sẻ với mọi người source code của 1st private-test solution, Zalo AI Challenge 2020 phần traffic sign detection. Một số tricks mà team có sử dụng: 1. Training data preprocessing - Chia ảnh thành các grid nhỏ hơn, sử dụng input size 160x160 và stried window 40x40. - Loại bớt các ảnh background và các ảnh có dính 1 phần box quá nhỏ (< 10% diện tích box), chỉ để tỉ lệ 1:3. 2. Training pharse - Augmentation: Mosaic (9) + mixup. - Model: Yolov5x, SGD Optimizer + Ema. - 5-Stratified CV (chia ảnh gốc, sau đó preprocess sau). 3. Inference - Chia ảnh thành các grid 384x384, scale-up lên input 608x608, stride 84x88. -Sau khi đi qua model, NMS được sử dụng giữa các ảnh nhỏ theo area của các box, iou 0.1. - WBF (Weighted boxes fusion) được sử dụng giữa các fold. - Best 5-fold fp16 model: 60.5 Các bạn có câu hỏi nào có thể viết dưới post này hoặc để lại ở phần issue của git. Các thành viên trong team: Hải Nam Nguyễn (xSeries - FUNiX), Thân Cường (AI Engineer - Asilla), Nguyen Hai Son (BKHN - AI Intern, Asilla). Chúc các bạn có thể áp dụng một trong các tricks ở trên vào cuộc thi của VinBigData Sắp tới :)",,,"#sharing, #cv",,
"Mn ơi em học đến bài này thì không biết sao lại có công thức thứ 2 ạ. Mn giúp em với.
https://machinelearningcoban.com/2017/01/16/gradientdescent2/",Mn ơi em học đến bài này thì không biết sao lại có công thức thứ 2 ạ. Mn giúp em với. https://machinelearningcoban.com/2017/01/16/gradientdescent2/,,,"#Q&A, #math",,
"VinAI Seminar - ""Ubiquitous 3D Vision in the Wild""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Minh Vo, Spree3D
Time: 10:00 am - 11:00 am (GMT+7), Mon, Jul 03, 2023","VinAI Seminar - ""Ubiquitous 3D Vision in the Wild"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Minh Vo, Spree3D Time: 10:00 am - 11:00 am (GMT+7), Mon, Jul 03, 2023",,,,,
"xin chào mọi người
mọi người cho em hỏi là cách lấy dữ liệu từ trang web như thế nào ạ,
ví dụ: em muốn lấy tất cả bài viết trong nhóm trên fb về, trong đó 1 số bài viết có ảnh, một số bài không, em muốn lấy về hết gồm : tiêu đề, nội dung, ảnh (nếu có)
em xin cảm ơn ạ","xin chào mọi người mọi người cho em hỏi là cách lấy dữ liệu từ trang web như thế nào ạ, ví dụ: em muốn lấy tất cả bài viết trong nhóm trên fb về, trong đó 1 số bài viết có ảnh, một số bài không, em muốn lấy về hết gồm : tiêu đề, nội dung, ảnh (nếu có) em xin cảm ơn ạ",,,"#Q&A, #data",,
Mn cho em hỏi xử lí các từ viết tắt tiếng như nào v ? E đang làm sentiment analysis mà đổi từng từ viết tắt sang hoàn chỉnh chắc chết mất.,Mn cho em hỏi xử lí các từ viết tắt tiếng như nào v ? E đang làm sentiment analysis mà đổi từng từ viết tắt sang hoàn chỉnh chắc chết mất.,,,"#Q&A, #nlp",,
"Mọi người có tài liệu, papers, books để bắt đầu tìm hiểu về derivative-free optimization hay blackbox optimization không ạ. Em xin cảm ơn","Mọi người có tài liệu, papers, books để bắt đầu tìm hiểu về derivative-free optimization hay blackbox optimization không ạ. Em xin cảm ơn",,,"#Q&A, #machine_learning",,
Chào mn ạ. Em đang mong muố tìm job intern/fresher Al ở Hà Nội ạ. Em cảm ơn ạ,Chào mn ạ. Em đang mong muố tìm job intern/fresher Al ở Hà Nội ạ. Em cảm ơn ạ,,,"#Q&A, #machine_learning",,
"Em chào mn. em tự học bài Gradient Descent đến đoạn công thức này thì không hiểu tại sao. mn giải thích giúp em với.
https://machinelearningcoban.com/2017/01/16/gradientdescent2/",Em chào mn. em tự học bài Gradient Descent đến đoạn công thức này thì không hiểu tại sao. mn giải thích giúp em với. https://machinelearningcoban.com/2017/01/16/gradientdescent2/,,,"#Q&A, #math",,
"Em chào cả nhà!
Em hiện làm việc cho một công ty về thiết kế xây dựng ở nước ngoài và có học thêm một Master về Data Science nhưng mọi thứ về DS đều khá mới với em, nên muốn hỏi các bậc cao nhân trong nhóm như sau ạ:
Mọi người cho em hỏi có ai xử dụng mô hình Neural Networks trong Deep learning để giải bài toán tối ưu trong kĩ thuật không ạ? Chẳng hạn, tìm nghiệm X=(x1,x2,...,xn) sao cho hàm f(X) đạt cực tiểu. Đây là bài toán tối ưu thuần Toán, tuy nhiên lại áp dụng nhiều trong kĩ thuật và thiết kế.
Em cảm ơn và mong mọi người cho em xin góp ý!","Em chào cả nhà! Em hiện làm việc cho một công ty về thiết kế xây dựng ở nước ngoài và có học thêm một Master về Data Science nhưng mọi thứ về DS đều khá mới với em, nên muốn hỏi các bậc cao nhân trong nhóm như sau ạ: Mọi người cho em hỏi có ai xử dụng mô hình Neural Networks trong Deep learning để giải bài toán tối ưu trong kĩ thuật không ạ? Chẳng hạn, tìm nghiệm X=(x1,x2,...,xn) sao cho hàm f(X) đạt cực tiểu. Đây là bài toán tối ưu thuần Toán, tuy nhiên lại áp dụng nhiều trong kĩ thuật và thiết kế. Em cảm ơn và mong mọi người cho em xin góp ý!",,,"#Q&A, #math",,
"Công ty trợ lý ảo, dùng trợ lý ảo, tuyển người phát triển trợ lý ảo🥳

Là một đơn vị tiên phong phát triển chatbot, trợ lý ảo thế hệ mới trên công nghệ ChatGPT tại Việt Nam, hiện mindmaid.ai đang có nhu cầu tuyển dụng Thực tập sinh Content Marketing & Chatbot Development. Nhiệm vụ chính bao gồm: nghĩ với ChatGPT, viết với ChatGPT, chuẩn bị dữ liệu huấn luyện trợ lý ảo cũng bằng ChatGPT. 

Đây là vị trí ưu tiên cho các bạn content, tuy nhiên vì chuẩn bị content cho trợ lý ảo, nên công ty cũng muốn ưu tiên một số suất cho các bạn có hiểu biết tốt về công nghệ, AI và muốn khám phá lĩnh vực trợ lý ảo này. 
Môi trường làm việc startup năng động, văn phòng khu vực trung tâm, công cụ làm việc Lark xịn sò, và đặc biệt là luôn được học hỏi về những skill mới nhất thông qua Growth Day hàng tuần.
Nhờ anh chị em bạn bè thấy có CV nào phù hợp giới thiệu giúp.
👉Thông tin chi tiết về JD, hình thức gửi CV...mời chat trực tiếp với trợ lý ảo ạ: https://aivtuyendung.mindmaid.ai

(p/s: xin phép ad đăng lên group vì không tìm thấy post gom đăng bài tuyển dụng trên nhóm)","Công ty trợ lý ảo, dùng trợ lý ảo, tuyển người phát triển trợ lý ảo Là một đơn vị tiên phong phát triển chatbot, trợ lý ảo thế hệ mới trên công nghệ ChatGPT tại Việt Nam, hiện mindmaid.ai đang có nhu cầu tuyển dụng Thực tập sinh Content Marketing & Chatbot Development. Nhiệm vụ chính bao gồm: nghĩ với ChatGPT, viết với ChatGPT, chuẩn bị dữ liệu huấn luyện trợ lý ảo cũng bằng ChatGPT. Đây là vị trí ưu tiên cho các bạn content, tuy nhiên vì chuẩn bị content cho trợ lý ảo, nên công ty cũng muốn ưu tiên một số suất cho các bạn có hiểu biết tốt về công nghệ, AI và muốn khám phá lĩnh vực trợ lý ảo này. Môi trường làm việc startup năng động, văn phòng khu vực trung tâm, công cụ làm việc Lark xịn sò, và đặc biệt là luôn được học hỏi về những skill mới nhất thông qua Growth Day hàng tuần. Nhờ anh chị em bạn bè thấy có CV nào phù hợp giới thiệu giúp. Thông tin chi tiết về JD, hình thức gửi CV...mời chat trực tiếp với trợ lý ảo ạ: https://aivtuyendung.mindmaid.ai (p/s: xin phép ad đăng lên group vì không tìm thấy post gom đăng bài tuyển dụng trên nhóm)",,,"#sharing, #nlp",,
"Dạ em chào mọi người, em là newbie tự học về ML được vài tháng. Em thấy nhiều anh chị khuyên nên làm project trên kaggle, vậy cụ thể là mình làm như thế nào, các bước ra sao ạ? Do em thấy trên kaggle không có mục project mà chỉ có mục dataset và competition ạ. Mong mọi người giải đáp, em xin cảm ơn ạ","Dạ em chào mọi người, em là newbie tự học về ML được vài tháng. Em thấy nhiều anh chị khuyên nên làm project trên kaggle, vậy cụ thể là mình làm như thế nào, các bước ra sao ạ? Do em thấy trên kaggle không có mục project mà chỉ có mục dataset và competition ạ. Mong mọi người giải đáp, em xin cảm ơn ạ",,,"#Q&A, #machine_learning",,
"🔥🔥🔥 𝙃𝒐̣𝙘 𝙙𝒂𝙩𝒂 𝒗𝙖̀ 𝙫𝒊𝙚̣̂𝒄 𝒍𝙖̀𝒎 𝑫𝙖𝒕𝙖. 🔥🔥🔥
Gần đây, trên các cộng đồng có nhiều tranh cãi ý kiến cho rằng “Đừng nên học data nữa, ngành data bão hòa rồi, học data không xin được việc đâu?”
Với quan điểm của mình, trước khi quyết định học hay theo đuổi một ngành nào đó các bạn hãy tự hỏi bản thân trước 3 câu hỏi:
- Cụ thể, mình đang muốn gì?
- Mục đích của việc học này là gì?
- Nếu có nó mình sẽ được những lợi ích gì? Nếu không có nó mình có thể sẽ mất đi cơ hội gì?
👉 Nếu các bạn cho rằng ngành data đã bão hòa và khó xin việc thì hãy hiểu nguyên nhân tại sao?
1. Hiện tại, tình hình kinh tế chung đang khó khăn, các doanh nghiệp còn đang lay off, nhiều ngành nghề khác cũng khó khăn và thất nghiệp không riêng gì trong lĩnh vực data.
2. Thực tế rằng nhu cầu tuyển dụng data vẫn rất lớn, cắt giảm tuyển level thấp tại vì doanh nghiệp cần phải tối ưu hóa chi phí, tập trung giải quyết vấn đề trước mắt nên họ muốn tìm những người có thể bắt đầu ngay với công việc và tạo ra giá trị thay vì đào tạo, nuôi dưỡng nguồn nhân lực mới.
- Bản chất của việc tuyển intern, fresher là đầu tư chi phí để đào tạo con người để đảm bảo nguồn nhân lực phát triển lâu dài chứ không phải giải quyết vấn đề trước mắt.
- Các doanh nghiệp tuyển Senior, Leader level là vì đa số doanh nghiệp, lĩnh vực đang bắt đầu ứng dụng và xây dựng data platform cần người có kinh nghiệm để bắt đầu, thiết kế, và đào tạo và đến một giai đoạn nào đó khi hệ thống đã phát triển họ k thể mãi tìm được các DE, DA level cao vì nếu không đào tạo lớp trẻ thì sẽ k có các senior tương lai. Vì vậy, sẽ có giai đoạn, nhu cầu tuyển dụng level thấp quay lại nhiều hơn… Nếu không học, không chuẩn bị trước cho tương lai thì làm sao bạn nắm bắt các cơ hội phí trước/
👉 Nếu các bạn bảo tại sao yêu cầu nhà tuyển dụng ngày càng cao, biết nhiều kỹ năng như: SQL, Power BI, Python,... vẫn không được tuyển?
1. Hãy đặt lại câu hỏi, tại sao họ phải tuyển bạn? Bạn có gì để mang lại giá trị cho công ty họ?
- Bạn hãy xem phỏng vấn, tìm việc (không phải xin việc =)) ) là đi chợ, việc làm, mức lương là hàng hóa và tiền. Bạn đang bán sức lao động và giá trị cho công ty nên “Thuận mua, vừa bán”. Mức lương và công việc sẽ đi theo quy luật cung cầu của thị trường (điều này đúng cho tất cả ngành nghề)
- Ngành data ngày càng hot vì mức lương, mức đãi ngộ hấp dẫn với nhiều cơ hội phát triển do vậy ngày càng nhiều người chú ý và học về Data nên sự cạnh tranh sẽ lên cao. Trước kia ít người học, khó tuyển người nên yêu cầu đơn giản, bây giờ đã có nhiều người học và có những người có tư duy, tư chất tốt hơn biết nhiều hơn thì họ có quyền chọn người tốt hơn, yêu cầu cao hơn chứ.
2. Hãy bỏ tư tưởng “ăn xổi”.
- Bạn bảo bạn biết SQL, BI tools, lập trình .. nhưng bạn thật sự biết đến đâu- thành thạo đến đâu? Bài đăng nào của các bạn em cũng chỉ thấy nói “em có học và biết một chút” chứ k nói em thành thạo và đã làm những dự án A, B, C về SQL, Data warehouse hay BI dashboard.
- Bạn hãy quên đi giấc mơ học 3, 6 tháng ở khóa học ngắn hạn và chắc chắn có việc. Các bạn học 3 -6 tháng và tin theo lời quảng cáo của trung tâm là chắc chắn có việc = )) . Hãy suy nghĩ các bạn lấy gì để cạnh tranh với những bạn đã học 4 năm đại học hay ngồi học ngày học đêm để nâng cao kỹ năng từng ngày mà chỉ tin tưởng và tuần 2 buổi học trong 3 tháng là giỏi rồi, đi làm tốt rồi ..
- Tất nhiều một số bạn, có thể nói là nhiều bạn xin việc trong 3 tháng , 6 tháng, thậm chí là 2 tháng học về data nhưng đó chắc chắn là nhiều sự đánh đổi cố gắng và đôi khi là may mắn nắm bắt được đúng cơ hội,..
Nhiều bạn học sinh của mình đã nhận được việc 2 tháng, 3 tháng học (dù chưa được học đầy đủ kiến thức nhưng cũng có nhiều bạn học xong rồi những loay hoay mãi chưa tìm được cơ hội, Mình nhận thấy rằng các bạn tìm được việc sớm có một số là do có tố chất tốt hơn về cả hard skill và soft skill, có một số chăm chỉ hơn, cố gắng nhiều hơn và đa số là do may mắn gặp thời điểm tốt hơn, nắm bắt được cơ hội =)) vì mình dạy và đánh giá được trình đội các bạn đôi lúc không chênh lệch nhiều thậm chí bạn k tìm được việc còn tốt hơn bạn đã tìm được =))
- Tìm việc là sự phù hợp vs công ty không chỉ về kỹ năng và nhiều yếu tố khác.
Khi mình tốt nghiệp đại học, mình tự tin có một nền tảng kiến thức vững chắc (mình đã skip rất nhiều về software để tập trung học AI và Data), mình có hơn 1 năm nghiên cứu tại Lab trí tuệ nhân tạo, Mình là first author 1 paper trong hội nghị RANK A, có 2 năm kinh nghiệm part time tại một công ty lớn nhưng mình vẫn “BỊ TRƯỢT PHỎNG VẤN VỊ TRÍ FRESHER CỦA MỘT CÔNG TY VN” (đoạn này khoe chỉ là phần nhỏ thôi, phần lớn để cho các bạn thấy phù hợp quan trọng thế nào. Nếu có dịp mình sẽ chia sẻ về lần PV trượt ấy) bởi vì tại thời điểm ấy họ k cần những người làm việc như mình, và họ có nhiều option có thể tốt hơn, có thể phù hợp hơn.
👉 Hãy xác định học để làm gì? Minh luôn quan điểm học để ấm vào thân, để xây dựng kiến thức trước.
- Nếu phần trên nói đến tư tưởng ăn xổi, có nhiều bạn gọi cho mình và hỏi học xong 1 khóa, 2 khóa học của anh em có xin việc không và mình trả lời “Xin được việc hay không do sự cố gắng và may mắn của bạn không phải ở mình và cái mình cam kết và chất lượng kiến thức và bạn cảm thấy mình đang hiểu biết lên, đang học được những kiến thức có ích chứ MÌNH KHÔNG BAO GIỜ CAM KẾT VIỆC LÀM” và đến 90 % các bạn thất vọng và không học. Mình xin chúc các bạn “may mắn hơn ở một nơi nào đó họ cam kết học xong có việc”
- Tại sao các bạn không nghĩ đến việc học để lấy kiến thức cho bản thân trước, rồi khi có kiến thức các cơ hội sẽ tự tìm thời và mình sẽ không bỏ lỡ những cơ hội trong tương lai?
- Tại sao các bạn luôn có tư tưởng học để chuyển một công việc mới sang làm Data luôn mà không nghĩ để phục vụ cho chuyên ngành, cho công việc khác của mình sau này. Các bạn hãy để ý thời kì công nghệ thông tin bắt đầu bùng nổ, giá trị của những người đã biết sử dụng máy tính, thành thạo tin học văn phòng trong CV cao như thế nào và hiện tại ai cũng cảm nhận rõ mình đang bước chân vào thời kỳ mới là “Data Driven” vậy những người có kỹ năng tốt về dữ liệu liệu có cơ hội, có thế mạnh để cạnh tranh hơn trong lĩnh vực của họ hay không?
Thôi viết dài quá, mỏi tay, đói bụng,... nếu có cơ hội mình sẽ chia sẻ sâu hơn về từng quan điểm trong bài viết. À có một vấn đề, mình vừa nghĩ vừa gõ máy sẽ bị các lỗi chính tả nên có gặp nhiều lỗi chính tả xin các bạn đừng cười, và tập trung vào nội dung bài viết và các luận điểm mình muốn truyền đạt, có ai đó đã từng nói rằng
“Một khi đã sai chính tả thì mọi lập luận đều vô nghĩa” =))","̣ ̀ ̣̂ ̀ . Gần đây, trên các cộng đồng có nhiều tranh cãi ý kiến cho rằng “Đừng nên học data nữa, ngành data bão hòa rồi, học data không xin được việc đâu?” Với quan điểm của mình, trước khi quyết định học hay theo đuổi một ngành nào đó các bạn hãy tự hỏi bản thân trước 3 câu hỏi: - Cụ thể, mình đang muốn gì? - Mục đích của việc học này là gì? - Nếu có nó mình sẽ được những lợi ích gì? Nếu không có nó mình có thể sẽ mất đi cơ hội gì? Nếu các bạn cho rằng ngành data đã bão hòa và khó xin việc thì hãy hiểu nguyên nhân tại sao? 1. Hiện tại, tình hình kinh tế chung đang khó khăn, các doanh nghiệp còn đang lay off, nhiều ngành nghề khác cũng khó khăn và thất nghiệp không riêng gì trong lĩnh vực data. 2. Thực tế rằng nhu cầu tuyển dụng data vẫn rất lớn, cắt giảm tuyển level thấp tại vì doanh nghiệp cần phải tối ưu hóa chi phí, tập trung giải quyết vấn đề trước mắt nên họ muốn tìm những người có thể bắt đầu ngay với công việc và tạo ra giá trị thay vì đào tạo, nuôi dưỡng nguồn nhân lực mới. - Bản chất của việc tuyển intern, fresher là đầu tư chi phí để đào tạo con người để đảm bảo nguồn nhân lực phát triển lâu dài chứ không phải giải quyết vấn đề trước mắt. - Các doanh nghiệp tuyển Senior, Leader level là vì đa số doanh nghiệp, lĩnh vực đang bắt đầu ứng dụng và xây dựng data platform cần người có kinh nghiệm để bắt đầu, thiết kế, và đào tạo và đến một giai đoạn nào đó khi hệ thống đã phát triển họ k thể mãi tìm được các DE, DA level cao vì nếu không đào tạo lớp trẻ thì sẽ k có các senior tương lai. Vì vậy, sẽ có giai đoạn, nhu cầu tuyển dụng level thấp quay lại nhiều hơn… Nếu không học, không chuẩn bị trước cho tương lai thì làm sao bạn nắm bắt các cơ hội phí trước/ Nếu các bạn bảo tại sao yêu cầu nhà tuyển dụng ngày càng cao, biết nhiều kỹ năng như: SQL, Power BI, Python,... vẫn không được tuyển? 1. Hãy đặt lại câu hỏi, tại sao họ phải tuyển bạn? Bạn có gì để mang lại giá trị cho công ty họ? - Bạn hãy xem phỏng vấn, tìm việc (không phải xin việc =)) ) là đi chợ, việc làm, mức lương là hàng hóa và tiền. Bạn đang bán sức lao động và giá trị cho công ty nên “Thuận mua, vừa bán”. Mức lương và công việc sẽ đi theo quy luật cung cầu của thị trường (điều này đúng cho tất cả ngành nghề) - Ngành data ngày càng hot vì mức lương, mức đãi ngộ hấp dẫn với nhiều cơ hội phát triển do vậy ngày càng nhiều người chú ý và học về Data nên sự cạnh tranh sẽ lên cao. Trước kia ít người học, khó tuyển người nên yêu cầu đơn giản, bây giờ đã có nhiều người học và có những người có tư duy, tư chất tốt hơn biết nhiều hơn thì họ có quyền chọn người tốt hơn, yêu cầu cao hơn chứ. 2. Hãy bỏ tư tưởng “ăn xổi”. - Bạn bảo bạn biết SQL, BI tools, lập trình .. nhưng bạn thật sự biết đến đâu- thành thạo đến đâu? Bài đăng nào của các bạn em cũng chỉ thấy nói “em có học và biết một chút” chứ k nói em thành thạo và đã làm những dự án A, B, C về SQL, Data warehouse hay BI dashboard. - Bạn hãy quên đi giấc mơ học 3, 6 tháng ở khóa học ngắn hạn và chắc chắn có việc. Các bạn học 3 -6 tháng và tin theo lời quảng cáo của trung tâm là chắc chắn có việc = )) . Hãy suy nghĩ các bạn lấy gì để cạnh tranh với những bạn đã học 4 năm đại học hay ngồi học ngày học đêm để nâng cao kỹ năng từng ngày mà chỉ tin tưởng và tuần 2 buổi học trong 3 tháng là giỏi rồi, đi làm tốt rồi .. - Tất nhiều một số bạn, có thể nói là nhiều bạn xin việc trong 3 tháng , 6 tháng, thậm chí là 2 tháng học về data nhưng đó chắc chắn là nhiều sự đánh đổi cố gắng và đôi khi là may mắn nắm bắt được đúng cơ hội,.. Nhiều bạn học sinh của mình đã nhận được việc 2 tháng, 3 tháng học (dù chưa được học đầy đủ kiến thức nhưng cũng có nhiều bạn học xong rồi những loay hoay mãi chưa tìm được cơ hội, Mình nhận thấy rằng các bạn tìm được việc sớm có một số là do có tố chất tốt hơn về cả hard skill và soft skill, có một số chăm chỉ hơn, cố gắng nhiều hơn và đa số là do may mắn gặp thời điểm tốt hơn, nắm bắt được cơ hội =)) vì mình dạy và đánh giá được trình đội các bạn đôi lúc không chênh lệch nhiều thậm chí bạn k tìm được việc còn tốt hơn bạn đã tìm được =)) - Tìm việc là sự phù hợp vs công ty không chỉ về kỹ năng và nhiều yếu tố khác. Khi mình tốt nghiệp đại học, mình tự tin có một nền tảng kiến thức vững chắc (mình đã skip rất nhiều về software để tập trung học AI và Data), mình có hơn 1 năm nghiên cứu tại Lab trí tuệ nhân tạo, Mình là first author 1 paper trong hội nghị RANK A, có 2 năm kinh nghiệm part time tại một công ty lớn nhưng mình vẫn “BỊ TRƯỢT PHỎNG VẤN VỊ TRÍ FRESHER CỦA MỘT CÔNG TY VN” (đoạn này khoe chỉ là phần nhỏ thôi, phần lớn để cho các bạn thấy phù hợp quan trọng thế nào. Nếu có dịp mình sẽ chia sẻ về lần PV trượt ấy) bởi vì tại thời điểm ấy họ k cần những người làm việc như mình, và họ có nhiều option có thể tốt hơn, có thể phù hợp hơn. Hãy xác định học để làm gì? Minh luôn quan điểm học để ấm vào thân, để xây dựng kiến thức trước. - Nếu phần trên nói đến tư tưởng ăn xổi, có nhiều bạn gọi cho mình và hỏi học xong 1 khóa, 2 khóa học của anh em có xin việc không và mình trả lời “Xin được việc hay không do sự cố gắng và may mắn của bạn không phải ở mình và cái mình cam kết và chất lượng kiến thức và bạn cảm thấy mình đang hiểu biết lên, đang học được những kiến thức có ích chứ MÌNH KHÔNG BAO GIỜ CAM KẾT VIỆC LÀM” và đến 90 % các bạn thất vọng và không học. Mình xin chúc các bạn “may mắn hơn ở một nơi nào đó họ cam kết học xong có việc” - Tại sao các bạn không nghĩ đến việc học để lấy kiến thức cho bản thân trước, rồi khi có kiến thức các cơ hội sẽ tự tìm thời và mình sẽ không bỏ lỡ những cơ hội trong tương lai? - Tại sao các bạn luôn có tư tưởng học để chuyển một công việc mới sang làm Data luôn mà không nghĩ để phục vụ cho chuyên ngành, cho công việc khác của mình sau này. Các bạn hãy để ý thời kì công nghệ thông tin bắt đầu bùng nổ, giá trị của những người đã biết sử dụng máy tính, thành thạo tin học văn phòng trong CV cao như thế nào và hiện tại ai cũng cảm nhận rõ mình đang bước chân vào thời kỳ mới là “Data Driven” vậy những người có kỹ năng tốt về dữ liệu liệu có cơ hội, có thế mạnh để cạnh tranh hơn trong lĩnh vực của họ hay không? Thôi viết dài quá, mỏi tay, đói bụng,... nếu có cơ hội mình sẽ chia sẻ sâu hơn về từng quan điểm trong bài viết. À có một vấn đề, mình vừa nghĩ vừa gõ máy sẽ bị các lỗi chính tả nên có gặp nhiều lỗi chính tả xin các bạn đừng cười, và tập trung vào nội dung bài viết và các luận điểm mình muốn truyền đạt, có ai đó đã từng nói rằng “Một khi đã sai chính tả thì mọi lập luận đều vô nghĩa” =))",,,"#sharing, #data",,
"Em chào anh chị trong group,
Em mới tìm hiểu về Machine Learning và em mong muốn tìm cuốn sách hoặc khoá học để có thể hiểu rõ về các thuật toán đằng sau các mô hình và cách sử dụng mô hình phù hợp cho từng bài toán khác nhau. Mong được anh chị giúp đỡ cho em ạ.
Em cảm ơn.","Em chào anh chị trong group, Em mới tìm hiểu về Machine Learning và em mong muốn tìm cuốn sách hoặc khoá học để có thể hiểu rõ về các thuật toán đằng sau các mô hình và cách sử dụng mô hình phù hợp cho từng bài toán khác nhau. Mong được anh chị giúp đỡ cho em ạ. Em cảm ơn.",,,"#Q&A, #machine_learning",,
"Thuật toán Monte Carlo Tree Search trong AlphaGo.
Chào các bạn, mình tìm thấy một bài viết khá hay về thuật toán Monte Carlo Tree Search - trái tim của hệ thống đánh cờ vây Alpha Go. Mình xin giới thiệu nó với các bạn, và tiện thể mình cũng dịch nó ra tiếng Việt luôn cho bạn nào quan tâm.
Và kèm theo cả python code sử dụng Monte Carlo cho trò cờ vây và tic-tac-toe (cờ caro 3*3).
Bài dịch: https://ngdmau.github.io/Monte-Carlo-Tree-Search/
Bài gốc: https://int8.io/monte-carlo-tree-search-beginners-guide/
Link code python:
Cờ vây: https://github.com/int8/gomcts
Tic-tac-toe: https://github.com/int8/monte-carlo-tree-search
Xin cảm ơn và chúc các bạn buổi tối vui vẻ ^^.
#python #montecarlo #mcts #alphago #gametheory","Thuật toán Monte Carlo Tree Search trong AlphaGo. Chào các bạn, mình tìm thấy một bài viết khá hay về thuật toán Monte Carlo Tree Search - trái tim của hệ thống đánh cờ vây Alpha Go. Mình xin giới thiệu nó với các bạn, và tiện thể mình cũng dịch nó ra tiếng Việt luôn cho bạn nào quan tâm. Và kèm theo cả python code sử dụng Monte Carlo cho trò cờ vây và tic-tac-toe (cờ caro 3*3). Bài dịch: https://ngdmau.github.io/Monte-Carlo-Tree-Search/ Bài gốc: https://int8.io/monte-carlo-tree-search-beginners-guide/ Link code python: Cờ vây: https://github.com/int8/gomcts Tic-tac-toe: https://github.com/int8/monte-carlo-tree-search Xin cảm ơn và chúc các bạn buổi tối vui vẻ ^^.",#python	#montecarlo	#mcts	#alphago	#gametheory,,"#sharing, #machine_learning",,
"Tác giả của ""Effective Pandas"" - anh Matt Harrison - tặng sách.
Mặc dù ai cũng biết chỗ để tải cuốn sách mình muốn, nhưng nếu được chính tác giả tặng thì cảm giác vẫn rất khác biệt.
Các bạn làm theo hướng dẫn để được tặng sách miễn phí nhé.","Tác giả của ""Effective Pandas"" - anh Matt Harrison - tặng sách. Mặc dù ai cũng biết chỗ để tải cuốn sách mình muốn, nhưng nếu được chính tác giả tặng thì cảm giác vẫn rất khác biệt. Các bạn làm theo hướng dẫn để được tặng sách miễn phí nhé.",,,#sharing,,
"Em 27 tuổi dev quèn đang gặm quyển thánh kinh của anh Tiệp để chuyển việc.
Công nhận ngành khó voãi mấy bác ạ, ko biết có bác nào lớn lớn chia sẻ ít kinh nghiệm tự học không, em ko có background khoa học máy tính theo có ổn hay đuối ko ợ.
(em có bằng đh mạng máy tính, chả LQ, dev quèn cũng ít dính luôn, hjx)
Em cảm ơn.","Em 27 tuổi dev quèn đang gặm quyển thánh kinh của anh Tiệp để chuyển việc. Công nhận ngành khó voãi mấy bác ạ, ko biết có bác nào lớn lớn chia sẻ ít kinh nghiệm tự học không, em ko có background khoa học máy tính theo có ổn hay đuối ko ợ. (em có bằng đh mạng máy tính, chả LQ, dev quèn cũng ít dính luôn, hjx) Em cảm ơn.",,,"#Q&A, #machine_learning",,
Xin chia sẻ với mọi người một thư viện hay ho giúp tối ưu bộ nhớ khi triển khai các mô hình LLM lên môi trường production ạ 😁,Xin chia sẻ với mọi người một thư viện hay ho giúp tối ưu bộ nhớ khi triển khai các mô hình LLM lên môi trường production ạ,,,"#Q&A, #deep_learning",,
"FinGPT: Open-Source Financial LLMs
Cung cấp toàn bộ quy trình LLM training and finetuning trong lĩnh vực tài chính.
paper: https://arxiv.org/abs/2306.06031
Code: https://github.com/AI4Finance-Foundation/FinGPT",FinGPT: Open-Source Financial LLMs Cung cấp toàn bộ quy trình LLM training and finetuning trong lĩnh vực tài chính. paper: https://arxiv.org/abs/2306.06031 Code: https://github.com/AI4Finance-Foundation/FinGPT,,,"#sharing, #deep_learning",,
"Chào các a/chị trong group ạ.
Em mới mọc mòi nghiên cứu về NLP thì có một thắc mắc mong nhận được một số ý kiến để tham khảo ạ:
Ở trong quá trình preprocess, em có thấy là đa số các hệ thống sẽ sử dụng BPE để tokenize đầu vào. Tuy nhiên em có đọc được paper của PhoBERT (Mô hình ngôn ngữ của VN) thì họ có đề cập là họ đã WordSegment trước khi áp dụng BPE.
Về mặt trực giác thì do PhoBERT là mô hình đơn ngôn ngữ (monolingual) nên việc segment các từ, từ ghép thì đúng là sẽ giúp ích rất nhiều cho việc tokenize để bổ trợ feature cho các task về sau.
Thắc mắc của em là nếu ta cần giải quyết một bài toán đa ngôn ngữ trong đó bao gồm ngôn ngữ các unit của nó được tách biệt rõ ràng như English (Whitespace) và các ngôn ngữ có cách biểu diễn khác như VN (từ ghép) hay JP (ko có whitespace) thì mình sẽ preprocess như nào ạ ?
Em cảm ơn a.chị.","Chào các a/chị trong group ạ. Em mới mọc mòi nghiên cứu về NLP thì có một thắc mắc mong nhận được một số ý kiến để tham khảo ạ: Ở trong quá trình preprocess, em có thấy là đa số các hệ thống sẽ sử dụng BPE để tokenize đầu vào. Tuy nhiên em có đọc được paper của PhoBERT (Mô hình ngôn ngữ của VN) thì họ có đề cập là họ đã WordSegment trước khi áp dụng BPE. Về mặt trực giác thì do PhoBERT là mô hình đơn ngôn ngữ (monolingual) nên việc segment các từ, từ ghép thì đúng là sẽ giúp ích rất nhiều cho việc tokenize để bổ trợ feature cho các task về sau. Thắc mắc của em là nếu ta cần giải quyết một bài toán đa ngôn ngữ trong đó bao gồm ngôn ngữ các unit của nó được tách biệt rõ ràng như English (Whitespace) và các ngôn ngữ có cách biểu diễn khác như VN (từ ghép) hay JP (ko có whitespace) thì mình sẽ preprocess như nào ạ ? Em cảm ơn a.chị.",,,"#Q&A, #nlp, #deep_learning",,
"Em đang có 1 vấn đề như này ạ, mong đc giải đáp
Dưới đây là 2 video em quay lại 2 ứng dụng sử dụng text-to-speech, bên trái là ứng dụng trên mạng, bên phải là em tự code. Thư viện e sử dụng là gTTS có sẵn của Python, nhưng không hiểu sao nghe nó khá là đuồi :(
Em có chỉnh lại tham số đầy đủ của gTTS cho nó sử dụng api của tên miền google.com.vn và ngôn là vi nhưng vẫn k đạt được chất lượng của voice như bên phải. Voice của video bên phải được sử dụng khá nhiều, bên J2Team cũng sử dụng voice này mà em không biết lấy đâu ra hay làm sao đạt được, ai có kinh nghiệm thì giúp em với, em cám ơn mọi người nhiều ạ","Em đang có 1 vấn đề như này ạ, mong đc giải đáp Dưới đây là 2 video em quay lại 2 ứng dụng sử dụng text-to-speech, bên trái là ứng dụng trên mạng, bên phải là em tự code. Thư viện e sử dụng là gTTS có sẵn của Python, nhưng không hiểu sao nghe nó khá là đuồi :( Em có chỉnh lại tham số đầy đủ của gTTS cho nó sử dụng api của tên miền google.com.vn và ngôn là vi nhưng vẫn k đạt được chất lượng của voice như bên phải. Voice của video bên phải được sử dụng khá nhiều, bên J2Team cũng sử dụng voice này mà em không biết lấy đâu ra hay làm sao đạt được, ai có kinh nghiệm thì giúp em với, em cám ơn mọi người nhiều ạ",,,"#Q&A, #nlp, #python",,
Em xin chia sẻ một bài ngắn về a == None và a is None cho bác nào quan tâm ạ 😁,Em xin chia sẻ một bài ngắn về a == None và a is None cho bác nào quan tâm ạ,,,"#sharing, #python",,
"Chào mọi người, em hiện tại sẽ tốt nghiệp vào tháng 8 và đang muốn tìm một job về AI Engineer. Trước đó thì em đã có khoảng 9 tháng intern về CV ở Viettel. Mọi người cho em hỏi là ở Hà Nội thì có những công ty nào tốt làm về AI ạ?","Chào mọi người, em hiện tại sẽ tốt nghiệp vào tháng 8 và đang muốn tìm một job về AI Engineer. Trước đó thì em đã có khoảng 9 tháng intern về CV ở Viettel. Mọi người cho em hỏi là ở Hà Nội thì có những công ty nào tốt làm về AI ạ?",,,"#Q&A, #machine_learning",,
"Giống như công cụ Warp của Photoshop, nhưng mạnh mẽ hơn nhiều.
Mô hình AI mới này giúp chỉnh sửa hình ảnh bằng thao tác bấm và kéo thả đơn giản. Xoay đối tượng của ảnh như thể đó là một mô hình 3D.
Xem thêm:","Giống như công cụ Warp của Photoshop, nhưng mạnh mẽ hơn nhiều. Mô hình AI mới này giúp chỉnh sửa hình ảnh bằng thao tác bấm và kéo thả đơn giản. Xoay đối tượng của ảnh như thể đó là một mô hình 3D. Xem thêm:",,,"#sharing, #cv, #machine_learning",,
"Xin chào tất cả mọi người,
Tôi muốn mua một cuốn sách để làm quà bằng tiếng Anh. Những cuốn sách hay nhất về Machine Learning, Deep Learning mà bạn có thể đề xuất là gì.
Cảm ơn các bạn rất nhiều.","Xin chào tất cả mọi người, Tôi muốn mua một cuốn sách để làm quà bằng tiếng Anh. Những cuốn sách hay nhất về Machine Learning, Deep Learning mà bạn có thể đề xuất là gì. Cảm ơn các bạn rất nhiều.",,,"#Q&A, #machine_learning",,
Chào mn ạ. Mình tốt nghiệp đại học Bách khoa Hà nội chuyên ngành Cơ điện tử. Mình định hướng chuyên sâu mảng robotics and Al. Mình đang muốn tìm job intern/fresher về Al ở Hà nội. Cảm ơn mn ạ,Chào mn ạ. Mình tốt nghiệp đại học Bách khoa Hà nội chuyên ngành Cơ điện tử. Mình định hướng chuyên sâu mảng robotics and Al. Mình đang muốn tìm job intern/fresher về Al ở Hà nội. Cảm ơn mn ạ,,,"#Q&A, #machine_learning",,
"Em chào anh chị trong nhóm ạ. Hiện em đã tốt nghiệp Đại học Bách Khoa Hà Nội chuyên ngành CƠ điện tử, em đang tìm việc Intern/fresher về Al tại Hà Nội. Không biết anh/chị ở công ty nào có còn open cho vị trí Intern/fresher không ạ?","Em chào anh chị trong nhóm ạ. Hiện em đã tốt nghiệp Đại học Bách Khoa Hà Nội chuyên ngành CƠ điện tử, em đang tìm việc Intern/fresher về Al tại Hà Nội. Không biết anh/chị ở công ty nào có còn open cho vị trí Intern/fresher không ạ?",,,"#Q&A, #machine_learning",,
"[ Góc INTERN ]
Chào mọi người
Em sinh viên năm cuối tại HCM ngành điện tử công nghiệp
Có kiến thức về Machine learning,Deep learning, Neutral Network.
Image Processing và các framework AI tensor flow, Numpy, keras.....
Một số project về sử dụng các thuật toán ứng dụng vào hệ thống nhúng chủ yếu là các vấn đề nhận diện ( nhận diện khuôn mặt để mở cửa, nhận diện biển số xe, nhận diện khuôn mặt đeo khẩu trang...)
Em mong muốn tìm công việc internship tại HCM ạ","[ Góc INTERN ] Chào mọi người Em sinh viên năm cuối tại HCM ngành điện tử công nghiệp Có kiến thức về Machine learning,Deep learning, Neutral Network. Image Processing và các framework AI tensor flow, Numpy, keras..... Một số project về sử dụng các thuật toán ứng dụng vào hệ thống nhúng chủ yếu là các vấn đề nhận diện ( nhận diện khuôn mặt để mở cửa, nhận diện biển số xe, nhận diện khuôn mặt đeo khẩu trang...) Em mong muốn tìm công việc internship tại HCM ạ",,,"#Q&A, #machine_learning",,
"[Nhờ trợ giúp]
Chào các bác, ví dụ có 1 text như này ""tôiđanghỏibạn"", bị lỗi dính chữ, mất dấu cách. Có thư viện nào có thể sửa được câu trên thành câu hoàn chỉnh không ạ? # output: ""tôi đang hỏi bạn"".
Nếu chuỗi là tiếng Anh thì làm được, còn tiếng Việt thì em chưa biết cách nào.🥲","[Nhờ trợ giúp] Chào các bác, ví dụ có 1 text như này ""tôiđanghỏibạn"", bị lỗi dính chữ, mất dấu cách. Có thư viện nào có thể sửa được câu trên thành câu hoàn chỉnh không ạ? # output: ""tôi đang hỏi bạn"". Nếu chuỗi là tiếng Anh thì làm được, còn tiếng Việt thì em chưa biết cách nào.",,,"#Q&A,  #data",,
"Em chào anh/chị ạ. Hiện tại em mới tốt nghiệp và đang mong muốn tìm việc Fresher/Intern AI hoặc liên quan tới Data tại Hà Nội ạ.
Anh/chị đang tuyển thì cho em xin JD với ạ hoặc em sẽ inbox gửi CV ạ.",Em chào anh/chị ạ. Hiện tại em mới tốt nghiệp và đang mong muốn tìm việc Fresher/Intern AI hoặc liên quan tới Data tại Hà Nội ạ. Anh/chị đang tuyển thì cho em xin JD với ạ hoặc em sẽ inbox gửi CV ạ.,,,"#Q&A, #machine_learning, #data",,
"Em chào các anh chị ạ
Em đang là sinh viên năm 2 em có định hướng muốn theo AI nhưng không biết bắt đầu từ đâu, anh/chị có thể cho em xin roadmap được không ạ?Em cảm ơn ạ","Em chào các anh chị ạ Em đang là sinh viên năm 2 em có định hướng muốn theo AI nhưng không biết bắt đầu từ đâu, anh/chị có thể cho em xin roadmap được không ạ?Em cảm ơn ạ",,,"#Q&A, #machine_learning",,
"Góc tìm đồng đội tham gia Team dự cuộc thi ""The 4th Annual International Competition in Data Science & Artificial Intelligence""
Chào các bạn, từ 01/7/2023 ~ 31/8/2023 ISODS, USA tổ chức cuộc thi quốc tế dành cho sinh viên, nghiên cứu viên về nội dung Data Science & Artificial Intelligence.
Nhóm mình cần tìm 05 thành viên tham gia Team, nếu bạn nào thấy yêu thích, phù hợp thì gửi CV mô tả năng lực và kinh nghiệm liên quan qua email cho mình nhé.
Hạn nhận Email: 25/06/2023.
Tiêu chí:
- Có kinh nghiệm, kỹ năng lập trình liên quan đến Data science, Machine learning, Deep learning và Computer vision.
- Tư duy thuật toán tốt.
- Ưu tiên các bạn sinh viên năm 4.
Quyền lợi:
- Làm việc, cộng tác với những bạn có kinh nghiệm trong lĩnh vực liên quan.
- Những bạn phù hợp, đáp ứng năng lực sẽ được nhận học bổng toàn phần Master/PhD tại Lab của mình tại trường Đại học Kyonggi, Hàn Quốc. Bắt đầu học vào kỳ mùa Xuân, tháng 3/2024.
- Tích lũy kinh nghiệm, hồ sơ xin học bổng Master/PhD tại nước ngoài.
Các thông tin liên quan:
Website về cuộc thi: http://isods.org/
Website của Lab: http://ctrl.kyonggi.ac.kr/
Email của mình: phamdinhlam@kgu.ac.kr
Cảm ơn Admin đã duyệt bài!","Góc tìm đồng đội tham gia Team dự cuộc thi ""The 4th Annual International Competition in Data Science & Artificial Intelligence"" Chào các bạn, từ 01/7/2023 ~ 31/8/2023 ISODS, USA tổ chức cuộc thi quốc tế dành cho sinh viên, nghiên cứu viên về nội dung Data Science & Artificial Intelligence. Nhóm mình cần tìm 05 thành viên tham gia Team, nếu bạn nào thấy yêu thích, phù hợp thì gửi CV mô tả năng lực và kinh nghiệm liên quan qua email cho mình nhé. Hạn nhận Email: 25/06/2023. Tiêu chí: - Có kinh nghiệm, kỹ năng lập trình liên quan đến Data science, Machine learning, Deep learning và Computer vision. - Tư duy thuật toán tốt. - Ưu tiên các bạn sinh viên năm 4. Quyền lợi: - Làm việc, cộng tác với những bạn có kinh nghiệm trong lĩnh vực liên quan. - Những bạn phù hợp, đáp ứng năng lực sẽ được nhận học bổng toàn phần Master/PhD tại Lab của mình tại trường Đại học Kyonggi, Hàn Quốc. Bắt đầu học vào kỳ mùa Xuân, tháng 3/2024. - Tích lũy kinh nghiệm, hồ sơ xin học bổng Master/PhD tại nước ngoài. Các thông tin liên quan: Website về cuộc thi: http://isods.org/ Website của Lab: http://ctrl.kyonggi.ac.kr/ Email của mình: phamdinhlam@kgu.ac.kr Cảm ơn Admin đã duyệt bài!",,,"#sharing, #machine_learning",,
"Chào mọi người, một người quen của em cần trợ giúp cho việc thử nghiệm các kỹ thuật học máy trên tập dữ liệu kinh tế, tất nhiên là có trả phí ạ.
Dữ liệu bao gồm: (1) time series tabular dataset, các cột đặc trưng bao gồm đặc trưng số thực và đặc trưng hạng mục, dặc trưng có dạng sai phân so với ""cùng kỳ năm xxx"", dạng tỷ lệ so với ""cùng kỳ năm xxx""; (2) dữ liệu binary thể hiện sự xuất hiện của keyword về kinh tế tại mỗi mốc thời gian. Các đặc trưng được phân về nhiều nhóm và giữa các nhóm có thể có sai khác về mốc thời gian thực hiện phép đo, sai khác về chu kỳ thực hiện phép đo; có thể có missing value trong các nhóm đặc trưng.
Cảm ơn mọi người đã đọc ạ.","Chào mọi người, một người quen của em cần trợ giúp cho việc thử nghiệm các kỹ thuật học máy trên tập dữ liệu kinh tế, tất nhiên là có trả phí ạ. Dữ liệu bao gồm: (1) time series tabular dataset, các cột đặc trưng bao gồm đặc trưng số thực và đặc trưng hạng mục, dặc trưng có dạng sai phân so với ""cùng kỳ năm xxx"", dạng tỷ lệ so với ""cùng kỳ năm xxx""; (2) dữ liệu binary thể hiện sự xuất hiện của keyword về kinh tế tại mỗi mốc thời gian. Các đặc trưng được phân về nhiều nhóm và giữa các nhóm có thể có sai khác về mốc thời gian thực hiện phép đo, sai khác về chu kỳ thực hiện phép đo; có thể có missing value trong các nhóm đặc trưng. Cảm ơn mọi người đã đọc ạ.",,,"#sharing, #machine_learning",,
"Các bác giúp em với ạ!
Em đang dùng thử VNCORENLP cho bài toán ner ạ , vấn đề là văn bản phải được chuẩn hóa(viết hoa tên người, địa điểm,...) thì mới phát huy được ạ còn để tất còn để ở chữ thường thì gần như là không phát hiện được gì ạ :(
Các bác có model nào để chuẩn hóa tiếng việt không ạ ,giới thiệu em với ạ !
Em cảm ơn","Các bác giúp em với ạ! Em đang dùng thử VNCORENLP cho bài toán ner ạ , vấn đề là văn bản phải được chuẩn hóa(viết hoa tên người, địa điểm,...) thì mới phát huy được ạ còn để tất còn để ở chữ thường thì gần như là không phát hiện được gì ạ :( Các bác có model nào để chuẩn hóa tiếng việt không ạ ,giới thiệu em với ạ ! Em cảm ơn",,,"#Q&A, #nlp, #data",,
"Giống như cây bút cần phải đọc rất nhiều truyện để trở thành một “tài năng kể chuyện”, các mô hình ngôn ngữ cần phải được huấn luyện trên một lượng lớn văn bản để đạt được sự thành thục. Càng đọc và học nhiều, chúng càng hiểu và sinh ra ngôn ngữ tốt hơn.","Giống như cây bút cần phải đọc rất nhiều truyện để trở thành một “tài năng kể chuyện”, các mô hình ngôn ngữ cần phải được huấn luyện trên một lượng lớn văn bản để đạt được sự thành thục. Càng đọc và học nhiều, chúng càng hiểu và sinh ra ngôn ngữ tốt hơn.",,,"#sharing, #nlp",,
"Chào mọi người!
Sau 1 tuần nhận được rất nhiều feedback từ mọi người, đặc biệt là từ một số bạn có hạn chế về GPU, team đã convert thành công Vietcuna sang C++ với thư viện GGML và chạy trên CPU chỉ với 3GB RAM.
Mọi người có thể tải model C++ tại đây
Bản quantized (4bit): https://huggingface.co/vilm/vietcuna-3b-ggml-fp16-q4_0
Bản non-quantized: https://huggingface.co/vilm/vietcuna-3b-ggml-fp16
Ngoài ra phiên bản 7B dự kiến sẽ sớm được phát hành.
https://github.com/vilm-ai/vietcuna.cpp","Chào mọi người! Sau 1 tuần nhận được rất nhiều feedback từ mọi người, đặc biệt là từ một số bạn có hạn chế về GPU, team đã convert thành công Vietcuna sang C++ với thư viện GGML và chạy trên CPU chỉ với 3GB RAM. Mọi người có thể tải model C++ tại đây Bản quantized (4bit): https://huggingface.co/vilm/vietcuna-3b-ggml-fp16-q4_0 Bản non-quantized: https://huggingface.co/vilm/vietcuna-3b-ggml-fp16 Ngoài ra phiên bản 7B dự kiến sẽ sớm được phát hành. https://github.com/vilm-ai/vietcuna.cpp",,,#sharing,,
"Em chào mọi người ạ.Em là sv năm 2 mới chập chững bước vào ML và mong muốn hướng đến sau này là ML engineer.Tuy mới bước đầu và trình độ chẳng bằng ai nhưng em có đam mê lớn vs ML cũng như AI ạ.Em đang tự học và tìm hiểu qua blog của anh Tiệp,nhưng mà em cũng muốn cày lại căn bản từ đầu lộ trình để hướng đến AI engineer bằng cách tham gia các khoá học trên mạng của coursera,udemy …
Vì mới chập chững nhập môn nên em rất mơ hồ và k rõ lộ trình cũng như thứ tự các khoá học mà mình nên focus 😞 Làm phiền anh/chị,cũng như cô/chú có thể chỉ giúp em 1 lộ trình và các khoá học từ căn bản đến nâng cao k ạ.
Từ 1 đứa chẳng có gì,em mong muốn đc sống vs đam mê và tạo ra 1 giá trị gì đó dù em biết AI là 1 trường phái rất rất khó và đòi hỏi 1 quá trình chăm chỉ lâu dài,nhưng em sẽ cố hết sức có thể ạ!
Em cảm ơn mọi người rất nhiều!!","Em chào mọi người ạ.Em là sv năm 2 mới chập chững bước vào ML và mong muốn hướng đến sau này là ML engineer.Tuy mới bước đầu và trình độ chẳng bằng ai nhưng em có đam mê lớn vs ML cũng như AI ạ.Em đang tự học và tìm hiểu qua blog của anh Tiệp,nhưng mà em cũng muốn cày lại căn bản từ đầu lộ trình để hướng đến AI engineer bằng cách tham gia các khoá học trên mạng của coursera,udemy … Vì mới chập chững nhập môn nên em rất mơ hồ và k rõ lộ trình cũng như thứ tự các khoá học mà mình nên focus Làm phiền anh/chị,cũng như cô/chú có thể chỉ giúp em 1 lộ trình và các khoá học từ căn bản đến nâng cao k ạ. Từ 1 đứa chẳng có gì,em mong muốn đc sống vs đam mê và tạo ra 1 giá trị gì đó dù em biết AI là 1 trường phái rất rất khó và đòi hỏi 1 quá trình chăm chỉ lâu dài,nhưng em sẽ cố hết sức có thể ạ! Em cảm ơn mọi người rất nhiều!!",,,"#Q&A, #machine_learning",,
GIẢI THÍCH CÁCH HOẠT ĐỘNG CỦA SELF- ATTENTION.,GIẢI THÍCH CÁCH HOẠT ĐỘNG CỦA SELF- ATTENTION.,,,"#sharing, #deep_learning",,
"Xin phép các bác admin cho em chia sẻ playlist Software Engineering Fundamentals tạo bởi anh em group MLOpsVN cho bác Data Scientist/AI Engineer nào quan tâm ạ.
https://www.youtube.com/playlist?list=PLvmLXlo5OR87Gifw5IT-YWllj67YHhaEw",Xin phép các bác admin cho em chia sẻ playlist Software Engineering Fundamentals tạo bởi anh em group MLOpsVN cho bác Data Scientist/AI Engineer nào quan tâm ạ. https://www.youtube.com/playlist?list=PLvmLXlo5OR87Gifw5IT-YWllj67YHhaEw,,,"#sharing, #machine_learning",,
"Chào tất cả ACE trong group.
Cho em hỏi là trong group mình có ai đã sử dụng MMtracking của openmmlab chưa ạ? Em gặp vấn đề về implement và modify; mặc dù đã tìm hiểu cả tuần nay nhưng vẫn chưa giải quyết được nên e đăng stt này hy vọng mọi người giúp e ạ. Do vấn đề dài dòng quá nên không viết lên đây được ạ.
Cảm ơn mọi người đã đọc tin <3",Chào tất cả ACE trong group. Cho em hỏi là trong group mình có ai đã sử dụng MMtracking của openmmlab chưa ạ? Em gặp vấn đề về implement và modify; mặc dù đã tìm hiểu cả tuần nay nhưng vẫn chưa giải quyết được nên e đăng stt này hy vọng mọi người giúp e ạ. Do vấn đề dài dòng quá nên không viết lên đây được ạ. Cảm ơn mọi người đã đọc tin <3,,,"#Q&A, #cv",,
"Microsoft is offering FREE courses in following areas AI, IOT, DATA SCIENCE, MACHINE LEARNING","Microsoft is offering FREE courses in following areas AI, IOT, DATA SCIENCE, MACHINE LEARNING",,,,,
"Giới thiệu với các bạn mô hình Vietcuna do bên mình mới train. Hiện chỉ public bản 3B, phiên bản 7B và 40B vẫn đang tiếp tục cải thiện và trong kế hoạch release","Giới thiệu với các bạn mô hình Vietcuna do bên mình mới train. Hiện chỉ public bản 3B, phiên bản 7B và 40B vẫn đang tiếp tục cải thiện và trong kế hoạch release",,,"#sharing, #machine_learning",,
"Chào anh chị trong nhóm
Anh chị cho em hỏi em học ngành cơ điện tử đang kiếm cơ hội thực tập các công ty liên quan điến ngành học hiện tại
Em có vài câu hỏi sau đây:
Mong anh chị trả lời
Cơ điện tử ở việt nam đi thực tập chủ yếu làm gì?
Tìm thông tin trên mạng mông lung quá
Đôi nét bản thân: em là sinh viên đầu năm 3 một trường kỹ thuật tại sài gòn
tiếng anh của em đọc hiểu nghe nói k vấn đề giao tiếp tốt, sài được ngôn ngữ c, vba, matlab ở mức căn bản, python, cad, solidworks, plc, inventor
Một số chứng chỉ trên coursera liên quan đến mấy phần mềm ở trên ,cs50p
Mong anh chị cho em thông tin thực tập tại công ty liên quan đến chuyên ngành của em
Cuối năm nay em thi thêm tiếng trung hsk thì liệu có tăng thêm cơ hội thực tập tại các công ty không
Em không ngại khó anh chị chỉ em chỗ thực tập để học hỏi và trau dồi cho bản thân.
Anh chị giải đáp với.
Còn lập trình nhúng , thiết kế cơ khí có anh chị nào làm mảng này không, mình có thể trao đổi tại bài post này.
Còn data science thì học kiến thức gì trước trước khi tìm hiểu mảng nay tại trên mạng nói nhiều cái mông lung quá
Em xin lỗi vì hỏi hơi ngớ ngớ.
Cảm ơn anh chị đọc bài!","Chào anh chị trong nhóm Anh chị cho em hỏi em học ngành cơ điện tử đang kiếm cơ hội thực tập các công ty liên quan điến ngành học hiện tại Em có vài câu hỏi sau đây: Mong anh chị trả lời Cơ điện tử ở việt nam đi thực tập chủ yếu làm gì? Tìm thông tin trên mạng mông lung quá Đôi nét bản thân: em là sinh viên đầu năm 3 một trường kỹ thuật tại sài gòn tiếng anh của em đọc hiểu nghe nói k vấn đề giao tiếp tốt, sài được ngôn ngữ c, vba, matlab ở mức căn bản, python, cad, solidworks, plc, inventor Một số chứng chỉ trên coursera liên quan đến mấy phần mềm ở trên ,cs50p Mong anh chị cho em thông tin thực tập tại công ty liên quan đến chuyên ngành của em Cuối năm nay em thi thêm tiếng trung hsk thì liệu có tăng thêm cơ hội thực tập tại các công ty không Em không ngại khó anh chị chỉ em chỗ thực tập để học hỏi và trau dồi cho bản thân. Anh chị giải đáp với. Còn lập trình nhúng , thiết kế cơ khí có anh chị nào làm mảng này không, mình có thể trao đổi tại bài post này. Còn data science thì học kiến thức gì trước trước khi tìm hiểu mảng nay tại trên mạng nói nhiều cái mông lung quá Em xin lỗi vì hỏi hơi ngớ ngớ. Cảm ơn anh chị đọc bài!",,,#Q&A,,
"Chào mọi người ạ, em đang thực hiện project cá nhân và trước đó sử dụng VOC dataset. Tuy nhiên để so sánh với các thực nghiệm của các tác giả trong paper thì em phải so sánh trên COCO. Tuy nhiên COCO có một vấn đề là nó tồn tại nhiều bức ảnh không có object nào thuộc COCO class, các module của em không được thiết kế cho việc này và chỉnh sửa lại rất mất thời gian. Bỏ những tấm ảnh này đi thì rất dễ nhưng em nghĩ là nó cũng sẽ ảnh hưởng đến việc train và tính mAP. Trong các paper thì các tác giả không đề cập đến việc này. Vậy hiện nay cách làm phổ biến là đơn giản bỏ những data như trên ra khỏi tập train và valid hay vẫn giữ vậy ạ.","Chào mọi người ạ, em đang thực hiện project cá nhân và trước đó sử dụng VOC dataset. Tuy nhiên để so sánh với các thực nghiệm của các tác giả trong paper thì em phải so sánh trên COCO. Tuy nhiên COCO có một vấn đề là nó tồn tại nhiều bức ảnh không có object nào thuộc COCO class, các module của em không được thiết kế cho việc này và chỉnh sửa lại rất mất thời gian. Bỏ những tấm ảnh này đi thì rất dễ nhưng em nghĩ là nó cũng sẽ ảnh hưởng đến việc train và tính mAP. Trong các paper thì các tác giả không đề cập đến việc này. Vậy hiện nay cách làm phổ biến là đơn giản bỏ những data như trên ra khỏi tập train và valid hay vẫn giữ vậy ạ.",,,"#Q&A, #data",,
Mọi người cho em hỏi có cách nào để xóa các đường thẳng trong các ảnh trong hình không ạ? Em định dùng Hough để nhận diện nhưng ngặt nỗi số 7 với số 1 nó cũng nhận vào. Em dự định xóa các đường để rồi sau đó tìm ra được khung nhỏ nhất chứa số,Mọi người cho em hỏi có cách nào để xóa các đường thẳng trong các ảnh trong hình không ạ? Em định dùng Hough để nhận diện nhưng ngặt nỗi số 7 với số 1 nó cũng nhận vào. Em dự định xóa các đường để rồi sau đó tìm ra được khung nhỏ nhất chứa số,,,"#Q&A, #cv",,
"If you are attending #CVPR2023 in-person, join us","If you are attending in-person, join us",#CVPR2023,,,,
"Chẳng là em có dùng MINST để đào tạo mô hình nhận dạng kí tự số, sử dụng mô hình MLP. Sau đó em thu thập thêm các kí tự số viết tay của nhiều người. Thì với tập dữ liệu mới này thì MNIST cho kết quả nhận diện không tốt lắm (khoảng 65%). Em định training tiếp mô hình trên tập dữ liệu mới, thì em nên chia tập training-test như thế nào ạ? Vì tập dữ mới của em không được cân cho lắm, ví dụ như tập số 0 có 100 ảnh, tập số 8 có 70 ảnh, trong khi đó tập số 1 chỉ có khoảng 15 ảnh thôi, hay tập số 4 chỉ có 20 ảnh","Chẳng là em có dùng MINST để đào tạo mô hình nhận dạng kí tự số, sử dụng mô hình MLP. Sau đó em thu thập thêm các kí tự số viết tay của nhiều người. Thì với tập dữ liệu mới này thì MNIST cho kết quả nhận diện không tốt lắm (khoảng 65%). Em định training tiếp mô hình trên tập dữ liệu mới, thì em nên chia tập training-test như thế nào ạ? Vì tập dữ mới của em không được cân cho lắm, ví dụ như tập số 0 có 100 ảnh, tập số 8 có 70 ảnh, trong khi đó tập số 1 chỉ có khoảng 15 ảnh thôi, hay tập số 4 chỉ có 20 ảnh",,,"#Q&A, #cv, #data",,
[Mong ad approve sớm ạ],[Mong ad approve sớm ạ],,,,,
"Xin chào các anh em trong nhóm,
Mình là người không chuyên, nhưng đang thử ứng dụng ML cho 1 bài toán của công ty. Mình nghĩ bài toán này dùng ML là đủ chứ ko cần DL.
Do chưa có nhiều kinh nghiệm nên mình có câu hỏi như sau. Hiện tại 1 sample của mình có 8 feauture (dimension ), nhưng thực tế chỉ có 2 LOẠI feauture thôi, vì 1 sample này được lấy từ 4 sensor, mỗi sensor cho thu về 2 loại feauture. 1 cái là Vận tốc, 1 là áp suất tại van đó. Nhưng 4 van này đổ về 1 khuôn chứa, từ đó tạo ra sản phẩm, nên ko thể bỏ được feauture nào hết.
Ngoài việc đưa cả 8 feauture vào, hoặc nghĩ ra 1 công thức để sinh feauture mới, thì có cách nào để Model hiểu 8 feauture, nhưng thực chất là có 2 loại chính được ko ạ?
Mình đang làm bài toán Clustering.
Mong được hướng dẫn hoặc cho keyword nào liên quan đến vấn đề này. Xin cảm ơn!","Xin chào các anh em trong nhóm, Mình là người không chuyên, nhưng đang thử ứng dụng ML cho 1 bài toán của công ty. Mình nghĩ bài toán này dùng ML là đủ chứ ko cần DL. Do chưa có nhiều kinh nghiệm nên mình có câu hỏi như sau. Hiện tại 1 sample của mình có 8 feauture (dimension ), nhưng thực tế chỉ có 2 LOẠI feauture thôi, vì 1 sample này được lấy từ 4 sensor, mỗi sensor cho thu về 2 loại feauture. 1 cái là Vận tốc, 1 là áp suất tại van đó. Nhưng 4 van này đổ về 1 khuôn chứa, từ đó tạo ra sản phẩm, nên ko thể bỏ được feauture nào hết. Ngoài việc đưa cả 8 feauture vào, hoặc nghĩ ra 1 công thức để sinh feauture mới, thì có cách nào để Model hiểu 8 feauture, nhưng thực chất là có 2 loại chính được ko ạ? Mình đang làm bài toán Clustering. Mong được hướng dẫn hoặc cho keyword nào liên quan đến vấn đề này. Xin cảm ơn!",,,"#Q&A, #machine_learning",,
"Chào mọi người,
Em hiện là sinh viên ngành Cơ Điện Tử muốn tìm hiểu về Machine Learning để áp dụng vào ngành của mình trong tương lai.
Hiện tại em đã học xong và nắm chắc các kiến thức liên quan đến Toán (Giải Tích, Xác suất, Đại Số Tuyến Tính,...). Về lập trình thì em đã biết C và R.
Nhờ mọi người tư vấn giúp em lộ trình cũng như các khóa học hay về Machine Learning cho sinh viên mới bắt đầu như em ạ.
Em xin cảm ơn.","Chào mọi người, Em hiện là sinh viên ngành Cơ Điện Tử muốn tìm hiểu về Machine Learning để áp dụng vào ngành của mình trong tương lai. Hiện tại em đã học xong và nắm chắc các kiến thức liên quan đến Toán (Giải Tích, Xác suất, Đại Số Tuyến Tính,...). Về lập trình thì em đã biết C và R. Nhờ mọi người tư vấn giúp em lộ trình cũng như các khóa học hay về Machine Learning cho sinh viên mới bắt đầu như em ạ. Em xin cảm ơn.",,,"#Q&A, #machine_learning",,
"Chào mọi người trong nhóm ạ
Em là sinh viên mới ra trường, trước đây thì e có theo hướng lập trình web, app bây giờ em muốn tìm hiểu sang cả học máy nữa nhưng chưa biết bắt đầu từ đâu. Mọi người có thể cho em xin một cái roadmap dành cho người mới bắt đầu như em không ạ? Em xin cảm ơn mn nhiều","Chào mọi người trong nhóm ạ Em là sinh viên mới ra trường, trước đây thì e có theo hướng lập trình web, app bây giờ em muốn tìm hiểu sang cả học máy nữa nhưng chưa biết bắt đầu từ đâu. Mọi người có thể cho em xin một cái roadmap dành cho người mới bắt đầu như em không ạ? Em xin cảm ơn mn nhiều",,,"#Q&A, #machine_learning",,
"Mình đang tìm hiểu về deep learning trong việc xử lý hình ảnh, sau khi tra internet thì mình có những thắc mắc sau đây, mọi người có thể chia sẻ với mình về các câu hỏi này được không ạ? Em cảm ơn.
1. Embedding là quá trình gì? Phân biệt word embedding và image embedding?
→ Hiểu: embedding là quá trình chuyển vector nhiều chiều thành vector có số chiều ít hơn bằng cách tạo ra một không gian nhúng dành cho các từ có ý nghĩa gần giống nhau. Word embedding và image embedding đều xử lý việc biến input từ nhiều dimension thành vector/tensor có ít dimension hơn (ví dụ, thay vì có 10 words, thì ta sẽ có dạng tensor để biểu diễn cho từng word là 10 dimension - (0,0,...,1,0) - 9 số 0 và 1 số 1. → ta sẽ phát hiện ra những từ có cùng ngữ nghĩa và tạo nên một embedded space (1 tensor). Sau này khi biểu diễn các ngôn ngữ khác thì chỉ cần 1 vector chỉ index của từ đó để nhân với embedded tensor.
2. Pre-trained model của quá trình nhận dạng hình ảnh được thực hiện qua các bước chính nào? inception_v3 của torchvision.models hỗ trợ những gì trong quá trình pre-trained hoặc nhận dạng hình ảnh?
3. Hidden layer và embed layer khác gì nhau?
4. Ecoder và embedding khác gì nhau?
Opinion: encoder biến text/iamge thành vector, embedding là quá trình giảm dimension của vector","Mình đang tìm hiểu về deep learning trong việc xử lý hình ảnh, sau khi tra internet thì mình có những thắc mắc sau đây, mọi người có thể chia sẻ với mình về các câu hỏi này được không ạ? Em cảm ơn. 1. Embedding là quá trình gì? Phân biệt word embedding và image embedding? → Hiểu: embedding là quá trình chuyển vector nhiều chiều thành vector có số chiều ít hơn bằng cách tạo ra một không gian nhúng dành cho các từ có ý nghĩa gần giống nhau. Word embedding và image embedding đều xử lý việc biến input từ nhiều dimension thành vector/tensor có ít dimension hơn (ví dụ, thay vì có 10 words, thì ta sẽ có dạng tensor để biểu diễn cho từng word là 10 dimension - (0,0,...,1,0) - 9 số 0 và 1 số 1. → ta sẽ phát hiện ra những từ có cùng ngữ nghĩa và tạo nên một embedded space (1 tensor). Sau này khi biểu diễn các ngôn ngữ khác thì chỉ cần 1 vector chỉ index của từ đó để nhân với embedded tensor. 2. Pre-trained model của quá trình nhận dạng hình ảnh được thực hiện qua các bước chính nào? inception_v3 của torchvision.models hỗ trợ những gì trong quá trình pre-trained hoặc nhận dạng hình ảnh? 3. Hidden layer và embed layer khác gì nhau? 4. Ecoder và embedding khác gì nhau? Opinion: encoder biến text/iamge thành vector, embedding là quá trình giảm dimension của vector",,,"#Q&A, #deep_learning, #cv",,
"Xin chào mọi người ạ. Hiện em đang bắt đầu học python để dùng cho ML, mọi người có thể giới thiệu cho em học các khóa học online nào thì phù hợp ạ.
P/S: Em có kiến thức về lập trình căn bản rồi nên em nghĩ em học sẽ khá nhanh
Cảm ơn mọi người ạ <3","Xin chào mọi người ạ. Hiện em đang bắt đầu học python để dùng cho ML, mọi người có thể giới thiệu cho em học các khóa học online nào thì phù hợp ạ. P/S: Em có kiến thức về lập trình căn bản rồi nên em nghĩ em học sẽ khá nhanh Cảm ơn mọi người ạ <3",,,"#Q&A, #machine_learning, #python",,
"Chào mọi người, em có người quen cần tìm giải pháp làm mềm ảnh bitmap thành ảnh vector có thể dùng cho thêu công nghiệp wilcom. Yêu cầu xử lý được pencil hatching pattern, lọc bỏ được chi tiết thừa để phù hợp với ảnh thêu. Không biết đã có ai cung cấp giải pháp chưa ạ? Cảm ơn mọi người rất nhiều.
Ảnh minh hoạ","Chào mọi người, em có người quen cần tìm giải pháp làm mềm ảnh bitmap thành ảnh vector có thể dùng cho thêu công nghiệp wilcom. Yêu cầu xử lý được pencil hatching pattern, lọc bỏ được chi tiết thừa để phù hợp với ảnh thêu. Không biết đã có ai cung cấp giải pháp chưa ạ? Cảm ơn mọi người rất nhiều. Ảnh minh hoạ",,,"#sharing, #cv",,
"Chào mọi người, tại sao AlexNet sử dụng Group Convolution nhưng hầu hết những implementation cộng đồng của AlexNet lại chỉ dùng Convolution thông thường ạ?","Chào mọi người, tại sao AlexNet sử dụng Group Convolution nhưng hầu hết những implementation cộng đồng của AlexNet lại chỉ dùng Convolution thông thường ạ?",,,"#Q&A, #deep_learning",,
"Xin chào mọi người, em có 1 vài vấn đề cần mọi người trong nhóm tư vấn và hỗ trợ ạ.
Em đang có 1 đề tài về ứng dụng AI trong giám sát mạng. Dự định của em sẽ sử dụng Zabbix cho phần giám sát mạng, nhưng em tìm hiểu thì zabbix nó hỗ trợ rất ít AI hoặc không có ứng dụng AI rõ ràng trong đó. Nên em định chuyển tiếp các thông tin log thu thập được từ zabbix về 1 công cụ AI của bên khác để phân tích rõ hơn.
Vì thế em muốn hỏi mọi người trong nhóm tư vấn cho em các tool về 1 vài công cụ AI đó ( chủ yếu sử dụng qua webui chứ không phải lập trình vì em không chuyên về lập trình ạ) hoặc ứng dụng có tích hợp AI trong giám mạng có thể thay thế được Zabbix ạ.
Em cảm ơn ạ.","Xin chào mọi người, em có 1 vài vấn đề cần mọi người trong nhóm tư vấn và hỗ trợ ạ. Em đang có 1 đề tài về ứng dụng AI trong giám sát mạng. Dự định của em sẽ sử dụng Zabbix cho phần giám sát mạng, nhưng em tìm hiểu thì zabbix nó hỗ trợ rất ít AI hoặc không có ứng dụng AI rõ ràng trong đó. Nên em định chuyển tiếp các thông tin log thu thập được từ zabbix về 1 công cụ AI của bên khác để phân tích rõ hơn. Vì thế em muốn hỏi mọi người trong nhóm tư vấn cho em các tool về 1 vài công cụ AI đó ( chủ yếu sử dụng qua webui chứ không phải lập trình vì em không chuyên về lập trình ạ) hoặc ứng dụng có tích hợp AI trong giám mạng có thể thay thế được Zabbix ạ. Em cảm ơn ạ.",,,"#Q&A, #machine_learning",,
"chào mọi người, mình xin phép hỏi đến một số vấn đề của face recognition.
1.Mình thắc mắc là trong các thư viện như face_recognition thì đầu ra của nó là 1 vector 128 chiều , hay là một index sau khi được softmax.
2. Mình có tìm hiểu thì thấy rằng họ sử dụng model facenet trong thư viện face_recognition. Mình cũng chưa hiểu facenet lắm, mk có đọc được là nó train bằng softmax ở lớp cuối, nhưng lâu lâu mình lại thấy đâu đó có tài liệu nói rằng nó sẽ nhúng ảnh thành vector 128 chiều. Vậy có phải là trước lớp cuối là 1 linear 128 chiều và lớp cuối là 1 softmax linear đúng k nhỉ, hay là lớp cuối cùng chỉ là 1 linear 128 chiều sau đó sử dụng l2_distance để đo độ tương đồng hoặc tính loss nhỉ ?
3. VD mình có 1 thư mục có 30 ảnh của 30 người khác nhau. Sau đó sử dụng face_recognition thì với vấn đề thứ 2 chưa giải quyết được nó sẽ sinh ra vấn đề tiếp là: nếu lớp cuối cùng là 1 softmax linear và trước nó là 1 linear 128 chiều thì có phải thư viện sẽ đóng băng toàn bộ weight trừ cái weight kết nối của 2 linear trên đúng không nhỉ? ( lý do mình nghĩ như vậy là vì weight kết nối mới khởi tạo rất dễ tạo ra loss lớn, mà lại phải update toàn bộ weight thì nó sẽ như kiểu big learning rate ấy). Cái này mình đoán mò th, ai thấy sai thì giúp mình với ạ.
Thực ra thì sáng nay mình có gặp một người và có kêu mình rằng thay vì em nhúng ra 128 vector rồi tính độ tương đồng giữa các khuôn mặt thì tại sao em không cho model chạy qua rồi 1 phát cho ra kết quả luôn. Mình đang nghĩ việc này không có khả thi cho lắm Tại vì nếu model càng sâu thì với dữ liệu ít ỏi chỉ có 30 hình ảnh thì rất dễ bị overfiting còn nếu model quá nông thì khó mà có thể nhận diện chính xác được. Điều này là mình đã thực nghiệm khi train mobilefacenet +arcMargin trên tập dữ liệu 5000 ảnh của hơn 1000 đối tượng vẫn bị overfitting. Nhưng mà do mình chưa data Agumentation nên cũng không chắc, mà mình hết colab ùi, ai từng làm qua kiểu 1 phát cho ra kết quả luôn thì chia sẽ cho mình với ạ.
Lỡ mình có hỏi ngu quá thì mong mn cũng đừng ném đá quá ha!!! cảm ơn mn đã đọc ạ.","chào mọi người, mình xin phép hỏi đến một số vấn đề của face recognition. 1.Mình thắc mắc là trong các thư viện như face_recognition thì đầu ra của nó là 1 vector 128 chiều , hay là một index sau khi được softmax. 2. Mình có tìm hiểu thì thấy rằng họ sử dụng model facenet trong thư viện face_recognition. Mình cũng chưa hiểu facenet lắm, mk có đọc được là nó train bằng softmax ở lớp cuối, nhưng lâu lâu mình lại thấy đâu đó có tài liệu nói rằng nó sẽ nhúng ảnh thành vector 128 chiều. Vậy có phải là trước lớp cuối là 1 linear 128 chiều và lớp cuối là 1 softmax linear đúng k nhỉ, hay là lớp cuối cùng chỉ là 1 linear 128 chiều sau đó sử dụng l2_distance để đo độ tương đồng hoặc tính loss nhỉ ? 3. VD mình có 1 thư mục có 30 ảnh của 30 người khác nhau. Sau đó sử dụng face_recognition thì với vấn đề thứ 2 chưa giải quyết được nó sẽ sinh ra vấn đề tiếp là: nếu lớp cuối cùng là 1 softmax linear và trước nó là 1 linear 128 chiều thì có phải thư viện sẽ đóng băng toàn bộ weight trừ cái weight kết nối của 2 linear trên đúng không nhỉ? ( lý do mình nghĩ như vậy là vì weight kết nối mới khởi tạo rất dễ tạo ra loss lớn, mà lại phải update toàn bộ weight thì nó sẽ như kiểu big learning rate ấy). Cái này mình đoán mò th, ai thấy sai thì giúp mình với ạ. Thực ra thì sáng nay mình có gặp một người và có kêu mình rằng thay vì em nhúng ra 128 vector rồi tính độ tương đồng giữa các khuôn mặt thì tại sao em không cho model chạy qua rồi 1 phát cho ra kết quả luôn. Mình đang nghĩ việc này không có khả thi cho lắm Tại vì nếu model càng sâu thì với dữ liệu ít ỏi chỉ có 30 hình ảnh thì rất dễ bị overfiting còn nếu model quá nông thì khó mà có thể nhận diện chính xác được. Điều này là mình đã thực nghiệm khi train mobilefacenet +arcMargin trên tập dữ liệu 5000 ảnh của hơn 1000 đối tượng vẫn bị overfitting. Nhưng mà do mình chưa data Agumentation nên cũng không chắc, mà mình hết colab ùi, ai từng làm qua kiểu 1 phát cho ra kết quả luôn thì chia sẽ cho mình với ạ. Lỡ mình có hỏi ngu quá thì mong mn cũng đừng ném đá quá ha!!! cảm ơn mn đã đọc ạ.",,,"#Q&A, #cv, #machine_learning",,
" Chào mọi người, mình 30t là người trái ngành và đang có hứng thú tìm hiểu về AI. Mình có tham khảo nhiều nguồn và đang theo lộ trình tự học AI 6 khóa trên udemy (khóa thứ 6 ở bên dưới là: Áp dụng AI:
Applied Machine Learning for Healthcare (Hadelin de Ponteves, Kirill Eremenko)Khóa học này tập trung vào ứng dụng Machine Learning trong lĩnh vực y tế và dữ liệu y tế.
) do con chatGPT đưa ra và đã học xong khóa 1 trên. Theo mọi người lộ trình này có ổn không. Thêm nữa mọi người cho mình xin tên/link các khóa học toán trên Youtube hoặc Udemy/Cousera để theo ngành này nữa ạ. Cảm ơn mọi người!","Chào mọi người, mình 30t là người trái ngành và đang có hứng thú tìm hiểu về AI. Mình có tham khảo nhiều nguồn và đang theo lộ trình tự học AI 6 khóa trên udemy (khóa thứ 6 ở bên dưới là: Áp dụng AI: Applied Machine Learning for Healthcare (Hadelin de Ponteves, Kirill Eremenko)Khóa học này tập trung vào ứng dụng Machine Learning trong lĩnh vực y tế và dữ liệu y tế. ) do con chatGPT đưa ra và đã học xong khóa 1 trên. Theo mọi người lộ trình này có ổn không. Thêm nữa mọi người cho mình xin tên/link các khóa học toán trên Youtube hoặc Udemy/Cousera để theo ngành này nữa ạ. Cảm ơn mọi người!",,,"#Q&A, #machine_learning",,
"Chào mọi người,
Em đang làm một sản phẩm để dự thi cuộc thi KHKT THPT về việc cải thiện thời gian đèn giao thông bằng ML. Em làm được phần Car Detection roi nhma tới khúc xử lý để đưa ra một thời gian chính xác thì em đang phân giữa việc tìm ra một công thức toán hay là dùng một model ML.
Em có một thắc mắc là liệu mình có thể xây dựng một mô hình có thể dự đoán được thời gian đèn xanh/đỏ/vàng ở một ngã tư nào đó dựa vào các features là : lưu lượng xe , số lượng xe tại thời điểm t ở cả bốn cột đèn giao thông không ạ.
Nếu được thì mình phải label bằng tay cho tập dữ liệu đúng k ạ? Liệu mình có còn cách nào khác nhanh hơn không ạ.
Em cảm ơn mọi người ạ.","Chào mọi người, Em đang làm một sản phẩm để dự thi cuộc thi KHKT THPT về việc cải thiện thời gian đèn giao thông bằng ML. Em làm được phần Car Detection roi nhma tới khúc xử lý để đưa ra một thời gian chính xác thì em đang phân giữa việc tìm ra một công thức toán hay là dùng một model ML. Em có một thắc mắc là liệu mình có thể xây dựng một mô hình có thể dự đoán được thời gian đèn xanh/đỏ/vàng ở một ngã tư nào đó dựa vào các features là : lưu lượng xe , số lượng xe tại thời điểm t ở cả bốn cột đèn giao thông không ạ. Nếu được thì mình phải label bằng tay cho tập dữ liệu đúng k ạ? Liệu mình có còn cách nào khác nhanh hơn không ạ. Em cảm ơn mọi người ạ.",,,"#Q&A, #cv, #machine_learning",,
"Em chào mọi người ạ,
Em mới chuyển qua làm NLP được vài tháng nay, trước đó em làm mảng vision, nên em có một vài câu hỏi muốn tham vấn mọi người ạ. Giả sử quy mô dữ liệu lớn (>= 10M short Arabic text sentences of less than 100 words), nguồn lực khoảng 50 human annotators, em gặp vấn đề như sau:
Đối với bài toán sentiment classification (SC), annotation của human thường rất subjective, đặc biệt đối với các sentences có cả POS và NEG sentiments thì rất khó gán nhãn
Một số nguồn gợi ý sử dụng clustering words thành POS/NEG/NEU trước, rồi đếm số POS/NEG/NEU trong mỗi sentences để tạo pseudo-label. Tuy nhiên em thấy cách này chưa toàn diện, vì nghĩa của từ còn phụ thuộc nhiều vào context
Em có thử sử dụng một số SC models (pre-trained on big data) trên huggingface để sinh benchmark trên nhiều public datasets khác nhau thì thấy độ chính xác chỉ ở mức 70% => Em đoán là do cách định nghĩa POS/NEU/NEG sentiments của mỗi bộ dataset cũng khác nhau nhiều
Ngay cả việc định nghĩa thế nào là NEU sentiment em cũng chưa tìm thấy thông tin đủ tốt ạ. Trong bài [1], tác giả của báo có đề xuất NEU sentiment nghĩa là ""no positive and no negative"". Cơ mà như thế nào là ""no positive"" và ""no negative"" thì cũng rất đưa ra 1 quy tắc nhất quán
Có một cách để generate consistent SC labels là sử dụng public pre-trained LLMs (e.g. GPT), tuy nhiên em có thử GPT thì thấy chất lượng khá tệ. Kể cả Google Dịch cũng perform badly on Arabic (em so kết quả dịch của Google với kết quả dịch của human translator) => Em đoán là NLP models hiện tại vẫn chưa hoạt động tốt với Arabic
Em rất mong nhận được sự giúp đỡ của mọi người ạ!
Trích dẫn:
[1] https://www.reddit.com/r/MachineLearning/comments/1my83q/the_importance_of_neutral_class_in_sentiment/","Em chào mọi người ạ, Em mới chuyển qua làm NLP được vài tháng nay, trước đó em làm mảng vision, nên em có một vài câu hỏi muốn tham vấn mọi người ạ. Giả sử quy mô dữ liệu lớn (>= 10M short Arabic text sentences of less than 100 words), nguồn lực khoảng 50 human annotators, em gặp vấn đề như sau: Đối với bài toán sentiment classification (SC), annotation của human thường rất subjective, đặc biệt đối với các sentences có cả POS và NEG sentiments thì rất khó gán nhãn Một số nguồn gợi ý sử dụng clustering words thành POS/NEG/NEU trước, rồi đếm số POS/NEG/NEU trong mỗi sentences để tạo pseudo-label. Tuy nhiên em thấy cách này chưa toàn diện, vì nghĩa của từ còn phụ thuộc nhiều vào context Em có thử sử dụng một số SC models (pre-trained on big data) trên huggingface để sinh benchmark trên nhiều public datasets khác nhau thì thấy độ chính xác chỉ ở mức 70% => Em đoán là do cách định nghĩa POS/NEU/NEG sentiments của mỗi bộ dataset cũng khác nhau nhiều Ngay cả việc định nghĩa thế nào là NEU sentiment em cũng chưa tìm thấy thông tin đủ tốt ạ. Trong bài [1], tác giả của báo có đề xuất NEU sentiment nghĩa là ""no positive and no negative"". Cơ mà như thế nào là ""no positive"" và ""no negative"" thì cũng rất đưa ra 1 quy tắc nhất quán Có một cách để generate consistent SC labels là sử dụng public pre-trained LLMs (e.g. GPT), tuy nhiên em có thử GPT thì thấy chất lượng khá tệ. Kể cả Google Dịch cũng perform badly on Arabic (em so kết quả dịch của Google với kết quả dịch của human translator) => Em đoán là NLP models hiện tại vẫn chưa hoạt động tốt với Arabic Em rất mong nhận được sự giúp đỡ của mọi người ạ! Trích dẫn: [1] https://www.reddit.com/r/MachineLearning/comments/1my83q/the_importance_of_neutral_class_in_sentiment/",,,"#Q&A, #nlp",,
"Em chào mọi người ạ
Hiện em đang làm 1 project cuối kì về ML
1.1.Bài toán: Bài toán ước lượng
Công ty tài chính A cần kiểm soát thanh khoản của dòng tiền. Tại thời điểm đầu tháng 04.2023 cần ước lượng số tiền thu của khách hàng trong tháng 4.2023 là bao nhiêu.
Dựa vào data có sẵn
Yêu cầu
Requirements:
1.Clean data and train the models
2.Diagnose and assess the results
3.Use tools
Tuy nhiên project khi làm ra được cmt là sai trong việc sử dụng thống kê suy luận, đang xài R. Hiện tại em vẫn chưa tìm ra chỗ fix chỗ đó nên mạo muội muốn hỏi thêm các anh chị a, em cảm ơn","Em chào mọi người ạ Hiện em đang làm 1 project cuối kì về ML 1.1.Bài toán: Bài toán ước lượng Công ty tài chính A cần kiểm soát thanh khoản của dòng tiền. Tại thời điểm đầu tháng 04.2023 cần ước lượng số tiền thu của khách hàng trong tháng 4.2023 là bao nhiêu. Dựa vào data có sẵn Yêu cầu Requirements: 1.Clean data and train the models 2.Diagnose and assess the results 3.Use tools Tuy nhiên project khi làm ra được cmt là sai trong việc sử dụng thống kê suy luận, đang xài R. Hiện tại em vẫn chưa tìm ra chỗ fix chỗ đó nên mạo muội muốn hỏi thêm các anh chị a, em cảm ơn",,,"#Q&A, #machine_learning",,
Mô hình AI chuyển đổi video 2D thành cấu trúc 3D chi tiết!,Mô hình AI chuyển đổi video 2D thành cấu trúc 3D chi tiết!,,,"#sharing, #machine_learning",,
"Có bác nào làm việc với segmentation trong YOLO chưa ạ, cho em hỏi với là sau khi em train xong model thì đầu vào ảnh của em phải chuẩn hóa như nào mới cho vào model nhận diện được vậy ạ","Có bác nào làm việc với segmentation trong YOLO chưa ạ, cho em hỏi với là sau khi em train xong model thì đầu vào ảnh của em phải chuẩn hóa như nào mới cho vào model nhận diện được vậy ạ",,,"#Q&A, #cv",,
"Em chào mn ạ,
Em đang là sinh viên đi thực tập và em có được giao một task là crawl thông tin về các teams cũng như lịch sử đấu, lịch thi đấu của giải đấu Premier League. Em có kiểm tra khi vào trang thì web có gọi API tới để về data. Tuy nhiên em vào API thì bị error 403. Em có dùng cheerio để crawl thuần nhưng cái HTML nó trả về thì lại không chứa thông tin do nó dyniamic render. Mn ai có kinh nghiệm có thể hướng dẫn giúp em với ạ.
Em cảm ơn mn.
https://www.premierleague.com/","Em chào mn ạ, Em đang là sinh viên đi thực tập và em có được giao một task là crawl thông tin về các teams cũng như lịch sử đấu, lịch thi đấu của giải đấu Premier League. Em có kiểm tra khi vào trang thì web có gọi API tới để về data. Tuy nhiên em vào API thì bị error 403. Em có dùng cheerio để crawl thuần nhưng cái HTML nó trả về thì lại không chứa thông tin do nó dyniamic render. Mn ai có kinh nghiệm có thể hướng dẫn giúp em với ạ. Em cảm ơn mn. https://www.premierleague.com/",,,"#Q&A, #data",,
"Cuối tuần sau CVPR khai mạc ở Vancouver, Canada. Mình ko có paper nhưng cũng có mặt ngày chủ nhật, bạn nào trong group cũng đi thì hội Việt Nam hangout nhé.","Cuối tuần sau CVPR khai mạc ở Vancouver, Canada. Mình ko có paper nhưng cũng có mặt ngày chủ nhật, bạn nào trong group cũng đi thì hội Việt Nam hangout nhé.",,,#sharing,,
"HCM - Decoding Generative AI - Startups
Thân mời cả nhà cùng đến tham dự sự kiện Decoding Generative AI - Startups do Techies Story, WeBuild và AWS Vietnam tổ chức.
Thời gian: 14:00 - 16:30, ngày 20/06/2023
Địa điểm: Quận 1, TP Hồ Chí Minh.
Link đăng ký: https://lu.ma/TechEventGenAI
Agenda: tiếng Anh
2:00pm - 2:05pm: Welcome remarks 2:05pm - 2:35pm: Sharing by Nathan Wangerin Lile, Chief of Staff, Stability AI
2:35pm - 3:05pm: Sharing by Nadav Magnezi, Business & Strategy, AI21 Labs
3:05pm - 3.20pm - Tea Break
3:20pm - 3.40pm: Sharing by Tin Nguyen, CEO & Founder, Ather Labs
3.40pm - 4pm: Sharing by Girish Dilip Patil, Head of Tech, Digital Native Business, ASEAN, Amazon Web Services (AWS)
4pm - 4:30pm: Q&A
About the speakers:
Amazon Web Service (AWS)'s mission for artificial intelligence (AI) is to empower developers to redefine the capabilities of the Internet by offering them robust tools and top-notch security measures. With a focus on effectiveness and innovation, AWS aims to provide the necessary resources for developers to build groundbreaking AI solutions.
Stability AI, headquartered in London with developers distributed across the globe, was founded to build the foundation to activate humanity's potential through artificial intelligence (AI). As the leader in multimodal, open-source AI model development and deployment, Stability AI collaborates with public and private sector partners to bring this next generation infrastructure to a global audience.
AI21 Labs is building state of the art language models with a laser focus on understanding meaning. And to do this, they are simultaneously introducing scientific innovations and tackling frontier software engineering challenges posed by models of this size and sophistication. AI21 Labs was founded by AI pioneers and technology veterans in 2017, including Prof. Yoav Shoham (Professor Emeritus at Stanford), Ori Goshen (Founder of CrowdX), and chairman, Prof. Amnon Shashua (Founder, Mobileye).
Ather Labs is a gaming & technology studio building compelling entertainment experiences using innovative technologies.
About the organizers:
Techie Story: The Untold shares the lives and works of profound individuals who work in tech, including tech veterans, the software engineers, the art wizards, the HRs. Techie Story is also dedicated to enriching the knowledge and skills of developers through a range of engaging events, both online and offline.
WeBuild Community is a forum that connects developers in Vietnam, where they can share their knowledge and experience while working, learning, and building cool stuffs together with more than 3,500 members on Discord.","HCM - Decoding Generative AI - Startups Thân mời cả nhà cùng đến tham dự sự kiện Decoding Generative AI - Startups do Techies Story, WeBuild và AWS Vietnam tổ chức. Thời gian: 14:00 - 16:30, ngày 20/06/2023 Địa điểm: Quận 1, TP Hồ Chí Minh. Link đăng ký: https://lu.ma/TechEventGenAI Agenda: tiếng Anh 2:00pm - 2:05pm: Welcome remarks 2:05pm - 2:35pm: Sharing by Nathan Wangerin Lile, Chief of Staff, Stability AI 2:35pm - 3:05pm: Sharing by Nadav Magnezi, Business & Strategy, AI21 Labs 3:05pm - 3.20pm - Tea Break 3:20pm - 3.40pm: Sharing by Tin Nguyen, CEO & Founder, Ather Labs 3.40pm - 4pm: Sharing by Girish Dilip Patil, Head of Tech, Digital Native Business, ASEAN, Amazon Web Services (AWS) 4pm - 4:30pm: Q&A About the speakers: Amazon Web Service (AWS)'s mission for artificial intelligence (AI) is to empower developers to redefine the capabilities of the Internet by offering them robust tools and top-notch security measures. With a focus on effectiveness and innovation, AWS aims to provide the necessary resources for developers to build groundbreaking AI solutions. Stability AI, headquartered in London with developers distributed across the globe, was founded to build the foundation to activate humanity's potential through artificial intelligence (AI). As the leader in multimodal, open-source AI model development and deployment, Stability AI collaborates with public and private sector partners to bring this next generation infrastructure to a global audience. AI21 Labs is building state of the art language models with a laser focus on understanding meaning. And to do this, they are simultaneously introducing scientific innovations and tackling frontier software engineering challenges posed by models of this size and sophistication. AI21 Labs was founded by AI pioneers and technology veterans in 2017, including Prof. Yoav Shoham (Professor Emeritus at Stanford), Ori Goshen (Founder of CrowdX), and chairman, Prof. Amnon Shashua (Founder, Mobileye). Ather Labs is a gaming & technology studio building compelling entertainment experiences using innovative technologies. About the organizers: Techie Story: The Untold shares the lives and works of profound individuals who work in tech, including tech veterans, the software engineers, the art wizards, the HRs. Techie Story is also dedicated to enriching the knowledge and skills of developers through a range of engaging events, both online and offline. WeBuild Community is a forum that connects developers in Vietnam, where they can share their knowledge and experience while working, learning, and building cool stuffs together with more than 3,500 members on Discord.",,,,,
"Chúc các bạn buổi tối chủ nhật vui vẻ và ấm áp bên người thân yêu. Trong tuần qua mình đã thực hiện 2 bài viết vể sử dụng Typst (được viết bằng Rust-lang) để soạn thảo luận văn/luận án và presentation slides. Mình hi vọng nó hữu ích cho các bạn muốn học và áp dụng vào quá trình biên soạn tài liệu cho các bạn.
Trong khoa học, các sinh viên còn có thể phải làm Poster để tham gia các Conferences. Vậy nên, trong chuỗi bài về Typst, mình xin chia sẻ bài còn lại sử dụng Typst để làm Poster tại đây: https://github.com/linhduongtuan/VNUHCM-typst-poster.
Mong rằng mấy bài về chủ đề Typst này có giá trị sử dụng trong công việc học tập và nghiên cứu của các bạn. Nếu các bạn thấy nó có ý nghĩa với bản thân mình thì xin đừng tiếc 1 Star cho các repositories của mình nhé. Cảm ơn các bạn nhiều.
Ps. Mình hi vọng, các Journals sẽ sớm chấp nhận Typst như 1 typsetting (như Word hay LaTeX) cho việc soạn thảo manuscripts gửi cho các tập san/nhà xuất bản trong thời gian gần nhất.","Chúc các bạn buổi tối chủ nhật vui vẻ và ấm áp bên người thân yêu. Trong tuần qua mình đã thực hiện 2 bài viết vể sử dụng Typst (được viết bằng Rust-lang) để soạn thảo luận văn/luận án và presentation slides. Mình hi vọng nó hữu ích cho các bạn muốn học và áp dụng vào quá trình biên soạn tài liệu cho các bạn. Trong khoa học, các sinh viên còn có thể phải làm Poster để tham gia các Conferences. Vậy nên, trong chuỗi bài về Typst, mình xin chia sẻ bài còn lại sử dụng Typst để làm Poster tại đây: https://github.com/linhduongtuan/VNUHCM-typst-poster. Mong rằng mấy bài về chủ đề Typst này có giá trị sử dụng trong công việc học tập và nghiên cứu của các bạn. Nếu các bạn thấy nó có ý nghĩa với bản thân mình thì xin đừng tiếc 1 Star cho các repositories của mình nhé. Cảm ơn các bạn nhiều. Ps. Mình hi vọng, các Journals sẽ sớm chấp nhận Typst như 1 typsetting (như Word hay LaTeX) cho việc soạn thảo manuscripts gửi cho các tập san/nhà xuất bản trong thời gian gần nhất.",,,#sharing,,
"Dạ em chào mọi người, em đang có một chút khúc mắc trong bài toán binary classification sử dụng dataset này https://github.com/IBM/TabFormer/tree/main/data/credit_card, cụ thể hơn là điểm F1 training của model thấp, nhưng điểm F1 của validation và testing thì lại cao.  Về dataset, đây là dataset về giao dịch thẻ tín dụng, và mục tiêu dự đoán là xem cuộc giao dịch đấy có phải lừa đảo hay không (""Is Fraud?""). Phân bố class này rất mất cân bằng, cụ thể là:
24 triệu cuộc giao dịch
30,000 giao dịch lừa đảo (0.1% of tổng giao dịch)
Sau phần tiền xử lý dữ liệu, em có chia ra 3 sets Training (Cuộc giao dịch trước 2018), Validation (trong 2018) và Testing (sau 2018), với phần phân bố class như sau (0 là giao dịch không lừa đảo, 1 là giao dịch lừa đảo)
Training Data: 
Class 0:  20579668 
Class 1: 25179 
Validation Data: 
Class 0: 1719124 
Class 1: 2491 
Testing Data: 
Class 0: 2058351 
Class 1: 2087
Em hiện tại đang sử dụng model XGBoost để dự đoán, và em có thu lại được một số kết quả như sau:
F1 Score on Training Data : 0.57417479049085 
F1 Score on Testing Data : 0.8719438392641008 
PR AUC score on Training Data : 0.9918559271777408 
PR AUC score on Testing Data : 0.9077624174590952
Training report
precision recall f1-score support

0 1.00 1.00 1.00 20579668
1 0.47 1.00 0.64 25179

accuracy 1.00 20604847
macro avg 0.73 1.00 0.82 20604847
weighted avg 1.00 1.00 1.00 20604847

Test report
precision recall f1-score support

0 1.00 1.00 1.00 2058351
1 0.83 0.93 0.87 2087

accuracy 1.00 2060438
macro avg 0.91 0.96 0.94 2060438
weighted avg 1.00 1.00 1.00 2060438
Như em thấy, thì model không học được tốt trên dữ liệu training nhưng lại có kết quả rất tốt ở trên dữ liệu testing (Và cả ở trên validation set), và bây giờ em cảm thấy hơi khó hiểu về trường hợp như vầy. Em có hiểu model sẽ bị underfit nếu như model không thể học đủ kiến thức từ dữ liệu training, và như vậy thì model sẽ không dự đoán tốt được các dữ liệu tương lai, như dữ liệu test. Còn model sẽ overfit nếu như model học quá khớp với dữ liệu training, và nhớ toàn bộ các thông tin của dữ liệu đó thay vì học cách phân loại, nên vì thế model sẽ không thực hiện tốt việc phân loại. Tuy nhiên ở trường hợp của em, model học kém ở trên dữ liệu training, nhưng lại trả kết quả rất cao cho dữ liệu testing và validation (Em không gửi kèm dữ liệu validation ở đây, nhưng kết quả cũng na ná phần testing).   Vầy nên em muốn hỏi mọi người rằng bài toán của em hiện tại đang gặp vấn đề gì, và ở trong trường hợp nào ạ? Em có gửi kèm thêm một số thông tin đằng sau, gồm loss của model lúc training, learning curve và các ma trận confusion. Em cảm ơn mọi người ạ!
Loss của model (validation_0 là dữ liệu training, validation_1 là dữ liệu testing)
[0] validation_0-aucpr:0.75831 validation_0-logloss:0.67418 validation_1-aucpr:0.17989 validation_1-logloss:0.67417
[10] validation_0-aucpr:0.78157 validation_0-logloss:0.52305 validation_1-aucpr:0.42574 validation_1-logloss:0.51965
[20] validation_0-aucpr:0.83228 validation_0-logloss:0.41181 validation_1-aucpr:0.79299 validation_1-logloss:0.40593
[30] validation_0-aucpr:0.84335 validation_0-logloss:0.32956 validation_1-aucpr:0.82845 validation_1-logloss:0.32171
[40] validation_0-aucpr:0.86026 validation_0-logloss:0.26683 validation_1-aucpr:0.86401 validation_1-logloss:0.25788
[50] validation_0-aucpr:0.87519 validation_0-logloss:0.21770 validation_1-aucpr:0.86298 validation_1-logloss:0.20919
[60] validation_0-aucpr:0.88714 validation_0-logloss:0.17906 validation_1-aucpr:0.86130 validation_1-logloss:0.17034
[70] validation_0-aucpr:0.89531 validation_0-logloss:0.14839 validation_1-aucpr:0.86285 validation_1-logloss:0.14016
[80] validation_0-aucpr:0.89770 validation_0-logloss:0.12463 validation_1-aucpr:0.86329 validation_1-logloss:0.11545
[90] validation_0-aucpr:0.90004 validation_0-logloss:0.10519 validation_1-aucpr:0.86052 validation_1-logloss:0.09647
[100] validation_0-aucpr:0.90534 validation_0-logloss:0.08897 validation_1-aucpr:0.87044 validation_1-logloss:0.07986
[110] validation_0-aucpr:0.91044 validation_0-logloss:0.07617 validation_1-aucpr:0.86994 validation_1-logloss:0.06662
[120] validation_0-aucpr:0.91458 validation_0-logloss:0.06538 validation_1-aucpr:0.86962 validation_1-logloss:0.05589
[130] validation_0-aucpr:0.91902 validation_0-logloss:0.05645 validation_1-aucpr:0.87092 validation_1-logloss:0.04684
[140] validation_0-aucpr:0.92276 validation_0-logloss:0.04895 validation_1-aucpr:0.87258 validation_1-logloss:0.03967
[150] validation_0-aucpr:0.92713 validation_0-logloss:0.04308 validation_1-aucpr:0.87285 validation_1-logloss:0.03377
[160] validation_0-aucpr:0.93179 validation_0-logloss:0.03788 validation_1-aucpr:0.87703 validation_1-logloss:0.02851
[170] validation_0-aucpr:0.93487 validation_0-logloss:0.03361 validation_1-aucpr:0.87967 validation_1-logloss:0.02426
[180] validation_0-aucpr:0.93875 validation_0-logloss:0.03013 validation_1-aucpr:0.88027 validation_1-logloss:0.02093
[190] validation_0-aucpr:0.94333 validation_0-logloss:0.02688 validation_1-aucpr:0.88284 validation_1-logloss:0.01781
[200] validation_0-aucpr:0.94592 validation_0-logloss:0.02454 validation_1-aucpr:0.88497 validation_1-logloss:0.01577
[210] validation_0-aucpr:0.95043 validation_0-logloss:0.02236 validation_1-aucpr:0.89025 validation_1-logloss:0.01363
[220] validation_0-aucpr:0.95464 validation_0-logloss:0.02033 validation_1-aucpr:0.89146 validation_1-logloss:0.01172
[230] validation_0-aucpr:0.95761 validation_0-logloss:0.01880 validation_1-aucpr:0.89327 validation_1-logloss:0.01044
[240] validation_0-aucpr:0.96080 validation_0-logloss:0.01747 validation_1-aucpr:0.89531 validation_1-logloss:0.00912
[250] validation_0-aucpr:0.96417 validation_0-logloss:0.01625 validation_1-aucpr:0.89891 validation_1-logloss:0.00802
[260] validation_0-aucpr:0.96675 validation_0-logloss:0.01519 validation_1-aucpr:0.90279 validation_1-logloss:0.00712
[270] validation_0-aucpr:0.96898 validation_0-logloss:0.01434 validation_1-aucpr:0.90530 validation_1-logloss:0.00645
[280] validation_0-aucpr:0.97143 validation_0-logloss:0.01353 validation_1-aucpr:0.90629 validation_1-logloss:0.00573
[290] validation_0-aucpr:0.97334 validation_0-logloss:0.01284 validation_1-aucpr:0.90836 validation_1-logloss:0.00520
[300] validation_0-aucpr:0.97506 validation_0-logloss:0.01216 validation_1-aucpr:0.90954 validation_1-logloss:0.00468
[310] validation_0-aucpr:0.97660 validation_0-logloss:0.01161 validation_1-aucpr:0.91150 validation_1-logloss:0.00427
[320] validation_0-aucpr:0.97800 validation_0-logloss:0.01108 validation_1-aucpr:0.91411 validation_1-logloss:0.00386
[330] validation_0-aucpr:0.97927 validation_0-logloss:0.01068 validation_1-aucpr:0.91551 validation_1-logloss:0.00361
[340] validation_0-aucpr:0.98054 validation_0-logloss:0.01019 validation_1-aucpr:0.91600 validation_1-logloss:0.00323
[350] validation_0-aucpr:0.98177 validation_0-logloss:0.00977 validation_1-aucpr:0.91776 validation_1-logloss:0.00299
[360] validation_0-aucpr:0.98272 validation_0-logloss:0.00938 validation_1-aucpr:0.92028 validation_1-logloss:0.00275
[370] validation_0-aucpr:0.98370 validation_0-logloss:0.00903 validation_1-aucpr:0.92015 validation_1-logloss:0.00256
[380] validation_0-aucpr:0.98444 validation_0-logloss:0.00877 validation_1-aucpr:0.92196 validation_1-logloss:0.00242
[390] validation_0-aucpr:0.98514 validation_0-logloss:0.00851 validation_1-aucpr:0.92389 validation_1-logloss:0.00229
[400] validation_0-aucpr:0.98580 validation_0-logloss:0.00828 validation_1-aucpr:0.92348 validation_1-logloss:0.00219
[410] validation_0-aucpr:0.98643 validation_0-logloss:0.00801 validation_1-aucpr:0.92514 validation_1-logloss:0.00203
[420] validation_0-aucpr:0.98711 validation_0-logloss:0.00774 validation_1-aucpr:0.92575 validation_1-logloss:0.00189
[430] validation_0-aucpr:0.98774 validation_0-logloss:0.00750 validation_1-aucpr:0.92427 validation_1-logloss:0.00177
[440] validation_0-aucpr:0.98832 validation_0-logloss:0.00725 validation_1-aucpr:0.92531 validation_1-logloss:0.00164
[450] validation_0-aucpr:0.98887 validation_0-logloss:0.00708 validation_1-aucpr:0.92623 validation_1-logloss:0.00160
[460] validation_0-aucpr:0.98931 validation_0-logloss:0.00690 validation_1-aucpr:0.92806 validation_1-logloss:0.00151
[470] validation_0-aucpr:0.98963 validation_0-logloss:0.00674 validation_1-aucpr:0.92860 validation_1-logloss:0.00146
[480] validation_0-aucpr:0.99005 validation_0-logloss:0.00656 validation_1-aucpr:0.92980 validation_1-logloss:0.00140
[490] validation_0-aucpr:0.99038 validation_0-logloss:0.00642 validation_1-aucpr:0.93051 validation_1-logloss:0.00135
[500] validation_0-aucpr:0.99077 validation_0-logloss:0.00628 validation_1-aucpr:0.93089 validation_1-logloss:0.00131
[510] validation_0-aucpr:0.99108 validation_0-logloss:0.00613 validation_1-aucpr:0.93270 validation_1-logloss:0.00126
[520] validation_0-aucpr:0.99138 validation_0-logloss:0.00601 validation_1-aucpr:0.93254 validation_1-logloss:0.00122
[530] validation_0-aucpr:0.99166 validation_0-logloss:0.00590 validation_1-aucpr:0.93199 validation_1-logloss:0.00119
[540] validation_0-aucpr:0.99197 validation_0-logloss:0.00577 validation_1-aucpr:0.93318 validation_1-logloss:0.00116
[550] validation_0-aucpr:0.99224 validation_0-logloss:0.00566 validation_1-aucpr:0.93408 validation_1-logloss:0.00112
[560] validation_0-aucpr:0.99250 validation_0-logloss:0.00554 validation_1-aucpr:0.93327 validation_1-logloss:0.00109
[570] validation_0-aucpr:0.99278 validation_0-logloss:0.00542 validation_1-aucpr:0.93397 validation_1-logloss:0.00106
[580] validation_0-aucpr:0.99300 validation_0-logloss:0.00530 validation_1-aucpr:0.93339 validation_1-logloss:0.00102
[590] validation_0-aucpr:0.99324 validation_0-logloss:0.00521 validation_1-aucpr:0.93372 validation_1-logloss:0.00100
[599] validation_0-aucpr:0.99338 validation_0-logloss:0.00513 validation_1-aucpr:0.93378 validation_1-logloss:0.00099","Dạ em chào mọi người, em đang có một chút khúc mắc trong bài toán binary classification sử dụng dataset này https://github.com/IBM/TabFormer/tree/main/data/credit_card, cụ thể hơn là điểm F1 training của model thấp, nhưng điểm F1 của validation và testing thì lại cao. Về dataset, đây là dataset về giao dịch thẻ tín dụng, và mục tiêu dự đoán là xem cuộc giao dịch đấy có phải lừa đảo hay không (""Is Fraud?""). Phân bố class này rất mất cân bằng, cụ thể là: 24 triệu cuộc giao dịch 30,000 giao dịch lừa đảo (0.1% of tổng giao dịch) Sau phần tiền xử lý dữ liệu, em có chia ra 3 sets Training (Cuộc giao dịch trước 2018), Validation (trong 2018) và Testing (sau 2018), với phần phân bố class như sau (0 là giao dịch không lừa đảo, 1 là giao dịch lừa đảo) Training Data: Class 0: 20579668 Class 1: 25179 Validation Data: Class 0: 1719124 Class 1: 2491 Testing Data: Class 0: 2058351 Class 1: 2087 Em hiện tại đang sử dụng model XGBoost để dự đoán, và em có thu lại được một số kết quả như sau: F1 Score on Training Data : 0.57417479049085 F1 Score on Testing Data : 0.8719438392641008 PR AUC score on Training Data : 0.9918559271777408 PR AUC score on Testing Data : 0.9077624174590952 Training report precision recall f1-score support 0 1.00 1.00 1.00 20579668 1 0.47 1.00 0.64 25179 accuracy 1.00 20604847 macro avg 0.73 1.00 0.82 20604847 weighted avg 1.00 1.00 1.00 20604847 Test report precision recall f1-score support 0 1.00 1.00 1.00 2058351 1 0.83 0.93 0.87 2087 accuracy 1.00 2060438 macro avg 0.91 0.96 0.94 2060438 weighted avg 1.00 1.00 1.00 2060438 Như em thấy, thì model không học được tốt trên dữ liệu training nhưng lại có kết quả rất tốt ở trên dữ liệu testing (Và cả ở trên validation set), và bây giờ em cảm thấy hơi khó hiểu về trường hợp như vầy. Em có hiểu model sẽ bị underfit nếu như model không thể học đủ kiến thức từ dữ liệu training, và như vậy thì model sẽ không dự đoán tốt được các dữ liệu tương lai, như dữ liệu test. Còn model sẽ overfit nếu như model học quá khớp với dữ liệu training, và nhớ toàn bộ các thông tin của dữ liệu đó thay vì học cách phân loại, nên vì thế model sẽ không thực hiện tốt việc phân loại. Tuy nhiên ở trường hợp của em, model học kém ở trên dữ liệu training, nhưng lại trả kết quả rất cao cho dữ liệu testing và validation (Em không gửi kèm dữ liệu validation ở đây, nhưng kết quả cũng na ná phần testing). Vầy nên em muốn hỏi mọi người rằng bài toán của em hiện tại đang gặp vấn đề gì, và ở trong trường hợp nào ạ? Em có gửi kèm thêm một số thông tin đằng sau, gồm loss của model lúc training, learning curve và các ma trận confusion. Em cảm ơn mọi người ạ! Loss của model (validation_0 là dữ liệu training, validation_1 là dữ liệu testing) [0] validation_0-aucpr:0.75831 validation_0-logloss:0.67418 validation_1-aucpr:0.17989 validation_1-logloss:0.67417 [10] validation_0-aucpr:0.78157 validation_0-logloss:0.52305 validation_1-aucpr:0.42574 validation_1-logloss:0.51965 [20] validation_0-aucpr:0.83228 validation_0-logloss:0.41181 validation_1-aucpr:0.79299 validation_1-logloss:0.40593 [30] validation_0-aucpr:0.84335 validation_0-logloss:0.32956 validation_1-aucpr:0.82845 validation_1-logloss:0.32171 [40] validation_0-aucpr:0.86026 validation_0-logloss:0.26683 validation_1-aucpr:0.86401 validation_1-logloss:0.25788 [50] validation_0-aucpr:0.87519 validation_0-logloss:0.21770 validation_1-aucpr:0.86298 validation_1-logloss:0.20919 [60] validation_0-aucpr:0.88714 validation_0-logloss:0.17906 validation_1-aucpr:0.86130 validation_1-logloss:0.17034 [70] validation_0-aucpr:0.89531 validation_0-logloss:0.14839 validation_1-aucpr:0.86285 validation_1-logloss:0.14016 [80] validation_0-aucpr:0.89770 validation_0-logloss:0.12463 validation_1-aucpr:0.86329 validation_1-logloss:0.11545 [90] validation_0-aucpr:0.90004 validation_0-logloss:0.10519 validation_1-aucpr:0.86052 validation_1-logloss:0.09647 [100] validation_0-aucpr:0.90534 validation_0-logloss:0.08897 validation_1-aucpr:0.87044 validation_1-logloss:0.07986 [110] validation_0-aucpr:0.91044 validation_0-logloss:0.07617 validation_1-aucpr:0.86994 validation_1-logloss:0.06662 [120] validation_0-aucpr:0.91458 validation_0-logloss:0.06538 validation_1-aucpr:0.86962 validation_1-logloss:0.05589 [130] validation_0-aucpr:0.91902 validation_0-logloss:0.05645 validation_1-aucpr:0.87092 validation_1-logloss:0.04684 [140] validation_0-aucpr:0.92276 validation_0-logloss:0.04895 validation_1-aucpr:0.87258 validation_1-logloss:0.03967 [150] validation_0-aucpr:0.92713 validation_0-logloss:0.04308 validation_1-aucpr:0.87285 validation_1-logloss:0.03377 [160] validation_0-aucpr:0.93179 validation_0-logloss:0.03788 validation_1-aucpr:0.87703 validation_1-logloss:0.02851 [170] validation_0-aucpr:0.93487 validation_0-logloss:0.03361 validation_1-aucpr:0.87967 validation_1-logloss:0.02426 [180] validation_0-aucpr:0.93875 validation_0-logloss:0.03013 validation_1-aucpr:0.88027 validation_1-logloss:0.02093 [190] validation_0-aucpr:0.94333 validation_0-logloss:0.02688 validation_1-aucpr:0.88284 validation_1-logloss:0.01781 [200] validation_0-aucpr:0.94592 validation_0-logloss:0.02454 validation_1-aucpr:0.88497 validation_1-logloss:0.01577 [210] validation_0-aucpr:0.95043 validation_0-logloss:0.02236 validation_1-aucpr:0.89025 validation_1-logloss:0.01363 [220] validation_0-aucpr:0.95464 validation_0-logloss:0.02033 validation_1-aucpr:0.89146 validation_1-logloss:0.01172 [230] validation_0-aucpr:0.95761 validation_0-logloss:0.01880 validation_1-aucpr:0.89327 validation_1-logloss:0.01044 [240] validation_0-aucpr:0.96080 validation_0-logloss:0.01747 validation_1-aucpr:0.89531 validation_1-logloss:0.00912 [250] validation_0-aucpr:0.96417 validation_0-logloss:0.01625 validation_1-aucpr:0.89891 validation_1-logloss:0.00802 [260] validation_0-aucpr:0.96675 validation_0-logloss:0.01519 validation_1-aucpr:0.90279 validation_1-logloss:0.00712 [270] validation_0-aucpr:0.96898 validation_0-logloss:0.01434 validation_1-aucpr:0.90530 validation_1-logloss:0.00645 [280] validation_0-aucpr:0.97143 validation_0-logloss:0.01353 validation_1-aucpr:0.90629 validation_1-logloss:0.00573 [290] validation_0-aucpr:0.97334 validation_0-logloss:0.01284 validation_1-aucpr:0.90836 validation_1-logloss:0.00520 [300] validation_0-aucpr:0.97506 validation_0-logloss:0.01216 validation_1-aucpr:0.90954 validation_1-logloss:0.00468 [310] validation_0-aucpr:0.97660 validation_0-logloss:0.01161 validation_1-aucpr:0.91150 validation_1-logloss:0.00427 [320] validation_0-aucpr:0.97800 validation_0-logloss:0.01108 validation_1-aucpr:0.91411 validation_1-logloss:0.00386 [330] validation_0-aucpr:0.97927 validation_0-logloss:0.01068 validation_1-aucpr:0.91551 validation_1-logloss:0.00361 [340] validation_0-aucpr:0.98054 validation_0-logloss:0.01019 validation_1-aucpr:0.91600 validation_1-logloss:0.00323 [350] validation_0-aucpr:0.98177 validation_0-logloss:0.00977 validation_1-aucpr:0.91776 validation_1-logloss:0.00299 [360] validation_0-aucpr:0.98272 validation_0-logloss:0.00938 validation_1-aucpr:0.92028 validation_1-logloss:0.00275 [370] validation_0-aucpr:0.98370 validation_0-logloss:0.00903 validation_1-aucpr:0.92015 validation_1-logloss:0.00256 [380] validation_0-aucpr:0.98444 validation_0-logloss:0.00877 validation_1-aucpr:0.92196 validation_1-logloss:0.00242 [390] validation_0-aucpr:0.98514 validation_0-logloss:0.00851 validation_1-aucpr:0.92389 validation_1-logloss:0.00229 [400] validation_0-aucpr:0.98580 validation_0-logloss:0.00828 validation_1-aucpr:0.92348 validation_1-logloss:0.00219 [410] validation_0-aucpr:0.98643 validation_0-logloss:0.00801 validation_1-aucpr:0.92514 validation_1-logloss:0.00203 [420] validation_0-aucpr:0.98711 validation_0-logloss:0.00774 validation_1-aucpr:0.92575 validation_1-logloss:0.00189 [430] validation_0-aucpr:0.98774 validation_0-logloss:0.00750 validation_1-aucpr:0.92427 validation_1-logloss:0.00177 [440] validation_0-aucpr:0.98832 validation_0-logloss:0.00725 validation_1-aucpr:0.92531 validation_1-logloss:0.00164 [450] validation_0-aucpr:0.98887 validation_0-logloss:0.00708 validation_1-aucpr:0.92623 validation_1-logloss:0.00160 [460] validation_0-aucpr:0.98931 validation_0-logloss:0.00690 validation_1-aucpr:0.92806 validation_1-logloss:0.00151 [470] validation_0-aucpr:0.98963 validation_0-logloss:0.00674 validation_1-aucpr:0.92860 validation_1-logloss:0.00146 [480] validation_0-aucpr:0.99005 validation_0-logloss:0.00656 validation_1-aucpr:0.92980 validation_1-logloss:0.00140 [490] validation_0-aucpr:0.99038 validation_0-logloss:0.00642 validation_1-aucpr:0.93051 validation_1-logloss:0.00135 [500] validation_0-aucpr:0.99077 validation_0-logloss:0.00628 validation_1-aucpr:0.93089 validation_1-logloss:0.00131 [510] validation_0-aucpr:0.99108 validation_0-logloss:0.00613 validation_1-aucpr:0.93270 validation_1-logloss:0.00126 [520] validation_0-aucpr:0.99138 validation_0-logloss:0.00601 validation_1-aucpr:0.93254 validation_1-logloss:0.00122 [530] validation_0-aucpr:0.99166 validation_0-logloss:0.00590 validation_1-aucpr:0.93199 validation_1-logloss:0.00119 [540] validation_0-aucpr:0.99197 validation_0-logloss:0.00577 validation_1-aucpr:0.93318 validation_1-logloss:0.00116 [550] validation_0-aucpr:0.99224 validation_0-logloss:0.00566 validation_1-aucpr:0.93408 validation_1-logloss:0.00112 [560] validation_0-aucpr:0.99250 validation_0-logloss:0.00554 validation_1-aucpr:0.93327 validation_1-logloss:0.00109 [570] validation_0-aucpr:0.99278 validation_0-logloss:0.00542 validation_1-aucpr:0.93397 validation_1-logloss:0.00106 [580] validation_0-aucpr:0.99300 validation_0-logloss:0.00530 validation_1-aucpr:0.93339 validation_1-logloss:0.00102 [590] validation_0-aucpr:0.99324 validation_0-logloss:0.00521 validation_1-aucpr:0.93372 validation_1-logloss:0.00100 [599] validation_0-aucpr:0.99338 validation_0-logloss:0.00513 validation_1-aucpr:0.93378 validation_1-logloss:0.00099",,,"#Q&A, #machine_learning",,
"Chào mọi người, sau nhiều tháng chậm trễ, em cũng đã hoàn thành đề tài OCR cho chữ Hán-Nôm mà đã được mọi người góp ý qua trước đây.  
Link bài post cũ: https://www.facebook.com/groups/machinelearningcoban/posts/1423658951424841 
Hiện tại, nhóm em đã xây dựng thành công 1 bộ dữ liệu OCR dành cho các tài liệu lịch sử cũ được viết tay bằng chữ Hán-Nôm với 2953 Page và 38318 Patch, phục vụ cho cả 2 bài toán Text Detection và Text Recognition. Bộ dữ liệu dựa trên 3 tác phẩm lớn gồm: 
- Trọn bộ 24 quyển của Đại Việt Sử Ký Toàn Thư. 
- Truyện Kiều các bản năm 1866, 1871, và 1872. 
- Lục Vân Tiên.  
Ngoài ra, em cũng đã cài đặt và thử nghiệm các mô hình theo sequence level thay vì character level như các công trình trước, trong đó:
- Text Detection: sử dụng 2 mô hình đại diện cho 2 hướng tiếp cận Regression-based và Segmentation-based, được tham khảo từ cuộc thi về Scene Text Detection trên bộ dữ liệu ICDAR 2015.
- Text Recognition: giải quyết theo 4 hướng tiếp cận khác nhau cùng với 2 phương pháp huấn luyện gồm Fine-tuning trên 1 bộ dữ liệu Synthetic với hơn 100k Patch và Retraining từ đầu trên dữ liệu thật để so sánh kết quả với khi Fine-tuning.
GitHub: https://github.com/ds4v/NomNaOCR
Dataset: https://www.kaggle.com/datasets/quandang/nomnaocr 
Rất mong nhận được các góp ý từ mọi người để em có thể học hỏi cũng như để cải thiện thêm cho project trong tương lai.
 — với Nguyễn Đức Duy Anh.","Chào mọi người, sau nhiều tháng chậm trễ, em cũng đã hoàn thành đề tài OCR cho chữ Hán-Nôm mà đã được mọi người góp ý qua trước đây. Link bài post cũ: https://www.facebook.com/groups/machinelearningcoban/posts/1423658951424841 Hiện tại, nhóm em đã xây dựng thành công 1 bộ dữ liệu OCR dành cho các tài liệu lịch sử cũ được viết tay bằng chữ Hán-Nôm với 2953 Page và 38318 Patch, phục vụ cho cả 2 bài toán Text Detection và Text Recognition. Bộ dữ liệu dựa trên 3 tác phẩm lớn gồm: - Trọn bộ 24 quyển của Đại Việt Sử Ký Toàn Thư. - Truyện Kiều các bản năm 1866, 1871, và 1872. - Lục Vân Tiên. Ngoài ra, em cũng đã cài đặt và thử nghiệm các mô hình theo sequence level thay vì character level như các công trình trước, trong đó: - Text Detection: sử dụng 2 mô hình đại diện cho 2 hướng tiếp cận Regression-based và Segmentation-based, được tham khảo từ cuộc thi về Scene Text Detection trên bộ dữ liệu ICDAR 2015. - Text Recognition: giải quyết theo 4 hướng tiếp cận khác nhau cùng với 2 phương pháp huấn luyện gồm Fine-tuning trên 1 bộ dữ liệu Synthetic với hơn 100k Patch và Retraining từ đầu trên dữ liệu thật để so sánh kết quả với khi Fine-tuning. GitHub: https://github.com/ds4v/NomNaOCR Dataset: https://www.kaggle.com/datasets/quandang/nomnaocr Rất mong nhận được các góp ý từ mọi người để em có thể học hỏi cũng như để cải thiện thêm cho project trong tương lai. — với Nguyễn Đức Duy Anh.",,,"#sharing, #cv, #deep_learning",,
Nghề của tương lai: #PromptEngineering,Nghề của tương lai:,#PromptEngineering,,#sharing,,
"Chào các anh,chị trong forum.
Em hiện đang học về GIS (hệ thống thông tin địa lý). Các anh chị cho em hỏi liệu Machine Learning có áp dụng được vào GIS không ạ và áp dụng như thế nào vậy ạ.
Em cảm ơn mọi người ạ🥰🥰","Chào các anh,chị trong forum. Em hiện đang học về GIS (hệ thống thông tin địa lý). Các anh chị cho em hỏi liệu Machine Learning có áp dụng được vào GIS không ạ và áp dụng như thế nào vậy ạ. Em cảm ơn mọi người ạ",,,"#Q&A, #machine_learning",,
"Xin chào mọi người,
Hè này em tính đầu tư thời gian để học Machine Learning. Trên Coursera có hai khóa Machine Learning tốt nhất đó là khóa Machine Learning Specialization của DeepLearning.ai và khóa Machine Learning Professional Certificate của IBM.
Mọi người cho em xin ý kiến là khóa nào là khóa tốt nhất và chuyên sâu nhất.
Em không cần khóa beginner-friendly, em chỉ cần khóa nào dạy đầy đủ nhất.
Cảm ơn mọi người nhiều.","Xin chào mọi người, Hè này em tính đầu tư thời gian để học Machine Learning. Trên Coursera có hai khóa Machine Learning tốt nhất đó là khóa Machine Learning Specialization của DeepLearning.ai và khóa Machine Learning Professional Certificate của IBM. Mọi người cho em xin ý kiến là khóa nào là khóa tốt nhất và chuyên sâu nhất. Em không cần khóa beginner-friendly, em chỉ cần khóa nào dạy đầy đủ nhất. Cảm ơn mọi người nhiều.",,,"#Q&A, #machine_learning",,
Chuyện là em đang làm 1 project về image classfication trên mobile. Em có quantizition model xuống int8. Em có xuất output thì nó ra 1 mảng giá trí như trong hình. Em muốn convert nó thành phần trăm tỉ lệ chính xác. Lên mạng kiếm tài liệu nhưng không biết như thế nào. Anh chị nào biết chỉ em với ạ. Em cám ơn.,Chuyện là em đang làm 1 project về image classfication trên mobile. Em có quantizition model xuống int8. Em có xuất output thì nó ra 1 mảng giá trí như trong hình. Em muốn convert nó thành phần trăm tỉ lệ chính xác. Lên mạng kiếm tài liệu nhưng không biết như thế nào. Anh chị nào biết chỉ em với ạ. Em cám ơn.,,,"#Q&A, #cv",,
"Chào các bạn,
Tuần trước mình nhận được một email rất dài nhờ tư vấn cho một dự án tiên đoán mức độ ung thư vú trong tương lai cho phụ nữ Việt Nam. Mình đã nói chuyện với team và thấy rằng đây là một dự án ý nghĩa, mình đã nhận lời kết nối dự án với cộng đồng.
Hiện nhóm đã có khá nhiều dữ liệu, cả về ảnh lẫn dữ liệu bảng, có chuyên gia về mặt y tế và có các bạn lập trình viên hỗ trợ. Tuy nhiên vẫn còn nhiều điểm có thể cải thiện bằng cả computer vision và data science. Theo mình đây cũng là một cơ hội tốt cho các bạn nghiên cứu về ML cho Y Sinh được tiếp cận nguồn dữ liệu tốt và làm với một dự án thực tế.
Vậy nên mình viết post này giúp kết nối các bạn có kinh nghiệm trong lĩnh vực này với dự án. Thông tin chi tiết về dự án và cách liên hệ hợp tác được cho trong comment.
Cảm ơn các bạn.","Chào các bạn, Tuần trước mình nhận được một email rất dài nhờ tư vấn cho một dự án tiên đoán mức độ ung thư vú trong tương lai cho phụ nữ Việt Nam. Mình đã nói chuyện với team và thấy rằng đây là một dự án ý nghĩa, mình đã nhận lời kết nối dự án với cộng đồng. Hiện nhóm đã có khá nhiều dữ liệu, cả về ảnh lẫn dữ liệu bảng, có chuyên gia về mặt y tế và có các bạn lập trình viên hỗ trợ. Tuy nhiên vẫn còn nhiều điểm có thể cải thiện bằng cả computer vision và data science. Theo mình đây cũng là một cơ hội tốt cho các bạn nghiên cứu về ML cho Y Sinh được tiếp cận nguồn dữ liệu tốt và làm với một dự án thực tế. Vậy nên mình viết post này giúp kết nối các bạn có kinh nghiệm trong lĩnh vực này với dự án. Thông tin chi tiết về dự án và cách liên hệ hợp tác được cho trong comment. Cảm ơn các bạn.",,,"#sharing, #machine_learning",,
"Mấy ngày trước mình có tạo ra template để viết luận văn/luận án sử dụng Typst tại đây https://github.com/linhduongtuan/BKHN-Thesis_template_typst.
Hôm nay, mình xin gửi đến các bạn 1 template khác với mục đích viết Slides để báo cáo. Xin tham khảo tại đây: https://github.com/linhduongtuan/DTU-typst-presentation
Hi vọng với combo thư viện này sẽ giúp chúng ta có thêm công cụ để soạn thảo văn bản và bài thuyết trình một cách hiệu quả.
Chúc các bạn cuối tuần vui vẻ.
Ps. Nếu các bạn thấy repositories của mình có ích, xin đừng tiếc một Star cho mỗi thư viện nói trên.
Trân trọng","Mấy ngày trước mình có tạo ra template để viết luận văn/luận án sử dụng Typst tại đây https://github.com/linhduongtuan/BKHN-Thesis_template_typst. Hôm nay, mình xin gửi đến các bạn 1 template khác với mục đích viết Slides để báo cáo. Xin tham khảo tại đây: https://github.com/linhduongtuan/DTU-typst-presentation Hi vọng với combo thư viện này sẽ giúp chúng ta có thêm công cụ để soạn thảo văn bản và bài thuyết trình một cách hiệu quả. Chúc các bạn cuối tuần vui vẻ. Ps. Nếu các bạn thấy repositories của mình có ích, xin đừng tiếc một Star cho mỗi thư viện nói trên. Trân trọng",,,#sharing,,
Xin chào các anh chị và các bạn. Mình làm về data ở Tây Ban Nha. Mình muốn join vào dự án nào đó ở VN mà có liên quan tới Crypto Curency thì tốt. Mình không yêu cầu trả lương chỉ mong được học hỏi từ các anh chị và các bạn.,Xin chào các anh chị và các bạn. Mình làm về data ở Tây Ban Nha. Mình muốn join vào dự án nào đó ở VN mà có liên quan tới Crypto Curency thì tốt. Mình không yêu cầu trả lương chỉ mong được học hỏi từ các anh chị và các bạn.,,,"#sharing, #data",,
"Em chào anh chị trong nhóm ạ. Hiện em đã tốt nghiệp Đại học Bách Khoa Hà Nội, em đang tìm việc Intern/fresher về Al tại Hà Nội. Không biết anh/chị ở công ty nào có còn open cho vị trí Intern/fresher không ạ?","Em chào anh chị trong nhóm ạ. Hiện em đã tốt nghiệp Đại học Bách Khoa Hà Nội, em đang tìm việc Intern/fresher về Al tại Hà Nội. Không biết anh/chị ở công ty nào có còn open cho vị trí Intern/fresher không ạ?",,,"#Q&A, #machine_learning",,
"Chào mọi người em đang tìm hiểu về chatbot tiếng việt sử dụng Underthesea. Nhưng em thấy ít tài liệu về đề tài này, mọi người có tài liệu hoặc code liên quan cho em xin tham khảo với ạ. Em cảm ơn","Chào mọi người em đang tìm hiểu về chatbot tiếng việt sử dụng Underthesea. Nhưng em thấy ít tài liệu về đề tài này, mọi người có tài liệu hoặc code liên quan cho em xin tham khảo với ạ. Em cảm ơn",,,"#Q&A, #nlp",,
Một bài báo của DeepMind sử dụng AlphaDev tìm ra được thuật toán sắp xếp hiệu quả hơn. Chi tiết bài báo:,Một bài báo của DeepMind sử dụng AlphaDev tìm ra được thuật toán sắp xếp hiệu quả hơn. Chi tiết bài báo:,,,"#sharing, #machine_learning",,
"Not only generating images but also drawing, editing, inpainting, outpainting, and removal. https://lunai.art/imagetool","Not only generating images but also drawing, editing, inpainting, outpainting, and removal. https://lunai.art/imagetool",,,,,
"[Sharing Generative AI google free courses]
Google has created a Generative AI learning path with 9 FREE courses!
Topics cover:
- Intro to LLMs
- Attention Mechanism
- Image Generation/Captioning
- Intro to Responsible AI
From the fundamentals of LLMs to creating & deploying generative AI solutions!",[Sharing Generative AI google free courses] Google has created a Generative AI learning path with 9 FREE courses! Topics cover: - Intro to LLMs - Attention Mechanism - Image Generation/Captioning - Intro to Responsible AI From the fundamentals of LLMs to creating & deploying generative AI solutions!,,,,,
"*** Câu hỏi phỏng vấn ***
Em là AI engineer, kinh nghiệm 2.5 năm, em sắp đi phỏng vấn mà tại đó giờ em làm 1 công ty, chủ yếu làm computer vision nên hơi tự ti 1 tí.
Mỗi người đi qua thấy post của em có thể cho em 1 vài câu hỏi phỏng vấn liên quan hoặc 1 bài toán gì mọi người được hỏi để giải quyết nhanh trong cuộc phỏng vấn được ko ạ?
*** Những kĩ năng của em
+ Pytorch, Tensoflow
+ Mô hình Yolov7, mask rcnn, efficientnet, efficientdet,...
+ Project thì chủ yếu liên quan đến defect detection, classification, segmentation, theo dõi đối tượng bằng deepsort.","*** Câu hỏi phỏng vấn *** Em là AI engineer, kinh nghiệm 2.5 năm, em sắp đi phỏng vấn mà tại đó giờ em làm 1 công ty, chủ yếu làm computer vision nên hơi tự ti 1 tí. Mỗi người đi qua thấy post của em có thể cho em 1 vài câu hỏi phỏng vấn liên quan hoặc 1 bài toán gì mọi người được hỏi để giải quyết nhanh trong cuộc phỏng vấn được ko ạ? *** Những kĩ năng của em + Pytorch, Tensoflow + Mô hình Yolov7, mask rcnn, efficientnet, efficientdet,... + Project thì chủ yếu liên quan đến defect detection, classification, segmentation, theo dõi đối tượng bằng deepsort.",,,"#Q&A, #cv, #machine_learning",,
"Cho hỏi có ai biết trang opnai.net ko? Mình thấy nó trả lời câu hỏi cũng giống chat gpt, ko biết có phải trang mạo danh nhằm mục đích gì ở người dùng ko hay là trang này muốn giúp cho người việt được dùng chatgpt tại vn?","Cho hỏi có ai biết trang opnai.net ko? Mình thấy nó trả lời câu hỏi cũng giống chat gpt, ko biết có phải trang mạo danh nhằm mục đích gì ở người dùng ko hay là trang này muốn giúp cho người việt được dùng chatgpt tại vn?",,,"#Q&A, #nlp",,
Tool cho ae dùng AI chỉnh sửa ảnh.,Tool cho ae dùng AI chỉnh sửa ảnh.,,,"#sharing, #machine_learning",,
"Chắc các bạn trong forum này nhiều người sử dụng LaTeX/Overleaf để soạn thảo văn bản, luận văn, bài báo,... Mặc dù cộng đồng LaTex rất mạnh, nhưng mình rất ghét khi muốn xuất file phải bấm `compile`, chờ nó quay khá lâu, rồi lỗi (nếu có) tương đối khó đọc. Gần đây ngôn ngữ Rust nổi lên thay thế C/C++, và rồi Typst (https://github.com/typst/typst) và (online ver. tương tự Overleaf https://typst.app/) được phát triển như là trình biên soạn mới, và mình nghĩ Typst tuy còn non trẻ nhưng nó sẽ có chỗ đứng của riêng nó. Typst xuất file pdf gần như theo thời gian thật mà không cần compile. Về syntax của Typst tương đối dễ đọc. Vậy nên mình thử tạo ra 1 template cho việc biên soạn luận văn sử dụng Typst tại đây https://github.com/linhduongtuan/BKHN-Thesis_template_typst. Hi vọng, với template này các bạn có thể từng bước học cách sử dụng Typst và xa hơn là học Rust.
Chúc mọi người buổi tối vui vẻ!
Nếu các bạn thấy repository của mình thú vị, xin đừng tiếc 1 Star cho nó nhé. Trân trọng cảm ơn.","Chắc các bạn trong forum này nhiều người sử dụng LaTeX/Overleaf để soạn thảo văn bản, luận văn, bài báo,... Mặc dù cộng đồng LaTex rất mạnh, nhưng mình rất ghét khi muốn xuất file phải bấm `compile`, chờ nó quay khá lâu, rồi lỗi (nếu có) tương đối khó đọc. Gần đây ngôn ngữ Rust nổi lên thay thế C/C++, và rồi Typst (https://github.com/typst/typst) và (online ver. tương tự Overleaf https://typst.app/) được phát triển như là trình biên soạn mới, và mình nghĩ Typst tuy còn non trẻ nhưng nó sẽ có chỗ đứng của riêng nó. Typst xuất file pdf gần như theo thời gian thật mà không cần compile. Về syntax của Typst tương đối dễ đọc. Vậy nên mình thử tạo ra 1 template cho việc biên soạn luận văn sử dụng Typst tại đây https://github.com/linhduongtuan/BKHN-Thesis_template_typst. Hi vọng, với template này các bạn có thể từng bước học cách sử dụng Typst và xa hơn là học Rust. Chúc mọi người buổi tối vui vẻ! Nếu các bạn thấy repository của mình thú vị, xin đừng tiếc 1 Star cho nó nhé. Trân trọng cảm ơn.",,,#sharing,,
"Xin giới thiệu với mọi người một danh sách dài tổng hợp những textbooks xuất sắc về các chủ đề từ Machine Learning, Statistical Learning, Optimization, Optimal Transport, Algebraic Statisics, etc được chia sẻ miễn phí đến với cộng đồng bởi chính những tác giả viết ra những cuốn sách này, với tên tuổi lớn hàng đầu trong giới khoa học như Sutton (RL), Szeliski (CompVis), Hastie (Stats), Villani (OT)
Danh sách được tổng hợp và chia sẻ bởi Dr. Frank Nielsen: https://franknielsen.github.io/Books/CuratedBookLists.html","Xin giới thiệu với mọi người một danh sách dài tổng hợp những textbooks xuất sắc về các chủ đề từ Machine Learning, Statistical Learning, Optimization, Optimal Transport, Algebraic Statisics, etc được chia sẻ miễn phí đến với cộng đồng bởi chính những tác giả viết ra những cuốn sách này, với tên tuổi lớn hàng đầu trong giới khoa học như Sutton (RL), Szeliski (CompVis), Hastie (Stats), Villani (OT) Danh sách được tổng hợp và chia sẻ bởi Dr. Frank Nielsen: https://franknielsen.github.io/Books/CuratedBookLists.html",,,"#sharing, #machine_learning, #math",,
"Chào mọi người, mình đang scan khả năng sử dụng YoLo vào những ứng dụng trong nghiên cứu của mình tại Nauy. Cụ thể là kiểu như hình và link phía dưới trong nhưng là monitor các hạt rắn trong lò phản ứng của mình thông qua camera.
Trong nhóm này có ACE nào đã làm với YoLo không cho mình xin chút ít kinh nghiệm/Input, comment hoặc ib mình sẽ liên lạc trao đổi thêm nhé.
Cảm ơn mọi
https://www.youtube.com/watch?v=dm3FcR_WrvQ","Chào mọi người, mình đang scan khả năng sử dụng YoLo vào những ứng dụng trong nghiên cứu của mình tại Nauy. Cụ thể là kiểu như hình và link phía dưới trong nhưng là monitor các hạt rắn trong lò phản ứng của mình thông qua camera. Trong nhóm này có ACE nào đã làm với YoLo không cho mình xin chút ít kinh nghiệm/Input, comment hoặc ib mình sẽ liên lạc trao đổi thêm nhé. Cảm ơn mọi https://www.youtube.com/watch?v=dm3FcR_WrvQ",,,"#Q&A, #cv, #deep_learning",,
"Xin chào mọi người
Mình đang muốn tìm học về AI nhưng chưa biết bắt đầu từ đâu và cần học những gì
Mong muốn của mình là có thể sử dụng AI áp dụng vào lĩnh vực phần mềm,các tool hỗ trợ trong lĩnh vực phát triển web , app
Mong mọi người tư vấn lộ trình ạ","Xin chào mọi người Mình đang muốn tìm học về AI nhưng chưa biết bắt đầu từ đâu và cần học những gì Mong muốn của mình là có thể sử dụng AI áp dụng vào lĩnh vực phần mềm,các tool hỗ trợ trong lĩnh vực phát triển web , app Mong mọi người tư vấn lộ trình ạ",,,"#Q&A, #machine_learning",,
"Chào mọi người em có 2 câu hỏi này về universal approximation theorem:
1/ MLP có thể xấp xỉ skip connection và recurrent connection không?
2/ CNN có thể được cài đặt bằng MLP, nhưng cũng có thể coi MLP là CNN với kernel spatial size bằng 1x1. Vậy CNN và MLP cái nào ""tổng quát"" hơn? Nếu CNN ""tổng quát"" hơn, nó có riêng cho mình một approximation theorem không? Tương tự, RNN, mạng có skip connection, v.v. có approximation theorem cho riêng nó không?
Cảm ơn mọi người.","Chào mọi người em có 2 câu hỏi này về universal approximation theorem: 1/ MLP có thể xấp xỉ skip connection và recurrent connection không? 2/ CNN có thể được cài đặt bằng MLP, nhưng cũng có thể coi MLP là CNN với kernel spatial size bằng 1x1. Vậy CNN và MLP cái nào ""tổng quát"" hơn? Nếu CNN ""tổng quát"" hơn, nó có riêng cho mình một approximation theorem không? Tương tự, RNN, mạng có skip connection, v.v. có approximation theorem cho riêng nó không? Cảm ơn mọi người.",,,"#Q&A, #deep_learning",,
"Em chào anh chị trong nhóm ạ. Hiện em đã tốt nghiệp Đại học Bách Khoa Hà Nội, em đang tìm việc Intern/fresher về Machine Learning/AI/Deep Learning tại Hà Nội. Không biết anh/chị ở công ty nào có còn open cho vị trí Intern/fresher không ạ? Em sẽ chủ động inbox gửi CV ạ.","Em chào anh chị trong nhóm ạ. Hiện em đã tốt nghiệp Đại học Bách Khoa Hà Nội, em đang tìm việc Intern/fresher về Machine Learning/AI/Deep Learning tại Hà Nội. Không biết anh/chị ở công ty nào có còn open cho vị trí Intern/fresher không ạ? Em sẽ chủ động inbox gửi CV ạ.",,,"#Q&A, #machine_learning",,
"Chào mọi người, mình xin giới thiệu chuỗi seminar thú vị về việc cung cấp nền tảng về việc triển khai ML models với những kiến thức trong industry.
1. Seminar 1 (5/6): MLOps Marathon Sample Solution
2. Seminar 2 (6/6): Fundamental Series: Linux basics
3. Seminar 3 (7/6): Fundamental Series: Git, Github and Github actions
4. Seminar 4 (8/6): Containerization and Orchestration
5. Seminar 5 (13/6): WebAPI, FastAPI and NGINX
6. Seminar 6 (14/6): Data Drift and Solutions","Chào mọi người, mình xin giới thiệu chuỗi seminar thú vị về việc cung cấp nền tảng về việc triển khai ML models với những kiến thức trong industry. 1. Seminar 1 (5/6): MLOps Marathon Sample Solution 2. Seminar 2 (6/6): Fundamental Series: Linux basics 3. Seminar 3 (7/6): Fundamental Series: Git, Github and Github actions 4. Seminar 4 (8/6): Containerization and Orchestration 5. Seminar 5 (13/6): WebAPI, FastAPI and NGINX 6. Seminar 6 (14/6): Data Drift and Solutions",,,"#sharing, #machine_learning",,
"Xin chào các anh em, hiện tại em đang gặp vấn đề về độ chính xác khi sử dụng Python để đọc kết quả trên mô hình ONNX. Cụ thể hơn, trên môi trường Visual Studio, em sử dụng mô hình tự động (AutoML) để phân lớp hình ảnh (Image Classification) của ML.NET, đầu vào là 7 lớp. Mô hình của AutoML sử dụng huấn luyện là DNN + ResNeXt-50. Kết quả đạt được là 100% độ chính xác, em có thử 10 hình ảnh test cho mỗi lớp, kết quả đều đúng. Tuy nhiên khi sử dụng mô hình ONNX để sử dụng trên môi trường Python của Google Colab. Kết quả đạt được là hoàn toàn không chính xác. Cụ thể, khi em đưa bất cứ hình ảnh nào vào, kết quả cũng chỉ đạt được là duy nhất. Em đã thử chuyển mô hình onnx sang Tensorflow bằng thư viện onnx2tf nhưng cũng chỉ cho ra một kết quả duy nhất. Em hi vọng mọi người có thể gợi ý giúp em có cách nào có kết quả mô hình ONNX đúng nhất. Dưới đây là link post issue trên Github, tuy vậy cũng khá lâu rồi chưa có cách nào cải thiện được vấn đề này cả.
https://github.com/microsoft/onnxruntime/issues/16001#issuecomment-1554819072
Cảm ơn mọi người đã xem bài và mong mọi người giúp đỡ em.","Xin chào các anh em, hiện tại em đang gặp vấn đề về độ chính xác khi sử dụng Python để đọc kết quả trên mô hình ONNX. Cụ thể hơn, trên môi trường Visual Studio, em sử dụng mô hình tự động (AutoML) để phân lớp hình ảnh (Image Classification) của ML.NET, đầu vào là 7 lớp. Mô hình của AutoML sử dụng huấn luyện là DNN + ResNeXt-50. Kết quả đạt được là 100% độ chính xác, em có thử 10 hình ảnh test cho mỗi lớp, kết quả đều đúng. Tuy nhiên khi sử dụng mô hình ONNX để sử dụng trên môi trường Python của Google Colab. Kết quả đạt được là hoàn toàn không chính xác. Cụ thể, khi em đưa bất cứ hình ảnh nào vào, kết quả cũng chỉ đạt được là duy nhất. Em đã thử chuyển mô hình onnx sang Tensorflow bằng thư viện onnx2tf nhưng cũng chỉ cho ra một kết quả duy nhất. Em hi vọng mọi người có thể gợi ý giúp em có cách nào có kết quả mô hình ONNX đúng nhất. Dưới đây là link post issue trên Github, tuy vậy cũng khá lâu rồi chưa có cách nào cải thiện được vấn đề này cả. https://github.com/microsoft/onnxruntime/issues/16001#issuecomment-1554819072 Cảm ơn mọi người đã xem bài và mong mọi người giúp đỡ em.",,,"#Q&A, #machine_learning, #python",,
"Dạ em chào mấy Thầy, Cô, Anh, Chị.
Em là thành viên mới của nhóm, hiện tại em đang bắt đầu nghiên cứu học về Machine Learning. Cho em hỏi Thầy, Cô, Anh, Chị nào có Lộ Trình Học hay Website để học Machine Learning cho em tham khảo với được không ạ.
Em cảm ơn các Thầy, Cô, Anh, Chị ạ 🥰","Dạ em chào mấy Thầy, Cô, Anh, Chị. Em là thành viên mới của nhóm, hiện tại em đang bắt đầu nghiên cứu học về Machine Learning. Cho em hỏi Thầy, Cô, Anh, Chị nào có Lộ Trình Học hay Website để học Machine Learning cho em tham khảo với được không ạ. Em cảm ơn các Thầy, Cô, Anh, Chị ạ",,,"#Q&A, #machine_learning",,
"VinAI Seminar - ""Annotation-Efficient Learning for Object Discovery and Detection""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Huy Vo, AI Research Scientist at Meta.
Time: 2:30 pm - 3:30 pm (GMT+7), Tue, Jun 06, 2023","VinAI Seminar - ""Annotation-Efficient Learning for Object Discovery and Detection"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Huy Vo, AI Research Scientist at Meta. Time: 2:30 pm - 3:30 pm (GMT+7), Tue, Jun 06, 2023",,,,,
"Xin chào, có thầy nào làm mentor data engineer hoặc machine learning theo project. Em xin theo học với.","Xin chào, có thầy nào làm mentor data engineer hoặc machine learning theo project. Em xin theo học với.",,,"#Q&A, #machine_learning",,
"Em chào mọi người ạ. Em đang gặp khó về việc cài tensorflow GPU. Em đã cài đầy đủ package và path như trên mạng hướng dẫn.
- tensorflow 2.12
cuda 12.1
cudnn 8.1.0.77
python 3.10.11
Nhưng khi kiểm tra thì vẫn không có GPU . Mong mọi người giúp em với ạ. Em xin cảm ơn ạ.",Em chào mọi người ạ. Em đang gặp khó về việc cài tensorflow GPU. Em đã cài đầy đủ package và path như trên mạng hướng dẫn. - tensorflow 2.12 cuda 12.1 cudnn 8.1.0.77 python 3.10.11 Nhưng khi kiểm tra thì vẫn không có GPU . Mong mọi người giúp em với ạ. Em xin cảm ơn ạ.,,,"#Q&A, #python",,
"Xin chào mọi người, hiện em đang học môn machine learning cơ bản và giảng viên giao cho làm một cái project cá nhân. 
Em đã chọn được một bài báo về y sinh học, cụ thể là em dùng mô hình Bio-Bert để làm project, nhưng vấn đề là em tìm hiểu và chạy code nhưng không được
Mọi người có thể giúp em hiểu rõ hơn về bài báo và đoạn code được không ạ, vì em cũng mới học về machine learning nên em thấy code khá nhiều
Em xin cảm ơn mọi người nhiều
Link bài báo: https://paperswithcode.com/paper/biobert-a-pre-trained-biomedical-language
Link code : https://github.com/dmis-lab/biobert/blob/master/README.md","Xin chào mọi người, hiện em đang học môn machine learning cơ bản và giảng viên giao cho làm một cái project cá nhân. Em đã chọn được một bài báo về y sinh học, cụ thể là em dùng mô hình Bio-Bert để làm project, nhưng vấn đề là em tìm hiểu và chạy code nhưng không được Mọi người có thể giúp em hiểu rõ hơn về bài báo và đoạn code được không ạ, vì em cũng mới học về machine learning nên em thấy code khá nhiều Em xin cảm ơn mọi người nhiều Link bài báo: https://paperswithcode.com/paper/biobert-a-pre-trained-biomedical-language Link code : https://github.com/dmis-lab/biobert/blob/master/README.md",,,"#Q&A, #machine_learning",,
"Em chào mọi người, em đang deploy model tensorflow sang graph(.pb) mà khi cv2.dnn.readNetFromTensorflow thì không nhận được layer nào từ model
https://colab.research.google.com/drive/1m-kOQRKsyrfxKVzGrZAx2WT8Fs9IQ_wY?usp=sharing
Mong mọi người giúp em với ạ. Em xin cảm ơn","Em chào mọi người, em đang deploy model tensorflow sang graph(.pb) mà khi cv2.dnn.readNetFromTensorflow thì không nhận được layer nào từ model https://colab.research.google.com/drive/1m-kOQRKsyrfxKVzGrZAx2WT8Fs9IQ_wY?usp=sharing Mong mọi người giúp em với ạ. Em xin cảm ơn",,,"#Q&A, #python",,
"Mình muốn hỏi cách xử lý bài toán hồi quy logistic như thế nào khi đầu vào như hình. Hàm logit được giả định là tuyến tính với các biến độc lập x nhưng Decision boundary lúc này không thể là dạng tuyến tính a+bX bình thường mà phải là phương trình đường tròn. Vậy làm thể nào để xử lý được vấn đề này, đặc biệt khi ta không dự đoán được hình dạng của tập dữ liệu. Tra cứu thì có giải pháp là chuyển sang hệ tọa độ cực mà mình không hiểu lắm.","Mình muốn hỏi cách xử lý bài toán hồi quy logistic như thế nào khi đầu vào như hình. Hàm logit được giả định là tuyến tính với các biến độc lập x nhưng Decision boundary lúc này không thể là dạng tuyến tính a+bX bình thường mà phải là phương trình đường tròn. Vậy làm thể nào để xử lý được vấn đề này, đặc biệt khi ta không dự đoán được hình dạng của tập dữ liệu. Tra cứu thì có giải pháp là chuyển sang hệ tọa độ cực mà mình không hiểu lắm.",,,"#Q&A, #machine_learning",,
"Chào mọi người, e đang là sinh viên đi thực tập, em đang làm 1 project liên quan đến bài toán mà tập train và test đến từ 2 phân phối khác nhau , anh chị có thể cho em hỏi có cách nào để train model có kết quả cao không ạ","Chào mọi người, e đang là sinh viên đi thực tập, em đang làm 1 project liên quan đến bài toán mà tập train và test đến từ 2 phân phối khác nhau , anh chị có thể cho em hỏi có cách nào để train model có kết quả cao không ạ",,,"#Q&A, #machine_learning",,
"Vấn đề cross-validation.
Chào mọi người.Mình đang tập tành thực hành các project machine learning và gặp một vấn đề mong mọi người giải đáp giúp ạ.
Mình có 1 bộ imbalanced dataset. Theo google thì để xử lý imbalanced thì mình dùng StratifiedKFold với weight để xử lý. Khi đó, mình thu được K model và tính trung bình các score trên tập valid để đánh giá model.
modelX = LogisticClassifier(para_1=a, ....., para_n = t)
kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=12).split(X,y)
Sau đó, mình lại có 1 bộ test set hoàn toàn tách biệt với bộ data trên để dự đoán. Lúc đấy mình suy nghĩ đến việc có phải nên lựa chọn model có perform tốt nhất trong K model đó để predict cho test set đúng không? Mình có tra google và theo khả năng đọc hiểu của bản thân thì, việc cross-valid trên K model trên thực chất chỉ là một cách để kiểm tra performance và đề phòng TH bộ train -val của mình bị chia lệch. Còn thực chất khi predict ta sẽ vẫn predict trên model ban đầu (kiểu y_pred=modelX.predict(X_test)) đúng không mọi người.Còn việc đổi performance thì phụ thuộc vào việc mình hiệu chỉnh parameter như thế nào.
Không biết mình hiểu vấn đề như vậy có đúng không mọi người? Mình mong nhận được sự giải đáp của mọi người ạ.","Vấn đề cross-validation. Chào mọi người.Mình đang tập tành thực hành các project machine learning và gặp một vấn đề mong mọi người giải đáp giúp ạ. Mình có 1 bộ imbalanced dataset. Theo google thì để xử lý imbalanced thì mình dùng StratifiedKFold với weight để xử lý. Khi đó, mình thu được K model và tính trung bình các score trên tập valid để đánh giá model. modelX = LogisticClassifier(para_1=a, ....., para_n = t) kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=12).split(X,y) Sau đó, mình lại có 1 bộ test set hoàn toàn tách biệt với bộ data trên để dự đoán. Lúc đấy mình suy nghĩ đến việc có phải nên lựa chọn model có perform tốt nhất trong K model đó để predict cho test set đúng không? Mình có tra google và theo khả năng đọc hiểu của bản thân thì, việc cross-valid trên K model trên thực chất chỉ là một cách để kiểm tra performance và đề phòng TH bộ train -val của mình bị chia lệch. Còn thực chất khi predict ta sẽ vẫn predict trên model ban đầu (kiểu y_pred=modelX.predict(X_test)) đúng không mọi người.Còn việc đổi performance thì phụ thuộc vào việc mình hiệu chỉnh parameter như thế nào. Không biết mình hiểu vấn đề như vậy có đúng không mọi người? Mình mong nhận được sự giải đáp của mọi người ạ.",,,"#Q&A, #machine_learning, #data",,
"Chào mọi người,
Mình hiện tại đang làm việc trong lĩnh vực Web scraping về quảng cáo bất động sản/xe hơi và MLOps. Mình muốn đóng góp/hỗ trợ các dự án công đồng để trao dồi thêm kinh nghiệm về MLOps.
Nếu mn có dự án công đồng nào cần thu thập public data hoặc xây dựng ML pipelines thì ping mình nhé.","Chào mọi người, Mình hiện tại đang làm việc trong lĩnh vực Web scraping về quảng cáo bất động sản/xe hơi và MLOps. Mình muốn đóng góp/hỗ trợ các dự án công đồng để trao dồi thêm kinh nghiệm về MLOps. Nếu mn có dự án công đồng nào cần thu thập public data hoặc xây dựng ML pipelines thì ping mình nhé.",,,"#Q&A, #machine_learning, #data",,
"Xin chào mọi người, mình vừa phỏng vấn ở một công ty ở Anh tuyển summer internship ở vị trí Machine Learning, mình muốn viết bài này để chia sẽ đợt phỏng vấn này mục đích để mọi người cùng nhau bàn luận vì mình cũng chưa biết là câu trả lời của mình là ổn chưa, cụ thể là như sau:
Công ty họ nghiên cứu về Hyperspectral Camera(Camera quang phổ) cụ thể là tấm hình sẽ có hơn 50 chiều không gian màu thay vì chỉ có 3. Khi sử dụng hyperspectral images thì dữ liệu sẽ chính xác và nhiều thông tin hơn. Mình đến đợt phỏng vấn 2 có câu hỏi như sau:
Thì họ hỏi mình đại khái là: Bạn có những cách nào để compress một bức hình và đối với hyperspectral images thì làm sao?
Mình suy nghĩ một chút và trả lời như sau: một cách mình có thể nghĩ ra liền là có thể dùng convolutional methods như một bức hình bình thường hoặc PCA
Họ hỏi thêm là: Vậy bạn có biết effects sau khi deploy methods như vậy là gì không?
Lúc đó mình trả lời đơn giản là sẽ giảm chất lượng và thông tin dữ liệu
Lúc đó mình hơi run nên cũng không hình dung được có cách nào nữa.
Lúc cuối mình hỏi thêm rằng có phải việc compress hyperspectral images là một trong những vấn đề team nghiên cứu đang đối mặt đúng không? Thì họ nói đúng vậy thật.
Thật sự mình tự học khá nhiều nên kinh nghiệm còn rất ít nên khi phỏng vấn mình chỉ trả lời những gì mình nghĩ ra liền lúc đó thôi.
Mọi người ai có kinh nghiệm trong mảng này, cho mình hỏi có câu trả lời nào tốt hơn cho bức hình 50+ không gian màu như vậy không? Mình cảm ơn","Xin chào mọi người, mình vừa phỏng vấn ở một công ty ở Anh tuyển summer internship ở vị trí Machine Learning, mình muốn viết bài này để chia sẽ đợt phỏng vấn này mục đích để mọi người cùng nhau bàn luận vì mình cũng chưa biết là câu trả lời của mình là ổn chưa, cụ thể là như sau: Công ty họ nghiên cứu về Hyperspectral Camera(Camera quang phổ) cụ thể là tấm hình sẽ có hơn 50 chiều không gian màu thay vì chỉ có 3. Khi sử dụng hyperspectral images thì dữ liệu sẽ chính xác và nhiều thông tin hơn. Mình đến đợt phỏng vấn 2 có câu hỏi như sau: Thì họ hỏi mình đại khái là: Bạn có những cách nào để compress một bức hình và đối với hyperspectral images thì làm sao? Mình suy nghĩ một chút và trả lời như sau: một cách mình có thể nghĩ ra liền là có thể dùng convolutional methods như một bức hình bình thường hoặc PCA Họ hỏi thêm là: Vậy bạn có biết effects sau khi deploy methods như vậy là gì không? Lúc đó mình trả lời đơn giản là sẽ giảm chất lượng và thông tin dữ liệu Lúc đó mình hơi run nên cũng không hình dung được có cách nào nữa. Lúc cuối mình hỏi thêm rằng có phải việc compress hyperspectral images là một trong những vấn đề team nghiên cứu đang đối mặt đúng không? Thì họ nói đúng vậy thật. Thật sự mình tự học khá nhiều nên kinh nghiệm còn rất ít nên khi phỏng vấn mình chỉ trả lời những gì mình nghĩ ra liền lúc đó thôi. Mọi người ai có kinh nghiệm trong mảng này, cho mình hỏi có câu trả lời nào tốt hơn cho bức hình 50+ không gian màu như vậy không? Mình cảm ơn",,,"#Q&A, #machine_learning",,
"Hôm nay check github của sách chợt nhận ra đã được gần 1k stars :)
Các bạn có thể tải sách miễn phí tại https://github.com/tiepvupsu/ebookMLCB. Đừng quên để lại một star nếu bạn thấy hữu ích.",Hôm nay check github của sách chợt nhận ra đã được gần 1k stars :) Các bạn có thể tải sách miễn phí tại https://github.com/tiepvupsu/ebookMLCB. Đừng quên để lại một star nếu bạn thấy hữu ích.,,,#sharing,,
"[Góc hỏi đáp]
Mọi người đã từng dùng Stacked Hourglass Network để nhận diện Pose Estimation cho em hỏi chút được không ạ?
Đầu vào của model là ảnh RGB và label là 15 keypoints(x, y). Còn đầu ra của model là 15 heatmaps.
1. Vậy để tính hàm loss thì mình tính dựa trên 15 heatmaps hay extract heatmap thành 15keypoints để tính loss ạ?
2. Nếu tính loss bằng heatmaps thì làm sao để có thể tính ạ? Vì ảnh vào là RGB và label là 15 keypoints còn outputs của model là 15 heatmaps?
3. Em có sai ở dữ liệu label hay input đầu vào không ạ? Mong mọi người góp ý giúp em ạ..
Em cám ơn..","[Góc hỏi đáp] Mọi người đã từng dùng Stacked Hourglass Network để nhận diện Pose Estimation cho em hỏi chút được không ạ? Đầu vào của model là ảnh RGB và label là 15 keypoints(x, y). Còn đầu ra của model là 15 heatmaps. 1. Vậy để tính hàm loss thì mình tính dựa trên 15 heatmaps hay extract heatmap thành 15keypoints để tính loss ạ? 2. Nếu tính loss bằng heatmaps thì làm sao để có thể tính ạ? Vì ảnh vào là RGB và label là 15 keypoints còn outputs của model là 15 heatmaps? 3. Em có sai ở dữ liệu label hay input đầu vào không ạ? Mong mọi người góp ý giúp em ạ.. Em cám ơn..",,,"#Q&A, #cv, #deep_learning",,
"Làm việc trong lĩnh vực ML trong một thời gian rồi mà chưa có bài nào trong forum mình. Nhân dịp đang làm việc về Diffusion Models, mình xin phép chia sẻ bài viết mình tổng hợp lại và giải thích một số kiến thức về Diffusion Models. Hy vọng sẽ có ích với các bạn tìm hiểu về nó.
https://viblo.asia/p/diffusion-models-co-ban-phan-1-E1XVOx884Mz","Làm việc trong lĩnh vực ML trong một thời gian rồi mà chưa có bài nào trong forum mình. Nhân dịp đang làm việc về Diffusion Models, mình xin phép chia sẻ bài viết mình tổng hợp lại và giải thích một số kiến thức về Diffusion Models. Hy vọng sẽ có ích với các bạn tìm hiểu về nó. https://viblo.asia/p/diffusion-models-co-ban-phan-1-E1XVOx884Mz",,,"#sharing, #machine_learning",,
"Hi mọi người.
Tình hình là em đang bắt đầu học và luyện bên ML/DL.
Về kiến thức thì e đã có các source cho mình rồi.
Nhưng về luyện code thì em chưa biết có nền tảng nào good hay không. Thường nghe luyện code thì nghĩ đến hackerrank và leetcode nhưng ở trên này hầu như luyện algorithms, data structures, ... chứ em ít thấy problem cho ML và DL.
Nhờ các anh/chị/bạn có thể share cho em các nền tảng ok để luyện code về ML/DL ạ.
Em xin cảm ơn ^^.","Hi mọi người. Tình hình là em đang bắt đầu học và luyện bên ML/DL. Về kiến thức thì e đã có các source cho mình rồi. Nhưng về luyện code thì em chưa biết có nền tảng nào good hay không. Thường nghe luyện code thì nghĩ đến hackerrank và leetcode nhưng ở trên này hầu như luyện algorithms, data structures, ... chứ em ít thấy problem cho ML và DL. Nhờ các anh/chị/bạn có thể share cho em các nền tảng ok để luyện code về ML/DL ạ. Em xin cảm ơn ^^.",,,,,
"Em chào mọi người!
Em đang tìm hiểu về zero-shot learning cho bài toán multi-label nhưng em đã đọc một số bài báo nhưng em vẫn chưa hiểu cách zero-shot learning có thể predict được những label không có trong tập train. Không biết mọi người có 1 bài viết nào đó về vấn đề này, có thể suggest cho em được không ạ, em cảm ơn !","Em chào mọi người! Em đang tìm hiểu về zero-shot learning cho bài toán multi-label nhưng em đã đọc một số bài báo nhưng em vẫn chưa hiểu cách zero-shot learning có thể predict được những label không có trong tập train. Không biết mọi người có 1 bài viết nào đó về vấn đề này, có thể suggest cho em được không ạ, em cảm ơn !",,,"#Q&A, #machine_learning",,
"Một bài tổng hợp về kiến trúc mô hình SSD, một trong những mô hình có tốc độ xử lý cao và độ chính xác tốt nhất trong lớp các mô hình object detection.","Một bài tổng hợp về kiến trúc mô hình SSD, một trong những mô hình có tốc độ xử lý cao và độ chính xác tốt nhất trong lớp các mô hình object detection.",,,"#sharing, #cv, #deep_learning",,
"[Transformer model pytorch]
Mọi người cho em hỏi một chút về Transformer trên Pytorch ạ.
Em đang gặp một vấn đề là khi set d_model = d_FFN = 512 thì model hội tụ bình thường - bất biến với số block encoder và decoder .
Còn khi set d_model = 512 , d_FFN = d_model x (2^n) ,
thì loss chỉ giảm một chút rồi sau đó nó gần như không đổi.
Em mong mọi người góp ý, em xin cảm ơn ạ.","[Transformer model pytorch] Mọi người cho em hỏi một chút về Transformer trên Pytorch ạ. Em đang gặp một vấn đề là khi set d_model = d_FFN = 512 thì model hội tụ bình thường - bất biến với số block encoder và decoder . Còn khi set d_model = 512 , d_FFN = d_model x (2^n) , thì loss chỉ giảm một chút rồi sau đó nó gần như không đổi. Em mong mọi người góp ý, em xin cảm ơn ạ.",,,"#Q&A, #deep_learning",,
"Cho em hỏi nhóm mình ai có hứng thú với dữ liệu tài chính không ạ?
Em đang học tại trường Keimyung,khoa của em tổ chức cuộc thi phân tích dữ liệu , thông tin mọi người xem ảnh nha.
Bộ dữ liệu mọi người có thể xem ở link này : https://drive.google.com/drive/folders/1Pxy1uRX3Zx4OvIsEloyH6tCKO_KCJQYO?usp=share_link
Kỹ năng của em chỉ ở mức cơ bản thôi , chủ yếu muốn học từ mọi người là chính ạ.
Giải nhất 570$
2 Giải nhì 400$
5 Giải ba 230$
Giải chia đều cho 3 người trong nhóm ạ, nhóm đã có 2 người rùi !","Cho em hỏi nhóm mình ai có hứng thú với dữ liệu tài chính không ạ? Em đang học tại trường Keimyung,khoa của em tổ chức cuộc thi phân tích dữ liệu , thông tin mọi người xem ảnh nha. Bộ dữ liệu mọi người có thể xem ở link này : https://drive.google.com/drive/folders/1Pxy1uRX3Zx4OvIsEloyH6tCKO_KCJQYO?usp=share_link Kỹ năng của em chỉ ở mức cơ bản thôi , chủ yếu muốn học từ mọi người là chính ạ. Giải nhất 570$ 2 Giải nhì 400$ 5 Giải ba 230$ Giải chia đều cho 3 người trong nhóm ạ, nhóm đã có 2 người rùi !",,,"#sharing, #data",,
"Mọi người cho e hỏi, ma trận dạng như thế này được gọi tên là gì và có những tính chất như thế nào? Cảm ơn mọi người ạ.","Mọi người cho e hỏi, ma trận dạng như thế này được gọi tên là gì và có những tính chất như thế nào? Cảm ơn mọi người ạ.",,,"#Q&A, #math",,
"Sau hơn 1 tháng phát triển, AnyLabeling - công cụ gán nhãn dữ liệu ảnh với AI models đã hoàn thiện hơn: 
🔥 Tích hợp Segment Anything, YOLOv5, YOLOv8. Hỗ trợ nạp Custom Models để có thể tạo thành vòng lặp Gán nhãn - Huấn luyện, phát triển model dần dần. 
🔥 Hỗ trợ gán nhãn OCR, KIE với trường text và tính năng grouping. 
🔥 Hỗ trợ đa ngôn ngữ: Tiếng Anh, Tiếng Việt, Tiếng Trung. 
🔥 Cài đặt dễ dàng với Pip, hoặc các tệp thực thi. 
🔥 Tài liệu đầy đủ tại https://anylabeling.com/docs.  
Công cụ này sẽ giúp tăng tốc việc gán nhãn cho nhiều bài toán khác nhau. Xin được giới thiệu đến tất cả mọi người!
- Mã nguồn: https://github.com/vietanhdev/anylabeling
- Blog: https://aicurious.io/.../2023-04-22-create-a-segment...
#anylabeling #segmentanything #yolo #yolov5 #yolov8","Sau hơn 1 tháng phát triển, AnyLabeling - công cụ gán nhãn dữ liệu ảnh với AI models đã hoàn thiện hơn: Tích hợp Segment Anything, YOLOv5, YOLOv8. Hỗ trợ nạp Custom Models để có thể tạo thành vòng lặp Gán nhãn - Huấn luyện, phát triển model dần dần. Hỗ trợ gán nhãn OCR, KIE với trường text và tính năng grouping. Hỗ trợ đa ngôn ngữ: Tiếng Anh, Tiếng Việt, Tiếng Trung. Cài đặt dễ dàng với Pip, hoặc các tệp thực thi. Tài liệu đầy đủ tại https://anylabeling.com/docs. Công cụ này sẽ giúp tăng tốc việc gán nhãn cho nhiều bài toán khác nhau. Xin được giới thiệu đến tất cả mọi người! - Mã nguồn: https://github.com/vietanhdev/anylabeling - Blog: https://aicurious.io/.../2023-04-22-create-a-segment...",#anylabeling	#segmentanything	#yolo	#yolov5	#yolov8,,"#sharing, #cv, #deep_learning",,
"PHƯƠNG PHÁP DẠY CHATGPT HỌC ĐƯỢC KIẾN THỨC MỚI
Xin chia sẻ tới các anh em trong group mô hình tổng quát và các kĩ thuật để dạy ChatGPT hiệu quả. Đây là các phương pháp đang được sử dụng thực tế với giải pháp GPT SaleBot của bọn mình. Rất mong sẽ nhận được thêm nhiều ý kiến đóng góp về các cách thức hiệu quả hơn.
A. NGUYÊN LÝ CHUNG
ChatGPT nói riêng và các mô hình ngôn ngữ lớn (LLM) nói chung có một tính chất rất đặc biệt. Đó là nó có một ""trí nhớ ngắn hạn"" mà nếu đưa dữ liệu mới vào vùng nhớ này, thì mô hình sẽ học được dữ liệu mới này và sử dụng trong nội dung trả lời. Thuật ngữ gọi là in-context learning, hoặc one/few-shot learning. Đây là một đặc điểm có tính cách mạng, bởi nó giúp huấn luyện ChatGPT nhanh chóng và dễ dàng hơn rất nhiều so với giải pháp ""fine-tune"" truyền thống.
So sánh đơn giản, cách huấn luyện AI truyền thống giống như một anh học trò phải bỏ công bỏ sức ôn tập để có kiến thức đi thi. Trong khi đó huấn luyện ChatGPT theo in-context learning thì giống như anh học trò đó sử dụng phao. Cùng một mục đích là trả lời được câu hỏi của người ra đề, thì rõ ràng dùng phao sẽ nhanh chóng và thậm chí trong nhiều trường hợp còn chính xác hơn là tự học, với điều kiện là học trò này phải rất thông minh, là điều mà ChatGPT có thừa.
Đặc điểm có tính cách mạng này của ChatGPT và LLM mở đường cho rất nhiều ứng dụng khác nhau có thể phát triển trên nền ChatGPT, với chi phí rẻ hơn rất nhiều so với truyền thống.
Về tổng thể những ứng dụng này sẽ đều hoạt động theo mô hình như sau. Mình tiếp tục lấy ví dụ thi cử ở trên cho dễ hiểu
Bước 1: Đọc đề, phân tích đề
Ở bước này, hệ thống sẽ lấy câu hỏi của người dùng, và phân tích ý đồ (intention) của câu hỏi đó là gì?
Bước 2: Tìm ""phao"" phù hợp
Phao ở đây chính là đoạn dữ liệu phù hợp cần đưa vào bộ nhớ ngắn hạn của ChatGPT để nó trả lời được câu hỏi. Việc chọn phao này gọi là truy vấn thông tin (information retrieval) mà mình sẽ giới thiệu các kĩ thuật chính của nó ở phần tiếp
Bước 3: Sử dụng ""phao"" để xây dựng đáp án
Ở bước này thì cho ChatGPT đọc hiểu thông tin trong ""phao"" và sử dụng thông tin này để trả lời cho người dùng. Có thể điều khiển quá trình này bằng ""prompt điều khiển"", giúp câu trả lời có tính cách và văn phong khác nhau. Hiểu đơn giản là cùng một đề bài, dùng cùng một phao, thì các anh học trò khác nhau sẽ cho ra các câu trả lời theo phong thái riêng.
B. CÁC KĨ THUẬT LẤY DỮ LIỆU TỪ BỘ NHỚ DÀI HẠN
Trong mô hình huấn luyện ChatGPT kể trên, thì khâu quan trọng nhất chính là chuẩn bị và chọn ra được đúng ""phao"" từ đống kiến thức lớn chung. Khâu này quyết định toàn bộ chất lượng trả lời của ChatGPT, hay hiểu đơn giản là chuẩn bị ""phao"" lệch tủ, hay dùng ""phao"" lệch đề bài thì đều khiến thi trượt.
Khâu này trong khoa học máy tính gọi là truy vấn thông tin (information retrieval - IR), là một lĩnh vực được phát triển rất lâu dài. Có rất nhiều kĩ thuật IR khác nhau, nhưng dưới đây là một số kĩ thuật có thể áp dụng hiệu quả với bài toán huấn luyện ChatGPT:
Fuzzy matching (so khớp văn bản)
Đây là kĩ thuật đơn giản nhất, nó đơn thuần so sánh sự khác biệt về từ khoá giữa câu hỏi với lại dữ liệu phù hợp để trả lời. Kĩ thuật này giống như anh học sinh dốt trong đầu không có kiến thức gì, chỉ xem đề bài nhắc đến từ khoá gì thì tìm phao có chứa nhiều từ trùng nhất để chép
BM25 
Đây là kĩ thuật cao cấp hơn Fuzzy matching, nó biết trong câu hỏi của người dùng và dữ liệu trả lời, thì đâu là từ khoá quan trọng, đâu là từ khoá ít quan trọng. Kĩ thuật này giống như anh học sinh có nghe giảng, dù cũng chưa thực sự hiểu lắm nhưng cũng nhớ được vài từ khoá thày cô hay nhắc tới, nên trong các phao na ná nhau, thì anh biết chọn phao chứa nhiều từ khoá quan trọng nhất có trong đề bài. Điều thú vị ở đây là trong thực tiễn, dù anh học sinh chẳng hiểu gì bài giảng, chỉ dùng ""mẹo"" thì mẹo này cũng rất hiệu quả.
Semantic search (vector search)
Cuộc đời không bao giờ đơn giản kiểu học gì thi nấy, mà đề bài luôn được các thày cô ra kiểu lắt léo mà phải tư duy thì mới hiểu được đây là dạng bài gì, áp dụng kiến thức đã học nào. Đây chính là chỗ mà kĩ thuật semantic search áp dụng. Từ cái tên cũng đã nói lên kĩ thuật này, đó là thay vì chỉ nhìn vào mặt chữ, thì ""đọc hiểu"" câu hỏi, rồi tìm xem kiến thức nào trong những cái đã biết thực sự liên quan tới nội dung này. Quá trình ""đọc hiểu"" này của máy gọi là embedding, dữ liệu ngữ nghĩa của câu hỏi được biểu diễn ở dạng vector, và quá trình tìm kiếm là quá trình so sánh độ gần-xa giữa các vector nên kĩ thuật này còn gọi là vector search. Lĩnh vực vector search đang được thúc đẩy phát triển rất nhanh sau khi ChatGPT xuất hiện.
C. ĐÁNH GIÁ
Từ thực tiễn áp dụng, phối hợp các kĩ thuật huấn luyện ChatGPT kể trên vào sản phẩm GPT SaleBot (demo tại: https://support.gptsalebot.com), mình rút ra được một số đánh giá:
Fuzzy matching rất thích hợp để áp dụng cho các câu hỏi cụ thể, rõ nghĩa, dùng chuẩn thuật ngữ
BM25 rất thích hợp với những câu hỏi dài, có nhiều chi tiết
Semantic search thích hợp với những nội dung ngắn, và giỏi trong việc phân biệt các câu na ná nhau về hình thức nhưng khác nhau về ngữ nghĩa
Trên thực tế, để huấn luyện ChatGPT hiệu quả, thì không chỉ sử dụng một phương pháp, mà cần kết hợp nhiều phương pháp lại với nhau. Và team GPT SaleBot hiện đã triển khai các kĩ thuật để huấn luyện ChatGPT đạt chất lượng tương đương chatbot Fin của Intercom, là phiên bản tiên tiến nhất của trùm chatbot quốc tế (https://www.intercom.com/fin). Ở trình độ này, chatbot không những cần hiểu tốt câu hỏi của người dùng, và trả lời chính xác theo thông tin được huấn luyện, mà còn phải có khả năng xử lý các dạng viết tắt, viết sai chính tả, từ lóng, có khả năng đặt lại câu hỏi làm rõ nghĩa và gợi ý các câu hỏi follow-up để tiếp tục hội thoại.
Các bạn có thể trải nghiệm thử các tính năng này tại: https://support.gptsalebot.com
D. MỞ RỘNG
Huấn luyện ChatGPT trên dữ liệu riêng, và áp dụng phương pháp này vào các bài toán cụ thể như chatbot đang là lĩnh vực phát triển nhanh và sẽ còn có nhiều giải pháp thú vị hơn trong tương lai. Nếu bạn muốn nắm bắt cơ hội và trở thành các chuyên gia về triển khai chatbot, chuyên gia huấn luyện ChatGPT theo bài toán đặc thù của doanh nghiệp thì có thể đăng ký trở thành partner triển khai hoặc nhân sự của GPT SaleBot theo link dưới đây: https://aivgroupworking.sg.larksuite.com/.../shrlgvtifZ8t...
Quyền lợi của partner là sẽ được tiếp cận những hiểu biết sâu về công nghệ ChatGPT, những insight trong việc triển khai chatbot, trợ lý ảo thành công và đón đầu làn sóng conversational marketing đang diễn ra nhanh chóng sắp tới.
Lộc Đặng","PHƯƠNG PHÁP DẠY CHATGPT HỌC ĐƯỢC KIẾN THỨC MỚI Xin chia sẻ tới các anh em trong group mô hình tổng quát và các kĩ thuật để dạy ChatGPT hiệu quả. Đây là các phương pháp đang được sử dụng thực tế với giải pháp GPT SaleBot của bọn mình. Rất mong sẽ nhận được thêm nhiều ý kiến đóng góp về các cách thức hiệu quả hơn. A. NGUYÊN LÝ CHUNG ChatGPT nói riêng và các mô hình ngôn ngữ lớn (LLM) nói chung có một tính chất rất đặc biệt. Đó là nó có một ""trí nhớ ngắn hạn"" mà nếu đưa dữ liệu mới vào vùng nhớ này, thì mô hình sẽ học được dữ liệu mới này và sử dụng trong nội dung trả lời. Thuật ngữ gọi là in-context learning, hoặc one/few-shot learning. Đây là một đặc điểm có tính cách mạng, bởi nó giúp huấn luyện ChatGPT nhanh chóng và dễ dàng hơn rất nhiều so với giải pháp ""fine-tune"" truyền thống. So sánh đơn giản, cách huấn luyện AI truyền thống giống như một anh học trò phải bỏ công bỏ sức ôn tập để có kiến thức đi thi. Trong khi đó huấn luyện ChatGPT theo in-context learning thì giống như anh học trò đó sử dụng phao. Cùng một mục đích là trả lời được câu hỏi của người ra đề, thì rõ ràng dùng phao sẽ nhanh chóng và thậm chí trong nhiều trường hợp còn chính xác hơn là tự học, với điều kiện là học trò này phải rất thông minh, là điều mà ChatGPT có thừa. Đặc điểm có tính cách mạng này của ChatGPT và LLM mở đường cho rất nhiều ứng dụng khác nhau có thể phát triển trên nền ChatGPT, với chi phí rẻ hơn rất nhiều so với truyền thống. Về tổng thể những ứng dụng này sẽ đều hoạt động theo mô hình như sau. Mình tiếp tục lấy ví dụ thi cử ở trên cho dễ hiểu Bước 1: Đọc đề, phân tích đề Ở bước này, hệ thống sẽ lấy câu hỏi của người dùng, và phân tích ý đồ (intention) của câu hỏi đó là gì? Bước 2: Tìm ""phao"" phù hợp Phao ở đây chính là đoạn dữ liệu phù hợp cần đưa vào bộ nhớ ngắn hạn của ChatGPT để nó trả lời được câu hỏi. Việc chọn phao này gọi là truy vấn thông tin (information retrieval) mà mình sẽ giới thiệu các kĩ thuật chính của nó ở phần tiếp Bước 3: Sử dụng ""phao"" để xây dựng đáp án Ở bước này thì cho ChatGPT đọc hiểu thông tin trong ""phao"" và sử dụng thông tin này để trả lời cho người dùng. Có thể điều khiển quá trình này bằng ""prompt điều khiển"", giúp câu trả lời có tính cách và văn phong khác nhau. Hiểu đơn giản là cùng một đề bài, dùng cùng một phao, thì các anh học trò khác nhau sẽ cho ra các câu trả lời theo phong thái riêng. B. CÁC KĨ THUẬT LẤY DỮ LIỆU TỪ BỘ NHỚ DÀI HẠN Trong mô hình huấn luyện ChatGPT kể trên, thì khâu quan trọng nhất chính là chuẩn bị và chọn ra được đúng ""phao"" từ đống kiến thức lớn chung. Khâu này quyết định toàn bộ chất lượng trả lời của ChatGPT, hay hiểu đơn giản là chuẩn bị ""phao"" lệch tủ, hay dùng ""phao"" lệch đề bài thì đều khiến thi trượt. Khâu này trong khoa học máy tính gọi là truy vấn thông tin (information retrieval - IR), là một lĩnh vực được phát triển rất lâu dài. Có rất nhiều kĩ thuật IR khác nhau, nhưng dưới đây là một số kĩ thuật có thể áp dụng hiệu quả với bài toán huấn luyện ChatGPT: Fuzzy matching (so khớp văn bản) Đây là kĩ thuật đơn giản nhất, nó đơn thuần so sánh sự khác biệt về từ khoá giữa câu hỏi với lại dữ liệu phù hợp để trả lời. Kĩ thuật này giống như anh học sinh dốt trong đầu không có kiến thức gì, chỉ xem đề bài nhắc đến từ khoá gì thì tìm phao có chứa nhiều từ trùng nhất để chép BM25 Đây là kĩ thuật cao cấp hơn Fuzzy matching, nó biết trong câu hỏi của người dùng và dữ liệu trả lời, thì đâu là từ khoá quan trọng, đâu là từ khoá ít quan trọng. Kĩ thuật này giống như anh học sinh có nghe giảng, dù cũng chưa thực sự hiểu lắm nhưng cũng nhớ được vài từ khoá thày cô hay nhắc tới, nên trong các phao na ná nhau, thì anh biết chọn phao chứa nhiều từ khoá quan trọng nhất có trong đề bài. Điều thú vị ở đây là trong thực tiễn, dù anh học sinh chẳng hiểu gì bài giảng, chỉ dùng ""mẹo"" thì mẹo này cũng rất hiệu quả. Semantic search (vector search) Cuộc đời không bao giờ đơn giản kiểu học gì thi nấy, mà đề bài luôn được các thày cô ra kiểu lắt léo mà phải tư duy thì mới hiểu được đây là dạng bài gì, áp dụng kiến thức đã học nào. Đây chính là chỗ mà kĩ thuật semantic search áp dụng. Từ cái tên cũng đã nói lên kĩ thuật này, đó là thay vì chỉ nhìn vào mặt chữ, thì ""đọc hiểu"" câu hỏi, rồi tìm xem kiến thức nào trong những cái đã biết thực sự liên quan tới nội dung này. Quá trình ""đọc hiểu"" này của máy gọi là embedding, dữ liệu ngữ nghĩa của câu hỏi được biểu diễn ở dạng vector, và quá trình tìm kiếm là quá trình so sánh độ gần-xa giữa các vector nên kĩ thuật này còn gọi là vector search. Lĩnh vực vector search đang được thúc đẩy phát triển rất nhanh sau khi ChatGPT xuất hiện. C. ĐÁNH GIÁ Từ thực tiễn áp dụng, phối hợp các kĩ thuật huấn luyện ChatGPT kể trên vào sản phẩm GPT SaleBot (demo tại: https://support.gptsalebot.com), mình rút ra được một số đánh giá: Fuzzy matching rất thích hợp để áp dụng cho các câu hỏi cụ thể, rõ nghĩa, dùng chuẩn thuật ngữ BM25 rất thích hợp với những câu hỏi dài, có nhiều chi tiết Semantic search thích hợp với những nội dung ngắn, và giỏi trong việc phân biệt các câu na ná nhau về hình thức nhưng khác nhau về ngữ nghĩa Trên thực tế, để huấn luyện ChatGPT hiệu quả, thì không chỉ sử dụng một phương pháp, mà cần kết hợp nhiều phương pháp lại với nhau. Và team GPT SaleBot hiện đã triển khai các kĩ thuật để huấn luyện ChatGPT đạt chất lượng tương đương chatbot Fin của Intercom, là phiên bản tiên tiến nhất của trùm chatbot quốc tế (https://www.intercom.com/fin). Ở trình độ này, chatbot không những cần hiểu tốt câu hỏi của người dùng, và trả lời chính xác theo thông tin được huấn luyện, mà còn phải có khả năng xử lý các dạng viết tắt, viết sai chính tả, từ lóng, có khả năng đặt lại câu hỏi làm rõ nghĩa và gợi ý các câu hỏi follow-up để tiếp tục hội thoại. Các bạn có thể trải nghiệm thử các tính năng này tại: https://support.gptsalebot.com D. MỞ RỘNG Huấn luyện ChatGPT trên dữ liệu riêng, và áp dụng phương pháp này vào các bài toán cụ thể như chatbot đang là lĩnh vực phát triển nhanh và sẽ còn có nhiều giải pháp thú vị hơn trong tương lai. Nếu bạn muốn nắm bắt cơ hội và trở thành các chuyên gia về triển khai chatbot, chuyên gia huấn luyện ChatGPT theo bài toán đặc thù của doanh nghiệp thì có thể đăng ký trở thành partner triển khai hoặc nhân sự của GPT SaleBot theo link dưới đây: https://aivgroupworking.sg.larksuite.com/.../shrlgvtifZ8t... Quyền lợi của partner là sẽ được tiếp cận những hiểu biết sâu về công nghệ ChatGPT, những insight trong việc triển khai chatbot, trợ lý ảo thành công và đón đầu làn sóng conversational marketing đang diễn ra nhanh chóng sắp tới. Lộc Đặng",,,"#sharing, #nlp",,
"Xin chào các bạn, mình làm mảng geospatial rẽ ngang, cụ thể là data capture & processing point cloud data & 3D modelling. Mình chỉ viết vài script cơ bản phục vụ công việc trên python. Giờ muốn tham gia sâu hơn vào ML.
Không biết trong group mình có ai làm về mảng này không ạ? Mình xin phép được kết bạn và học hỏi thêm.
Mình ở Melbourne, Australia. Nếu có bạn nào ở đây mình xin mời cà phê ạ 🙂","Xin chào các bạn, mình làm mảng geospatial rẽ ngang, cụ thể là data capture & processing point cloud data & 3D modelling. Mình chỉ viết vài script cơ bản phục vụ công việc trên python. Giờ muốn tham gia sâu hơn vào ML. Không biết trong group mình có ai làm về mảng này không ạ? Mình xin phép được kết bạn và học hỏi thêm. Mình ở Melbourne, Australia. Nếu có bạn nào ở đây mình xin mời cà phê ạ",,,"#Q&A, #machine_learning",,
"[HCM]
Chào các anh/chị.
Em là người trái ngành, có đam mê về lĩnh vực AI và đang mong muốn tìm kiếm công việc bên mảng này (MLE, DS, DE)
Hiện tại thì tình hình xin việc fresher cho ML/DL cũng khó khăn, nên em muốn dành thêm thời gian để tiếp tục trau dồi bản thân, nâng cao khả năng của mình lên.
Vậy nên, em viết post này mong được anh/chị nào có thể cho em cơ hội, nhận em vào làm để học hỏi thêm, có cơ hội tiếp xúc với dự án thực tế. (công ty hay freelance đều được ạ).
Em ko quan trọng chuyện lương.
E ở HCM.
Anh/chị cmt e sẽ inbox CV ạ.
Em cảm ơn nhiều.","[HCM] Chào các anh/chị. Em là người trái ngành, có đam mê về lĩnh vực AI và đang mong muốn tìm kiếm công việc bên mảng này (MLE, DS, DE) Hiện tại thì tình hình xin việc fresher cho ML/DL cũng khó khăn, nên em muốn dành thêm thời gian để tiếp tục trau dồi bản thân, nâng cao khả năng của mình lên. Vậy nên, em viết post này mong được anh/chị nào có thể cho em cơ hội, nhận em vào làm để học hỏi thêm, có cơ hội tiếp xúc với dự án thực tế. (công ty hay freelance đều được ạ). Em ko quan trọng chuyện lương. E ở HCM. Anh/chị cmt e sẽ inbox CV ạ. Em cảm ơn nhiều.",,,"#sharing, #machine_learning",,
"Xin phép spam group một chút. Hành động này đáng bị lên án.
Tôi luôn phản đối chuyện đạo văn không trích nguồn. Đằng này bạn 'thầy Henry' này còn lấy luôn cả slide rồi ghi tên mình vào.
Tài khoản này trước đây đăng khá nhiều bài trong group và tự xưng là 'thầy Henry', tôi xin phép xóa tài khoản và các bài liên quan. Bạn nào quan tâm có thể qua group của 'thầy' theo dõi.","Xin phép spam group một chút. Hành động này đáng bị lên án. Tôi luôn phản đối chuyện đạo văn không trích nguồn. Đằng này bạn 'thầy Henry' này còn lấy luôn cả slide rồi ghi tên mình vào. Tài khoản này trước đây đăng khá nhiều bài trong group và tự xưng là 'thầy Henry', tôi xin phép xóa tài khoản và các bài liên quan. Bạn nào quan tâm có thể qua group của 'thầy' theo dõi.",,,#sharing,,
"Mọi người nghĩ sao về Tensorflow Developer Certificate ở thời điểm hiện tại ? 
Em/mình background về computer science + đã làm việc/nghiên cứu với ML gần 2 năm, đang băn khoăn có nên ôn thi làm đẹp CV không vì đọc review có vẻ không yêu cầu chuyên môn cao và công việc hiện tại ở công ty chưa nhiều áp lực.
Cảm ơn mọi người đã đọc.","Mọi người nghĩ sao về Tensorflow Developer Certificate ở thời điểm hiện tại ? Em/mình background về computer science + đã làm việc/nghiên cứu với ML gần 2 năm, đang băn khoăn có nên ôn thi làm đẹp CV không vì đọc review có vẻ không yêu cầu chuyên môn cao và công việc hiện tại ở công ty chưa nhiều áp lực. Cảm ơn mọi người đã đọc.",,,"#sharing, #machine_learning",,
"Em chào mọi người ạ
Có ai có thể cho em tài liệu tham khảo về việc sử dụng Bayesian Neural Network ( BNN Model) để dự đoán giá BTC bằng python không ạ. Em làm đồ án nhưng kiếm tài liệu về thuật toán này thì lại rất ít ạ",Em chào mọi người ạ Có ai có thể cho em tài liệu tham khảo về việc sử dụng Bayesian Neural Network ( BNN Model) để dự đoán giá BTC bằng python không ạ. Em làm đồ án nhưng kiếm tài liệu về thuật toán này thì lại rất ít ạ,,,"#Q&A, #deep_learning",,
"Chào các anh,chị trong forum.
Em sn 2000 đã ra trường. Tất cả kiến thức về AI em đều tự học nên nhiều phần kiến thức lại thiếu mà có phần lại tốt, và em không kinh nghiệm các dự án thực tế, vì vậy em viết post này để mong anh/chị nào có thể cho em cơ hội làm việc để học hỏi thêm, được làm việc trong dự án thực tế. Em có thể thực tập hay fresher tại HCM.
Em tự tin về phần giao tiếp tiếng Anh hay tốt nghiệp ở đh tốt, mà cho em hỏi ngu là mấy anh chị tuyển dụng giờ có nhìn vào những yếu tố này không ạ?","Chào các anh,chị trong forum. Em sn 2000 đã ra trường. Tất cả kiến thức về AI em đều tự học nên nhiều phần kiến thức lại thiếu mà có phần lại tốt, và em không kinh nghiệm các dự án thực tế, vì vậy em viết post này để mong anh/chị nào có thể cho em cơ hội làm việc để học hỏi thêm, được làm việc trong dự án thực tế. Em có thể thực tập hay fresher tại HCM. Em tự tin về phần giao tiếp tiếng Anh hay tốt nghiệp ở đh tốt, mà cho em hỏi ngu là mấy anh chị tuyển dụng giờ có nhìn vào những yếu tố này không ạ?",,,"#Q&A, #machine_learning",,
"Em chào anh chị trong group ạ, hiện tại em đang dùng yolov5 để detect object nhưng gặp tình trạng box của vật thể bị chia thành 2 box nhỏ thì có cách nào giải quyết không ạ ?
Em cảm ơn nhiều.","Em chào anh chị trong group ạ, hiện tại em đang dùng yolov5 để detect object nhưng gặp tình trạng box của vật thể bị chia thành 2 box nhỏ thì có cách nào giải quyết không ạ ? Em cảm ơn nhiều.",,,"#Q&A, #deep_learning, #cv",,
"Chào mọi người,
Cho em hỏi về bài toán Object Detection, liệu mình có phương pháp nào để cung cấp thông tin bổ sung cho mô hình không ạ.
Ví dụ như số lượng object không quá n, hoặc background của các ảnh luôn giống nhau.
Em cảm ơn ạ.","Chào mọi người, Cho em hỏi về bài toán Object Detection, liệu mình có phương pháp nào để cung cấp thông tin bổ sung cho mô hình không ạ. Ví dụ như số lượng object không quá n, hoặc background của các ảnh luôn giống nhau. Em cảm ơn ạ.",,,"#Q&A, #cv",,
"CHia sẻ lại cho các bạn một bài viết cực hay về ứng dụng của ML DS trong ngân hàng và các tổ chức tài chính.
Mình đang làm về risk management cho bank thì thấy bài viết đặc biệt hữu ích khi mình hiểu thêm về các mô hình machine learning đang được áp dụng và khai thác trong các hệ thống của ngân hàng.
https://www.facebook.com/groups/1083842528922368/posts/1259824431324176",CHia sẻ lại cho các bạn một bài viết cực hay về ứng dụng của ML DS trong ngân hàng và các tổ chức tài chính. Mình đang làm về risk management cho bank thì thấy bài viết đặc biệt hữu ích khi mình hiểu thêm về các mô hình machine learning đang được áp dụng và khai thác trong các hệ thống của ngân hàng. https://www.facebook.com/groups/1083842528922368/posts/1259824431324176,,,"#sharing, #machine_learning",,
"Chào mọi người ạ, em đang optimize lại code của em, trong lúc optimize thì em nhận thấy một điều kì lạ.
Cụ thể là như hình 1, em tạo 1 tensor là mathched, sau đó truyền images vào network, rồi em đo thời gian gán matched[0] thì kết quả là lần lặp đầu luôn tốn rất nhiều thời gian mà em không biết tại sao. Vấn đề đó chỉ là một phép gán.
Nhưng như trong hình 2, nếu em không truyền images vào network và đo thời gian em gán matched[0] thì nó lại rất nhanh. Vậy lí do là do đâu vậy ạ.
// em đang để config.device = ""cuda""","Chào mọi người ạ, em đang optimize lại code của em, trong lúc optimize thì em nhận thấy một điều kì lạ. Cụ thể là như hình 1, em tạo 1 tensor là mathched, sau đó truyền images vào network, rồi em đo thời gian gán matched[0] thì kết quả là lần lặp đầu luôn tốn rất nhiều thời gian mà em không biết tại sao. Vấn đề đó chỉ là một phép gán. Nhưng như trong hình 2, nếu em không truyền images vào network và đo thời gian em gán matched[0] thì nó lại rất nhanh. Vậy lí do là do đâu vậy ạ. // em đang để config.device = ""cuda""",,,"#Q&A, #deep_learning",,
"Chào cả nhà. Mình đang bị vấn đề này khi chạy TheBloke/vicuna-7B-GPTQ-4bit-128g hoặc Neko-Institute-of-Science/LLaMA-13B-4bit-128g trên text-generation-webui và không biết tại sao lại bị vậy. 
Mình cũng đã test chạy anon8231489123/vicuna-13b-GPTQ-4bit-128g với cùng thông số và không bị vấn đề gì cả.

- Đây là câu lệnh để mình chạy server:
 python server.py --share --auto-devices --chat --model-menu --wbits 4 --groupsize 128
Mong mọi người giúp mình. Mình cám ơn rất nhiều!",Chào cả nhà. Mình đang bị vấn đề này khi chạy TheBloke/vicuna-7B-GPTQ-4bit-128g hoặc Neko-Institute-of-Science/LLaMA-13B-4bit-128g trên text-generation-webui và không biết tại sao lại bị vậy. Mình cũng đã test chạy anon8231489123/vicuna-13b-GPTQ-4bit-128g với cùng thông số và không bị vấn đề gì cả. - Đây là câu lệnh để mình chạy server: python server.py --share --auto-devices --chat --model-menu --wbits 4 --groupsize 128 Mong mọi người giúp mình. Mình cám ơn rất nhiều!,,,#Q&A,,
"MEDIAPIPE: LOW-CODE NO-CODE AI FOR EVERYONE
Bạn nào quan tâm đến ứng dụng AI thì nhớ dùng thử MediaPipe là thư viện để các bạn developer có thể tích hợp AI vào ứng dụng của mình một cách dễ dàng chỉ với một vài dòng code nhé. Team mình mới ra mắt phiên bản mới của MediaPipe ở Google I/O năm nay với 14 API phục vụ nhiều use case khác nhau như face landmark detection, selfie segmentation v.v. Các bạn muốn biết cụ thể hơn thì xem video I/O này của mình nhé. Hy vọng thư viện này sẽ hữu ích cho các bạn! https://youtu.be/yOP_FY2KTm8P/S: Bonus thêm dưới comment là video về Project Gameface, công cụ để dùng khuôn mặt điều khiển con chuột máy tính bọn mình build với MediaPipe. Source code có trên GitHub cho những bạn muốn tìm hiểu kỹ hơn nhé.","MEDIAPIPE: LOW-CODE NO-CODE AI FOR EVERYONE Bạn nào quan tâm đến ứng dụng AI thì nhớ dùng thử MediaPipe là thư viện để các bạn developer có thể tích hợp AI vào ứng dụng của mình một cách dễ dàng chỉ với một vài dòng code nhé. Team mình mới ra mắt phiên bản mới của MediaPipe ở Google I/O năm nay với 14 API phục vụ nhiều use case khác nhau như face landmark detection, selfie segmentation v.v. Các bạn muốn biết cụ thể hơn thì xem video I/O này của mình nhé. Hy vọng thư viện này sẽ hữu ích cho các bạn! https://youtu.be/yOP_FY2KTm8P/S: Bonus thêm dưới comment là video về Project Gameface, công cụ để dùng khuôn mặt điều khiển con chuột máy tính bọn mình build với MediaPipe. Source code có trên GitHub cho những bạn muốn tìm hiểu kỹ hơn nhé.",,,"#sharing, #machine_learning",,
"Hi mọi người,

Mình muốn hỏi chút về cách freeze 1 custom model.

Mình có 1 model gồm 2 blocks,
Mình muốn train từng block theo cách như sau:
Freeze block 2, train block 1 và lưu weight
Load weight đã lưu, freeze block 1 và train block 2.
Mình muốn hỏi là làm cách nào để freeze các layer ở mỗi block dùng pytorch?
Mình đã thử:

for param in block1.parameters():
    param.requires_grad = True (False)

Nhưng khi làm như này thì tất cả param đều bị set requires_grad = False hết. Nếu k dùng đoạn code trên (nghĩa là k set gì cả) thì requires_grad của các param trong 2 blocks vẫn là True.

Ae ai gặp phải vấn đề như vậy cho mình ý kiến vs nhé. Thank you!","Hi mọi người, Mình muốn hỏi chút về cách freeze 1 custom model. Mình có 1 model gồm 2 blocks, Mình muốn train từng block theo cách như sau: Freeze block 2, train block 1 và lưu weight Load weight đã lưu, freeze block 1 và train block 2. Mình muốn hỏi là làm cách nào để freeze các layer ở mỗi block dùng pytorch? Mình đã thử: for param in block1.parameters(): param.requires_grad = True (False) Nhưng khi làm như này thì tất cả param đều bị set requires_grad = False hết. Nếu k dùng đoạn code trên (nghĩa là k set gì cả) thì requires_grad của các param trong 2 blocks vẫn là True. Ae ai gặp phải vấn đề như vậy cho mình ý kiến vs nhé. Thank you!",,,"#Q&A, #machine_learning",,
"Xin chào mọi người, em có một câu hỏi như này ạ, e tính hè này sẽ học hai khoá về ML trên cousera nhưng mọi người nói học mấy khoá đó để có kiến thức nền tảng chứ basic quá, đi làm không ăn thua.
Vậy nếu như học xong thì mình nên làm gì, học gì tiếp theo để có thể đi làm được ạ. Em vẫn chưa hình dung ra. Bởi vì e thấy cac cty chỉ tuyển người đã có kinh nghiệm về ML đi làm chứ k tuyển intern.
Mong mn giải đáp ạ.","Xin chào mọi người, em có một câu hỏi như này ạ, e tính hè này sẽ học hai khoá về ML trên cousera nhưng mọi người nói học mấy khoá đó để có kiến thức nền tảng chứ basic quá, đi làm không ăn thua. Vậy nếu như học xong thì mình nên làm gì, học gì tiếp theo để có thể đi làm được ạ. Em vẫn chưa hình dung ra. Bởi vì e thấy cac cty chỉ tuyển người đã có kinh nghiệm về ML đi làm chứ k tuyển intern. Mong mn giải đáp ạ.",,,"#Q&A, #machine_learning",,
em có một bài tập training model nhưng bị lỗi SPPF không biết mọi người có ai bị lỗi này như em không ạ. Em đang tìm cách fix mong anh chị giúp đỡ,em có một bài tập training model nhưng bị lỗi SPPF không biết mọi người có ai bị lỗi này như em không ạ. Em đang tìm cách fix mong anh chị giúp đỡ,,,"#Q&A, #machine_learning",,
"Dạ chào mọi người, hiện em đang muốn fine-tune mô hình TrOCR cho dữ liệu bao gồm công thức Latex, tiếng Việt và cả tiếng Anh nhưng trước hết em đang thử với mỗi Latex không thôi xem như thế nào thì CER mãi vẫn không xuống quá 0.27, không biết mọi người có kinh nghiệm gì khi fine-tune mô hình trên một tập dataset custom và có thể cho em vài gợi ý để thực hiện bài toán này được không ạ? Vì em chưa có nhiều kinh nghiệm nên gặp khó khăn, em cảm ơn mọi người đã đọc bài.","Dạ chào mọi người, hiện em đang muốn fine-tune mô hình TrOCR cho dữ liệu bao gồm công thức Latex, tiếng Việt và cả tiếng Anh nhưng trước hết em đang thử với mỗi Latex không thôi xem như thế nào thì CER mãi vẫn không xuống quá 0.27, không biết mọi người có kinh nghiệm gì khi fine-tune mô hình trên một tập dataset custom và có thể cho em vài gợi ý để thực hiện bài toán này được không ạ? Vì em chưa có nhiều kinh nghiệm nên gặp khó khăn, em cảm ơn mọi người đã đọc bài.",,,"#Q&A, #deep_learning",,
"Chào các anh,chị trong forum.
Em sn 2000 đã ra trường. Do lúc trước đi học không có định hướng sớm nên phải đến gần lúc ra trường mới xác định theo AI. Em đã đi thực tập nhưng hiện tại không có việc làm.
Tất cả kiến thức về AI em đều tự học nên thiếu kinh nghiệm làm việc tại các dự án thực tế, vì vậy em viết post này để mong anh/chị nào có thể cho em cơ hội làm việc để học hỏi thêm, được làm việc trong dự án thực tế. Em có thể đi làm không lương và làm việc tại Hà Nội.
Em cảm ơn nhiều.","Chào các anh,chị trong forum. Em sn 2000 đã ra trường. Do lúc trước đi học không có định hướng sớm nên phải đến gần lúc ra trường mới xác định theo AI. Em đã đi thực tập nhưng hiện tại không có việc làm. Tất cả kiến thức về AI em đều tự học nên thiếu kinh nghiệm làm việc tại các dự án thực tế, vì vậy em viết post này để mong anh/chị nào có thể cho em cơ hội làm việc để học hỏi thêm, được làm việc trong dự án thực tế. Em có thể đi làm không lương và làm việc tại Hà Nội. Em cảm ơn nhiều.",,,"#Q&A, #machine_learning",,
Em đang có bài tập là về seq2seq cho bài toán summarization nhưng trên model nó có độ chính xác là khoảng 0.4% là có thể dự đoán các từ tiếp theo. Các bác có thể xem code dưới đây cho em hỏi code em dự đoán đúng chưa hay em đang làm sai chỗ nào. Em xin cám ơn.,Em đang có bài tập là về seq2seq cho bài toán summarization nhưng trên model nó có độ chính xác là khoảng 0.4% là có thể dự đoán các từ tiếp theo. Các bác có thể xem code dưới đây cho em hỏi code em dự đoán đúng chưa hay em đang làm sai chỗ nào. Em xin cám ơn.,,,"#Q&A, #deep_learning, #nlp",,
"Chào mọi người ạ! E là sinh viên, e mới mua được chiếc vga rtx 3060 và 32GB RAM, e muốn build 1 PC để học tập và làm nghiên cứu về computer vision, vì budget của e không được cao lắm nên e đang tính mua con CPU i3 12100F. Mọi người cho e hỏi con CPU này có đủ cho việc học tập và nghiên cứu về Computer Vision, deep learning không ạ? E cảm ơn mọi người","Chào mọi người ạ! E là sinh viên, e mới mua được chiếc vga rtx 3060 và 32GB RAM, e muốn build 1 PC để học tập và làm nghiên cứu về computer vision, vì budget của e không được cao lắm nên e đang tính mua con CPU i3 12100F. Mọi người cho e hỏi con CPU này có đủ cho việc học tập và nghiên cứu về Computer Vision, deep learning không ạ? E cảm ơn mọi người",,,"#Q&A, #cv",,
"Chào mọi người, không biết ở đây có bạn sinh viên nào đang làm project về computer vision k ạ, có thể cho mình 1 chân vô hỗ trợ không công được k tại mình muốn tích lũy thêm kinh nghiệm từ những project ấy nên chủ yếu vừa giúp lại vừa học hỏi cái mới luôn ạ. Ai có nhu cầu thì cứ cmt mình giúp hết mình nha !!!!","Chào mọi người, không biết ở đây có bạn sinh viên nào đang làm project về computer vision k ạ, có thể cho mình 1 chân vô hỗ trợ không công được k tại mình muốn tích lũy thêm kinh nghiệm từ những project ấy nên chủ yếu vừa giúp lại vừa học hỏi cái mới luôn ạ. Ai có nhu cầu thì cứ cmt mình giúp hết mình nha !!!!",,,"#Q&A, #cv",,
"Mojo Programming Language

Chris Lattner (tác giả LLVM, ngôn ngữ Swift), Tim Davis và team Modular vừa cho ra mắt ngôn ngữ lập trình Mojo. Được cho là dễ đọc như Python, nhanh hơn tốc độ của C++ và safety như Rust - gần như là thế mạnh của từng ngôn ngữ - vào trong Mojo. Điểm hay ho của Mojo là cho phép truy cập toàn bộ hệ sinh thái có sẵn của Python, ví dụ như là NumPy, Pandas, Matplotlib trực tiếp trong Mojo (Mojo is actually a superset of Python, so I can use my Python code).

Hiện tại Mojo đang ở trong giai đoạn beta.

Product Launch 2023 Keynote ngày 03/05/2023 còn nêu ra những vấn đề về chi phí, phần cứng, cơ sở hạ tầng khi triển khai và xây dựng các hệ thống AI.

Nếu những gì team Modular nói là đúng, thì đây sẽ là một bước ngoặt tương đối lớn 👀

Link: https://www.youtube.com/watch?v=-3Kf2ZZU-dg","Mojo Programming Language Chris Lattner (tác giả LLVM, ngôn ngữ Swift), Tim Davis và team Modular vừa cho ra mắt ngôn ngữ lập trình Mojo. Được cho là dễ đọc như Python, nhanh hơn tốc độ của C++ và safety như Rust - gần như là thế mạnh của từng ngôn ngữ - vào trong Mojo. Điểm hay ho của Mojo là cho phép truy cập toàn bộ hệ sinh thái có sẵn của Python, ví dụ như là NumPy, Pandas, Matplotlib trực tiếp trong Mojo (Mojo is actually a superset of Python, so I can use my Python code). Hiện tại Mojo đang ở trong giai đoạn beta. Product Launch 2023 Keynote ngày 03/05/2023 còn nêu ra những vấn đề về chi phí, phần cứng, cơ sở hạ tầng khi triển khai và xây dựng các hệ thống AI. Nếu những gì team Modular nói là đúng, thì đây sẽ là một bước ngoặt tương đối lớn Link: https://www.youtube.com/watch?v=-3Kf2ZZU-dg",,,"#sharing, #python",,
"Chào cả nhà,
Hiện em đang đọc hiểu paper này (https://arxiv.org/abs/1907.02189) về convergence proof của federated learning. Khi chứng minh Theorem 1 ở cuối trang 14 thì tác giả có chuyển từ F^*_k sang F^* như trong hình đính kèm. Mọi người cho em hỏi là tại sao lại làm như vậy được không ạ?
Cảm ơn mọi người!","Chào cả nhà, Hiện em đang đọc hiểu paper này (https://arxiv.org/abs/1907.02189) về convergence proof của federated learning. Khi chứng minh Theorem 1 ở cuối trang 14 thì tác giả có chuyển từ F^*_k sang F^* như trong hình đính kèm. Mọi người cho em hỏi là tại sao lại làm như vậy được không ạ? Cảm ơn mọi người!",,,"#Q&A, #math",,
,nan,,,,,
"Hello chào cả nhà! Mình muốn hỏi build 1 con server để training với cấu hình sau:
- cpu: intel core i7 12700K
- gpu: x2 RTX 3060 12GB
Thì liệu chip core có ảnh hưởng đến hiệu năng khi sử dụng cùng với 2 card rời không hay main quyết định ạ.
Mong được giải đáp.
Mình cám ơn.",Hello chào cả nhà! Mình muốn hỏi build 1 con server để training với cấu hình sau: - cpu: intel core i7 12700K - gpu: x2 RTX 3060 12GB Thì liệu chip core có ảnh hưởng đến hiệu năng khi sử dụng cùng với 2 card rời không hay main quyết định ạ. Mong được giải đáp. Mình cám ơn.,,,#Q&A,,
"Hello everyone, I need a data set of Danish language including handwriting or typing. Anyone who can share or sell, please comment below. Main purpose for research research and study.Thank you everyone","Hello everyone, I need a data set of Danish language including handwriting or typing. Anyone who can share or sell, please comment below. Main purpose for research research and study.Thank you everyone",,,#data,,
"Chào mọi người , em là một học sinh cấp 3 đang tìm tòi về các kiến thức của Machine learning và xử lý ảnh cho cuộc thi nghiên cứu khoa học . Em đang làm một mini-project về phân loại rác thì em có thắc mắc là giữa rác tái chế và rác không tái chế thì khi xử lý ảnh em cần phải phân tích các yếu tố nào của rác để có thể đưa vào tập dữ liệu . Song với đó là em cũng muốn được mọi người chỉ giáo về các hướng để phân tích ảnh ạ.
Em cảm ơn mọi người.","Chào mọi người , em là một học sinh cấp 3 đang tìm tòi về các kiến thức của Machine learning và xử lý ảnh cho cuộc thi nghiên cứu khoa học . Em đang làm một mini-project về phân loại rác thì em có thắc mắc là giữa rác tái chế và rác không tái chế thì khi xử lý ảnh em cần phải phân tích các yếu tố nào của rác để có thể đưa vào tập dữ liệu . Song với đó là em cũng muốn được mọi người chỉ giáo về các hướng để phân tích ảnh ạ. Em cảm ơn mọi người.",,,"#Q&A, #machine_learning, #cv",,
"🔥 #Neuron #Rendering, những bước tiến mới nhất trong xử lý đồ họa với AI",những bước tiến mới nhất trong xử lý đồ họa với AI,"#Neuron	#Rendering,",,"#sharing, #cv",,
"Hello cả nhà cho mình xin phép chia sẻ một tập podcast episode mà bên Forward Vietnam Podcast bọn mình vừa thu với anh Phong Nguyen - Chief AI Officer tại FPT Software. Nếu mọi người trong group muốn tìm hiểu thêm về Al, ChatGPT, và FPT thì có thể check it out ở đây ạ:
Youtube: https://youtu.be/UhP417Xn-L0
Spotify: https://open.spotify.com/episode/5tiEqSwcZALcJ6Z6OP4T18?si=KJcNKw7sTi2zCaDjh7HzVw
Apple Podcast: https://podcasts.apple.com/us/podcast/forward-vietnam-podcast/id1653799053?i=1000611506073
Forward Vietnam Podcast là nơi chúng mình bàn luận và chia sẻ tới cộng đồng những cuộc thảo luận về công việc, tài chính, kinh doanh, công nghệ và cuộc sống của các bạn trẻ từ Việt Nam. Các bạn có thể tìm hiểu thêm về podcast của bọn mình tại đường link dưới đây nha. Chúng mình rất mong nhận được sự phản hồi từ mọi người nhé!
https://linktr.ee/forwardvietnampodcast","Hello cả nhà cho mình xin phép chia sẻ một tập podcast episode mà bên Forward Vietnam Podcast bọn mình vừa thu với anh Phong Nguyen - Chief AI Officer tại FPT Software. Nếu mọi người trong group muốn tìm hiểu thêm về Al, ChatGPT, và FPT thì có thể check it out ở đây ạ: Youtube: https://youtu.be/UhP417Xn-L0 Spotify: https://open.spotify.com/episode/5tiEqSwcZALcJ6Z6OP4T18?si=KJcNKw7sTi2zCaDjh7HzVw Apple Podcast: https://podcasts.apple.com/us/podcast/forward-vietnam-podcast/id1653799053?i=1000611506073 Forward Vietnam Podcast là nơi chúng mình bàn luận và chia sẻ tới cộng đồng những cuộc thảo luận về công việc, tài chính, kinh doanh, công nghệ và cuộc sống của các bạn trẻ từ Việt Nam. Các bạn có thể tìm hiểu thêm về podcast của bọn mình tại đường link dưới đây nha. Chúng mình rất mong nhận được sự phản hồi từ mọi người nhé! https://linktr.ee/forwardvietnampodcast",,,"#sharing, #machine_learning",,
"Chào mọi người, hiện em là thạc sỹ ngôn ngữ học, e đang tìm hiểu về ứng dụng ngôn ngữ vào khoa học máy tính, e có thấy mảng xử lí ngôn ngữ tự nhiên (NLP) thì kết hợp cả kiến thức về ngôn ngữ học và máy tính. Do em chưa có kiến thức gì về ngành khoa học máy tính, các anh/ chị cho em hỏi đối với người chưa có nền tảng IT như em thì nên bắt đầu học từ đâu và những mảng nào trong ngành có thể ứng dụng được kiến thức chuyên sâu về ngôn ngữ, ngữ âm được ạ. Em cám ơn ạ","Chào mọi người, hiện em là thạc sỹ ngôn ngữ học, e đang tìm hiểu về ứng dụng ngôn ngữ vào khoa học máy tính, e có thấy mảng xử lí ngôn ngữ tự nhiên (NLP) thì kết hợp cả kiến thức về ngôn ngữ học và máy tính. Do em chưa có kiến thức gì về ngành khoa học máy tính, các anh/ chị cho em hỏi đối với người chưa có nền tảng IT như em thì nên bắt đầu học từ đâu và những mảng nào trong ngành có thể ứng dụng được kiến thức chuyên sâu về ngôn ngữ, ngữ âm được ạ. Em cám ơn ạ",,,"#Q&A, #nlp",,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 04/2023 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 04/2023 vào comment của post này.",,,#sharing,,
"Có thể câu hỏi đã cũ nhưng mà các bác cho em hỏi với là chi tiết cách xác định Anchor box trong YOLO với ạ ! không thì bác nào có tài liệu hay cho em xin đọc với, chứ em đọc miết mà những câu trả lời chưa làm em thấy thỏa mãn hic !
P/S: Em cảm ơn và ghi nhận tất cả các ý kiến đóng góp từ mọi người ạ","Có thể câu hỏi đã cũ nhưng mà các bác cho em hỏi với là chi tiết cách xác định Anchor box trong YOLO với ạ ! không thì bác nào có tài liệu hay cho em xin đọc với, chứ em đọc miết mà những câu trả lời chưa làm em thấy thỏa mãn hic ! P/S: Em cảm ơn và ghi nhận tất cả các ý kiến đóng góp từ mọi người ạ",,,"#Q&A, #deep_learning",,
"MLOpsVN tiếp tục tổ chức seminar FREE về công nghệ search hay được sử dụng trong các hệ thống khuyến nghị (recommendation system) vào tối thứ 5 tuần này, mời các bác quan tâm đăng ký tham gia ạ 😁","MLOpsVN tiếp tục tổ chức seminar FREE về công nghệ search hay được sử dụng trong các hệ thống khuyến nghị (recommendation system) vào tối thứ 5 tuần này, mời các bác quan tâm đăng ký tham gia ạ",,,#webinar,,
"Xin chào mọi người, mấy ngày nay e bắt đầu tìm hiểu về Transformer model và e cũng đã tìm đọc các blog để xem kĩ hơn về các thuật toán của nó nhưng mà e thấy có khá ít blog bằng Tiếng Việt nói đến . Vậy nên trong quá trình tìm hiểu, e đã viết 1 bài viết nói chi tiết từng bước một số thuật toán cơ bản của Transformer model theo cái cách mà e đã hiểu . Bởi vì e cũng mới học nên sự nhầm lẫn về kiến thức sẽ xảy ra nên e hy vọng sẽ nhận được phản hồi của mn ạ @@ e cảm ơn ạ !","Xin chào mọi người, mấy ngày nay e bắt đầu tìm hiểu về Transformer model và e cũng đã tìm đọc các blog để xem kĩ hơn về các thuật toán của nó nhưng mà e thấy có khá ít blog bằng Tiếng Việt nói đến . Vậy nên trong quá trình tìm hiểu, e đã viết 1 bài viết nói chi tiết từng bước một số thuật toán cơ bản của Transformer model theo cái cách mà e đã hiểu . Bởi vì e cũng mới học nên sự nhầm lẫn về kiến thức sẽ xảy ra nên e hy vọng sẽ nhận được phản hồi của mn ạ @@ e cảm ơn ạ !",,,,,"#sharing, #deep_learning"
"MLOps Marathon là cuộc thi về MLOps đầu tiên được tổ chức tại Việt Nam nhằm tạo sân chơi cho các đội vừa xây dựng các AI/ML model vừa triển khai và vận hành nó trên môi trường production.
Không chỉ vậy, giải nhất có giá trị lên tới 100 triệu đồng, đăng ký tham gia thôi nào mọi người ơi 🥳","MLOps Marathon là cuộc thi về MLOps đầu tiên được tổ chức tại Việt Nam nhằm tạo sân chơi cho các đội vừa xây dựng các AI/ML model vừa triển khai và vận hành nó trên môi trường production. Không chỉ vậy, giải nhất có giá trị lên tới 100 triệu đồng, đăng ký tham gia thôi nào mọi người ơi",,,,,#sharing
"[ Hỏi về thư viện Python] Mình đang làm bài toán ước lượng phân bố của P(Y |X), với Y là multi dependent/correlated output, và X là multivariate continuous (not categorical) features. Mình muốn tìm thư viện Python mà implement được xác suất theo kiểu :(1) approximate P(Y|X) ~ N(m, S), where off-diagonal entries are not zeros hoặc là có thể sample được Y~ P(Y|X).
Đây là bài toán Bayesan multi-dependent output regression. Tuy nhiên mấy thư viện như Gaussian process ở sklearn thì hiện tại implementation, treat các output independent với nha","[ Hỏi về thư viện Python] Mình đang làm bài toán ước lượng phân bố của P(Y |X), với Y là multi dependent/correlated output, và X là multivariate continuous (not categorical) features. Mình muốn tìm thư viện Python mà implement được xác suất theo kiểu :(1) approximate P(Y|X) ~ N(m, S), where off-diagonal entries are not zeros hoặc là có thể sample được Y~ P(Y|X). Đây là bài toán Bayesan multi-dependent output regression. Tuy nhiên mấy thư viện như Gaussian process ở sklearn thì hiện tại implementation, treat các output independent với nha",,,,,"#Q&A, #math, #python"
"Hi everyone, I am trying to use some voice conversion model. Due to hardware limitation, each voice input file can only be as long as N seconds. I can still naively cut the input into various segments if there are enough silence segments. Howevers, some inputs have long stretches of non-silence segments. My idea is that I can split the input into overlapped segments, feed them into the model, then merge the outputs into one single outputs. So my question is how to properly do the merging. Not sure if it can be solved with audio englneering or signal processing or even deep learning.
Thank you very much. Have a great day.","Hi everyone, I am trying to use some voice conversion model. Due to hardware limitation, each voice input file can only be as long as N seconds. I can still naively cut the input into various segments if there are enough silence segments. Howevers, some inputs have long stretches of non-silence segments. My idea is that I can split the input into overlapped segments, feed them into the model, then merge the outputs into one single outputs. So my question is how to properly do the merging. Not sure if it can be solved with audio englneering or signal processing or even deep learning. Thank you very much. Have a great day.",,,,,bỏ
"Hello mọi người!
Mình có bài toán build một regression model để predict target variable có distribution như này. Mình đã thử nhìu cách, dùng SOTA gradient boosted algorithm như LGBM hay XGBoost mà RMSE vẫn rất cao. Theo mình tìm hiểu thì đây là Tweedie Distribution, ko biết có cao nhân nào biết cách giải quyết vấn đề này không ạ?","Hello mọi người! Mình có bài toán build một regression model để predict target variable có distribution như này. Mình đã thử nhìu cách, dùng SOTA gradient boosted algorithm như LGBM hay XGBoost mà RMSE vẫn rất cao. Theo mình tìm hiểu thì đây là Tweedie Distribution, ko biết có cao nhân nào biết cách giải quyết vấn đề này không ạ?",,,,,"#Q&A, #machine_learning"
"Vì vụ việc này liên quan tới ba thứ: machine learning, Penn State University, và việc đạo văn, tôi thấy cần phải lên tiếng. Hy vọng chưa có bạn nào trong group từng bỏ ra số tiền lớn để theo học tại đây:

https://forum.machinelearningcoban.com/t/mot-vai-nghi-van-ve-vien-ung-dung-toan-hoc-big-data-data-analytics-data-mining/5420","Vì vụ việc này liên quan tới ba thứ: machine learning, Penn State University, và việc đạo văn, tôi thấy cần phải lên tiếng. Hy vọng chưa có bạn nào trong group từng bỏ ra số tiền lớn để theo học tại đây: https://forum.machinelearningcoban.com/t/mot-vai-nghi-van-ve-vien-ung-dung-toan-hoc-big-data-data-analytics-data-mining/5420",,,,,"#sharing, #machine_learning"
"#PyTorch Multi-GPUs training trong PyTorch
Chào mọi người. Xin phép hỏi các anh chị có kinh nghiệm về training model bằng PyTorch trên nhiều GPUs. Hiện tại em đang gặp vấn đề khi chạy code trên 1 GPU thì ko sảy ra vấn đề gì, nhưng khi đẩy lên nhiều GPUs thì bị lỗi khi thực hiện backward cho loss. Em xin mô tả code ở ví dụ đơn giản như ảnh bên dưới đây.
Vấn đề này em có tìm kiếm thì biết rằng cho training trên nhiều GPUs thì tương đương có chừng ấy outputs được trả về cùng lúc. Do vậy việc backprop thông qua `loss.backward()` sẽ sảy ra lỗi, do `loss` lúc này là 1 Tensor nhiều chiều (PyTorch chỉ cho phép backward với scalar). Một cách khắc phục em tìm được là sử dụng `loss.sum().backward()` hoặc `loss.mean().backward()`.
Câu hỏi em đặt ra là việc lấy `sum` hoặc lấy `mean` của các losses trên sau đó backward thì có thay đổi bản chất hoặc kết quả của quá trình backpropagation hay không ? Ngoài ra có cách nào khác đơn giản hơn để không phải dùng sum hoặc mean của loss trước khi backward hay không ? Em xin cảm ơn mọi người.","Multi-GPUs training trong PyTorch Chào mọi người. Xin phép hỏi các anh chị có kinh nghiệm về training model bằng PyTorch trên nhiều GPUs. Hiện tại em đang gặp vấn đề khi chạy code trên 1 GPU thì ko sảy ra vấn đề gì, nhưng khi đẩy lên nhiều GPUs thì bị lỗi khi thực hiện backward cho loss. Em xin mô tả code ở ví dụ đơn giản như ảnh bên dưới đây. Vấn đề này em có tìm kiếm thì biết rằng cho training trên nhiều GPUs thì tương đương có chừng ấy outputs được trả về cùng lúc. Do vậy việc backprop thông qua `loss.backward()` sẽ sảy ra lỗi, do `loss` lúc này là 1 Tensor nhiều chiều (PyTorch chỉ cho phép backward với scalar). Một cách khắc phục em tìm được là sử dụng `loss.sum().backward()` hoặc `loss.mean().backward()`. Câu hỏi em đặt ra là việc lấy `sum` hoặc lấy `mean` của các losses trên sau đó backward thì có thay đổi bản chất hoặc kết quả của quá trình backpropagation hay không ? Ngoài ra có cách nào khác đơn giản hơn để không phải dùng sum hoặc mean của loss trước khi backward hay không ? Em xin cảm ơn mọi người.",#PyTorch,,,,"#Q&A, #python"
5 CÔNG NGHỆ MỚI TRONG DATA ENGINEERING.,5 CÔNG NGHỆ MỚI TRONG DATA ENGINEERING.,,,,,#sharing
"Chào m.n và a Tiep VuHuu, e đang có bài toán như thế này nhờ m.n giúp đỡ:
Export ra file báo cáo dựa trên câu thoại trên zalo
Ví dụ câu này: ""Em là Văn Thanh, hôm nay 01/01/2023 em đã làm xong task a, b ,c""
Expect: Ngày 01/01/2023, Văn Thanh, task a, b, c: đã xong
Hoặc: ""Chào anh Tuấn, em là Thanh, hôm nay 31/01/2023 e vô trễ nên làm chưa kịp task a, mới xong b, c thôi. Ngoài ra task d e cũng chưa làm""
Expect: Ngày 31/01/2023, Thanh, task b, c: đã xong; task a, d chưa xong
Em bỏ ML 6 năm r nên giờ quên hết, m.n cho e hỏi đây là dạng bài toán gì (e nghĩ là speech to text và data mining, ko biết đúng ko); và có thư viện nào mì ăn liền ko ạ (e prefer Java vì các dự án hiện tại đều viết bằng Java)
Thank m.n","Chào m.n và a Tiep VuHuu, e đang có bài toán như thế này nhờ m.n giúp đỡ: Export ra file báo cáo dựa trên câu thoại trên zalo Ví dụ câu này: ""Em là Văn Thanh, hôm nay 01/01/2023 em đã làm xong task a, b ,c"" Expect: Ngày 01/01/2023, Văn Thanh, task a, b, c: đã xong Hoặc: ""Chào anh Tuấn, em là Thanh, hôm nay 31/01/2023 e vô trễ nên làm chưa kịp task a, mới xong b, c thôi. Ngoài ra task d e cũng chưa làm"" Expect: Ngày 31/01/2023, Thanh, task b, c: đã xong; task a, d chưa xong Em bỏ ML 6 năm r nên giờ quên hết, m.n cho e hỏi đây là dạng bài toán gì (e nghĩ là speech to text và data mining, ko biết đúng ko); và có thư viện nào mì ăn liền ko ạ (e prefer Java vì các dự án hiện tại đều viết bằng Java) Thank m.n",,,,,"#Q&A, #python, #nlp"
"Chào mn ạ, theo kết quả của giải tích 2 thì nếu tồn tại đạo hàm có hướng tại điểm theta và vector gradient tại theta khác 0 thì kể từ điểm theta hàm y sẽ giảm nhiều nhất theo hướng của vector gradient. Vậy thường hàm lỗi trong học sâu có thoả mãn điều kiện tồn tại đạo hàm có hướng tại mọi điểm không ạ? Nếu không thì với những tính chất nào của mô hình và hàm lỗi thì hàm lỗi sẽ có đạo hàm có hướng tại mọi điểm ạ? Em cảm ơn mọi người","Chào mn ạ, theo kết quả của giải tích 2 thì nếu tồn tại đạo hàm có hướng tại điểm theta và vector gradient tại theta khác 0 thì kể từ điểm theta hàm y sẽ giảm nhiều nhất theo hướng của vector gradient. Vậy thường hàm lỗi trong học sâu có thoả mãn điều kiện tồn tại đạo hàm có hướng tại mọi điểm không ạ? Nếu không thì với những tính chất nào của mô hình và hàm lỗi thì hàm lỗi sẽ có đạo hàm có hướng tại mọi điểm ạ? Em cảm ơn mọi người",,,,,"#Q&A, #math, #machine_learning"
#AR và #AI đang thay đổi cuộc chơi!,và đang thay đổi cuộc chơi!,#AR	#AI,,,,#sharing
"Mình đang tìm hiểu về Mạng MLP và lan truyền ngược
mn cho em hỏi về các biến dạng của lan truyền ngược được không ạ! em tìm trên mạng thì thấy ít tài liệu về nó và hỏi chatgpt thì câu trả lời mỗi lúc mỗi khác ạ! em cảm ơn",Mình đang tìm hiểu về Mạng MLP và lan truyền ngược mn cho em hỏi về các biến dạng của lan truyền ngược được không ạ! em tìm trên mạng thì thấy ít tài liệu về nó và hỏi chatgpt thì câu trả lời mỗi lúc mỗi khác ạ! em cảm ơn,,,,,"#Q&A, #machine_learning"
"hello all, cho mình hỏi
ngoài Elastic có engine nào support Search sử dụng ML để cá nhân hóa kết quả k mn?
mình muốn làm chức năng search cho trang tìm kiếm Job sử dụng ML, giả sử user tìm kiếm job PHP ở hà nội, hoặc user click xem các job ở hà nội (không thực hiện search) , thì nó sẽ suggest và đưa các job ở hà nội lên hàng đầu
vì project của mình nhỏ ( search khoảng 1k item thôi ) nên dùng Elastic là hơi thừa, với lại nó ngốn Ram quá, mình chỉ định chạy trên con Vps 1gb RAM thôi","hello all, cho mình hỏi ngoài Elastic có engine nào support Search sử dụng ML để cá nhân hóa kết quả k mn? mình muốn làm chức năng search cho trang tìm kiếm Job sử dụng ML, giả sử user tìm kiếm job PHP ở hà nội, hoặc user click xem các job ở hà nội (không thực hiện search) , thì nó sẽ suggest và đưa các job ở hà nội lên hàng đầu vì project của mình nhỏ ( search khoảng 1k item thôi ) nên dùng Elastic là hơi thừa, với lại nó ngốn Ram quá, mình chỉ định chạy trên con Vps 1gb RAM thôi",,,,,#Q&A
AI mới tự phát triển dựa trên Thuyết tiến hóa Darwin.,AI mới tự phát triển dựa trên Thuyết tiến hóa Darwin.,,,,,#sharing
"Xin hỏi có anh/chị/em/bạn nào đang tập trung học cuốn ""Dive into Deep Learning"" ko ạ? Mình muốn tìm kiếm một nhóm bạn cùng học để tiện trao đổi và học hỏi. MXNet thấy hơi khó nhằn. Hi","Xin hỏi có anh/chị/em/bạn nào đang tập trung học cuốn ""Dive into Deep Learning"" ko ạ? Mình muốn tìm kiếm một nhóm bạn cùng học để tiện trao đổi và học hỏi. MXNet thấy hơi khó nhằn. Hi",,,,,#Q&A
"Cần giúp đỡ về cách sử dụng pretrained model.
Em là software engineer đang làm project trong đó có chức năng về similarity search. Vì không phải dân chuyên AI nên em chỉ học cách dùng pretrained model. Process của em như sau:
1. Dùng pyvi.ViTokenizer để tokenize input
2. Dùng model vinai/phobert-large để encode thành vector:
3. Lưu vector này vào Postgres với extension ankane/pgvector
4. Search vector bằng cosine_distance, provided by pgvector
Mọi thứ khá ổn khi em làm POC, em search bằng những câu hoàn chỉnh như: ""nhà cung cấp mainboard H110i tại HCM"". Nhưng đưa vào sử dụng thì khách hàng query rất vắn tắt kiểu: ""H110i"" và kết quả trả về còn thua cả full text search, thậm chí không match được câu nào có chữ ""H110i"", trong khi search nguyên câu thì lại có.
Expect của em là kể cả khi khách hàng chỉ nhập tên mã sản phẩm thì ít nhất cũng trả về những câu có chứa mã sản phẩm đó.
Em khá bế tắc ở chỗ này, mong được mọi người tư vấn.","Cần giúp đỡ về cách sử dụng pretrained model. Em là software engineer đang làm project trong đó có chức năng về similarity search. Vì không phải dân chuyên AI nên em chỉ học cách dùng pretrained model. Process của em như sau: 1. Dùng pyvi.ViTokenizer để tokenize input 2. Dùng model vinai/phobert-large để encode thành vector: 3. Lưu vector này vào Postgres với extension ankane/pgvector 4. Search vector bằng cosine_distance, provided by pgvector Mọi thứ khá ổn khi em làm POC, em search bằng những câu hoàn chỉnh như: ""nhà cung cấp mainboard H110i tại HCM"". Nhưng đưa vào sử dụng thì khách hàng query rất vắn tắt kiểu: ""H110i"" và kết quả trả về còn thua cả full text search, thậm chí không match được câu nào có chữ ""H110i"", trong khi search nguyên câu thì lại có. Expect của em là kể cả khi khách hàng chỉ nhập tên mã sản phẩm thì ít nhất cũng trả về những câu có chứa mã sản phẩm đó. Em khá bế tắc ở chỗ này, mong được mọi người tư vấn.",,,,,"#Q&A, #nlp, #deep_learning"
"Xin chào mọi người, em đang làm bài về object detection và đang dùng yolo v8. Mọi người cho em hỏi ở trong phần gắn nhãn thì gắn nhãn kiểu rectangle hay polygon có hiệu quả hơn ạ, và yolo có hỗ trợ detect bounding box dạng polygon không ạ, em xin cảm ơn","Xin chào mọi người, em đang làm bài về object detection và đang dùng yolo v8. Mọi người cho em hỏi ở trong phần gắn nhãn thì gắn nhãn kiểu rectangle hay polygon có hiệu quả hơn ạ, và yolo có hỗ trợ detect bounding box dạng polygon không ạ, em xin cảm ơn",,,,,"#Q&A, #cv, #deep_learning"
"Chào mọi người ạ, em đang đọc bài viết về PCA của thầy Tiệp. Em có một chỗ thắc mắc là : trong bài viết có một câu thế này ""PCA có thể được coi là phương pháp đi tìm một hệ cơ sở trực chuẩn đóng vai trò một phép xoay, sao cho trong hệ cơ sở mới này, phương sai theo một số chiều nào đó là rất nhỏ, và ta có thể bỏ qua"". Tuy nhiên giả sử em có một ma trận dữ liệu X và một phép xoay ứng với ma trận trực giao U, tức là X = UY. Rõ ràng ma trận hiệp phương sai của X và Y là như nhau, tuy em lại thấy điều này rất phản trực giác. Ví dụ cụ thể như hình bên dưới, rõ ràng khi áp dụng một phép xoay thì phương sai đã bị thay đổi, tuy nhiên trong công thức trên thì ma trận hiệp phương sai là như nhau. Em đã hiểu sai ở đâu vậy ạ.","Chào mọi người ạ, em đang đọc bài viết về PCA của thầy Tiệp. Em có một chỗ thắc mắc là : trong bài viết có một câu thế này ""PCA có thể được coi là phương pháp đi tìm một hệ cơ sở trực chuẩn đóng vai trò một phép xoay, sao cho trong hệ cơ sở mới này, phương sai theo một số chiều nào đó là rất nhỏ, và ta có thể bỏ qua"". Tuy nhiên giả sử em có một ma trận dữ liệu X và một phép xoay ứng với ma trận trực giao U, tức là X = UY. Rõ ràng ma trận hiệp phương sai của X và Y là như nhau, tuy em lại thấy điều này rất phản trực giác. Ví dụ cụ thể như hình bên dưới, rõ ràng khi áp dụng một phép xoay thì phương sai đã bị thay đổi, tuy nhiên trong công thức trên thì ma trận hiệp phương sai là như nhau. Em đã hiểu sai ở đâu vậy ạ.",,,,,"#Q&A, #math, #machine_learning"
"TƯƠNG LAI CỦA TRÍ TUỆ NHÂN TẠO TẠO SINH 2023 - NIC
Thân mời a Tiệp cùng các bạn bên Forum machine learning cơ bản tới tham gia sự kiện: "" The future of Generative AI 2023"" do bên NIC tổ chức.
Trung tâm Đổi mới sáng tạo Quốc gia (NIC), Bộ Kế hoạch và Đầu tư phối hợp với Mạng lưới Đổi mới sáng tạo Việt Nam tại Thung lũng Silicon tổ chức hội thảo “Tương lai của sự sáng tạo từ công nghệ AI 2023”.
Hội thảo có sự góp mặt của các chuyên gia AI hàng đầu từ Thung lũng Silicon và Việt Nam, dự kiến thu hút được sự quan tâm đáng kể từ các nhà lãnh đạo, nhà nghiên cứu và những người đam mê ngành AI trên toàn thế giới, mang đến cơ hội duy nhất để kết nối với các công ty công nghệ và tài năng hàng đầu của Việt Nam và Hoa Kỳ.
Anh Kim Pham - Cohost AI
Anh Hung Tran - GotIt AI
Chị Tâm Lê - Turing
Anh Phong Nguyen FPT AI
Anh Võ Minh Tuệ - Kỹ sư AI
Chị Lan Shuezhao - BasisSet
Ngoài ra, với mục đích mang đến cho người tham dự cơ hội trao đổi trực tiếp với đại diện các công ty công nghệ từ Silicon Valley, các xu hướng mới nhất và cách áp dụng AI để cải thiện hoạt động kinh doanh, phát triển thị trường.
Thời gian: 8:30 - 12:00 ngày 20/04/2023
Link đăng ký: bit.ly/cohost-nic
Sự kiện Online qua Zoom và Offline tại VP NIC số 7 Tôn Thất Thuyết, Cầu Giấy, Hà Nội.
 — với Hung Tran và 4 người khác.","TƯƠNG LAI CỦA TRÍ TUỆ NHÂN TẠO TẠO SINH 2023 - NIC Thân mời a Tiệp cùng các bạn bên Forum machine learning cơ bản tới tham gia sự kiện: "" The future of Generative AI 2023"" do bên NIC tổ chức. Trung tâm Đổi mới sáng tạo Quốc gia (NIC), Bộ Kế hoạch và Đầu tư phối hợp với Mạng lưới Đổi mới sáng tạo Việt Nam tại Thung lũng Silicon tổ chức hội thảo “Tương lai của sự sáng tạo từ công nghệ AI 2023”. Hội thảo có sự góp mặt của các chuyên gia AI hàng đầu từ Thung lũng Silicon và Việt Nam, dự kiến thu hút được sự quan tâm đáng kể từ các nhà lãnh đạo, nhà nghiên cứu và những người đam mê ngành AI trên toàn thế giới, mang đến cơ hội duy nhất để kết nối với các công ty công nghệ và tài năng hàng đầu của Việt Nam và Hoa Kỳ. Anh Kim Pham - Cohost AI Anh Hung Tran - GotIt AI Chị Tâm Lê - Turing Anh Phong Nguyen FPT AI Anh Võ Minh Tuệ - Kỹ sư AI Chị Lan Shuezhao - BasisSet Ngoài ra, với mục đích mang đến cho người tham dự cơ hội trao đổi trực tiếp với đại diện các công ty công nghệ từ Silicon Valley, các xu hướng mới nhất và cách áp dụng AI để cải thiện hoạt động kinh doanh, phát triển thị trường. Thời gian: 8:30 - 12:00 ngày 20/04/2023 Link đăng ký: bit.ly/cohost-nic Sự kiện Online qua Zoom và Offline tại VP NIC số 7 Tôn Thất Thuyết, Cầu Giấy, Hà Nội. — với Hung Tran và 4 người khác.",,,,,"#sharing, #webinar"
"Seminar tiếp theo tổ chức bởi MLOpsVN với chủ đề Model Optimization sẽ diễn ra online lúc 8h tối thứ 5 tuần này, mời cả nhà đăng ký tham gia nếu quan tâm ạ 😁","Seminar tiếp theo tổ chức bởi MLOpsVN với chủ đề Model Optimization sẽ diễn ra online lúc 8h tối thứ 5 tuần này, mời cả nhà đăng ký tham gia nếu quan tâm ạ",,,,,"#sharing, #webinar"
"Mọi người cho em hỏi tại sao accuracy_score của decision tree lại cao hơn cả Random Forest vậy ạ?
code: https://github.com/akirayorunoe/MLLearning/blob/main/SONAR%20Rock%20vs%20Mine%20Prediction/rock_vs_mine_score.py",Mọi người cho em hỏi tại sao accuracy_score của decision tree lại cao hơn cả Random Forest vậy ạ? code: https://github.com/akirayorunoe/MLLearning/blob/main/SONAR%20Rock%20vs%20Mine%20Prediction/rock_vs_mine_score.py,,,,,"#Q&A, #machine_learning"
"Hello all, mình đang tìm thông tin về ranh giới lat/long chi tiết của từng tỉnh/huyện/xã ở VN cập nhật mới nhất. Check trang https://gadm.org/download_country.html rồi chọn VN thì có đây đủ thông tin, nhưng thông tin hơi cũ, 1 số nơi ở VN đã xác nhập, đổi tên như Thành Phố Thủ Đức thì ko có. Nên lên đây nhờ mọi người, nếu ai có data mới nhất thì share giúp mình với. Many thanks.","Hello all, mình đang tìm thông tin về ranh giới lat/long chi tiết của từng tỉnh/huyện/xã ở VN cập nhật mới nhất. Check trang https://gadm.org/download_country.html rồi chọn VN thì có đây đủ thông tin, nhưng thông tin hơi cũ, 1 số nơi ở VN đã xác nhập, đổi tên như Thành Phố Thủ Đức thì ko có. Nên lên đây nhờ mọi người, nếu ai có data mới nhất thì share giúp mình với. Many thanks.",,,,,"#Q&A, #data"
"Xin chào mọi người!
Mình đang phát triển một tool gán nhãn dữ liệu mới base trên LabelMe và mô hình Segment Anything mới nhất của Facebook.
Xin phép được chia sẻ đến toàn thể nhóm mình để xin gạch đá và comment để tiếp tục cải tiến.
- Link: https://github.com/vietanhdev/anylabeling
- Demo: https://www.youtube.com/watch?v=5iQSGL7ebXE",Xin chào mọi người! Mình đang phát triển một tool gán nhãn dữ liệu mới base trên LabelMe và mô hình Segment Anything mới nhất của Facebook. Xin phép được chia sẻ đến toàn thể nhóm mình để xin gạch đá và comment để tiếp tục cải tiến. - Link: https://github.com/vietanhdev/anylabeling - Demo: https://www.youtube.com/watch?v=5iQSGL7ebXE,,,,,#sharing
"Em chào mọi người, Hiện tại, em vừa học xong 2 machine learning và deep learning specializations của thầy Andrew Ng. Mọi người có thể gợi ý cho em một số project áp dụng kiến thức đã học và học tiếp những kiến thức gì cho thị giác máy tính được không ạ. Em cảm ơn mọi người","Em chào mọi người, Hiện tại, em vừa học xong 2 machine learning và deep learning specializations của thầy Andrew Ng. Mọi người có thể gợi ý cho em một số project áp dụng kiến thức đã học và học tiếp những kiến thức gì cho thị giác máy tính được không ạ. Em cảm ơn mọi người",,,,,#Q&A
"Một thông tin mà mình cho là quan trọng nếu bạn nào muốn sử dụng mô hình ngôn lớn (LLM) vào việc phát triển sản phẩm thương mại.
Trích nguồn:",Một thông tin mà mình cho là quan trọng nếu bạn nào muốn sử dụng mô hình ngôn lớn (LLM) vào việc phát triển sản phẩm thương mại. Trích nguồn:,,,,,"#sharing, #nlp"
,nan,,,,,bỏ
"Nếu bạn hỏi tôi sau những hào quang ChatGPT, GPT-4 thì tiếp theo sẽ là gì, tôi sẽ trả lời là AutoGPT, với khoảng 30.000 sao ở repo trên Github.
Với AutoGPT các prompt sẽ được liên kết với nhau tạo ra một agents, có thể lên suy nghĩ, lên kế hoạch, thực hiện từng bước một và đạt được mục tiêu. Ví dụ, bạn hỏi cách tạo một startup với 100$ funding, thay vì trả lời trực tiếp, AutoGPT sẽ lên plan:
1. Research low-cost business models that require minimal funding.
2. Identify potential target markets and their needs.
3. Develop a lean MVP and test with target market to validate demand.
Sau đó, với mỗi task mô hình GPT sẽ sinh ra các câu trả lời dựa vào thông tin trên internet, và có thể sẽ có các task nhỏ được break down ra nữa. Cuối cùng nó sẽ tổng hợp lại và cho ra kết quả cuối cùng. Và tất cả đều tự động ^^","Nếu bạn hỏi tôi sau những hào quang ChatGPT, GPT-4 thì tiếp theo sẽ là gì, tôi sẽ trả lời là AutoGPT, với khoảng 30.000 sao ở repo trên Github. Với AutoGPT các prompt sẽ được liên kết với nhau tạo ra một agents, có thể lên suy nghĩ, lên kế hoạch, thực hiện từng bước một và đạt được mục tiêu. Ví dụ, bạn hỏi cách tạo một startup với 100$ funding, thay vì trả lời trực tiếp, AutoGPT sẽ lên plan: 1. Research low-cost business models that require minimal funding. 2. Identify potential target markets and their needs. 3. Develop a lean MVP and test with target market to validate demand. Sau đó, với mỗi task mô hình GPT sẽ sinh ra các câu trả lời dựa vào thông tin trên internet, và có thể sẽ có các task nhỏ được break down ra nữa. Cuối cùng nó sẽ tổng hợp lại và cho ra kết quả cuối cùng. Và tất cả đều tự động ^^",,,,,"#shairng, #deep_learning, #nlp"
"Hi mọi người, hôm nay mình muốn chia sẻ một tool khá hay ứng dụng của chatgpt/LLM được phát triển bởi CNext, giúp hỗ trợ người dùng trả lời câu hỏi, tìm insight từ dữ liệu bằng ngôn ngữ tự nhiên.
Theo mình thấy CNext hoạt động khá ấn tưởng, dù bạn có kiến thức về lập trình hay không thì một số task yêu cầu phức tạp bạn chỉ cần mô tả ý tưởng của mình CNext có thể hỗ trợ bạn triển khai ý tưởng đó trên dữ liệu của bạn.
Mình thấy tool có khá nhiều api hỗ trợ:
Import data: Tải lên một file dưới định dạng .csv (đây sẽ là file bạn mong muốn phân tích và tìm insight)
Table ops: Thao tác xử lý dữ liệu trên các trường dữ liệu
Google search: cho phép bạn search thông tin trên google và trả về định dạng json dễ dàng để khai thác dữ liệu
Twitter search hoặc Twitter Entities: Cho phép lấy thông tin trên twitter (rất phù hợp trong việc phân tích một trend hoặc một sản phẩm được giới thiệu trên twitter, giúp người dùng có thể theo dõi mức độ quan tâm hoặc thái độ của mọi người đối với tweet của mình
Plotting: Cho phép visualize dữ liệu, design biểu đồ thông qua ý tưởng ngôn ngữ tự nhiên nhất có thể
Ngoài ra còn một số api khác giúp cho các bạn HR có thể tìm kiếm ứng viên trên linkedin phù hợp với tiêu chí của mọi người đặt ra,...
Một số hướng dẫn và ví dụ bạn có thể xem ở đây: https://docs.cnext.io/sample-playbooks
https://twitter.com/cnextdotio
Dưới đây là một demo khá thú vị về cách CNext xử lý dữ liệu. Hy vọng bài viết hữu ích với mọi người. Chúc mọi người một ngày vui vẻ 🤗","Hi mọi người, hôm nay mình muốn chia sẻ một tool khá hay ứng dụng của chatgpt/LLM được phát triển bởi CNext, giúp hỗ trợ người dùng trả lời câu hỏi, tìm insight từ dữ liệu bằng ngôn ngữ tự nhiên. Theo mình thấy CNext hoạt động khá ấn tưởng, dù bạn có kiến thức về lập trình hay không thì một số task yêu cầu phức tạp bạn chỉ cần mô tả ý tưởng của mình CNext có thể hỗ trợ bạn triển khai ý tưởng đó trên dữ liệu của bạn. Mình thấy tool có khá nhiều api hỗ trợ: Import data: Tải lên một file dưới định dạng .csv (đây sẽ là file bạn mong muốn phân tích và tìm insight) Table ops: Thao tác xử lý dữ liệu trên các trường dữ liệu Google search: cho phép bạn search thông tin trên google và trả về định dạng json dễ dàng để khai thác dữ liệu Twitter search hoặc Twitter Entities: Cho phép lấy thông tin trên twitter (rất phù hợp trong việc phân tích một trend hoặc một sản phẩm được giới thiệu trên twitter, giúp người dùng có thể theo dõi mức độ quan tâm hoặc thái độ của mọi người đối với tweet của mình Plotting: Cho phép visualize dữ liệu, design biểu đồ thông qua ý tưởng ngôn ngữ tự nhiên nhất có thể Ngoài ra còn một số api khác giúp cho các bạn HR có thể tìm kiếm ứng viên trên linkedin phù hợp với tiêu chí của mọi người đặt ra,... Một số hướng dẫn và ví dụ bạn có thể xem ở đây: https://docs.cnext.io/sample-playbooks https://twitter.com/cnextdotio Dưới đây là một demo khá thú vị về cách CNext xử lý dữ liệu. Hy vọng bài viết hữu ích với mọi người. Chúc mọi người một ngày vui vẻ",,,,,"#shairng, #deep_learning, #nlp"
"Hello mọi người, việc các công cụ như ChatGPT và Midjourney trở nên phổ biến trong thời gian gần đây đã gây ra vô số ý kiến trái chiều trong nhiều lĩnh vực khác nhau. Trong bài viết này mình phân tích hai khía cạnh về sự sụt giảm chất lượng và số lượng content tạo ra bởi con người cùng với việc thiếu dữ liệu dùng cho training generative model trong tương lai. Nếu mọi người có insight về sự ảnh hưởng của AI trong các lĩnh vực như design hoặc content writing thì cũng có thể cùng chia sẻ nhé.","Hello mọi người, việc các công cụ như ChatGPT và Midjourney trở nên phổ biến trong thời gian gần đây đã gây ra vô số ý kiến trái chiều trong nhiều lĩnh vực khác nhau. Trong bài viết này mình phân tích hai khía cạnh về sự sụt giảm chất lượng và số lượng content tạo ra bởi con người cùng với việc thiếu dữ liệu dùng cho training generative model trong tương lai. Nếu mọi người có insight về sự ảnh hưởng của AI trong các lĩnh vực như design hoặc content writing thì cũng có thể cùng chia sẻ nhé.",,,,,"#sharing, #deep_learning"
"VinAI Seminar - ""An Integrated Framework for Controllable Text Generation""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Wei Xu, Assist. Prof. at Georgia Institute of Technology
Time: 9:30 am - 10:30 am (GMT+7), Tue, Apr 11, 2023","VinAI Seminar - ""An Integrated Framework for Controllable Text Generation"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Wei Xu, Assist. Prof. at Georgia Institute of Technology Time: 9:30 am - 10:30 am (GMT+7), Tue, Apr 11, 2023",,,,,bỏ
"Gần đây (r-)Polars nổi lên như 1 thư viện cạnh tranh với Pandas/Tidyverse về tốc độ xử lý dữ liệu lớn dạng bảng. Trong repository (https://github.com/linhduongtuan/Polars_vs_Pandas) này của mình, mình sẽ thử tốc độ dữ liệu lớn có tên là 69M_reddit_account.csv. Dữ liệu này có 69 triệu dòng và 7 biến số (cột). Khi giải nén file *gz thành *csv nó nặng ~3.3Gb. Mình sẽ so sánh hiệu năng đọc các files này (định dạng CSV, Parquet, và Feather) bằng Polars so với Pandas (version 2) sử dụng NumPy  và PyArrow backends.

Kết quả cho thấy:
Polars thắng trong tất cả các thí nghiệm;
Pandas với PyArrow backend tiệm cận tốc độ Polars, và nhanh hơn đáng kể khi đọc files *parquet và *feather (nhưng lại chậm hơn với *csv).
Lời tạm kết:
Mình nghĩ Polars đã và đang là đối thủ lớn của Pandas;
Rust sẽ là ngôn ngôn lập trình rất thú vị và đáng chúng ta đầu tư thời gian để học.
Dữ liệu tải tại đây (https://files.pushshift.io/reddit/69M_reddit_accounts.csv.gz)
Các bạn có thể tham khảo kết quả thí nghiệm của mình tại đây (https://github.com/linhduongtuan/Polars_vs_Pandas) ","Gần đây (r-)Polars nổi lên như 1 thư viện cạnh tranh với Pandas/Tidyverse về tốc độ xử lý dữ liệu lớn dạng bảng. Trong repository (https://github.com/linhduongtuan/Polars_vs_Pandas) này của mình, mình sẽ thử tốc độ dữ liệu lớn có tên là 69M_reddit_account.csv. Dữ liệu này có 69 triệu dòng và 7 biến số (cột). Khi giải nén file *gz thành *csv nó nặng ~3.3Gb. Mình sẽ so sánh hiệu năng đọc các files này (định dạng CSV, Parquet, và Feather) bằng Polars so với Pandas (version 2) sử dụng NumPy và PyArrow backends. Kết quả cho thấy: Polars thắng trong tất cả các thí nghiệm; Pandas với PyArrow backend tiệm cận tốc độ Polars, và nhanh hơn đáng kể khi đọc files *parquet và *feather (nhưng lại chậm hơn với *csv). Lời tạm kết: Mình nghĩ Polars đã và đang là đối thủ lớn của Pandas; Rust sẽ là ngôn ngôn lập trình rất thú vị và đáng chúng ta đầu tư thời gian để học. Dữ liệu tải tại đây (https://files.pushshift.io/reddit/69M_reddit_accounts.csv.gz) Các bạn có thể tham khảo kết quả thí nghiệm của mình tại đây (https://github.com/linhduongtuan/Polars_vs_Pandas)",,,,,"#sharing, #python"
"[MLOps] 
Xin chào mọi người, hiện tại mình đang làm Data Scientist cho 1 công ty Big 4 ở Bắc Mỹ. Do mình muốn học thêm mảng MLOps để chuẩn bị đổi việc, nên có vài thắc mắc, rất mong được mọi người chỉ giúp ạ.
1. Với working directory như hình, thì mình có vẻ như đang dockerize chưa đúng, mọi người có thể xem giúp mình với ạ. Vì khi mình chạy app từ docker image (  Network URL: http://172.17.0.4:8501,   External URL: http://76.64.53.12:8501) thì browser không load được.
2. Có cách nào để khi chạy app thì tự động mở browser luôn thay vì phải click tay vào URL không ạ?
3. Sau khi Dockerized xong, mọi người có suggest gì về Cloud để mình deploy app này lên free và làm portfolio để mình xin việc.
Mình cảm ơn nhiều ạ. Và rất mong được kết bạn với mọi người để học hỏi.","[MLOps] Xin chào mọi người, hiện tại mình đang làm Data Scientist cho 1 công ty Big 4 ở Bắc Mỹ. Do mình muốn học thêm mảng MLOps để chuẩn bị đổi việc, nên có vài thắc mắc, rất mong được mọi người chỉ giúp ạ. 1. Với working directory như hình, thì mình có vẻ như đang dockerize chưa đúng, mọi người có thể xem giúp mình với ạ. Vì khi mình chạy app từ docker image ( Network URL: http://172.17.0.4:8501, External URL: http://76.64.53.12:8501) thì browser không load được. 2. Có cách nào để khi chạy app thì tự động mở browser luôn thay vì phải click tay vào URL không ạ? 3. Sau khi Dockerized xong, mọi người có suggest gì về Cloud để mình deploy app này lên free và làm portfolio để mình xin việc. Mình cảm ơn nhiều ạ. Và rất mong được kết bạn với mọi người để học hỏi.",,,,,#Q&A
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 03/2023 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 03/2023 vào comment của post này.",,,,,bỏ
"Em chào mọi người, em cần tìm gpu để train dữ liệu khoảng 70gb, ảnh là dạng ảnh 3d google map nên rất nặng ạ. Xin mọi người tư vấn cho em về google driver và google colab ạ","Em chào mọi người, em cần tìm gpu để train dữ liệu khoảng 70gb, ảnh là dạng ảnh 3d google map nên rất nặng ạ. Xin mọi người tư vấn cho em về google driver và google colab ạ",,,,,#Q&A
"VinAI Seminar - ""Neural scene representations for learning-based view synthesis and its applications""
Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams.
Speaker: Phong Nguyen-Ha, Ph.D. candidate at University of Oulu, Finland
Time: 2:30 pm - 3:30 pm (GMT+7), Fri, Apr 07, 2023","VinAI Seminar - ""Neural scene representations for learning-based view synthesis and its applications"" Register here [https://forms.office.com/r/uSc77kJJx7] to access seminar via Ms. Teams. Speaker: Phong Nguyen-Ha, Ph.D. candidate at University of Oulu, Finland Time: 2:30 pm - 3:30 pm (GMT+7), Fri, Apr 07, 2023",,,,,bỏ
"[OpenSource] VnGPT - Mã nguồn mở giúp dựng server ChatGPT riêng trên máy cá nhân cùng nhiều ứng dụng AI thú vị khác

Lâu lâu mới lại có project opensource để chia sẻ cùng các anh em trong group. Sản phẩm này cũng khá đơn giản, sử dụng Gradio để call API ChatGPT, Whisper từ OpenAI và tiếp tục mở rộng, kết nối luồng với nhau để tạo ra giao diện tiện dụng cuối cho người dùng. Phù hợp để các anh em nghiên cứu về AI tự dựng server để test ChatGPT, hoặc dựng server ChatGPT dùng riêng cho bạn bè, gia đình, công ty. 

Còn ý nghĩa sâu xa của project thì mời các anh em đọc tại đây:
https://www.facebook.com/photo/?fbid=159818203592346&set=a.116382481269252

Những ưu điểm của công cụ này:
Mã nguồn mở và miễn phí
Cài đặt dễ dàng trên máy tính cá nhân (Windows, MacOS, Ubuntu) chỉ với một click
Tích hợp sẵn: ChatGPT, Whisper...và liên tục bổ sung các dịch vụ AI mới
Tự setup server riêng để sử dụng cá nhân, hoặc chia sẻ qua Internet để bạn bè, người thân cùng sử dụng
Cấu hình các tham số nâng cao, giúp mở khóa nhiều chức năng mới cho ChatGPT, Whisper, VnAlert...

VnGPT hiện được cam kết duy trì quản lý, phát triển bởi AIV Group. Tuy nhiên là sản phẩm nguồn mở nên các anh em clone về thoải mái. Nếu có thể commit back trở lại mã nguồn gốc thì càng tốt. 
-------------------------
👉 Dùng thử VnGPT tại: https://vngpt.aivgroup.vn
👉 Tải và cài đặt VnGPT tại: https://github.com/AIV-Group/VnGPT-CE
👉 Cộng đồng người dùng & phát triển VnGPT: Cộng đồng người dùng VnGPT
👉 Hỗ trợ sử dụng trực tiếp trên Zalo: Hỏi đáp cách dùng VnGPT","[OpenSource] VnGPT - Mã nguồn mở giúp dựng server ChatGPT riêng trên máy cá nhân cùng nhiều ứng dụng AI thú vị khác Lâu lâu mới lại có project opensource để chia sẻ cùng các anh em trong group. Sản phẩm này cũng khá đơn giản, sử dụng Gradio để call API ChatGPT, Whisper từ OpenAI và tiếp tục mở rộng, kết nối luồng với nhau để tạo ra giao diện tiện dụng cuối cho người dùng. Phù hợp để các anh em nghiên cứu về AI tự dựng server để test ChatGPT, hoặc dựng server ChatGPT dùng riêng cho bạn bè, gia đình, công ty. Còn ý nghĩa sâu xa của project thì mời các anh em đọc tại đây: https://www.facebook.com/photo/?fbid=159818203592346&set=a.116382481269252 Những ưu điểm của công cụ này: Mã nguồn mở và miễn phí Cài đặt dễ dàng trên máy tính cá nhân (Windows, MacOS, Ubuntu) chỉ với một click Tích hợp sẵn: ChatGPT, Whisper...và liên tục bổ sung các dịch vụ AI mới Tự setup server riêng để sử dụng cá nhân, hoặc chia sẻ qua Internet để bạn bè, người thân cùng sử dụng Cấu hình các tham số nâng cao, giúp mở khóa nhiều chức năng mới cho ChatGPT, Whisper, VnAlert... VnGPT hiện được cam kết duy trì quản lý, phát triển bởi AIV Group. Tuy nhiên là sản phẩm nguồn mở nên các anh em clone về thoải mái. Nếu có thể commit back trở lại mã nguồn gốc thì càng tốt. ------------------------- Dùng thử VnGPT tại: https://vngpt.aivgroup.vn Tải và cài đặt VnGPT tại: https://github.com/AIV-Group/VnGPT-CE Cộng đồng người dùng & phát triển VnGPT: Cộng đồng người dùng VnGPT Hỗ trợ sử dụng trực tiếp trên Zalo: Hỏi đáp cách dùng VnGPT",,,,,"#sharing, #nlp"
"""With >30 hours of video content (all free, no ads!), you'll learn how to create and train a Stable Diffusion model starting from pure Python""","""With >30 hours of video content (all free, no ads!), you'll learn how to create and train a Stable Diffusion model starting from pure Python""",,,,,bỏ
"Tiếp tục chủ đề chatbot. Lần nhóm mình tiếp cận finetune cho model Bloomz-7b1-mt, kết hợp với Low-rank adaptation cho cơ sở dữ liệu hỏi đáp với bác sĩ về bệnh tật (bằng tiếng Anh). Bài báo gốc tại đây https://arxiv.org/pdf/2303.14070.pdf.
Lý do nhóm mình chọn Bloomz-7b1-mt làm model gốc là vấn đề bản quyền mở của BigScience. Nó khác với những ràng buộc bản quyền cho LLaMA. Và Bloom được train trên dataset có tên là ROOT có kha khá dữ liệu là tiếng Việt. Nó có thể giúp các bạn finetune thêm bằng tiếng Việt thuận lợi hơn nếu muốn.
Kết quả prompt mình có so sánh trong source code của nhóm mình. Mình đặt tên cho finetuned model là Doctor with Bloom (tạm dịch bác sĩ với hoa). Đây là source code của mình.  https://github.com/linhduongtuan/doctorwithbloom
Hi vọng nó hữu ích với mọi người. Và xin đừng tiếc **** nếu bạn thích repository này.
 — với Phạm Ngọc Ninh.","Tiếp tục chủ đề chatbot. Lần nhóm mình tiếp cận finetune cho model Bloomz-7b1-mt, kết hợp với Low-rank adaptation cho cơ sở dữ liệu hỏi đáp với bác sĩ về bệnh tật (bằng tiếng Anh). Bài báo gốc tại đây https://arxiv.org/pdf/2303.14070.pdf. Lý do nhóm mình chọn Bloomz-7b1-mt làm model gốc là vấn đề bản quyền mở của BigScience. Nó khác với những ràng buộc bản quyền cho LLaMA. Và Bloom được train trên dataset có tên là ROOT có kha khá dữ liệu là tiếng Việt. Nó có thể giúp các bạn finetune thêm bằng tiếng Việt thuận lợi hơn nếu muốn. Kết quả prompt mình có so sánh trong source code của nhóm mình. Mình đặt tên cho finetuned model là Doctor with Bloom (tạm dịch bác sĩ với hoa). Đây là source code của mình. https://github.com/linhduongtuan/doctorwithbloom Hi vọng nó hữu ích với mọi người. Và xin đừng tiếc **** nếu bạn thích repository này. — với Phạm Ngọc Ninh.",,,,,"#sharing, #deep_learning"
Em đang muốn build server có khoảng 10 GPU. Các anh chị cho em hỏi mua GPU và mother board ở Hà Nội ở đâu ạ? Em nên mua GPU và mother board loại nào ạ.,Em đang muốn build server có khoảng 10 GPU. Các anh chị cho em hỏi mua GPU và mother board ở Hà Nội ở đâu ạ? Em nên mua GPU và mother board loại nào ạ.,,,,,#sharing
"Dạ em xin chào mọi người!
Mọi người có các bộ data liên quan đến các biểu hiện trên khuôn mặt không ạ? Ví dụ các bộ data để phân biệt các biểu hiện sau:
1- Dữ liệu khuôn mặt của người bị ốm (bị bệnh/ mệt mỏi)
2- Dữ liệu khuôn mặt của người bị đau đớn (khó chịu)
Em xin cảm ơn mọi người ạ. Chúc mọi người tuần mới làm việc hiệu quả .",Dạ em xin chào mọi người! Mọi người có các bộ data liên quan đến các biểu hiện trên khuôn mặt không ạ? Ví dụ các bộ data để phân biệt các biểu hiện sau: 1- Dữ liệu khuôn mặt của người bị ốm (bị bệnh/ mệt mỏi) 2- Dữ liệu khuôn mặt của người bị đau đớn (khó chịu) Em xin cảm ơn mọi người ạ. Chúc mọi người tuần mới làm việc hiệu quả .,,,,,"#Q&A, #data"
"[Nhờ giúp đỡ về dự báo thời tiết Timeseries]
Chào mọi người, hiện tại em đang làm project dự báo thời tiết và em đang có bảng dữ liệu như hình (có thể làm thêm các feild khác nếu cần để training model).
Cụ thể là:
-Mỗi 1h, hệ thống sẽ thu dữ liệu Nhiệt độ, độ ẩm, áp suất từ cảm biến và đưa lên database MySQL sau đó đưa lên web như hình. Sau đó toàn bộ dữ liệu của hệ thống sẽ được sử dụng để train model và đưa ra dự báo.
Em muốn dự báo thời tiết trong 7 ngày tới (time series) nhưng chưa biết tìm hiểu từ đâu, hay dùng model gì (em mới tìm hiểu về AI ML). Em xin cảm ơn ạ 🥹🥹🥹","[Nhờ giúp đỡ về dự báo thời tiết Timeseries] Chào mọi người, hiện tại em đang làm project dự báo thời tiết và em đang có bảng dữ liệu như hình (có thể làm thêm các feild khác nếu cần để training model). Cụ thể là: -Mỗi 1h, hệ thống sẽ thu dữ liệu Nhiệt độ, độ ẩm, áp suất từ cảm biến và đưa lên database MySQL sau đó đưa lên web như hình. Sau đó toàn bộ dữ liệu của hệ thống sẽ được sử dụng để train model và đưa ra dự báo. Em muốn dự báo thời tiết trong 7 ngày tới (time series) nhưng chưa biết tìm hiểu từ đâu, hay dùng model gì (em mới tìm hiểu về AI ML). Em xin cảm ơn ạ",,,,,"#Q&A, #data, #machine_learning"
AI CỦA DUOLINGO,AI CỦA DUOLINGO,,,,,bỏ
"Báo cáo 150 trang bởi team Microsofts về GPT-4.
Với GPT-4, từ ""GPT có thể làm được gì"" đã chuyển thành ""Có gì mà GPT không làm được"".
Một cách đầy bất ngờ, LLMs đã khiến những người làm AI thấy được một tia sáng trong công cuộc tiếp cận AGI.
https://vuanhtran.substack.com/p/2-sparks-of-artificial-general-intelligence?sd=pf
1 tuần mình sẽ viết 1-2 blogs, mọi người subscribe nhé","Báo cáo 150 trang bởi team Microsofts về GPT-4. Với GPT-4, từ ""GPT có thể làm được gì"" đã chuyển thành ""Có gì mà GPT không làm được"". Một cách đầy bất ngờ, LLMs đã khiến những người làm AI thấy được một tia sáng trong công cuộc tiếp cận AGI. https://vuanhtran.substack.com/p/2-sparks-of-artificial-general-intelligence?sd=pf 1 tuần mình sẽ viết 1-2 blogs, mọi người subscribe nhé",,,,,#sharing
"Chào anh chị ạ, em có một vài thắc mắc về các vị trí nghề nghiệp trong ngành AI.
Về mảng data thì e đã biết một vài vị trí như Data Scientist, Data Analyst, ... Tuy nhiên các vị trí khác - em tạm gọi là mảng AI (làm việc với models, ...) thì e chưa hình dung rõ và chưa biết tên ạ.
Anh chị cho e xin review về các jobs liên quan đến mảng AI (tên vị trí, các tasks khi đi làm) với ạ.
Cảm ơn admin và mng rất nhiều","Chào anh chị ạ, em có một vài thắc mắc về các vị trí nghề nghiệp trong ngành AI. Về mảng data thì e đã biết một vài vị trí như Data Scientist, Data Analyst, ... Tuy nhiên các vị trí khác - em tạm gọi là mảng AI (làm việc với models, ...) thì e chưa hình dung rõ và chưa biết tên ạ. Anh chị cho e xin review về các jobs liên quan đến mảng AI (tên vị trí, các tasks khi đi làm) với ạ. Cảm ơn admin và mng rất nhiều",,,,,#Q&A
Em xin phép share một webinar khác diễn ra vào 8h tối nay (30/3) tổ chức bởi MLOpsVN,Em xin phép share một webinar khác diễn ra vào 8h tối nay (30/3) tổ chức bởi MLOpsVN,,,,,#webinar
"Xin phép mọi người trong nhóm. Hiện tại em đang chuẩn bị làm đồ án tốt nghiệp hướng của em muốn làm là về human pose, em muốn bài toán hướng đến 1 số ứng dụng như phát hiện người bị đuối nước, người bị ngã cho người già đột quỵ hay trẻ nhỏ, hành vi người tham gia giao thông như chuẩn bị băng qua đường áp dụng cho ôtô. Vì lần đầu tiên em tiếp cận với hướng này không biết những bài toán trên của em có khả thi hay khó khăn nào không em xin phép xin ý kiến mọi người đã từng làm về ứng dụng về hướng này ạ. Em cảm ơn mọi người.","Xin phép mọi người trong nhóm. Hiện tại em đang chuẩn bị làm đồ án tốt nghiệp hướng của em muốn làm là về human pose, em muốn bài toán hướng đến 1 số ứng dụng như phát hiện người bị đuối nước, người bị ngã cho người già đột quỵ hay trẻ nhỏ, hành vi người tham gia giao thông như chuẩn bị băng qua đường áp dụng cho ôtô. Vì lần đầu tiên em tiếp cận với hướng này không biết những bài toán trên của em có khả thi hay khó khăn nào không em xin phép xin ý kiến mọi người đã từng làm về ứng dụng về hướng này ạ. Em cảm ơn mọi người.",,,,,"#Q&A, #cv"
"Em chào mọi người ạ,
Mọi người có thể giới thiệu cho em một số paper nổi tiếng, kinh điển cho task Face Verification được không ạ. Kinh điển theo ý em tức là khi nhắc tới task này thì mọi người nghĩ ngay tới paper nào ý ạ.
Em xin chân thành cảm ơn ạ.","Em chào mọi người ạ, Mọi người có thể giới thiệu cho em một số paper nổi tiếng, kinh điển cho task Face Verification được không ạ. Kinh điển theo ý em tức là khi nhắc tới task này thì mọi người nghĩ ngay tới paper nào ý ạ. Em xin chân thành cảm ơn ạ.",,,,,"#Q&A, #cv"
"Em chào mọi người,
Em đang cần tìm một encoder model để encode rồi làm clustering. Hiện tại em mới chỉ tìm ra BERT từ năm 2018 là phổ biến nhất. Không biết là có pretrained encoder nào tốt hơn BERT không ạ. Nếu có bài bào nào đánh giá tổng quan các encoder model dùng transformer thì càng tốt ạ","Em chào mọi người, Em đang cần tìm một encoder model để encode rồi làm clustering. Hiện tại em mới chỉ tìm ra BERT từ năm 2018 là phổ biến nhất. Không biết là có pretrained encoder nào tốt hơn BERT không ạ. Nếu có bài bào nào đánh giá tổng quan các encoder model dùng transformer thì càng tốt ạ",,,,,"#Q&A, #deep_learning"
"NLP. Chào mọi người ạ. Em mới học và tìm hiểu xử lý về NLP trong tiếng việt. Thầy em cho em một số thư viện khá phổ biến như NLTK. Cho em hỏi là thư việ NLTK có áp dụng được cho tiếng việt không ạ? Em cảm ơn.
Tiện thể em muốn hỏi là xử lý văn bản tiếng việt thì ra trường có cơ hội tìm kiếm việc làm tốt không ạ?",NLP. Chào mọi người ạ. Em mới học và tìm hiểu xử lý về NLP trong tiếng việt. Thầy em cho em một số thư viện khá phổ biến như NLTK. Cho em hỏi là thư việ NLTK có áp dụng được cho tiếng việt không ạ? Em cảm ơn. Tiện thể em muốn hỏi là xử lý văn bản tiếng việt thì ra trường có cơ hội tìm kiếm việc làm tốt không ạ?,,,,,"#Q&A, #nlp, #python"
"Bất kể ai là người chiến thắng trong cuộc đua AI, thì đây là công ty vẫn ung dung hưởng lợi! Công ty nắm giữ vũ khí tối thượng của công nghệ AI.
#A100 #SmartTechnology #FutureTech","Bất kể ai là người chiến thắng trong cuộc đua AI, thì đây là công ty vẫn ung dung hưởng lợi! Công ty nắm giữ vũ khí tối thượng của công nghệ AI.",#A100	#SmartTechnology	#FutureTech,,,,#sharing
"Bởi những vấn đề về giới hạn bản quyền của model LLaMA, mình và bạn Phạm Ngọc Ninh đã sử dụng model BLOOM không bị giới hạn bản quyền (bài báo tại đâyhttps://arxiv.org/abs/2211.05100) để train models theo hướng Alpaca-LoRA. Vì BLOOM được train với 2,7% dataset là tiếng Việt (tham khảo tại đây https://huggingface.co/bigscience/bloom). Hơn nữa, mình có thấy Andrej Karpathy khuyên khích nên dùng BLOOM như hình bên dưới. Với những lý do trên, mình và bạn Ninh đã train model BLOOM-56M và BLOOM-7b1 kết hợp với LoRA, cũng như sử dụng dữ liệu Alpaca_data_cleaned.json tại đây (https://github.com/gururise/AlpacaDataCleaned).
Đây là repository mà mình và bạn Ninh đã reimplemnt https://github.com/linhduongtuan/BLOOM-LORA.
Hi vọng cuối tuần có thứ để mua vui với mọi người.
 — với Phạm Ngọc Ninh.","Bởi những vấn đề về giới hạn bản quyền của model LLaMA, mình và bạn Phạm Ngọc Ninh đã sử dụng model BLOOM không bị giới hạn bản quyền (bài báo tại đâyhttps://arxiv.org/abs/2211.05100) để train models theo hướng Alpaca-LoRA. Vì BLOOM được train với 2,7% dataset là tiếng Việt (tham khảo tại đây https://huggingface.co/bigscience/bloom). Hơn nữa, mình có thấy Andrej Karpathy khuyên khích nên dùng BLOOM như hình bên dưới. Với những lý do trên, mình và bạn Ninh đã train model BLOOM-56M và BLOOM-7b1 kết hợp với LoRA, cũng như sử dụng dữ liệu Alpaca_data_cleaned.json tại đây (https://github.com/gururise/AlpacaDataCleaned). Đây là repository mà mình và bạn Ninh đã reimplemnt https://github.com/linhduongtuan/BLOOM-LORA. Hi vọng cuối tuần có thứ để mua vui với mọi người. — với Phạm Ngọc Ninh.",,,,,"#sharing, #deep_learning"
"Chào các bạn.
Mình vừa hoàn thành các jupyter notebook sử dụng pandas và sqlalchemy để mô phỏng MySQL code, hy vọng sẽ giúp ích được ai đó.
Cảm ơn các bạn đã quan tâm ạ.","Chào các bạn. Mình vừa hoàn thành các jupyter notebook sử dụng pandas và sqlalchemy để mô phỏng MySQL code, hy vọng sẽ giúp ích được ai đó. Cảm ơn các bạn đã quan tâm ạ.",,,,,#sharing
"Xin chào anh em, nhân dịp đang tìm hiểu về Data Visualize nên chia sẻ cùng anh em một công cụ nhỏ. Hi vọng giúp được anh em mới học!","Xin chào anh em, nhân dịp đang tìm hiểu về Data Visualize nên chia sẻ cùng anh em một công cụ nhỏ. Hi vọng giúp được anh em mới học!",,,,,"#sharing, #data"
Mọi người đều biết về sức mạnh vượt trội của GPT-4 trong các bài toán xử lý ngôn ngữ tự nhiên. Nay Microsft mang sức mạnh của GPT-4 lên Github với công cụ GitHub Copilot X. Với lượng dữ liệu (code) khổng lồ trên Github thì AI sẽ hỗ trợ rất nhiều cho lập trình viên trong nhiều công việc khác nhau.,Mọi người đều biết về sức mạnh vượt trội của GPT-4 trong các bài toán xử lý ngôn ngữ tự nhiên. Nay Microsft mang sức mạnh của GPT-4 lên Github với công cụ GitHub Copilot X. Với lượng dữ liệu (code) khổng lồ trên Github thì AI sẽ hỗ trợ rất nhiều cho lập trình viên trong nhiều công việc khác nhau.,,,,,"#sharing, #nlp"
"Sau tiếng vang lớn của ChatGPT, mình thấy có người reimplement lại mô hình này dưới cái tên minChatGPT (sử dụng kiến trúc GPT-2) tại đây https://github.com/ethanyanjiali/minChatGPT. Có lẽ đây là phong cách được định hình bởi Andrej Karpathy, khi Andrej reimplemt mô hình GPT-2 với 2 repos là minGPT và nanoGPT.
Có bạn nào thích thú với ý tưởng này, mình nghĩ hoàn toàn có thể sử dụng các kiến trúc khác như LLaMA, BLOOM, GPT-NEOX,...","Sau tiếng vang lớn của ChatGPT, mình thấy có người reimplement lại mô hình này dưới cái tên minChatGPT (sử dụng kiến trúc GPT-2) tại đây https://github.com/ethanyanjiali/minChatGPT. Có lẽ đây là phong cách được định hình bởi Andrej Karpathy, khi Andrej reimplemt mô hình GPT-2 với 2 repos là minGPT và nanoGPT. Có bạn nào thích thú với ý tưởng này, mình nghĩ hoàn toàn có thể sử dụng các kiến trúc khác như LLaMA, BLOOM, GPT-NEOX,...",,,,,"#sharing, #deep_learning"
"Nay mình giới thiệu với mọi người một công cụ hỗ trợ viết code còn thông minh hơn Copilot.
Cursor là một phần mềm để lập trình (IDE) với sự hỗ trợ AI. Hiện tại một số tính năng mà Cursor hỗ trợ: - Write: Sinh code với AI, thông minh hơn Copilot - Diff: Yêu cầu AI sửa và cải tiến đoạn code - Chat: Dạng ChatGPT nhưng hiểu ngữ cảnh của file, project Ngoài ra có các tính năng như: tự động sinh ra comment, test case, xác định vùng code có khả năng bị lỗi, và sửa luôn...","Nay mình giới thiệu với mọi người một công cụ hỗ trợ viết code còn thông minh hơn Copilot. Cursor là một phần mềm để lập trình (IDE) với sự hỗ trợ AI. Hiện tại một số tính năng mà Cursor hỗ trợ: - Write: Sinh code với AI, thông minh hơn Copilot - Diff: Yêu cầu AI sửa và cải tiến đoạn code - Chat: Dạng ChatGPT nhưng hiểu ngữ cảnh của file, project Ngoài ra có các tính năng như: tự động sinh ra comment, test case, xác định vùng code có khả năng bị lỗi, và sửa luôn...",,,,,#sharing
"Em chào anh chị ạ, em muốn xin review từ các anh chị đã theo học chương trình Khoa học dữ liệu của đại học khoa học tự nhiên hà nội. Vì lý do tài chính và gia đình nên em quyết định theo học thạc sĩ ở Hà Nội thay vì đi du học. Các anh chị thấy chương trình đào tạo và chất lượng giảng dạy thạc sĩ của trường thế nào ạ?
Em cảm ơn anh chị nhiều ạ","Em chào anh chị ạ, em muốn xin review từ các anh chị đã theo học chương trình Khoa học dữ liệu của đại học khoa học tự nhiên hà nội. Vì lý do tài chính và gia đình nên em quyết định theo học thạc sĩ ở Hà Nội thay vì đi du học. Các anh chị thấy chương trình đào tạo và chất lượng giảng dạy thạc sĩ của trường thế nào ạ? Em cảm ơn anh chị nhiều ạ",,,,,#Q&A
"Vừa rồi mình có tham gia challenge Player Contact Detection ở kaggle và may mắn được top1. Mình xin chia sẽ code và solution hi vọng sẽ có ích cho các bạn mới.
Challenge page: https://www.kaggle.com/competitions/nfl-player-contact-detection/leaderboard
Solution: shorturl.at/kJNW0
Source code: https://github.com/nvnnghia/nfl3_1st",Vừa rồi mình có tham gia challenge Player Contact Detection ở kaggle và may mắn được top1. Mình xin chia sẽ code và solution hi vọng sẽ có ích cho các bạn mới. Challenge page: https://www.kaggle.com/competitions/nfl-player-contact-detection/leaderboard Solution: shorturl.at/kJNW0 Source code: https://github.com/nvnnghia/nfl3_1st,,,,,#sharing
"Chỉ trong vài năm qua, tác động của Microsoft trong việc thúc đẩy lĩnh vực AI là rất ấn tượng. Microsoft đang hoàn toàn thống trị, chạy nhanh hơn và bỏ xa các đối thủ khác 😳","Chỉ trong vài năm qua, tác động của Microsoft trong việc thúc đẩy lĩnh vực AI là rất ấn tượng. Microsoft đang hoàn toàn thống trị, chạy nhanh hơn và bỏ xa các đối thủ khác",,,,,bỏ
"#chatbot
Em chào mọi người ạ,
Hiện tại em đang phải build chatbot cho 1 công ty về tư vấn tâm lý, người ta muốn chatbot có thể đặt nhiều câu hỏi kiểu open-question cho người dùng để hiểu được vấn đề tâm lý của người dùng.
Em đã thử ParlAI và dùng Empathetic Dialogues, thì model khá tốt, thể hiện được sự đồng cảm với người dùng nhưng chưa đặt thêm được câu hỏi cho người dùng. Đây là github link ạ https://github.com/facebookresearch/ParlAI
Em đã thử thêm dataset và train lại model dựa trên trained model, em có để --gpu 1 nhưng lúc ở task manager thì em lại thấy GPU không hoạt động gì, còn CPU và memnory thì tầm 90% - 100% luôn. Nên em chưa thành công train lại model khi dùng ParlAI ạ.
Em sợ build model từ đầu, không dùng pretrained model thì kết quả chưa chắc đã bằng pretrained model. Nên em không biết làm thế nào.
https://dl.acm.org/doi/abs/10.1145/3313831.3376131
Hướng em muốn build chatbot là như bài báo này ạ. Nhưng mà em không tìm thấy code của bài báo này, nên em cũng không biết làm như nào.
Mọi người có thể tư vẫn giúp em, em nên làm gì được không ạ. Em xin chân thành cảm ơn mọi người ạ. 😄🙏","Em chào mọi người ạ, Hiện tại em đang phải build chatbot cho 1 công ty về tư vấn tâm lý, người ta muốn chatbot có thể đặt nhiều câu hỏi kiểu open-question cho người dùng để hiểu được vấn đề tâm lý của người dùng. Em đã thử ParlAI và dùng Empathetic Dialogues, thì model khá tốt, thể hiện được sự đồng cảm với người dùng nhưng chưa đặt thêm được câu hỏi cho người dùng. Đây là github link ạ https://github.com/facebookresearch/ParlAI Em đã thử thêm dataset và train lại model dựa trên trained model, em có để --gpu 1 nhưng lúc ở task manager thì em lại thấy GPU không hoạt động gì, còn CPU và memnory thì tầm 90% - 100% luôn. Nên em chưa thành công train lại model khi dùng ParlAI ạ. Em sợ build model từ đầu, không dùng pretrained model thì kết quả chưa chắc đã bằng pretrained model. Nên em không biết làm thế nào. https://dl.acm.org/doi/abs/10.1145/3313831.3376131 Hướng em muốn build chatbot là như bài báo này ạ. Nhưng mà em không tìm thấy code của bài báo này, nên em cũng không biết làm như nào. Mọi người có thể tư vẫn giúp em, em nên làm gì được không ạ. Em xin chân thành cảm ơn mọi người ạ.",#chatbot,,,,"#sharing, #Q&A, #nlp"
"Chào mọi người ạ, hiện em đang có một project nho nhỏ, em đang imple model SSD để test với dataset là VOC sau đó nếu được kết quả tốt thì chỉnh sửa lại kiến trúc một tí và thử trên một dataset của riêng em. Tuy nhiên em không có chỗ train, máy em không đủ vram, còn colab pro em có nghe nói là chỉ nới rộng thời gian train thôi chứ random ra gpu cũng không ngon lắm. Mà bản pro plus thì em đang là sv nên không đủ điều kiện mua. Vậy nếu em muốn train SSD thì có giải pháp nào cho em không ạ, em cảm ơn mọi người rất nhiều.","Chào mọi người ạ, hiện em đang có một project nho nhỏ, em đang imple model SSD để test với dataset là VOC sau đó nếu được kết quả tốt thì chỉnh sửa lại kiến trúc một tí và thử trên một dataset của riêng em. Tuy nhiên em không có chỗ train, máy em không đủ vram, còn colab pro em có nghe nói là chỉ nới rộng thời gian train thôi chứ random ra gpu cũng không ngon lắm. Mà bản pro plus thì em đang là sv nên không đủ điều kiện mua. Vậy nếu em muốn train SSD thì có giải pháp nào cho em không ạ, em cảm ơn mọi người rất nhiều.",,,,,"#Q&A, #deep_learning"
"Chào mọi người, em là sinh viên năm cuối ngành CNTT trường BK. Tầm tháng 8 năm nay em sẽ tốt nghiệp. Em đang có dự định đi học Master ML/DL ở Úc.
Background của em thì em có GPA tích luỹ tới kì thứ 7 ~ 3.8, ielts 7.0, có nền tảng về thống kê, xử lí, trực quan hoá, handle dữ liệu không cân bằng, nói chung là em có biết cơ bản về khdl, implement các model bằng keras và tf, nhưng mà linear algebra thì hiện tại em quên hết rồi ạ tại học từ năm 1 😞 nhưng mà em có thể tự học lại tại em cũng thích học toán.
Hiện tại em hơi mất định hướng, kiểu các bạn cùng khoá của em (90% không theo hướng ML) đã đi làm rồi ạ, nhưng mà em vẫn chưa đi vì em chưa nhận được offer nào phù hợp. Job ML cho level thấp như em khá ít và khi người ta nghe em nói em chỉ có thể làm đến hết 2023 không thể commit với công ty được người ta cũng không muốn tuyển em í 😞. Em muốn em học Master xong có thể ở đó làm các ngành liên quan đến major của em vài năm rồi mới về nước ạ. Nhưng mà em sợ nếu em không có work experience thì không thể đi làm được ở Úc. Em hi vọng mn có thể cho ý kiến giúp em rằng em nên ở đây làm các job liên quan rồi mới đi học, hay là tốt nghiệp xong em đi luôn ạ? Và không biết là ở Úc thì con đường cho ML có rộng mở không ạ? Em cần chuẩn bị những kiến thức cơ bản nào trước khi em chính thức vào học Master ML không ạ? Em không có vấn đề gì về tài chính, bố mẹ em có thể chu cấp cho em đi học ms liền sau khi em tốt nghiệp bk ạ, chỉ là em thấy các bạn đi làm em áp lực và em đang hoài nghi về chuyện em muốn theo đuổi ML huhu 😞 Em mong mọi người cho em lời khuyên, em cảm ơn mnn","Chào mọi người, em là sinh viên năm cuối ngành CNTT trường BK. Tầm tháng 8 năm nay em sẽ tốt nghiệp. Em đang có dự định đi học Master ML/DL ở Úc. Background của em thì em có GPA tích luỹ tới kì thứ 7 ~ 3.8, ielts 7.0, có nền tảng về thống kê, xử lí, trực quan hoá, handle dữ liệu không cân bằng, nói chung là em có biết cơ bản về khdl, implement các model bằng keras và tf, nhưng mà linear algebra thì hiện tại em quên hết rồi ạ tại học từ năm 1 nhưng mà em có thể tự học lại tại em cũng thích học toán. Hiện tại em hơi mất định hướng, kiểu các bạn cùng khoá của em (90% không theo hướng ML) đã đi làm rồi ạ, nhưng mà em vẫn chưa đi vì em chưa nhận được offer nào phù hợp. Job ML cho level thấp như em khá ít và khi người ta nghe em nói em chỉ có thể làm đến hết 2023 không thể commit với công ty được người ta cũng không muốn tuyển em í . Em muốn em học Master xong có thể ở đó làm các ngành liên quan đến major của em vài năm rồi mới về nước ạ. Nhưng mà em sợ nếu em không có work experience thì không thể đi làm được ở Úc. Em hi vọng mn có thể cho ý kiến giúp em rằng em nên ở đây làm các job liên quan rồi mới đi học, hay là tốt nghiệp xong em đi luôn ạ? Và không biết là ở Úc thì con đường cho ML có rộng mở không ạ? Em cần chuẩn bị những kiến thức cơ bản nào trước khi em chính thức vào học Master ML không ạ? Em không có vấn đề gì về tài chính, bố mẹ em có thể chu cấp cho em đi học ms liền sau khi em tốt nghiệp bk ạ, chỉ là em thấy các bạn đi làm em áp lực và em đang hoài nghi về chuyện em muốn theo đuổi ML huhu Em mong mọi người cho em lời khuyên, em cảm ơn mnn",,,,,#Q&A
"#imageclassification
Hi mọi người, hiện giờ e đang muốn build một image classifcation model có thể phân loại flowers and leaf. Về phần training, e đang train trên dataset tầm 10GB ảnh, 124 training classes. Giờ vấn đề e gặp phải là nếu mà trong test images mà có images mà ko thuộc training class nào, thì làm thế nào model của e có thể detect được là cái images đấy ko thuộc training class nào. E định dùng neural network với output layer có sigmoid activation function. Nếu mà sigmoid score cho từng class < 0.5, thì e sẽ classify tấm images đấy ko thuộc training class nào. Liệu rằng approach của e có khả dĩ ko hay mọi ng có approach nào khác?","Hi mọi người, hiện giờ e đang muốn build một image classifcation model có thể phân loại flowers and leaf. Về phần training, e đang train trên dataset tầm 10GB ảnh, 124 training classes. Giờ vấn đề e gặp phải là nếu mà trong test images mà có images mà ko thuộc training class nào, thì làm thế nào model của e có thể detect được là cái images đấy ko thuộc training class nào. E định dùng neural network với output layer có sigmoid activation function. Nếu mà sigmoid score cho từng class < 0.5, thì e sẽ classify tấm images đấy ko thuộc training class nào. Liệu rằng approach của e có khả dĩ ko hay mọi ng có approach nào khác?",#imageclassification,,,,"#Q&A, #cv, #deep_learning"
"***** Điểm tin về LLaMA (https://arxiv.org/abs/2302.13971; https://github.com/facebookresearch/llama) và các biến thể cũng như giải pháp kĩ thuật cải tiến LLaMA *****
Như chúng ta đã biến sức nóng của mô hình ngôn ngữ lớn (Large Language Models ~ LLM) lớn tới mức như thế nào, đặc biệt là ChatGPT và GPT-4 do OpenAI tạo ra. Tuy nhiên, rất tiếc OpenAI nhưng lại không Open, nhưng FacebookResearch (FAIR) được lãnh đạo bởi bác Yan LeCun chủ trương Open (source code và data). Có lần mình nghe bác LeCun nói, kiến trúc Convolutional Neural Network (CNN) được bác ấy phát minh từ năm 1988-89, nhưng vì nhiều lý về bản quyền (nơi bác ấy từng làm việc, ví dụ Bell Labs) nên AI bị rơi vào ""ngủ đông"". Rút kinh nghiệm từ đó, bác LeCun chủ trương Open Science (các bạn có thể sẽ gặp thuật ngữ này ngày một nhiều hơn, ví dụ OPEN: source code, data, access, review,...) giúp khoa học dissemination and exploitation nhanh và tốt hơn.
Quay lại LLaMA mở mã nguồn và trained weights ta sẽ thấy điều trên đúng với những cải tiến mà tôi được biết cho tới nay (9AM GMT+7, ngày 18/3/2023):
1/ Nhóm nghiên cứu ở Đại Học Stanford đã sử dụng kĩ thuật mà họ công bố trước đó là Self-Instruction (https://arxiv.org/abs/2212.10560) để cải tiến hiệu năng mô hình và đặt tên biến thể là Alpaca. Source code tại đây: https://github.com/tatsu-lab/stanford_alpaca; ứng dung Demo giống như ChatGPT tại đây: https://alpaca-ai-custom4.ngrok.io/ (tạm thời đang ngừng hoạt động để nâng cấp)
2/ Tiếp thu kinh nghiệm của nhóm Tastu ở ĐH Stanford, Eric J. Wang đã sử dụng kỹ thuật Low-Rank LLaMA Instruct-Tuning (https://arxiv.org/pdf/2106.09685.pdf) để finetune LLaMA và đặt tên là LLaMA-lora (source code tại đây: https://github.com/tloen/alpaca-lora). Hơn nữa, LLaMA-lora có thể train model với precision=8-bit sử dụng thư viện bitsandbytes mà chỉ cần 1 GPU như RTX 4090. Mình đang chờ xem có ai sử dụng accelerate để train model với precision=8-bit (fp8=True)???
3/ Tiếp theo vấn đề liên quan tới quantization để giảm bộ nhớ, cấu hình máy tính, các bạn có thể tham khảo tại đây https://github.com/qwopqwop200/GPTQ-for-LLaMa (paper: https://arxiv.org/abs/2302.13971 và original code tại đây: https://github.com/IST-DASLab/gptq)
4/ Kết hợp giữa quantization, chuyển trained weighted của LLaMA sang C/C++ và deploy lên edged device các bạn có thể theo dõi GitHub này: https://github.com/ggerganov/llama.cpp. Tương tự, với trained weighted của Alpaca xuống 4-bit và C/C++ tại đây https://github.com/antimatter15/alpaca.cpp
5/ Last but not least, mình thấy có 1 nhóm ở Bồ Đào Nha, sử dụng ChatGPT để dịch dữ liệu alpaca_data.json dùng trong Alpaca (mục 1/) sang tiếng Bồ rồi train mô hình giống với LLaMA-lora (mục 2/) tại đây: https://github.com/22-hours/cabrita. Bạn nào có nhã hứng với tiếng Việt, theo mình có thể sử dụng ý tướng của nhóm Bồ Đào Nha này. Hóng các bạn reimplementation source code này. Lưu ý 1 chút, Eric J. Wang có phàn này về chất lượng của dataset dùng để finetune Alpaca, nên các bạn có thể tham khảo thêm dataset này alpaca_data_cleaned.json tại đây (https://github.com/tloen/alpaca-lora/blob/main/alpaca_data_cleaned.json).
...
n/ và chắc chắn sẽ còn rất nhiều ý tưởng thú vị liên quan tới chủ đề này sẽ được giới thiệu trong thời gian ngắn tới","***** Điểm tin về LLaMA (https://arxiv.org/abs/2302.13971; https://github.com/facebookresearch/llama) và các biến thể cũng như giải pháp kĩ thuật cải tiến LLaMA ***** Như chúng ta đã biến sức nóng của mô hình ngôn ngữ lớn (Large Language Models ~ LLM) lớn tới mức như thế nào, đặc biệt là ChatGPT và GPT-4 do OpenAI tạo ra. Tuy nhiên, rất tiếc OpenAI nhưng lại không Open, nhưng FacebookResearch (FAIR) được lãnh đạo bởi bác Yan LeCun chủ trương Open (source code và data). Có lần mình nghe bác LeCun nói, kiến trúc Convolutional Neural Network (CNN) được bác ấy phát minh từ năm 1988-89, nhưng vì nhiều lý về bản quyền (nơi bác ấy từng làm việc, ví dụ Bell Labs) nên AI bị rơi vào ""ngủ đông"". Rút kinh nghiệm từ đó, bác LeCun chủ trương Open Science (các bạn có thể sẽ gặp thuật ngữ này ngày một nhiều hơn, ví dụ OPEN: source code, data, access, review,...) giúp khoa học dissemination and exploitation nhanh và tốt hơn. Quay lại LLaMA mở mã nguồn và trained weights ta sẽ thấy điều trên đúng với những cải tiến mà tôi được biết cho tới nay (9AM GMT+7, ngày 18/3/2023): 1/ Nhóm nghiên cứu ở Đại Học Stanford đã sử dụng kĩ thuật mà họ công bố trước đó là Self-Instruction (https://arxiv.org/abs/2212.10560) để cải tiến hiệu năng mô hình và đặt tên biến thể là Alpaca. Source code tại đây: https://github.com/tatsu-lab/stanford_alpaca; ứng dung Demo giống như ChatGPT tại đây: https://alpaca-ai-custom4.ngrok.io/ (tạm thời đang ngừng hoạt động để nâng cấp) 2/ Tiếp thu kinh nghiệm của nhóm Tastu ở ĐH Stanford, Eric J. Wang đã sử dụng kỹ thuật Low-Rank LLaMA Instruct-Tuning (https://arxiv.org/pdf/2106.09685.pdf) để finetune LLaMA và đặt tên là LLaMA-lora (source code tại đây: https://github.com/tloen/alpaca-lora). Hơn nữa, LLaMA-lora có thể train model với precision=8-bit sử dụng thư viện bitsandbytes mà chỉ cần 1 GPU như RTX 4090. Mình đang chờ xem có ai sử dụng accelerate để train model với precision=8-bit (fp8=True)??? 3/ Tiếp theo vấn đề liên quan tới quantization để giảm bộ nhớ, cấu hình máy tính, các bạn có thể tham khảo tại đây https://github.com/qwopqwop200/GPTQ-for-LLaMa (paper: https://arxiv.org/abs/2302.13971 và original code tại đây: https://github.com/IST-DASLab/gptq) 4/ Kết hợp giữa quantization, chuyển trained weighted của LLaMA sang C/C++ và deploy lên edged device các bạn có thể theo dõi GitHub này: https://github.com/ggerganov/llama.cpp. Tương tự, với trained weighted của Alpaca xuống 4-bit và C/C++ tại đây https://github.com/antimatter15/alpaca.cpp 5/ Last but not least, mình thấy có 1 nhóm ở Bồ Đào Nha, sử dụng ChatGPT để dịch dữ liệu alpaca_data.json dùng trong Alpaca (mục 1/) sang tiếng Bồ rồi train mô hình giống với LLaMA-lora (mục 2/) tại đây: https://github.com/22-hours/cabrita. Bạn nào có nhã hứng với tiếng Việt, theo mình có thể sử dụng ý tướng của nhóm Bồ Đào Nha này. Hóng các bạn reimplementation source code này. Lưu ý 1 chút, Eric J. Wang có phàn này về chất lượng của dataset dùng để finetune Alpaca, nên các bạn có thể tham khảo thêm dataset này alpaca_data_cleaned.json tại đây (https://github.com/tloen/alpaca-lora/blob/main/alpaca_data_cleaned.json). ... n/ và chắc chắn sẽ còn rất nhiều ý tưởng thú vị liên quan tới chủ đề này sẽ được giới thiệu trong thời gian ngắn tới",,,,,#sharing
"Có bạn nào trong group này tải được trained weights của LLaMA (bài báo tại đây https://arxiv.org/abs/2302.13971v1) và link chia sẻ torrent tại đây https://github.com/facebookresearch/llama/pull/73/files, cụ thể là link magnet torrent này magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA; Mình thử thì traffic để download == 0%.
==> Vậy bạn nào đã tải được trained weights của LLaMA thì cho mình xin với nhé.
Xin đa tạ trước với mọi người.","Có bạn nào trong group này tải được trained weights của LLaMA (bài báo tại đây https://arxiv.org/abs/2302.13971v1) và link chia sẻ torrent tại đây https://github.com/facebookresearch/llama/pull/73/files, cụ thể là link magnet torrent này magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA; Mình thử thì traffic để download == 0%. ==> Vậy bạn nào đã tải được trained weights của LLaMA thì cho mình xin với nhé. Xin đa tạ trước với mọi người.",,,,,"#Q&A, #sharing"
"What's New with OpenAI GPT-4?
#AI #DataScience #MachineLearning #GPT4",What's New with OpenAI GPT-4?,#AI	#DataScience	#MachineLearning	#GPT4,,,,
"Mình muốn tạo 1 mô hình dự đoán số lượng bệnh nhân tới khám từng phòng khám. Dữ liệu với các dòng là: phòng khám, thời gian tới khám, thời gian khám xong, bệnh nhân.
Dữ liệu muốn dự đoán là. Tên phòng khám + khoảng thời gian —> dự đoán số lượng bệnh nhân khám, bệnh nhân
Nhờ các bạn tư vấn giúp mình ít keyword để có hướng nghiên cứu. Xin cảm ơn đã đọc bài.","Mình muốn tạo 1 mô hình dự đoán số lượng bệnh nhân tới khám từng phòng khám. Dữ liệu với các dòng là: phòng khám, thời gian tới khám, thời gian khám xong, bệnh nhân. Dữ liệu muốn dự đoán là. Tên phòng khám + khoảng thời gian —> dự đoán số lượng bệnh nhân khám, bệnh nhân Nhờ các bạn tư vấn giúp mình ít keyword để có hướng nghiên cứu. Xin cảm ơn đã đọc bài.",,,,,"#Q&A, #machine_learning"
🎯 Google giới thiệu trình tìm kiếm bằng chat giống ChatGPT,Google giới thiệu trình tìm kiếm bằng chat giống ChatGPT,,,,,#sharing
"Chào các bạn. Tôi xin chia sẻ bài nói chuyện của tôi về AI-for-dev (trước Tết 1 tuần mà giờ phải cập nhật slides rất nhiều rồi) hy vọng có vài thông tin hữu ích cho các bạn trẻ đang là lập trình viên và muốn tìm hiểu phát triển AI. 

Nếu các bạn seniors có insights & resources gì khác cũng xin comment để tôi cập nhật thêm nhé. Cheers!

Talk Title: AI4Dev: Landscapes, Toolsets, Roadmaps, Collabs
Slides & video recording: https://gem.cot.ai/p/-9_Wl26kz#/
Abstract:
The field of Artificial Intelligence (AI) is radically changing every aspect of human life, from the way we shop and entertain, lend and earn, to the way we learn and create. There are exponentially many AI ideas, principles, procedures, applications, tools and platforms being developed and shared freely. This on the one hand provides young talents with tremendous opportunities, on the other hand poses new challenges in the development of their knowledge and skills.
Specifically, AI is now widely considered software 2.0. As a dev, you need to be well prepared for what’s coming next. In this talk I will walk you through comprehensive landscapes of the many exciting topics in AI, introduce wonderful AI-powered dev tools, recommend pragmatic training roadmaps for you to quickly upgrade your AI capabilities, and invite you to collaborate & altogether build a strong community of AI4Dev by joining study groups and contributing to many exciting fullstack production AI projects.

#AI4Dev #AI4VN #Startup","Chào các bạn. Tôi xin chia sẻ bài nói chuyện của tôi về AI-for-dev (trước Tết 1 tuần mà giờ phải cập nhật slides rất nhiều rồi) hy vọng có vài thông tin hữu ích cho các bạn trẻ đang là lập trình viên và muốn tìm hiểu phát triển AI. Nếu các bạn seniors có insights & resources gì khác cũng xin comment để tôi cập nhật thêm nhé. Cheers! Talk Title: AI4Dev: Landscapes, Toolsets, Roadmaps, Collabs Slides & video recording: https://gem.cot.ai/p/-9_Wl26kz#/ Abstract: The field of Artificial Intelligence (AI) is radically changing every aspect of human life, from the way we shop and entertain, lend and earn, to the way we learn and create. There are exponentially many AI ideas, principles, procedures, applications, tools and platforms being developed and shared freely. This on the one hand provides young talents with tremendous opportunities, on the other hand poses new challenges in the development of their knowledge and skills. Specifically, AI is now widely considered software 2.0. As a dev, you need to be well prepared for what’s coming next. In this talk I will walk you through comprehensive landscapes of the many exciting topics in AI, introduce wonderful AI-powered dev tools, recommend pragmatic training roadmaps for you to quickly upgrade your AI capabilities, and invite you to collaborate & altogether build a strong community of AI4Dev by joining study groups and contributing to many exciting fullstack production AI projects.",#AI4Dev	#AI4VN	#Startup,,,,#sharing
"[Poe của Quora - 1 ứng dụng thay thế tuyệt vời cho ChatGPT]

Hello mọi người,
Là một cộng đồng về ML, mình chắc rằng nhu cầu sử dụng ChatGPT của mọi người rất cao nhưng việc truy cập và sử dụng lại tương đối khó khăn (khó tạo account, thường phải dùng VPN, lại hay bị rate limit, etc. ). Mình muốn giới thiệu với mọi người 1 ứng dụng mới, mà theo quan điểm chủ quan của mình là tiện lợi hơn ChatGPT rất nhiều, đó là Poe của Quora. Vâng, chính là trang hỏi đáp Quora mà bạn hay thấy đó.

1 vài ưu điểm của Poe so với ChatGPT mà mình thấy:
📚Poe của Quora là ứng dụng tích hợp nhiều chatbot, nên bạn có thể dùng ChatGPT thông qua Poe.
📧 Tạo account rất đơn giản, không có giới hạn gì cả. Lại còn không cần dùng VPN.
💻 Giao diện xinh xắn dễ thương.
🏃Tốc độ rất nhanh, nhiều bạn ở VN dùng bảo là sử dụng ChatGPT ở Poe cho kết quả nhanh gấp 3, 4 lần so với ChatGPT Pro từ web chính.
💰Không có rate limit và hoàn toàn miễn phí .
🧐 Không chỉ có ChatGPT, ở đây còn có Claude là model của Anthropic, công ty vừa được Google đầu tư $300 triệu và Sage là custom model của Quora dựa vào OpenAI. Tại sao lại vất vả dùng ChatGPT trong khi ở Poe, bạn vừa dùng được ChatGPT, vừa được dùng những state-of-the-art model mới nhất?

Hiện nay bạn có thể dùng Poe trên iOS hoặc là desktop nhé. Bản Android cũng sẽ sớm được ra mắt.

[Disclaimer]: mình làm việc trong team Poe ở Quora nên dĩ nhiên là sẽ khen nhiều hơn chê. Ứng dụng của bọn mình sinh sau đẻ muộn nên sẽ còn khuyết điểm, mọi người có đóng góp gì thì cứ comment vào post nhé, không chừng sẽ thấy ý kiến của bản thân được đưa vào app đó 😜","[Poe của Quora - 1 ứng dụng thay thế tuyệt vời cho ChatGPT] Hello mọi người, Là một cộng đồng về ML, mình chắc rằng nhu cầu sử dụng ChatGPT của mọi người rất cao nhưng việc truy cập và sử dụng lại tương đối khó khăn (khó tạo account, thường phải dùng VPN, lại hay bị rate limit, etc. ). Mình muốn giới thiệu với mọi người 1 ứng dụng mới, mà theo quan điểm chủ quan của mình là tiện lợi hơn ChatGPT rất nhiều, đó là Poe của Quora. Vâng, chính là trang hỏi đáp Quora mà bạn hay thấy đó. 1 vài ưu điểm của Poe so với ChatGPT mà mình thấy: Poe của Quora là ứng dụng tích hợp nhiều chatbot, nên bạn có thể dùng ChatGPT thông qua Poe. Tạo account rất đơn giản, không có giới hạn gì cả. Lại còn không cần dùng VPN. Giao diện xinh xắn dễ thương. Tốc độ rất nhanh, nhiều bạn ở VN dùng bảo là sử dụng ChatGPT ở Poe cho kết quả nhanh gấp 3, 4 lần so với ChatGPT Pro từ web chính. Không có rate limit và hoàn toàn miễn phí . Không chỉ có ChatGPT, ở đây còn có Claude là model của Anthropic, công ty vừa được Google đầu tư $300 triệu và Sage là custom model của Quora dựa vào OpenAI. Tại sao lại vất vả dùng ChatGPT trong khi ở Poe, bạn vừa dùng được ChatGPT, vừa được dùng những state-of-the-art model mới nhất? Hiện nay bạn có thể dùng Poe trên iOS hoặc là desktop nhé. Bản Android cũng sẽ sớm được ra mắt. [Disclaimer]: mình làm việc trong team Poe ở Quora nên dĩ nhiên là sẽ khen nhiều hơn chê. Ứng dụng của bọn mình sinh sau đẻ muộn nên sẽ còn khuyết điểm, mọi người có đóng góp gì thì cứ comment vào post nhé, không chừng sẽ thấy ý kiến của bản thân được đưa vào app đó",,,,,#sharing
"Người tây rất hay chơi chữ kiểu LLaMA (là một loài động vật họ lạc đà được nuôi ở Nam Mỹ để lấy lông) và cũng là tên model xử lý ngôn ngữ gần đây được FacebookResearch công bố. Nửa đêm qua, 1 nhóm nghiên cứu ở Đại học Stanford có finetune nhỏ nhất model LLaMA 7B theo cơ chế Self-Instruct (bài báo tại đây https://arxiv.org/abs/2212.10560). Họ đặt tên cho model của họ là Alpaca (một loài thuộc họ lạc đà, có đặc điểm sinh học gần giống với Llama, cũng được nuôi ở dãy Andes, Nam Mỹ để lấy lông). Đây là source code cho Alpaca (https://github.com/tatsu-lab/stanford_alpaca). Bên cạnh đó, họ cũng đang merge request lên HuggingFace/transformers, và cần thêm 1 người review code trước khi PR lên transformers. Ngoài ra, họ cũng xây dựng giao diện nền web giống ChatGPT (nhưng k cần đăng ký để xử dụng) tại đây https://alpaca-ai-custom1.ngrok.io/
***** Hi vọng mọi người sẽ thích thú với những công cụ sinh ra ngôn ngữ *****","Người tây rất hay chơi chữ kiểu LLaMA (là một loài động vật họ lạc đà được nuôi ở Nam Mỹ để lấy lông) và cũng là tên model xử lý ngôn ngữ gần đây được FacebookResearch công bố. Nửa đêm qua, 1 nhóm nghiên cứu ở Đại học Stanford có finetune nhỏ nhất model LLaMA 7B theo cơ chế Self-Instruct (bài báo tại đây https://arxiv.org/abs/2212.10560). Họ đặt tên cho model của họ là Alpaca (một loài thuộc họ lạc đà, có đặc điểm sinh học gần giống với Llama, cũng được nuôi ở dãy Andes, Nam Mỹ để lấy lông). Đây là source code cho Alpaca (https://github.com/tatsu-lab/stanford_alpaca). Bên cạnh đó, họ cũng đang merge request lên HuggingFace/transformers, và cần thêm 1 người review code trước khi PR lên transformers. Ngoài ra, họ cũng xây dựng giao diện nền web giống ChatGPT (nhưng k cần đăng ký để xử dụng) tại đây https://alpaca-ai-custom1.ngrok.io/ ***** Hi vọng mọi người sẽ thích thú với những công cụ sinh ra ngôn ngữ *****",,,,,"#sharing, #deep_learning"
Hi mn. Hiện tại e đang làm project cá nhân Crawl data shopee. Đa số các fields đều lấy đúng nhưng chỉ có Price và Stock là bị sai số. Mn chỉ e chỗ này với ạ. E xin cảm ơn,Hi mn. Hiện tại e đang làm project cá nhân Crawl data shopee. Đa số các fields đều lấy đúng nhưng chỉ có Price và Stock là bị sai số. Mn chỉ e chỗ này với ạ. E xin cảm ơn,,,,,"#Q&A, #data"
Chào mọi người. Hiện e đang có bộ data có cấu trúc tương tự như hình. E thấy data đang bị duplicate theo hàng của vài trăm cột đầu. E đang tìm hiểu theo hướng grouping lại thay vì để duplicates như v mà train model nhưng ko có key word để tìm hiểu. Mong các bác chỉ giáo vs ạ.,Chào mọi người. Hiện e đang có bộ data có cấu trúc tương tự như hình. E thấy data đang bị duplicate theo hàng của vài trăm cột đầu. E đang tìm hiểu theo hướng grouping lại thay vì để duplicates như v mà train model nhưng ko có key word để tìm hiểu. Mong các bác chỉ giáo vs ạ.,,,,,"#Q&A, #data"
"[RoomGPT - Room Generation]

Các bạn có muốn thiết kế văn phòng, phòng khách, phòng ngủ, phòng tắm,... của mình trở nên đẹp hơn, nhưng lại bị thiếu ý tưởng. Hãy đề RoomGPT giúp bạn. Các bạn chọn loại phòng và phong cách mong muốn (modern, vintage, minimalist,...), sau đó tải ảnh căn phòng hiện tại của bạn lên, RoomGPT sẽ giúp bạn làm căn phòng trở lên lộng lẫy hơn.

Ứng dụng hoàn toàn miễn phí tại: https://www.roomgpt.io/

Các bạn còn chờ gì nữa mà không biến cái ổ chuột của mình trở nên sang trọng hơn ^^","[RoomGPT - Room Generation] Các bạn có muốn thiết kế văn phòng, phòng khách, phòng ngủ, phòng tắm,... của mình trở nên đẹp hơn, nhưng lại bị thiếu ý tưởng. Hãy đề RoomGPT giúp bạn. Các bạn chọn loại phòng và phong cách mong muốn (modern, vintage, minimalist,...), sau đó tải ảnh căn phòng hiện tại của bạn lên, RoomGPT sẽ giúp bạn làm căn phòng trở lên lộng lẫy hơn. Ứng dụng hoàn toàn miễn phí tại: https://www.roomgpt.io/ Các bạn còn chờ gì nữa mà không biến cái ổ chuột của mình trở nên sang trọng hơn ^^",,,,,"#sharing, #cv, #deep_learning"
"Chắc các bạn quan tâm tới chủ đề GPT-xx không thể không biết codebase transformers của HuggingFace. Hay gần đây, Andrej Karpathy có làm series bài giảng và source code về GPT-2 có tên là nanoGPT (https://github.com/karpathy/nanoGPT/tree/master). Trước đó vài năm Andrej cũng tạo repo có tên là minGPT (https://github.com/karpathy/minGPT/tree/master). Sáng sớm nay, TySam có làm repo tương tự nanoGPT có tên là hlb-GPT ~hyperlightspeedbench-gpt tại đây (https://github.com/tysam-code/hlb-gpt/tree/main), anh này có sở trường làm mọi thứ đơn giản, hiệu quả, dễ đọc, dễ hiểu, mà benchmark lại rất ổn! Hi vọng với thông tin này, các bạn sẽ có thêm kiến thức, kĩ năng mới cho mình.","Chắc các bạn quan tâm tới chủ đề GPT-xx không thể không biết codebase transformers của HuggingFace. Hay gần đây, Andrej Karpathy có làm series bài giảng và source code về GPT-2 có tên là nanoGPT (https://github.com/karpathy/nanoGPT/tree/master). Trước đó vài năm Andrej cũng tạo repo có tên là minGPT (https://github.com/karpathy/minGPT/tree/master). Sáng sớm nay, TySam có làm repo tương tự nanoGPT có tên là hlb-GPT ~hyperlightspeedbench-gpt tại đây (https://github.com/tysam-code/hlb-gpt/tree/main), anh này có sở trường làm mọi thứ đơn giản, hiệu quả, dễ đọc, dễ hiểu, mà benchmark lại rất ổn! Hi vọng với thông tin này, các bạn sẽ có thêm kiến thức, kĩ năng mới cho mình.",,,,,"#sharing, #deep_learning"
"[Person Re-Identification]
Hiện nay các paper sota trên tập dữ liệu Market-101 (chủ yếu là resnet) cho bài toán ReID khi áp dụng trên dữ liệu thực tế đều không cho kết quả đáng mong đợi (dù đã retrain model trên data của doanh nghiệp). Mong mọi người đã có kinh nghiệm trong mảng này có thể cho em xin cao kiến. Em xin phép được hỏi một vài câu như sau:
1. Liệu dữ liệu dùng để retrain vẫn còn quá nhỏ để model có thể học được?
2. Có phải các paper public sẽ không áp dụng vào thưc tế được?
3. Có cách nào để cải thiện một model ReID để có thể áp dụng vào trong bài toán thực tế mà không cần phải thu thập số lượng data lớn?
Rất mong nhận được câu trả lời từ tất cả mọi người, em xin cảm ơn.","[Person Re-Identification] Hiện nay các paper sota trên tập dữ liệu Market-101 (chủ yếu là resnet) cho bài toán ReID khi áp dụng trên dữ liệu thực tế đều không cho kết quả đáng mong đợi (dù đã retrain model trên data của doanh nghiệp). Mong mọi người đã có kinh nghiệm trong mảng này có thể cho em xin cao kiến. Em xin phép được hỏi một vài câu như sau: 1. Liệu dữ liệu dùng để retrain vẫn còn quá nhỏ để model có thể học được? 2. Có phải các paper public sẽ không áp dụng vào thưc tế được? 3. Có cách nào để cải thiện một model ReID để có thể áp dụng vào trong bài toán thực tế mà không cần phải thu thập số lượng data lớn? Rất mong nhận được câu trả lời từ tất cả mọi người, em xin cảm ơn.",,,,,"#Q&A, #data, #cv, #deep_learning"
"Hello mọi người,
Hôm nay mình có một discussion với đồng nghiệp của mình về một cái khá là basic trong ML nhưng cũng khá hay để thảo luận. Mình muốn đưa ra đây để mọi người cùng thảo luận:
Vấn đề của mình là một bài toán về time series based và deployment in production.
Mình có một ML model, được train mỗi ngày với dữ liệu mới được cập nhật (feedback của model hôm trước) và deploy mỗi ngày vì data freshness rất rất quan trọng khi có quá nhiều data drift. đặc biệt là data của ngày N-1 là quan trọng nhất vì nó chứa nhiều thông tin hữu ích nhất.
Cách làm nào là tốt nhất và có thể là best practice trong các trường hợp dưới đây:
Suppose là ngày N là ngày deployment, mình chỉ có dữ liệu đến ngày N-1, mục tiêu là probability predictions tại ngày N là tốt nhất và được calibrated tốt nhất.
Train model trên data đến N-3, sau đó optimize (hyperparams search) trên data N-2 rồi calibration trên N-1
Train model trên data đến N-2, sau đó optimize (hyperparams search) trên data N- 1. rồi calibration trên N - 1
Hyperparams search với time-series cross validation (5 folds) trên toàn bộ data để obtain best params, sau đó train trên toàn bộ data đến N với best params đó. 
Mời mọi người cùng thảo luận xem cách nào theoretically là hiệu quả nhất.","Hello mọi người, Hôm nay mình có một discussion với đồng nghiệp của mình về một cái khá là basic trong ML nhưng cũng khá hay để thảo luận. Mình muốn đưa ra đây để mọi người cùng thảo luận: Vấn đề của mình là một bài toán về time series based và deployment in production. Mình có một ML model, được train mỗi ngày với dữ liệu mới được cập nhật (feedback của model hôm trước) và deploy mỗi ngày vì data freshness rất rất quan trọng khi có quá nhiều data drift. đặc biệt là data của ngày N-1 là quan trọng nhất vì nó chứa nhiều thông tin hữu ích nhất. Cách làm nào là tốt nhất và có thể là best practice trong các trường hợp dưới đây: Suppose là ngày N là ngày deployment, mình chỉ có dữ liệu đến ngày N-1, mục tiêu là probability predictions tại ngày N là tốt nhất và được calibrated tốt nhất. Train model trên data đến N-3, sau đó optimize (hyperparams search) trên data N-2 rồi calibration trên N-1 Train model trên data đến N-2, sau đó optimize (hyperparams search) trên data N- 1. rồi calibration trên N - 1 Hyperparams search với time-series cross validation (5 folds) trên toàn bộ data để obtain best params, sau đó train trên toàn bộ data đến N với best params đó. Mời mọi người cùng thảo luận xem cách nào theoretically là hiệu quả nhất.",,,,,"#Q&A, #machine_learning"
"[Visual ChatGPT: Sử dụng ChatGPT với hình ảnh]

Mô hình ChatGPT chỉ hỗ trợ mọi người tương tác bằng ngôn ngữ. Tuy nhiên có rất nhiều mô hình dạng hình ảnh như Visual Transformers hay Stable Diffusion.  Thế nên các nhà nghiên cứu đến từ Microsoft đã xây dựng mô hình Visual ChatGPT kết hợp ChatGPT với các mô hình hình ảnh để hỗ trợ người dùng có thể gửi hình ảnh khi chat và hỗ trợ trả lời các câu hỏi liên quan tới nội dung bức ảnh.

Github: https://github.com/microsoft/visual-chatgpt",[Visual ChatGPT: Sử dụng ChatGPT với hình ảnh] Mô hình ChatGPT chỉ hỗ trợ mọi người tương tác bằng ngôn ngữ. Tuy nhiên có rất nhiều mô hình dạng hình ảnh như Visual Transformers hay Stable Diffusion. Thế nên các nhà nghiên cứu đến từ Microsoft đã xây dựng mô hình Visual ChatGPT kết hợp ChatGPT với các mô hình hình ảnh để hỗ trợ người dùng có thể gửi hình ảnh khi chat và hỗ trợ trả lời các câu hỏi liên quan tới nội dung bức ảnh. Github: https://github.com/microsoft/visual-chatgpt,,,,,"#sharing, #deep_learning"
"Hi cả nhà, em phép được chia sẻ một seminar khác tổ chức bởi MLOpsVN cho các bác nào quan tâm :D. Cảm ơn cả nhà nhiều.","Hi cả nhà, em phép được chia sẻ một seminar khác tổ chức bởi MLOpsVN cho các bác nào quan tâm :D. Cảm ơn cả nhà nhiều.",,,,,#webinar
"Có lẽ khái niệm AGI (Artificial General Intelligence) vẫn là gì đó chưa tới, nhưng cách GPT-4 đạt kết quả trong các bài kiểm tra như BAR, LSAT, GRE, AP, đặc biệt ấn tượng. Ví dụ như GRE, thì bài Quantitative đạt 163/170, bài Verbal đạt 169/170 và bài Writing đạt 4/6.
Hiện tại các bạn có thể thử GPT-4 bản chỉ có text trên ChatGPT bản plus.","Có lẽ khái niệm AGI (Artificial General Intelligence) vẫn là gì đó chưa tới, nhưng cách GPT-4 đạt kết quả trong các bài kiểm tra như BAR, LSAT, GRE, AP, đặc biệt ấn tượng. Ví dụ như GRE, thì bài Quantitative đạt 163/170, bài Verbal đạt 169/170 và bài Writing đạt 4/6. Hiện tại các bạn có thể thử GPT-4 bản chỉ có text trên ChatGPT bản plus.",,,,,#sharing
"KHẢO SÁT NHU CẦU THEO DÕI THÔNG TIN VỀ AI-ML 

Em xin phép anh Tiep VuHuu thực hiện khảo sát trong group. 

Hiện tại lĩnh vực AI nói chung, ML nói riêng đang phát triển quá nhanh chóng, với các bước tiến từng ngày. Lượng thông tin liên quan tới AI, việc ứng dụng AI...vượt quá khả năng theo dõi thông thường của bất kì cá nhân nào, trong khi hiểu và sử dụng AI ngày càng trở thành nhu cầu quan trọng trong công việc và đời sống. 

Hiện tại em và team đã có một giải pháp để giải quyết bài toán quá tải thông tin, và dự định sẽ tạo một phiên bản dành riêng tối ưu cho lĩnh vực AI/ML nhằm giúp những người theo dõi, nhà nghiên cứu, doanh nhân khởi nghiệp trong lĩnh vực này có thể theo dõi thông tin tốt hơn. 

Rất mong các anh chị em trong Group dành chút thời gian thực hiện khảo sát để team điều chỉnh sản phẩm tốt nhất. 

Kết quả thực hiện khảo sát sẽ được phân tích và public trở lại với cộng đồng để mọi người cùng có hiểu biết sâu hơn về sự quan tâm của người Việt tới AI nói chung. 

Em xin cám ơn!

👉Link thực hiện khảo sát: http://bit.ly/3Za2mPd","KHẢO SÁT NHU CẦU THEO DÕI THÔNG TIN VỀ AI-ML Em xin phép anh Tiep VuHuu thực hiện khảo sát trong group. Hiện tại lĩnh vực AI nói chung, ML nói riêng đang phát triển quá nhanh chóng, với các bước tiến từng ngày. Lượng thông tin liên quan tới AI, việc ứng dụng AI...vượt quá khả năng theo dõi thông thường của bất kì cá nhân nào, trong khi hiểu và sử dụng AI ngày càng trở thành nhu cầu quan trọng trong công việc và đời sống. Hiện tại em và team đã có một giải pháp để giải quyết bài toán quá tải thông tin, và dự định sẽ tạo một phiên bản dành riêng tối ưu cho lĩnh vực AI/ML nhằm giúp những người theo dõi, nhà nghiên cứu, doanh nhân khởi nghiệp trong lĩnh vực này có thể theo dõi thông tin tốt hơn. Rất mong các anh chị em trong Group dành chút thời gian thực hiện khảo sát để team điều chỉnh sản phẩm tốt nhất. Kết quả thực hiện khảo sát sẽ được phân tích và public trở lại với cộng đồng để mọi người cùng có hiểu biết sâu hơn về sự quan tâm của người Việt tới AI nói chung. Em xin cám ơn! Link thực hiện khảo sát: http://bit.ly/3Za2mPd",,,,,#Q&A
"#Ask Vấn đề Crawling Data ở các sàn
Chào mọi người em là sinh viên đang làm project De tốt nghiệp. Mục tiêu của em là crawl Data ở 1 thị trường ngách sản phẩm làm đẹp từ các sàn thương mại điện tử Lazada, Shopee,Tiki gồm cả Batch lẫn Stream phục vụ nhu cầu phân tích, mọi thứ khá ok cho đến khi em cần phân tích doanh số thị trường ngành lên Dashboard và số lượng sản phẩm thì em gặp 1 vài vấn đề :
Tiki thì 1 số sản phẩm bán missing value khá là nhiều( dao động từ 10-20% ) và có những case thiếu tầm 50% , nhiều sản phẩm quantity_sold chỉ rơi vào 1 dù đăng khá lâu , em khá là đau đầu khi fill value vào những case kiểu này, thường thì em sẽ so sánh quy mô giữa Tiki và Shoppe với cùng 1 loại sản phẩm/tầm giá/số lượng review tuỳ từng th đó và dựa vào tỉ lệ để điền giá trị, nhưng với những case mà chỉ Tiki có thì bó tay.
Đến câu chuyện tổng doanh số thị trường , thì công thức em sử dụng vẫn là giá bán x số lượng bán cộng tất cả. Với Shopee thì khá ổn do giá và số lượng sản phẩm bán số lượng missing value ít, Nhưng lúc tính thì thị phần Tiki thọt so với Shopee khá nhiều, 1 phần ảnh hưởng do dữ liệu . Ngoài ra khi cần real-time sẽ phải tính toán mỗi khung thời gian thì cách của em tính với vài chục nghìn sản phẩm cá nhân em thấy khá là ngu nhưng vẫn chưa có giải pháp tốt hơn. Vậy làm thế nào để mình xử lý trường hợp này thì em cần cao kiến
Dữ liệu em crawl thông qua API của các bên cung cấp, em có tham khảo 1 số sản phẩm như case của Metric.vn thì còn có thể crawl dữ liệu bán của từng sản phẩm trong 1 mốc thời gian thứ mà lúc em crawl ko có, vậy làm thế nào để có thể crawl số lượng bán của 1 sản phẩm trên các sàn ạ?
Do tài năng và kiến thức hạn hẹp,rất mong được các bác chỉ giáo 🙁","Vấn đề Crawling Data ở các sàn Chào mọi người em là sinh viên đang làm project De tốt nghiệp. Mục tiêu của em là crawl Data ở 1 thị trường ngách sản phẩm làm đẹp từ các sàn thương mại điện tử Lazada, Shopee,Tiki gồm cả Batch lẫn Stream phục vụ nhu cầu phân tích, mọi thứ khá ok cho đến khi em cần phân tích doanh số thị trường ngành lên Dashboard và số lượng sản phẩm thì em gặp 1 vài vấn đề : Tiki thì 1 số sản phẩm bán missing value khá là nhiều( dao động từ 10-20% ) và có những case thiếu tầm 50% , nhiều sản phẩm quantity_sold chỉ rơi vào 1 dù đăng khá lâu , em khá là đau đầu khi fill value vào những case kiểu này, thường thì em sẽ so sánh quy mô giữa Tiki và Shoppe với cùng 1 loại sản phẩm/tầm giá/số lượng review tuỳ từng th đó và dựa vào tỉ lệ để điền giá trị, nhưng với những case mà chỉ Tiki có thì bó tay. Đến câu chuyện tổng doanh số thị trường , thì công thức em sử dụng vẫn là giá bán x số lượng bán cộng tất cả. Với Shopee thì khá ổn do giá và số lượng sản phẩm bán số lượng missing value ít, Nhưng lúc tính thì thị phần Tiki thọt so với Shopee khá nhiều, 1 phần ảnh hưởng do dữ liệu . Ngoài ra khi cần real-time sẽ phải tính toán mỗi khung thời gian thì cách của em tính với vài chục nghìn sản phẩm cá nhân em thấy khá là ngu nhưng vẫn chưa có giải pháp tốt hơn. Vậy làm thế nào để mình xử lý trường hợp này thì em cần cao kiến Dữ liệu em crawl thông qua API của các bên cung cấp, em có tham khảo 1 số sản phẩm như case của Metric.vn thì còn có thể crawl dữ liệu bán của từng sản phẩm trong 1 mốc thời gian thứ mà lúc em crawl ko có, vậy làm thế nào để có thể crawl số lượng bán của 1 sản phẩm trên các sàn ạ? Do tài năng và kiến thức hạn hẹp,rất mong được các bác chỉ giáo",#Ask,,,,"#Q&A, #data"
"👉🏻Với trung bình hơn 3 nghìn lượt tải về gói cài đặt mỗi tháng, Deploy AI Systems Yourself (D.AI.S.Y) toolkit - một sản phẩm của Neural Việt Nam là một bộ công cụ bao gồm các thuật toán trí tuệ nhân tạo đã được xây dựng và đóng gói. Mục tiêu hướng đến các bạn trẻ có niềm say mê với công nghệ trí tuệ nhân tạo và muốn thực hành các ứng dụng trí tuệ nhân tạo vào đời sống thông qua các câu lệnh đơn giản.
🙋🏻‍♂️🙋🏻‍♂️Bên cạnh các ứng dụng phổ biến đã được tích hợp sẵn như: nhận diện người đeo khẩu trang, phát hiện dáng người,... Team Daisykit - Neural Việt Nam sẽ tiếp tục phát triển và cho ra mắt các ứng dụng mới trong thời gian sắp tới.","Với trung bình hơn 3 nghìn lượt tải về gói cài đặt mỗi tháng, Deploy AI Systems Yourself (D.AI.S.Y) toolkit - một sản phẩm của Neural Việt Nam là một bộ công cụ bao gồm các thuật toán trí tuệ nhân tạo đã được xây dựng và đóng gói. Mục tiêu hướng đến các bạn trẻ có niềm say mê với công nghệ trí tuệ nhân tạo và muốn thực hành các ứng dụng trí tuệ nhân tạo vào đời sống thông qua các câu lệnh đơn giản. Bên cạnh các ứng dụng phổ biến đã được tích hợp sẵn như: nhận diện người đeo khẩu trang, phát hiện dáng người,... Team Daisykit - Neural Việt Nam sẽ tiếp tục phát triển và cho ra mắt các ứng dụng mới trong thời gian sắp tới.",,,,,#sharing
"Em chào mọi người ạ. Em muốn xin lời khuyên của các anh chị để bổ sung kiến thức cho việc học Machine Learning.
Em hiện là sinh viên năm 2 ngành Khoa học dữ liệu. Sau khi tốt nghiệp em muốn làm bên mảng Machine Learning, cụ thể là Machine Learning Engineer hoặc NLP Scientist cho các công ty về sản phẩm công nghệ số như voice assistant, xe tự hành, ...
Nhưng do ngành này đòi hỏi kiến thức cao (đặc biệt là về Toán), và em cũng được nghe thường người ta tuyển dân Machine Learning từ PhD thôi nên em muốn tìm hiểu nhiều nhất về ML để cạnh tranh và có được công việc trong mảng này.
Các kiến thức em đã biết:
Lập trình cơ bản
Cấu trúc dữ liệu và giải thuật
Cách hoạt động của một model (ở mức cơ bản, chưa đào sâu về Toán)
Cách dùng libray như sklearn để giải quyết bài toán regression cơ bản trên Kaggle
Hy vọng các anh chị có thể cho em xin tham khảo về:
Tài liệu ML, cụ thể là các nền tảng về Toán, Thống Kê cần thiết cho ngành
Các dự án cá nhân để bỏ vô portfolio cần thể hiện kiến thức ở mảng nào
Sinh viên có thể có paper tốt không? Làm sao để bắt đầu nghiên cứu làm paper? Hiện tại em đang làm paper ứng dụng ML trong việc dự báo, vẫn là Regression bằng cách dùng library, nhưng em thấy cách làm này đơn giản, không chuyên sâu và chỉ cần nghiên cứu chút là có thể làm được. ","Em chào mọi người ạ. Em muốn xin lời khuyên của các anh chị để bổ sung kiến thức cho việc học Machine Learning. Em hiện là sinh viên năm 2 ngành Khoa học dữ liệu. Sau khi tốt nghiệp em muốn làm bên mảng Machine Learning, cụ thể là Machine Learning Engineer hoặc NLP Scientist cho các công ty về sản phẩm công nghệ số như voice assistant, xe tự hành, ... Nhưng do ngành này đòi hỏi kiến thức cao (đặc biệt là về Toán), và em cũng được nghe thường người ta tuyển dân Machine Learning từ PhD thôi nên em muốn tìm hiểu nhiều nhất về ML để cạnh tranh và có được công việc trong mảng này. Các kiến thức em đã biết: Lập trình cơ bản Cấu trúc dữ liệu và giải thuật Cách hoạt động của một model (ở mức cơ bản, chưa đào sâu về Toán) Cách dùng libray như sklearn để giải quyết bài toán regression cơ bản trên Kaggle Hy vọng các anh chị có thể cho em xin tham khảo về: Tài liệu ML, cụ thể là các nền tảng về Toán, Thống Kê cần thiết cho ngành Các dự án cá nhân để bỏ vô portfolio cần thể hiện kiến thức ở mảng nào Sinh viên có thể có paper tốt không? Làm sao để bắt đầu nghiên cứu làm paper? Hiện tại em đang làm paper ứng dụng ML trong việc dự báo, vẫn là Regression bằng cách dùng library, nhưng em thấy cách làm này đơn giản, không chuyên sâu và chỉ cần nghiên cứu chút là có thể làm được.",,,,,"#Q&A, #machine_learning"
"Xin hỏi:
Mình đang làm cho một startup về thời trang, và đang tìm kiếm giải pháp đọc một hình, xem trong hình đó người dùng mặt áo quần gì.
Đơn giản hơn là cho hai bức hình, trả lại xác xuất hai bức hình đó có cùng một loại vật thể .
Nếu phải xây dựng training data thì cần cỡ bao nhiêu training data để xây dựng mô hình kiểu này. 100k đủ không ?","Xin hỏi: Mình đang làm cho một startup về thời trang, và đang tìm kiếm giải pháp đọc một hình, xem trong hình đó người dùng mặt áo quần gì. Đơn giản hơn là cho hai bức hình, trả lại xác xuất hai bức hình đó có cùng một loại vật thể . Nếu phải xây dựng training data thì cần cỡ bao nhiêu training data để xây dựng mô hình kiểu này. 100k đủ không ?",,,,,"#Q&A, #data, #cv"
"#nlp #question #documentclustering
Chào mọi người, Em đang cần thực hiện bài toán phân cụm văn bản, bài báo,... ( document clustering) với số cụm không xác định trước. Về phần phân cụm sử dụng DBSCAN em đã hiểu và thấy ok. Tuy nhiên phần embedding văn bản, em muốn tham khảo ý kiến mọi người phương pháp nào tốt nhất hiện nay. Em được biết về phần câu có các mô hình sentence embedding kết quả khá ok như sbert,... Tuy nhiên với cả văn bản dài thì em tìm hiểu không thấy có nhiều thông tin. Mong mọi người cho ý kiến về giải pháp hoặc các keyword, bài viết liên quan để em tìm hiểu ạ.  Em cảm ơn","Chào mọi người, Em đang cần thực hiện bài toán phân cụm văn bản, bài báo,... ( document clustering) với số cụm không xác định trước. Về phần phân cụm sử dụng DBSCAN em đã hiểu và thấy ok. Tuy nhiên phần embedding văn bản, em muốn tham khảo ý kiến mọi người phương pháp nào tốt nhất hiện nay. Em được biết về phần câu có các mô hình sentence embedding kết quả khá ok như sbert,... Tuy nhiên với cả văn bản dài thì em tìm hiểu không thấy có nhiều thông tin. Mong mọi người cho ý kiến về giải pháp hoặc các keyword, bài viết liên quan để em tìm hiểu ạ. Em cảm ơn",#nlp	#question	#documentclustering,,,,"#Q&A, #nlp, #deep_learning, #machine_learning"
"Hi mọi người, mình có một thắc mắc muốn nhờ mọi người giải thích giúp với.
Mình đang thực hiện một dự án cá nhân, trong đó có một phần xử lý đọc các ký tự chữ số trên mặt đồng hồ. Framework mình xử dụng là detectron2.
Khi mình training và predict thử trên 2 version khác nhau là 0.6 và 0.1.3 thì kết quả là version 0.1.3 mình thấy tốc độ hội tụ hanh hơn nhiều, chỉ cần training 2000 iter với dataset 1300/150 là đã cho kết quả rất ấn tượng rồi. Trong khi mình training trên bản 0.6 với tập dataset lớn hơn 1 chút 1600/250 với 80000 iter nhưng kết quả tốc độ hội tụ không ổn định, nhiều số không thể nhận diện được. Không rõ có ai gặp phải tình trạng như mình không vậy?","Hi mọi người, mình có một thắc mắc muốn nhờ mọi người giải thích giúp với. Mình đang thực hiện một dự án cá nhân, trong đó có một phần xử lý đọc các ký tự chữ số trên mặt đồng hồ. Framework mình xử dụng là detectron2. Khi mình training và predict thử trên 2 version khác nhau là 0.6 và 0.1.3 thì kết quả là version 0.1.3 mình thấy tốc độ hội tụ hanh hơn nhiều, chỉ cần training 2000 iter với dataset 1300/150 là đã cho kết quả rất ấn tượng rồi. Trong khi mình training trên bản 0.6 với tập dataset lớn hơn 1 chút 1600/250 với 80000 iter nhưng kết quả tốc độ hội tụ không ổn định, nhiều số không thể nhận diện được. Không rõ có ai gặp phải tình trạng như mình không vậy?",,,,,"#Q&A, #cv, #deep_learning"
"🤖🤖🤖 ChatGPT đang làm mưa làm gió trên toàn thế giới những ngày vừa qua. Hãy cùng team NeuralVN tìm hiểu về công nghệ phía sau chatbot đình đám này qua bài viết ""Bí mật công nghệ đằng sau ChatGPT"".","ChatGPT đang làm mưa làm gió trên toàn thế giới những ngày vừa qua. Hãy cùng team NeuralVN tìm hiểu về công nghệ phía sau chatbot đình đám này qua bài viết ""Bí mật công nghệ đằng sau ChatGPT"".",,,,,#sharing
"Chào mọi người ạ, em đang tìm hiểu về AdaDelta optimizer, có một đoạn em vẫn chưa hiểu đó là trong paper, tác giả có đề cập về sự không thống nhất trong đơn vị và đưa ra giải pháp đề xuất. Tuy nhiên em vẫn chưa thể nào hình dung được sự sai lệch về đơn vị này là như thế nào mặc dù đã search và đọc thêm từ nhiều nguồn. Mọi người có thể cho em một ví dụ cụ thể hay một giải thích trực quan với ạ.
Đây là paper em đang đọc, đề cập về đơn vị ở phần 3.2 (trang 3) : https://arxiv.org/pdf/1212.5701.pdf
Em cảm ơn mọi người đã quan tâm, chúc mọi người một ngày tốt lành.","Chào mọi người ạ, em đang tìm hiểu về AdaDelta optimizer, có một đoạn em vẫn chưa hiểu đó là trong paper, tác giả có đề cập về sự không thống nhất trong đơn vị và đưa ra giải pháp đề xuất. Tuy nhiên em vẫn chưa thể nào hình dung được sự sai lệch về đơn vị này là như thế nào mặc dù đã search và đọc thêm từ nhiều nguồn. Mọi người có thể cho em một ví dụ cụ thể hay một giải thích trực quan với ạ. Đây là paper em đang đọc, đề cập về đơn vị ở phần 3.2 (trang 3) : https://arxiv.org/pdf/1212.5701.pdf Em cảm ơn mọi người đã quan tâm, chúc mọi người một ngày tốt lành.",,,,,"#Q&A, #deep_learning"
"Chào mọi người ạ, em đang có một thắc mắc về ý nghĩa của việc chuẩn hóa phân phối của data về dạng phân phối chuẩn. 
Có một câu trả lời ở đây mà em đang quan tâm : https://www.quora.com/Why-in-machine-learning-do-lots-of-people-want-to-convert-skewed-data-into-normal-distribution
Tác giả nói rằng, giả sử dữ liệu của chúng ta tuân theo hàm y = f(x) + e với f(x) là một hàm số cố định và e là biến ngẫu nhiên tuân theo phân phối chuẩn, tác giả chỉ ra việc chuyển data về pp chuẩn giúp cho e không phụ thuộc vào dữ liệu nữa, tức là e đang độc lập và có mean bằng 0, nếu e không độc lập tức là e = bias + t với t là biến ngẫu nhiên độc lập có mean = 0. Tuy nhiên lấy ví dụ trong linear regression, mặt phẳng cần tìm có dạng như w1x1 + w2x2 + b, thì rõ ràng là mình đã có thành phần bias trong đây rồi, thế thì em mới nảy ra 2 câu hỏi như sau :
Như đã nói ở trên thì có phải những thuật tương tự như linear regression không cần chuyển về pp chuẩn ?
Xét các trường hợp khác, nếu ta đã chuyển pp dữ liệu về pp chuẩn thì không cần thành phần bias ?
Cảm ơn mọi người đã quan tâm, chúc mọi người một ngày tốt lành","Chào mọi người ạ, em đang có một thắc mắc về ý nghĩa của việc chuẩn hóa phân phối của data về dạng phân phối chuẩn. Có một câu trả lời ở đây mà em đang quan tâm : https://www.quora.com/Why-in-machine-learning-do-lots-of-people-want-to-convert-skewed-data-into-normal-distribution Tác giả nói rằng, giả sử dữ liệu của chúng ta tuân theo hàm y = f(x) + e với f(x) là một hàm số cố định và e là biến ngẫu nhiên tuân theo phân phối chuẩn, tác giả chỉ ra việc chuyển data về pp chuẩn giúp cho e không phụ thuộc vào dữ liệu nữa, tức là e đang độc lập và có mean bằng 0, nếu e không độc lập tức là e = bias + t với t là biến ngẫu nhiên độc lập có mean = 0. Tuy nhiên lấy ví dụ trong linear regression, mặt phẳng cần tìm có dạng như w1x1 + w2x2 + b, thì rõ ràng là mình đã có thành phần bias trong đây rồi, thế thì em mới nảy ra 2 câu hỏi như sau : Như đã nói ở trên thì có phải những thuật tương tự như linear regression không cần chuyển về pp chuẩn ? Xét các trường hợp khác, nếu ta đã chuyển pp dữ liệu về pp chuẩn thì không cần thành phần bias ? Cảm ơn mọi người đã quan tâm, chúc mọi người một ngày tốt lành",,,,,"#Q&A, #math, #machine_learning"
Bác nào gợi ý giúp mình cái cloud nào train tốt với chứ gg colab chạy thhế kia bao giờ mới xong :3,Bác nào gợi ý giúp mình cái cloud nào train tốt với chứ gg colab chạy thhế kia bao giờ mới xong :3,,,,,#Q&A
"Chào mọi người,
Mình là lập trình viên không chuyên AI, ML nhưng muốn vọc vạch tý nên có dự định làm hệ thống phân loại tài liệu.
Mình đang có hàng TB tài liệu để lộn xộn, nhiều định dạng khác nhau, còn bị trùng lặp do tên file khác nhau vì download ở nhiều nguồn.
Nay mình muốn phát triển tool tự động phân loại rồi gom chúng vào các thư mục phân theo thể loại.
Nhờ mọi người vạch ra giúp mình xem cần học những gì và thực hiện như thế nào?
Mình cảm ơn.","Chào mọi người, Mình là lập trình viên không chuyên AI, ML nhưng muốn vọc vạch tý nên có dự định làm hệ thống phân loại tài liệu. Mình đang có hàng TB tài liệu để lộn xộn, nhiều định dạng khác nhau, còn bị trùng lặp do tên file khác nhau vì download ở nhiều nguồn. Nay mình muốn phát triển tool tự động phân loại rồi gom chúng vào các thư mục phân theo thể loại. Nhờ mọi người vạch ra giúp mình xem cần học những gì và thực hiện như thế nào? Mình cảm ơn.",,,,,"#Q&A, #nlp"
"[Final Project - Machine Learning In Production - MLOps]
[Implement MLOps for Credit Risk scoring in Banking]
This is a final project thesis about “Banking Credit Risk” of “course 5 - Machine Learning in Production – MLOps”, which was implemented by young excellent students: Tran Thi Hoang Anh, Le Thi Duyen, Nguyen Bui Hoang Long.
One of my students asked me: Dear teacher, I am in the banking sector, it is really necessary to train credit risk models on a regular time base because the economic context is constantly shifting time by time. Usually, implementing these banking models according to traditional ways is difficult and brain-teasing due to manually handling Machine Learning steps. Then, I responded why don't you think about MLOps? Maybe, MLOps is a perfect remedy to overcome these hurts efficiently.
In this final project thesis, my student group has carried out a lot of experiments in terms of models and data feature engineering to boost model performance on the training dataset whilst still considerably assured the model latency as well as computational cost. Thanks to accelerating the MLOps lifecycle, models are updated with the daily dataset without imposing model degradation.
It is an enjoyable feeling mixed with mild pride when my students were able to comprehend MLOps methodologies and competently handle Banking risk models.","[Final Project - Machine Learning In Production - MLOps] [Implement MLOps for Credit Risk scoring in Banking] This is a final project thesis about “Banking Credit Risk” of “course 5 - Machine Learning in Production – MLOps”, which was implemented by young excellent students: Tran Thi Hoang Anh, Le Thi Duyen, Nguyen Bui Hoang Long. One of my students asked me: Dear teacher, I am in the banking sector, it is really necessary to train credit risk models on a regular time base because the economic context is constantly shifting time by time. Usually, implementing these banking models according to traditional ways is difficult and brain-teasing due to manually handling Machine Learning steps. Then, I responded why don't you think about MLOps? Maybe, MLOps is a perfect remedy to overcome these hurts efficiently. In this final project thesis, my student group has carried out a lot of experiments in terms of models and data feature engineering to boost model performance on the training dataset whilst still considerably assured the model latency as well as computational cost. Thanks to accelerating the MLOps lifecycle, models are updated with the daily dataset without imposing model degradation. It is an enjoyable feeling mixed with mild pride when my students were able to comprehend MLOps methodologies and competently handle Banking risk models.",,,,,bỏ
"Anh em đang học hoặc đã làm thì cùng mình trao đổi 1 số câu hỏi về CV nhé:
Tại sao Yolov8 pretrained task object detection bằng tập COCO, trong khi task classification lại dùng tập ImageNet. Câu hỏi vận dụng là nếu mình có 1 bài toán đầu vào ảnh grayscale cho task object detection thì việc dùng pretrain trên tập COCO có hiệu quả bằng việc dùng bộ dataset grayscale khác ( như mnist) hay không ?
Việc chọn backbone khi xây dựng model thì nên dựa vào yếu tố nào ?
Làm thế nào để đánh giá việc data  agumentation thủ công có hiệu quả hơn so với dùng có sẵn của yolo ?","Anh em đang học hoặc đã làm thì cùng mình trao đổi 1 số câu hỏi về CV nhé: Tại sao Yolov8 pretrained task object detection bằng tập COCO, trong khi task classification lại dùng tập ImageNet. Câu hỏi vận dụng là nếu mình có 1 bài toán đầu vào ảnh grayscale cho task object detection thì việc dùng pretrain trên tập COCO có hiệu quả bằng việc dùng bộ dataset grayscale khác ( như mnist) hay không ? Việc chọn backbone khi xây dựng model thì nên dựa vào yếu tố nào ? Làm thế nào để đánh giá việc data agumentation thủ công có hiệu quả hơn so với dùng có sẵn của yolo ?",,,,,"#Q&A, #data, #deep_learning, #cv"
"Xin giới thiệu với mọi người một tài liệu được viết bởi 2 giáo sư hàng đầu của Stanford University, Prof. Jure Leskovec & Prof. Jeffrey D. Ullman, và cùng một entrepreneur nổi tiếng của Thung Lũng Silicon Anand Rajaraman, về chủ đề ""Mining Massive Data"". Sách xoay quanh những phương pháp khai thác dữ liệu hiệu quả, chính xác, và nhanh chóng. Link sách: http://www.mmds.org/, Link course: https://online.stanford.edu/courses/soe-ycs0007-mining-massive-data-sets
Đồng thời, xin chia sẻ luôn một cuốn sách cùng chủ đề Data Mining nhưng được viết theo hướng ứng dụng hơn kèm code do Dr. Ron Zacharski chia sẻ kinh nghiệm của ông miễn phí cho cộng đồng: http://guidetodatamining.com/","Xin giới thiệu với mọi người một tài liệu được viết bởi 2 giáo sư hàng đầu của Stanford University, Prof. Jure Leskovec & Prof. Jeffrey D. Ullman, và cùng một entrepreneur nổi tiếng của Thung Lũng Silicon Anand Rajaraman, về chủ đề ""Mining Massive Data"". Sách xoay quanh những phương pháp khai thác dữ liệu hiệu quả, chính xác, và nhanh chóng. Link sách: http://www.mmds.org/, Link course: https://online.stanford.edu/courses/soe-ycs0007-mining-massive-data-sets Đồng thời, xin chia sẻ luôn một cuốn sách cùng chủ đề Data Mining nhưng được viết theo hướng ứng dụng hơn kèm code do Dr. Ron Zacharski chia sẻ kinh nghiệm của ông miễn phí cho cộng đồng: http://guidetodatamining.com/",,,,,"#Q&A, #data"
"Đợt vừa rồi mình và team có tham gia một cuộc thi về trích xuất thông tin từ hóa đơn có tên gọi là The Mobile capture receipts Optical Character Recognition (MC-OCR).
Mình có một bài viết chia sẻ về giải pháp của team mình, mọi người tham khảo và góp ý ạ.","Đợt vừa rồi mình và team có tham gia một cuộc thi về trích xuất thông tin từ hóa đơn có tên gọi là The Mobile capture receipts Optical Character Recognition (MC-OCR). Mình có một bài viết chia sẻ về giải pháp của team mình, mọi người tham khảo và góp ý ạ.",,,,,"#sharing, #cv"
Mọi người cho e hỏi. Em định encode dữ liệu nến nhật để train CNN phân loại mẫu hình thì liệu có hiệu quả không ạ. Có paper hay dự án nào làm r chưa ạ?,Mọi người cho e hỏi. Em định encode dữ liệu nến nhật để train CNN phân loại mẫu hình thì liệu có hiệu quả không ạ. Có paper hay dự án nào làm r chưa ạ?,,,,,"#Q&A, #cv, #deep_learning"
"[Data-Centric AI Course]
Thông thường các khóa học về Machine Learning sẽ dạy nhiều về các mô hình. Tuy nhiên khi làm việc trong môi trường thực tế, dữ liệu thường nhiễu và hỗn độn (messy), thế nên bên cạnh việc cải thiện mô hình, chúng ta cũng nên tập trung vào cải thiện các vấn đề của dữ liệu. Data-Centric AI (DCAI) là hướng nghiên cứu mới, tập trung vào việc cải thiện dữ liệu để tăng độ hiệu quả của mô hình.
Khóa này dạy các bạn giải quyết các vấn đề về dữ liệu trong các project thực tế như:
- Data-Centric AI vs. Model-Centric AI
- Label Errors
- Dataset Creation and Curation
- Data-centric Evaluation of ML Models
- Class Imbalance, Outliers, and Distribution Shift
- Growing or Compressing Datasets
- Interpretability in Data-Centric ML
- Encoding Human Priors: Data Augmentation and Prompt Engineering
- Data Privacy and Security","[Data-Centric AI Course] Thông thường các khóa học về Machine Learning sẽ dạy nhiều về các mô hình. Tuy nhiên khi làm việc trong môi trường thực tế, dữ liệu thường nhiễu và hỗn độn (messy), thế nên bên cạnh việc cải thiện mô hình, chúng ta cũng nên tập trung vào cải thiện các vấn đề của dữ liệu. Data-Centric AI (DCAI) là hướng nghiên cứu mới, tập trung vào việc cải thiện dữ liệu để tăng độ hiệu quả của mô hình. Khóa này dạy các bạn giải quyết các vấn đề về dữ liệu trong các project thực tế như: - Data-Centric AI vs. Model-Centric AI - Label Errors - Dataset Creation and Curation - Data-centric Evaluation of ML Models - Class Imbalance, Outliers, and Distribution Shift - Growing or Compressing Datasets - Interpretability in Data-Centric ML - Encoding Human Priors: Data Augmentation and Prompt Engineering - Data Privacy and Security",,,,,"#sharing, #data"
"Ai đã làm về hệ thống điểm danh bằng camera cho mình xin ý kiến về ưu/nhược điểm (hoặc nên/không nên) về các phương hướng sau:
1- Face_recognition
2- VGG + ArcFace
3- YOLO
4- gợi ý thêm 😅
Thank.",Ai đã làm về hệ thống điểm danh bằng camera cho mình xin ý kiến về ưu/nhược điểm (hoặc nên/không nên) về các phương hướng sau: 1- Face_recognition 2- VGG + ArcFace 3- YOLO 4- gợi ý thêm Thank.,,,,,"#Q&A, #cv, #deep_learning"
Em xin phép chia sẻ thông tin về buổi webinar tối nay cho các bác quan tâm tới MLOps 😁,Em xin phép chia sẻ thông tin về buổi webinar tối nay cho các bác quan tâm tới MLOps,,,,,#webinar
"Kính chào các bác. Đợt này nhân dịp đang quay lại học Reinforcement Learrning nên em mạnh dạn chia sẻ cùng cả nhà 1 video về ""Deep Q Learrning"".
Vẫn với phương châm đơn giản, hi vọng giúp được các anh em mới học thôi!","Kính chào các bác. Đợt này nhân dịp đang quay lại học Reinforcement Learrning nên em mạnh dạn chia sẻ cùng cả nhà 1 video về ""Deep Q Learrning"". Vẫn với phương châm đơn giản, hi vọng giúp được các anh em mới học thôi!",,,,,"#sharing, #machine_learning"
"[ChatGPT for Google - Chrome Extension]
Plugin ChatGPT for Google này sẽ giúp bạn hiện thị nội dung trả lời của ChatGPT cho nội dung bạn tìm kiếm bên cạnh kết quả trả về của Google.
Với sức mạnh của ChatGPT thì đây là một tính năng cực kì hữu ích cho người dùng. Ví dụ như tìm kiếm các câu hỏi liên quan tới lập trình, ChatGPT sẽ là bản tổng hợp của Stackoverflow, Github,.., khi cho bạn cả câu trả lời và giải thích luôn. Nói chung giờ anh em dev cũng nhàn :)))
Như ở dưới mình hỏi cách vẽ Bar Graph trong Python, nó sinh ra cả code có cả comment luôn.","[ChatGPT for Google - Chrome Extension] Plugin ChatGPT for Google này sẽ giúp bạn hiện thị nội dung trả lời của ChatGPT cho nội dung bạn tìm kiếm bên cạnh kết quả trả về của Google. Với sức mạnh của ChatGPT thì đây là một tính năng cực kì hữu ích cho người dùng. Ví dụ như tìm kiếm các câu hỏi liên quan tới lập trình, ChatGPT sẽ là bản tổng hợp của Stackoverflow, Github,.., khi cho bạn cả câu trả lời và giải thích luôn. Nói chung giờ anh em dev cũng nhàn :))) Như ở dưới mình hỏi cách vẽ Bar Graph trong Python, nó sinh ra cả code có cả comment luôn.",,,,,#sharing
"Hi mọi người, em đã kết thúc 6 tháng intern AI Engineer và cảm thấy chút không hợp do phần lớn thời gian sẽ sử dụng để training mô hình. Hiện tại em muốn chuyển hướng sang Data Scientist hoặc SE nên em muốn hỏi mọi người trong Group, công việc của 1 Data Scientist sẽ làm những gì và sẽ dành nhiều thời gian làm việc gì? Em thì hay thấy bảo làm Data Scientist ở VN mà chỉ tốt nghiệp Cử nhân/ Kỹ sư thì làm không hiệu quả, mong mọi người cho lời khuyên.
Cảm ơn mọi người.","Hi mọi người, em đã kết thúc 6 tháng intern AI Engineer và cảm thấy chút không hợp do phần lớn thời gian sẽ sử dụng để training mô hình. Hiện tại em muốn chuyển hướng sang Data Scientist hoặc SE nên em muốn hỏi mọi người trong Group, công việc của 1 Data Scientist sẽ làm những gì và sẽ dành nhiều thời gian làm việc gì? Em thì hay thấy bảo làm Data Scientist ở VN mà chỉ tốt nghiệp Cử nhân/ Kỹ sư thì làm không hiệu quả, mong mọi người cho lời khuyên. Cảm ơn mọi người.",,,,,#Q&A
Em đang làm model nhận diện biển báo giao thông bằng CNN. Chạy file train thì nó đến epoch 446/2000 bị dừng như vậy và kết thúc chương trình luôn. Có ai biết lỗi này là lỗi gì và cách fix không ạ.,Em đang làm model nhận diện biển báo giao thông bằng CNN. Chạy file train thì nó đến epoch 446/2000 bị dừng như vậy và kết thúc chương trình luôn. Có ai biết lỗi này là lỗi gì và cách fix không ạ.,,,,,"#Q&A, #cv, #deep_learning"
"Xin chào mọi người.
Bài toán phân loại hình ảnh (image classification) là một trong những bài toán quan trọng trong lĩnh vực Computer Vision. Một cách giải quyết hiệu quả cho bài toán này là sử dụng kĩ thuật transfer learning vốn không yêu cầu quá nhiều về data hay resource mà vẫn mang lại kết quả tốt. Mình vừa xây dựng repository tổng hợp các thuật image classification mà pytorch có hỗ trợ transfer learning: Efficientnet, resnet, vgg, googlenet. Nó có thể sẽ hữu ích với các bạn mới tiếp xúc với bài toán này, mới tiếp xúc với pytorch hay đơn giản muốn sử dụng code nhanh gọn lẹ để giải quyết bài toán. Đối với những bạn đã thành thạo pytorch, muốn custom model và data nhiều hơn thì repo này có vẻ sẽ kém hữu ích với các bạn.
Trong repo này sẽ có các phần code:
- Làm thế nào load data do mình tự thu thập (custom dataset) vào model.
- Thay đổi hàm loss, hàm optimize
- Train và predict trên dữ liệu mới.
link:","Xin chào mọi người. Bài toán phân loại hình ảnh (image classification) là một trong những bài toán quan trọng trong lĩnh vực Computer Vision. Một cách giải quyết hiệu quả cho bài toán này là sử dụng kĩ thuật transfer learning vốn không yêu cầu quá nhiều về data hay resource mà vẫn mang lại kết quả tốt. Mình vừa xây dựng repository tổng hợp các thuật image classification mà pytorch có hỗ trợ transfer learning: Efficientnet, resnet, vgg, googlenet. Nó có thể sẽ hữu ích với các bạn mới tiếp xúc với bài toán này, mới tiếp xúc với pytorch hay đơn giản muốn sử dụng code nhanh gọn lẹ để giải quyết bài toán. Đối với những bạn đã thành thạo pytorch, muốn custom model và data nhiều hơn thì repo này có vẻ sẽ kém hữu ích với các bạn. Trong repo này sẽ có các phần code: - Làm thế nào load data do mình tự thu thập (custom dataset) vào model. - Thay đổi hàm loss, hàm optimize - Train và predict trên dữ liệu mới. link:",,,,,"#Q&A, #cv, #deep_learning"
Chào mọi người. Mình là newbie. Mình đang có nhu cầu tìm hiểu về việc render 3D từ ảnh chụp selfie/camera điện thoại thành đối tượng có thể chỉnh sửa được. Các bạn cho mình hỏi mình nên tìm hiểu theo hướng nào và keywords nào nhé. Cám ơn các bạn đã giúp đỡ,Chào mọi người. Mình là newbie. Mình đang có nhu cầu tìm hiểu về việc render 3D từ ảnh chụp selfie/camera điện thoại thành đối tượng có thể chỉnh sửa được. Các bạn cho mình hỏi mình nên tìm hiểu theo hướng nào và keywords nào nhé. Cám ơn các bạn đã giúp đỡ,,,,,
"Em chào mọi người ạ
Hiện em đang tìm hiểu và thực hiện bài toán Recommender system với mô hình VAE trên tập dữ liệu MovieLens. Mọi người cho em hỏi là sau khi xây dựng được utility matrix (chứa các rating từ 1 đến 5, và 0 là giá trị thể hiện cho item chưa được user đánh giá) thì khi đưa vào mô hình ta có cần bước normalize không? Và nếu phải normalize thì ta sẽ chỉ normalize các giá trị khác 0 thôi phải không ạ (bởi vì mục đích của em là sẽ không tính toán gradient trên các giá trị bằng 0 vì các giá trị này để cho phần test)?","Em chào mọi người ạ Hiện em đang tìm hiểu và thực hiện bài toán Recommender system với mô hình VAE trên tập dữ liệu MovieLens. Mọi người cho em hỏi là sau khi xây dựng được utility matrix (chứa các rating từ 1 đến 5, và 0 là giá trị thể hiện cho item chưa được user đánh giá) thì khi đưa vào mô hình ta có cần bước normalize không? Và nếu phải normalize thì ta sẽ chỉ normalize các giá trị khác 0 thôi phải không ạ (bởi vì mục đích của em là sẽ không tính toán gradient trên các giá trị bằng 0 vì các giá trị này để cho phần test)?",,,,,
"Em xin chào mọi người ạ.
Hiện em đang định thêm một số từ để tokenizer và em có tham khảo một số cách với underthesea và vncorenlp nhưng chưa tìm được cách nào khả quan.
Mong được mọi người giúp đỡ ạ.
Em xin cảm ơn ạ!!!",Em xin chào mọi người ạ. Hiện em đang định thêm một số từ để tokenizer và em có tham khảo một số cách với underthesea và vncorenlp nhưng chưa tìm được cách nào khả quan. Mong được mọi người giúp đỡ ạ. Em xin cảm ơn ạ!!!,,,,,
"Hướng Dẫn Tích Hợp OpenAI GPT-3 vào Web App với Streamlit
Chia sẻ với mọi người cách tích hợp OpenAI GPT-3 vào Streamlit để tạo một trợ lý cho chính mình. Streamlit là công cụ khá quen thuộc với ai làm AI để demo sản phẩm, đặc biệt là những ai không quen với HTML và CSS, chúng ta có thể custom dễ dàng. Hy vọng video sẽ giúp ích được cho mọi người.
Cảm ơn admin đã duyệt bài.
https://youtu.be/E1I9mBcDQSo","Hướng Dẫn Tích Hợp OpenAI GPT-3 vào Web App với Streamlit Chia sẻ với mọi người cách tích hợp OpenAI GPT-3 vào Streamlit để tạo một trợ lý cho chính mình. Streamlit là công cụ khá quen thuộc với ai làm AI để demo sản phẩm, đặc biệt là những ai không quen với HTML và CSS, chúng ta có thể custom dễ dàng. Hy vọng video sẽ giúp ích được cho mọi người. Cảm ơn admin đã duyệt bài. https://youtu.be/E1I9mBcDQSo",,,,,
"Chào các anh chị, hiện em đang tìm hiểu về “Personalized Recommendation System”, có thể do khả năng tìm kiếm còn hạn chế nên hầu hết các document em tìm được chỉ dừng lại ở mức lý thuyết, không đa dạng như những đề tài khác. Nếu anh/chị/bạn nào đã có kinh nghiệm về đề tài này em xin một số tài liệu để có thể tìm hiểu sâu hơn ạ. Mục đích là có thể tạo ra một PRS mini như kiểu youtube hoặc các sàn tmdt khác ạ.","Chào các anh chị, hiện em đang tìm hiểu về “Personalized Recommendation System”, có thể do khả năng tìm kiếm còn hạn chế nên hầu hết các document em tìm được chỉ dừng lại ở mức lý thuyết, không đa dạng như những đề tài khác. Nếu anh/chị/bạn nào đã có kinh nghiệm về đề tài này em xin một số tài liệu để có thể tìm hiểu sâu hơn ạ. Mục đích là có thể tạo ra một PRS mini như kiểu youtube hoặc các sàn tmdt khác ạ.",,,,,
"Có lẽ ChatGPT đã mang đến cho con người một cơ hội được làm những thứ hay ho nhất của mọi việc đó là hiểu rõ mục đích, bản chất của vấn đề, sáng tạo cách giải quyết vấn đề, còn những việc tay chân thì đã có ChatGPT lo.","Có lẽ ChatGPT đã mang đến cho con người một cơ hội được làm những thứ hay ho nhất của mọi việc đó là hiểu rõ mục đích, bản chất của vấn đề, sáng tạo cách giải quyết vấn đề, còn những việc tay chân thì đã có ChatGPT lo.",,,,,
"Chào mọi người. Em tìm thấy mộ repo trên github nói về PP-YOLO. Các bác có ai từng triển khai nó chưa ạ. Cho em xin lời khyên và hướng dẫn, e đang định triển khai YOLOv7 theo cách này. Em xn cảm ơn
https://github.com/PaddlePaddle/PaddleDetection","Chào mọi người. Em tìm thấy mộ repo trên github nói về PP-YOLO. Các bác có ai từng triển khai nó chưa ạ. Cho em xin lời khyên và hướng dẫn, e đang định triển khai YOLOv7 theo cách này. Em xn cảm ơn https://github.com/PaddlePaddle/PaddleDetection",,,,,
"Em chào anh chị và các bạn, em muốn hỏi về cuộc thi VLSP2022 vừa qua kết quả các task (cụ thể 2 task Constituency Parsing và Machine Translation) công bố ở đâu ạ, em đã tìm kiếm nhưng không thấy thông tin :( 
#VLSP2022 ","Em chào anh chị và các bạn, em muốn hỏi về cuộc thi VLSP2022 vừa qua kết quả các task (cụ thể 2 task Constituency Parsing và Machine Translation) công bố ở đâu ạ, em đã tìm kiếm nhưng không thấy thông tin :(",#VLSP2022,,,,
"Chào các bác.
Em đang học ML, có câu lệnh
%matplotlib widget
em run trên jupiter với vs code nó đều lỗi, cmd install ipymlp rồi vẫn hiện ra là k tìm thấy module.
Các bác đã gặp lỗi này chưa và giải quết thế nào ạ? em cảm ơn!","Chào các bác. Em đang học ML, có câu lệnh %matplotlib widget em run trên jupiter với vs code nó đều lỗi, cmd install ipymlp rồi vẫn hiện ra là k tìm thấy module. Các bác đã gặp lỗi này chưa và giải quết thế nào ạ? em cảm ơn!",,,,,
"Cơn bão ChatGPT mới đổ bộ gần đây vẫn chưa có dấu hiệu suy yếu!
Sau khi thử chat chit với ChatGPT thì mình nghĩ ngay tới việc xây dựng một Chat Bot tương tự. Và mình tìm được một video khá thú vị của Andrej Karpathy (cựu giám đốc nghiên cứu AI của Tesla), các bạn cùng tham khảo nhé 🙂","Cơn bão ChatGPT mới đổ bộ gần đây vẫn chưa có dấu hiệu suy yếu! Sau khi thử chat chit với ChatGPT thì mình nghĩ ngay tới việc xây dựng một Chat Bot tương tự. Và mình tìm được một video khá thú vị của Andrej Karpathy (cựu giám đốc nghiên cứu AI của Tesla), các bạn cùng tham khảo nhé",,,,,
"xin phép ad.
hiện công ty mình đang có bài toán đếm vật tư từ 1 ảnh.
hiện với ảnh mà các vật tư xếp sát nhau thì ko nhận ra được.
còn ảnh ko sát thì nhận được hết.
hiện đang sử dụng yolo7 ah.
AE cho thêm ý kiến để em chỉnh ah.",xin phép ad. hiện công ty mình đang có bài toán đếm vật tư từ 1 ảnh. hiện với ảnh mà các vật tư xếp sát nhau thì ko nhận ra được. còn ảnh ko sát thì nhận được hết. hiện đang sử dụng yolo7 ah. AE cho thêm ý kiến để em chỉnh ah.,,,,,
"Hello mọi người,
Em vừa áp dụng thử một bài NN đơn giản từ kênh Mì AI:https://www.miai.vn/2019/09/23/xay-dung-he-thong-chong-trom-don-gian-bang-yolo-va-opencv/
Em áp dụng y xì vậy và kết quả bị báo lỗi như các ảnh dưới.
Các bác có biết bị thế này vì sao ko ạ?
Em cảm ơn trước.","Hello mọi người, Em vừa áp dụng thử một bài NN đơn giản từ kênh Mì AI:https://www.miai.vn/2019/09/23/xay-dung-he-thong-chong-trom-don-gian-bang-yolo-va-opencv/ Em áp dụng y xì vậy và kết quả bị báo lỗi như các ảnh dưới. Các bác có biết bị thế này vì sao ko ạ? Em cảm ơn trước.",,,,,
"Kính chào các bác.
Hôm nay em mạnh dạn chia sẻ với anh em một chút về ChatGPT và cùng nhau tìm hiểu cáchtích hợp ChatGPT vào ứng dụng cá nhân thông qua OpenAI API nhé! Bài này có một chút về Streamlit cho anh em nào cần nhé!
Chỉ hi vọng giúp được các bạn newbie mới học như mình!",Kính chào các bác. Hôm nay em mạnh dạn chia sẻ với anh em một chút về ChatGPT và cùng nhau tìm hiểu cáchtích hợp ChatGPT vào ứng dụng cá nhân thông qua OpenAI API nhé! Bài này có một chút về Streamlit cho anh em nào cần nhé! Chỉ hi vọng giúp được các bạn newbie mới học như mình!,,,,,
"Em chào anh chị và các bạn ạ.
Em đang làm đồ án về đề tài Tổng hợp tiếng nói và đang đến phần đánh gía kết quả bằng điểm MOS. Tuy nhiên em đang gặp khó khăn về việc tìm người đánh giá với số lượng đủ lớn để đạt độ tin cậy cao.
Vì vậy em tha thiết hy vọng mọi người có thể giúp em đánh giá kết quả và góp ý cùng em.Em xin cảm ơn rất nhiều ạ.
Em gửi link khảo sát :",Em chào anh chị và các bạn ạ. Em đang làm đồ án về đề tài Tổng hợp tiếng nói và đang đến phần đánh gía kết quả bằng điểm MOS. Tuy nhiên em đang gặp khó khăn về việc tìm người đánh giá với số lượng đủ lớn để đạt độ tin cậy cao. Vì vậy em tha thiết hy vọng mọi người có thể giúp em đánh giá kết quả và góp ý cùng em.Em xin cảm ơn rất nhiều ạ. Em gửi link khảo sát :,,,,,
"Dạ em xin chào mọi người. Xin cho phép em hỏi về model KNN, dạ em train model KNN dùng để nhận diện khuôn mặt với data là 150 hình/ người, có 3 người. Em thử test nhận diện thì nó trả về kết quả đúng nhưng thời gian thì lâu ạ. Nhanh nhất là 1.5s đến 3s lận. Anh/chị có kinh nghiệm có thể giúp em có cách nào khắc phục và trả về kết quả dưới < 1s ạ hoặc sử dụng kĩ thuật khác ạ . Em xin cảm ơn anh/chị đã bỏ thời gian đọc bài viết của em.","Dạ em xin chào mọi người. Xin cho phép em hỏi về model KNN, dạ em train model KNN dùng để nhận diện khuôn mặt với data là 150 hình/ người, có 3 người. Em thử test nhận diện thì nó trả về kết quả đúng nhưng thời gian thì lâu ạ. Nhanh nhất là 1.5s đến 3s lận. Anh/chị có kinh nghiệm có thể giúp em có cách nào khắc phục và trả về kết quả dưới < 1s ạ hoặc sử dụng kĩ thuật khác ạ . Em xin cảm ơn anh/chị đã bỏ thời gian đọc bài viết của em.",,,,,
"mọi người ơi , em có 1 câu hỏi về việc train với tripeloss khi có nhiều đối tượng . như mọi người đã biết thì việc train này sẽ cố làm cho neg xa ancho, và cố gắng làm gần ancho vs pos. Em sẽ đặt ancho là A, pos là A1, neg là B, một đối tượng khác neg vs ancho là C. Câu hỏi của em là khi train tripeloss thì A sẽ càng gần A1, A sẽ càng xa B, nhưng nếu như việc chỉnh sửa A sao cho xa B lại vô tình khiến cho A gần C hơn thì s ạ. Ai có kinh nghiệm giải thích cái này giúp em với, em cảm ơn ạ","mọi người ơi , em có 1 câu hỏi về việc train với tripeloss khi có nhiều đối tượng . như mọi người đã biết thì việc train này sẽ cố làm cho neg xa ancho, và cố gắng làm gần ancho vs pos. Em sẽ đặt ancho là A, pos là A1, neg là B, một đối tượng khác neg vs ancho là C. Câu hỏi của em là khi train tripeloss thì A sẽ càng gần A1, A sẽ càng xa B, nhưng nếu như việc chỉnh sửa A sao cho xa B lại vô tình khiến cho A gần C hơn thì s ạ. Ai có kinh nghiệm giải thích cái này giúp em với, em cảm ơn ạ",,,,,
"Mình thấy có dự án cộng đồng với mục đích thu thập dữ liệu để cho ra mắt chat bot cạnh tranh với ChatGPT và các chủ đề generative khác như text-to-image, image-to-text,... Dự án này được dẫn dắt bởi LAION-AI https://laion.ai/. Hi vọng Việt Nam có nhiều đóng góp vào dự án mở này, dự án tại đây https://github.com/LAION-AI/Open-Assistant.","Mình thấy có dự án cộng đồng với mục đích thu thập dữ liệu để cho ra mắt chat bot cạnh tranh với ChatGPT và các chủ đề generative khác như text-to-image, image-to-text,... Dự án này được dẫn dắt bởi LAION-AI https://laion.ai/. Hi vọng Việt Nam có nhiều đóng góp vào dự án mở này, dự án tại đây https://github.com/LAION-AI/Open-Assistant.",,,,,
"MÔ HÌNH NGÔN NGỮ THUẦN ÂM TIẾT TIẾNG VIỆT
Mọi người nghĩ sao về thử nghiệm làm mô hình ngôn ngữ cho tập dữ liệu lấy âm tiết tiếng Việt làm trung tâm!
Lấy cảm hứng từ BabyLM chanllenge, và cramming paper (huấn luyện LM trên 1 GPU trong 24h), cổ vũ việc scaling down LM để nhiều người có thể tham gia và có những khám phá mới mẻ trong lĩnh vực đang rất hot này.
TẠI SAO LẠI THUẦN ÂM TIẾT?
1/ Vì nó chiếm ~80% text corpus tiếng Việt. Tập trung vào âm tiết sẽ làm nổi bật các đặc trưng của ngôn ngữ.
2/ Có nhiều sáng tạo hơn trong việc phân tích và phân tách dữ liệu (ví dụ xây dựng bộ vocab, custom tokenizer ...)
3/ Nhanh đánh giá độ hiệu quả của các mô hình. Người ta hay dùng character LM để thử nghiệm và so sánh độ hiệu quả của mô hình. Tập trung vào âm tiết cũng gần giống như character LM vậy.
Mình nghĩ thử nghiệm này đủ dễ để bất kỳ ai cũng tham gia được và ai cũng có thể có đóng góp / sáng tạo của riêng mình. Và trên hết nó sẽ vui và kéo cộng đồng lại gần nhau hơn.","MÔ HÌNH NGÔN NGỮ THUẦN ÂM TIẾT TIẾNG VIỆT Mọi người nghĩ sao về thử nghiệm làm mô hình ngôn ngữ cho tập dữ liệu lấy âm tiết tiếng Việt làm trung tâm! Lấy cảm hứng từ BabyLM chanllenge, và cramming paper (huấn luyện LM trên 1 GPU trong 24h), cổ vũ việc scaling down LM để nhiều người có thể tham gia và có những khám phá mới mẻ trong lĩnh vực đang rất hot này. TẠI SAO LẠI THUẦN ÂM TIẾT? 1/ Vì nó chiếm ~80% text corpus tiếng Việt. Tập trung vào âm tiết sẽ làm nổi bật các đặc trưng của ngôn ngữ. 2/ Có nhiều sáng tạo hơn trong việc phân tích và phân tách dữ liệu (ví dụ xây dựng bộ vocab, custom tokenizer ...) 3/ Nhanh đánh giá độ hiệu quả của các mô hình. Người ta hay dùng character LM để thử nghiệm và so sánh độ hiệu quả của mô hình. Tập trung vào âm tiết cũng gần giống như character LM vậy. Mình nghĩ thử nghiệm này đủ dễ để bất kỳ ai cũng tham gia được và ai cũng có thể có đóng góp / sáng tạo của riêng mình. Và trên hết nó sẽ vui và kéo cộng đồng lại gần nhau hơn.",,,,,
"Chào mọi người, mình muốn làm bài toán về OCR cho tiếng việt. Mình google thì không thấy có chỗ tải dữ liệu huấn luyện cho tiếng việt. Vậy có ai cho mình xin bộ dữ liệu huấn luyện được không ạ?","Chào mọi người, mình muốn làm bài toán về OCR cho tiếng việt. Mình google thì không thấy có chỗ tải dữ liệu huấn luyện cho tiếng việt. Vậy có ai cho mình xin bộ dữ liệu huấn luyện được không ạ?",,,,,
"[ChatGPT Cheat Sheet]
Bạn đã từng nghe về ChatGPT, nhưng chưa biết ChatGPT có thể làm những gì?
Post này sẽ giúp bạn tổng hợp những khả năng của ChatGPT, để từ đó bạn có thể ứng dụng để tăng hiệu suất công việc của mình. Một số ứng dụng có thể kể đến như:
- Tự động viết code, giải thích code, sinh document cho code
- Tự động vết bài ielts writing, trả lời ielts speaking
- Tự động viết CV dựa theo thông tin bạn cung cấp
- Tự động viết email, blog, essay,...
- Hỏi đáp thay Google :))","[ChatGPT Cheat Sheet] Bạn đã từng nghe về ChatGPT, nhưng chưa biết ChatGPT có thể làm những gì? Post này sẽ giúp bạn tổng hợp những khả năng của ChatGPT, để từ đó bạn có thể ứng dụng để tăng hiệu suất công việc của mình. Một số ứng dụng có thể kể đến như: - Tự động viết code, giải thích code, sinh document cho code - Tự động vết bài ielts writing, trả lời ielts speaking - Tự động viết CV dựa theo thông tin bạn cung cấp - Tự động viết email, blog, essay,... - Hỏi đáp thay Google :))",,,,,
"Chào mọi người ạ, em hiện đang một thắc mắc về cách design một mạng neural.
Hiện em đang thực hành xây dựng các model đơn giản để làm quen (stock prediction, heart disease prediction...), tuy nhiên em đến lúc build model, em thường chỉ xem theo hướng dẫn để build các layer. Em chưa hiểu tại sao có khi người ta dùng tới 3 lớp Conv2D, có khi lại ít hơn (hoặc nhiều hơn), hoặc maxpooling thì dùng như thế nào để hiệu quả (mất ít thông tin nhất) cũng như lựa chọn số chiều đầu ra cho mỗi layer như thế nào. Em có tìm hiểu thì chỉ bắt gặp vài câu trả lời đại khái như ""tùy thuộc vào data"". Nhưng mà cụ thể là tùy thuộc như thế nào thì em chưa rõ và em chưa tìm được nguồn giải thích kèm theo ví dụ cụ thể. Vậy làm sao để tự design cũng như đánh giá và đề xuất hướng cải tiến neural network của riêng mình?
Em mong mọi người giúp em, em cảm ơn mọi người rất nhiều ạ, chúc mọi người cuối tuần vui vẻ!","Chào mọi người ạ, em hiện đang một thắc mắc về cách design một mạng neural. Hiện em đang thực hành xây dựng các model đơn giản để làm quen (stock prediction, heart disease prediction...), tuy nhiên em đến lúc build model, em thường chỉ xem theo hướng dẫn để build các layer. Em chưa hiểu tại sao có khi người ta dùng tới 3 lớp Conv2D, có khi lại ít hơn (hoặc nhiều hơn), hoặc maxpooling thì dùng như thế nào để hiệu quả (mất ít thông tin nhất) cũng như lựa chọn số chiều đầu ra cho mỗi layer như thế nào. Em có tìm hiểu thì chỉ bắt gặp vài câu trả lời đại khái như ""tùy thuộc vào data"". Nhưng mà cụ thể là tùy thuộc như thế nào thì em chưa rõ và em chưa tìm được nguồn giải thích kèm theo ví dụ cụ thể. Vậy làm sao để tự design cũng như đánh giá và đề xuất hướng cải tiến neural network của riêng mình? Em mong mọi người giúp em, em cảm ơn mọi người rất nhiều ạ, chúc mọi người cuối tuần vui vẻ!",,,,,
Cho mình hỏi những bạn đã có kinh nghiệm về việc đăng ký khoá học Deep Learning Specialization của Coursera. Nếu mình đăng ký từng khoá học riêng lẻ thì khi khi hoàn tất khoá cuối cùng mình có được Deep Learning Certificate không? Hay mình phải đăng ký trọn bộ từ đầu thì mới được chứng chỉ đó? Thanks in advance,Cho mình hỏi những bạn đã có kinh nghiệm về việc đăng ký khoá học Deep Learning Specialization của Coursera. Nếu mình đăng ký từng khoá học riêng lẻ thì khi khi hoàn tất khoá cuối cùng mình có được Deep Learning Certificate không? Hay mình phải đăng ký trọn bộ từ đầu thì mới được chứng chỉ đó? Thanks in advance,,,,,
,nan,,,,,
Tổng hợp các nguồn cung cấp data miễn phí. Các bạn nào đang làm dự án mà không biết lấy data ở đâu thì lưu lại ngay nhé,Tổng hợp các nguồn cung cấp data miễn phí. Các bạn nào đang làm dự án mà không biết lấy data ở đâu thì lưu lại ngay nhé,,,,,
Google vừa ra mắt Bard để cạnh tranh trực tiếp với ChatGPT và sẽ sớm đến tay người dùng.,Google vừa ra mắt Bard để cạnh tranh trực tiếp với ChatGPT và sẽ sớm đến tay người dùng.,,,,,
"[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models]
Nếu như mô hình ChatGPT chỉ đơn thuần là giao tiếp dạng ngôn ngữ, thì gần đây mô hình BLIP-2 kết hợp giữa nội dung bức ảnh và ngôn ngữ đối thoại. Cụ thể, bạn có thể input một bức ảnh, và các câu hỏi, mô hình BLIP-2 sẽ hiểu được nội dung bức ảnh, và có thể lấy thêm các thông tin như trong các mô hình ngôn ngữ để trả lời.
Ví dụ: như input ảnh xe audi, bạn hỏi những đặc điểm nổi bật của xe trong bức ảnh, máy có thể kể ra những đặc điểm nổi bật của xe audi như khả năng tăng tốc nhanh, xe lai điện,....","[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models] Nếu như mô hình ChatGPT chỉ đơn thuần là giao tiếp dạng ngôn ngữ, thì gần đây mô hình BLIP-2 kết hợp giữa nội dung bức ảnh và ngôn ngữ đối thoại. Cụ thể, bạn có thể input một bức ảnh, và các câu hỏi, mô hình BLIP-2 sẽ hiểu được nội dung bức ảnh, và có thể lấy thêm các thông tin như trong các mô hình ngôn ngữ để trả lời. Ví dụ: như input ảnh xe audi, bạn hỏi những đặc điểm nổi bật của xe trong bức ảnh, máy có thể kể ra những đặc điểm nổi bật của xe audi như khả năng tăng tốc nhanh, xe lai điện,....",,,,,
"Mọi người cho em hỏi là có trường hợp nào do dùng khác version của library mà số liệu tính toán nó ra khác nhau không ạ. Tình trạng là em có lấy code của bài toán dự đoán ""doanh số bán hàng thông qua hình ảnh sản phẩm"" của trên mạng về chạy, sử dụng các library torch, pytorch_lightning,... để tính toán MAR, WAPE. Tuy nhiên kết quả em chạy ra so với paper của họ lại hơi khác xíu (ví dụ 97 và 96.2). Như vậy là lỗi do em sai hay là do version vậy ạ?","Mọi người cho em hỏi là có trường hợp nào do dùng khác version của library mà số liệu tính toán nó ra khác nhau không ạ. Tình trạng là em có lấy code của bài toán dự đoán ""doanh số bán hàng thông qua hình ảnh sản phẩm"" của trên mạng về chạy, sử dụng các library torch, pytorch_lightning,... để tính toán MAR, WAPE. Tuy nhiên kết quả em chạy ra so với paper của họ lại hơi khác xíu (ví dụ 97 và 96.2). Như vậy là lỗi do em sai hay là do version vậy ạ?",,,,,
"PhoNLP: A joint multi-task learning toolkit for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing
https://github.com/VinAIResearch/PhoNLP by Linh The Nguyen & Dat Quoc Nguyen
We present the first multi-task learning model---named PhoNLP---for joint Vietnamese part-of-speech tagging, named entity recognition and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT for each task independently.  We publicly release PhoNLP as an open-source toolkit under the MIT License. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future research and applications in  Vietnamese NLP.
PhoNLP paper will be released on ArXiv very very soon. ","PhoNLP: A joint multi-task learning toolkit for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing https://github.com/VinAIResearch/PhoNLP by Linh The Nguyen & Dat Quoc Nguyen We present the first multi-task learning model---named PhoNLP---for joint Vietnamese part-of-speech tagging, named entity recognition and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT for each task independently. We publicly release PhoNLP as an open-source toolkit under the MIT License. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future research and applications in Vietnamese NLP. PhoNLP paper will be released on ArXiv very very soon.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 01/2023 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 01/2023 vào comment của post này.",,,,,
"Mn cho em hỏi làm thế nào mà công thức 1 vế trái có transpose, công thức 2 vế trái k có transpose mà vế phải cả 2 công thức là như nhau ạ","Mn cho em hỏi làm thế nào mà công thức 1 vế trái có transpose, công thức 2 vế trái k có transpose mà vế phải cả 2 công thức là như nhau ạ",,,,,
"[DetectGPT: vỏ quýt dày có móng tay nhọn]
Sau khi các sinh viên đại học có ý định dùng ChatGPT để viết assignment, essay thì gần đây nhóm nghiên cứu tới từ đại học Stanford đã cho ra mắt mô hình DetectGPT để xác định xem văn bản được sinh bởi ChatGPT hay viết bởi con người.","[DetectGPT: vỏ quýt dày có móng tay nhọn] Sau khi các sinh viên đại học có ý định dùng ChatGPT để viết assignment, essay thì gần đây nhóm nghiên cứu tới từ đại học Stanford đã cho ra mắt mô hình DetectGPT để xác định xem văn bản được sinh bởi ChatGPT hay viết bởi con người.",,,,,
"Kính chào các bác!
Đợt này nhiều bạn hỏi trên  Group  về việc làm sao để train YOLOv8. Hôm nay em xin mạnh dạn chia sẻ bài này hi vọng giúp được các bạn mới học thôi ạ!
Chúc các bạn thành công!",Kính chào các bác! Đợt này nhiều bạn hỏi trên Group về việc làm sao để train YOLOv8. Hôm nay em xin mạnh dạn chia sẻ bài này hi vọng giúp được các bạn mới học thôi ạ! Chúc các bạn thành công!,,,,,
"Đầu xuân năm mới gửi đến mọi người bài Blog mình mới viết về Conformer: Convolution Augmented Transformer.
Trong đó mình có trình bày chi tiết về Conformer, cách áp dụng nó trong ASR và TTS. Cũng như nêu ra một số câu hỏi đáng chú ý về việc: Có cần thiết áp dụng Macron-style và Relative Position Encoding cho Conformer trong các bài toán xử lý tiếng nói hay không?
https://www.aiourlife.com/2023/01/convolution-augmented-transformer.html","Đầu xuân năm mới gửi đến mọi người bài Blog mình mới viết về Conformer: Convolution Augmented Transformer. Trong đó mình có trình bày chi tiết về Conformer, cách áp dụng nó trong ASR và TTS. Cũng như nêu ra một số câu hỏi đáng chú ý về việc: Có cần thiết áp dụng Macron-style và Relative Position Encoding cho Conformer trong các bài toán xử lý tiếng nói hay không? https://www.aiourlife.com/2023/01/convolution-augmented-transformer.html",,,,,
"A.C.E Nào đã từng dùng MMdetection này trên GG colboratory chưa ạ. Cho e xin chỉ giáo với ạ. Nó yêu cầu phải có GPU mà máy e mỗi con AMD ghẻ k chạy được tìm cách đẩy lên GG thì chưa thấy có ai làm mấy. 🙁 doc này viết hơi khó chịu ạ (Của China ). A/C đã dùng thì cho e xin hỏi đôi chút ❤
#Github: https://github.com/open-mmlab/mmdetection
#Docs : https://mmdetection.readthedocs.io/en/latest/",A.C.E Nào đã từng dùng MMdetection này trên GG colboratory chưa ạ. Cho e xin chỉ giáo với ạ. Nó yêu cầu phải có GPU mà máy e mỗi con AMD ghẻ k chạy được tìm cách đẩy lên GG thì chưa thấy có ai làm mấy. doc này viết hơi khó chịu ạ (Của China ). A/C đã dùng thì cho e xin hỏi đôi chút https://github.com/open-mmlab/mmdetection : https://mmdetection.readthedocs.io/en/latest/,#Github:	#Docs,,,,
"Dạ chào mọi người, lại là em đây ạ.
Hôm nay em đem đến 1 vấn đề như sau: Gần đây thì có rất nhiều paper ứng dụng các method Transformer-based cho time series data. Tuy nhiên sau khi tiếp thụ tri thức từ 1 vài nguồn thì em nhận được 1 số câu hỏi như này:
1. Khác với ngôn ngữ, dữ liệu dạng time series có sự liên hệ ở thời gian t với thời gian t-1. Nên liệu việc ta sử dụng self-attention cho dữ liệu dạng này có ổn không?
2. Liệu transformer-based có là 1 lựa chọn tốt cho các dữ liệu khác như dữ liệu tài chính, nhiệt độ, ...
Và từ 2 câu hỏi trên thì em có tham khảo 1 paper (https://arxiv.org/abs/2205.13504) nhận được câu trả lời cho 2 câu hỏi trên.
Vậy liệu self-attention có thực sự làm mất đi ít nhiều thông tin từ sự tương quan của data không? Nếu transformer có kết quả thấp hơn các model linear như trong paper đã trình bày thì có những hướng tiếp cận nào tốt hơn cho dạng data này không?
Em xin cảm ơn","Dạ chào mọi người, lại là em đây ạ. Hôm nay em đem đến 1 vấn đề như sau: Gần đây thì có rất nhiều paper ứng dụng các method Transformer-based cho time series data. Tuy nhiên sau khi tiếp thụ tri thức từ 1 vài nguồn thì em nhận được 1 số câu hỏi như này: 1. Khác với ngôn ngữ, dữ liệu dạng time series có sự liên hệ ở thời gian t với thời gian t-1. Nên liệu việc ta sử dụng self-attention cho dữ liệu dạng này có ổn không? 2. Liệu transformer-based có là 1 lựa chọn tốt cho các dữ liệu khác như dữ liệu tài chính, nhiệt độ, ... Và từ 2 câu hỏi trên thì em có tham khảo 1 paper (https://arxiv.org/abs/2205.13504) nhận được câu trả lời cho 2 câu hỏi trên. Vậy liệu self-attention có thực sự làm mất đi ít nhiều thông tin từ sự tương quan của data không? Nếu transformer có kết quả thấp hơn các model linear như trong paper đã trình bày thì có những hướng tiếp cận nào tốt hơn cho dạng data này không? Em xin cảm ơn",,,,,
"CodeGPT - VSCode SẼ MẠNH MẼ hơn với tính năng tương tự ChatGPT
Mình xin chia sẻ với mọi người full hướng dẫn cài đặt, setting và sử dụng Code GPT extension trong VSCode. Code GPT là một công cụ tương tự như GitHub Copilot. Bây giờ mọi người có thể thao tác trực tiếp ngay trong VSCode và có thêm một công cụ hỗ trợ đắc lực hơn nữa trong việc coding.
Cảm ơn admin đã duyệt bài.
https://youtu.be/kwXn0s31fpE","CodeGPT - VSCode SẼ MẠNH MẼ hơn với tính năng tương tự ChatGPT Mình xin chia sẻ với mọi người full hướng dẫn cài đặt, setting và sử dụng Code GPT extension trong VSCode. Code GPT là một công cụ tương tự như GitHub Copilot. Bây giờ mọi người có thể thao tác trực tiếp ngay trong VSCode và có thêm một công cụ hỗ trợ đắc lực hơn nữa trong việc coding. Cảm ơn admin đã duyệt bài. https://youtu.be/kwXn0s31fpE",,,,,
"anh chị giúp đỡ em với ạ
em chưa hiểu khúc X[label == 0, :] hoạt động sao ạ
Anh chị giúp em hiểu rõ khúc đó đc ko ã
Em newbie nên mong mn chỉ giáo ạ :(","anh chị giúp đỡ em với ạ em chưa hiểu khúc X[label == 0, :] hoạt động sao ạ Anh chị giúp em hiểu rõ khúc đó đc ko ã Em newbie nên mong mn chỉ giáo ạ :(",,,,,
"Chào mọi người, trước hết chúc mọi người có những ngày tết vui vẻ.
Hiện tại mình đang học khóa Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization tuần 1 ( Khóa 2 của DLS). Mình có chỗ không hiểu như sau:
- L2 regularization:
1. Về phương pháp L2 regularization việc tăng lambda khi cập nhật W giống như tăng alpha khi update W (W = W - alpha * dW).
2. Như slide How does regularization prevent overfitting bên dưới thì đoạn màu đỏ đó là việc regularization nó trượt xuống khi hàm tối ưu từ 1 về 0 nhưng mình nghe khi lambda cao quá nó sẽ làm g(z) gần như về tuyến tính. Mình không hiểu lắm, mọi người có thể giải thích rõ hơn hoặc đưa ra ví dụ được không ạ.
- Dropout:
1. Nó sẽ tắt ngẫu nhiên từng nhân và sử dụng ""Inverted dropout"" để giữ cho layer sau hoạt động một cách bình thường. Tuy nhiên, ở clip Understanding dropout việc tắt ngẫu nhiên nhằm: ""Can't rely on any one feature, so have to spread out weights."". Theo mình hiểu là nó sẽ tắt một số nhân khi forward propagation và việc update (back propagation) là nó sẽ vẫn bình thường đúng không ạ?
2. Mình vẫn mơ hồ về việc tắt 1 số nhân giống như L2 regularization. Mình mong nhận được ví dụ hay giải thích rõ ràng hơn ạ.
Cảm ơn mọi người đã quan tâm.","Chào mọi người, trước hết chúc mọi người có những ngày tết vui vẻ. Hiện tại mình đang học khóa Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization tuần 1 ( Khóa 2 của DLS). Mình có chỗ không hiểu như sau: - L2 regularization: 1. Về phương pháp L2 regularization việc tăng lambda khi cập nhật W giống như tăng alpha khi update W (W = W - alpha * dW). 2. Như slide How does regularization prevent overfitting bên dưới thì đoạn màu đỏ đó là việc regularization nó trượt xuống khi hàm tối ưu từ 1 về 0 nhưng mình nghe khi lambda cao quá nó sẽ làm g(z) gần như về tuyến tính. Mình không hiểu lắm, mọi người có thể giải thích rõ hơn hoặc đưa ra ví dụ được không ạ. - Dropout: 1. Nó sẽ tắt ngẫu nhiên từng nhân và sử dụng ""Inverted dropout"" để giữ cho layer sau hoạt động một cách bình thường. Tuy nhiên, ở clip Understanding dropout việc tắt ngẫu nhiên nhằm: ""Can't rely on any one feature, so have to spread out weights."". Theo mình hiểu là nó sẽ tắt một số nhân khi forward propagation và việc update (back propagation) là nó sẽ vẫn bình thường đúng không ạ? 2. Mình vẫn mơ hồ về việc tắt 1 số nhân giống như L2 regularization. Mình mong nhận được ví dụ hay giải thích rõ ràng hơn ạ. Cảm ơn mọi người đã quan tâm.",,,,,
"Dạ chào các anh chị, hiện em có nhận được thông tin cuộc thi AI City challenge 2023 đã công bố các tracks và em cũng đang muốn tham gia cuộc thi để học hỏi thêm kinh nghiệm.
Em viết bài viết này để tìm kiếm mentor hoặc may mắn hơn là tìm kiếm 1 team tham gia cuộc thi. Hiện em đang có mong muốn tham gia track 2 nhưng nếu được thì em vẫn mong muốn có thể học ở nhưng track khác nếu được.
Em xin cảm ơn","Dạ chào các anh chị, hiện em có nhận được thông tin cuộc thi AI City challenge 2023 đã công bố các tracks và em cũng đang muốn tham gia cuộc thi để học hỏi thêm kinh nghiệm. Em viết bài viết này để tìm kiếm mentor hoặc may mắn hơn là tìm kiếm 1 team tham gia cuộc thi. Hiện em đang có mong muốn tham gia track 2 nhưng nếu được thì em vẫn mong muốn có thể học ở nhưng track khác nếu được. Em xin cảm ơn",,,,,
"Xin giới thiệu với mọi người một ấn phẩm mới và đang dần hoàn thiện của Dr. Simon J.D. Prince, Senior Lecturer của University College London. Tác giả này từng cho ra một cuốn sách được đánh giá rất rất cao trong lĩnh vực Computer Vision là ""Computer Vision: Models, Learning, and Inference.""
Ấn phẩm dự kiến xuất bản năm 2024, bản phát thảo được published hôm 24/01 và cover cả những chủ đề rất nóng như Diffusion models. Dr. Prince đang kêu gọi mọi người đề xuất và chỉnh lý nếu có sai sót.
Link:","Xin giới thiệu với mọi người một ấn phẩm mới và đang dần hoàn thiện của Dr. Simon J.D. Prince, Senior Lecturer của University College London. Tác giả này từng cho ra một cuốn sách được đánh giá rất rất cao trong lĩnh vực Computer Vision là ""Computer Vision: Models, Learning, and Inference."" Ấn phẩm dự kiến xuất bản năm 2024, bản phát thảo được published hôm 24/01 và cover cả những chủ đề rất nóng như Diffusion models. Dr. Prince đang kêu gọi mọi người đề xuất và chỉnh lý nếu có sai sót. Link:",,,,,
"Học bổng hội thảo Wikimania 2023 vào 16–19 August 2023 tại Singapore.
Nếu ai có đóng góp hoặc nghiên cứu về Wikipedia và các dự án chị em liên quan như Wikidata, Wiktionary,... thì có thể xem thông tin để đăng ký!
https://wikimania.wikimedia.org/wiki/2023:Scholarships
https://wikimania.wikimedia.org/wiki/2023:Scholarships/Travel_Scholarship_application","Học bổng hội thảo Wikimania 2023 vào 16–19 August 2023 tại Singapore. Nếu ai có đóng góp hoặc nghiên cứu về Wikipedia và các dự án chị em liên quan như Wikidata, Wiktionary,... thì có thể xem thông tin để đăng ký! https://wikimania.wikimedia.org/wiki/2023:Scholarships https://wikimania.wikimedia.org/wiki/2023:Scholarships/Travel_Scholarship_application",,,,,
Distributed Training cho huấn luyện mô hình học sâu,Distributed Training cho huấn luyện mô hình học sâu,,,,,
"Hi mọi người,
Em đang làm một dự án NLP về tóm tắt. Hiện tại em gặp vấn đề khi tóm tắt một lượng text dài thì cost chạy model nhiều. Có cách nào để em có thể giữ được context không ạ? Em là newbie trong mảng này nên mọi người biết keyword, bài báo gì em nên cần học thì chỉ em với. Em cảm ơn mọi người nhiều ^^","Hi mọi người, Em đang làm một dự án NLP về tóm tắt. Hiện tại em gặp vấn đề khi tóm tắt một lượng text dài thì cost chạy model nhiều. Có cách nào để em có thể giữ được context không ạ? Em là newbie trong mảng này nên mọi người biết keyword, bài báo gì em nên cần học thì chỉ em với. Em cảm ơn mọi người nhiều ^^",,,,,
"Chúc mọi người một năm mới mạnh khỏe. Chúc cho AI Việt Nam sẽ có nhiều bước tiến mới, khai phá những vùng đất mới để anh em có nhiều việc làm. Chúc diễn đàn ngày càng có nhiều câu hỏi và post chia sẻ hay.","Chúc mọi người một năm mới mạnh khỏe. Chúc cho AI Việt Nam sẽ có nhiều bước tiến mới, khai phá những vùng đất mới để anh em có nhiều việc làm. Chúc diễn đàn ngày càng có nhiều câu hỏi và post chia sẻ hay.",,,,,
"Dạ chào mọi người, hôm nay em có câu hỏi như sau:
Trong paper về Batch Norm thì tác giả có nhắc đến định nghĩa Internal Covariate Shift. Theo cá nhân em hiểu thì đây là sự thay đổi về phân phối của dữ liệu sau khi đi qua các layer của mạng. Tuy nhiên có một lập luận là do BN sử dụng 2 hyperparameters là γ và β nên 2 tham số này cũng góp phần tạo ra Internal Covariate Shift chứ không giải quyết ICS.
Còn lý do mà BN có thể làm tăng tốc training vì optimizer chỉ cần quản lý phân phối giữa các layer bằng 2 tham số trên.
Anh/chị có thể giải đáp cho em về vấn đề này không ạ?
https://blog.paperspace.com/busting-the-myths-about-batch-normalization/","Dạ chào mọi người, hôm nay em có câu hỏi như sau: Trong paper về Batch Norm thì tác giả có nhắc đến định nghĩa Internal Covariate Shift. Theo cá nhân em hiểu thì đây là sự thay đổi về phân phối của dữ liệu sau khi đi qua các layer của mạng. Tuy nhiên có một lập luận là do BN sử dụng 2 hyperparameters là γ và β nên 2 tham số này cũng góp phần tạo ra Internal Covariate Shift chứ không giải quyết ICS. Còn lý do mà BN có thể làm tăng tốc training vì optimizer chỉ cần quản lý phân phối giữa các layer bằng 2 tham số trên. Anh/chị có thể giải đáp cho em về vấn đề này không ạ? https://blog.paperspace.com/busting-the-myths-about-batch-normalization/",,,,,
"💗💗💗🐱🐱🐱
Năm mới mèo chỗ mọi người như thế nào, còn đây là mèo AI của mình từ Midjouney nhìn nó lạ lắm.
Midjourney cũng tương tự như DALL-E 2 và Stable Diffusion giúp sinh ảnh từ text nhập vào. Có người đã dùng ảnh được tạo bằng Midjourney và chiến thắng trong một cuộc thi về ảnh. Mọi người cùng tìm hiểu về cách sử dụng Midjourney để sinh ảnh.
Cảm ơn admin đã duyệt bài.","Năm mới mèo chỗ mọi người như thế nào, còn đây là mèo AI của mình từ Midjouney nhìn nó lạ lắm. Midjourney cũng tương tự như DALL-E 2 và Stable Diffusion giúp sinh ảnh từ text nhập vào. Có người đã dùng ảnh được tạo bằng Midjourney và chiến thắng trong một cuộc thi về ảnh. Mọi người cùng tìm hiểu về cách sử dụng Midjourney để sinh ảnh. Cảm ơn admin đã duyệt bài.",,,,,
"Mình đang tìm hiểu về cơ chế Attention trong Deep Learning và đang có một thắc mắc là cơ chế embedding có phải là một bước của Attention không? Hay là 2 cơ chế này có liên quan gì đến nhau không ạ?
Theo mình hiểu thì
Embedding: giảm dimensions của input --> mục đích giảm nhẹ khối lượng input (?) - mình không chắc lắm
Attention: tập trung vào và biểu diễn chỉ những điểm quan trọng của input để đưa qua các layer tiếp theo (?) - Link tham khảo: https://viblo.asia/p/tim-hieu-ve-co-che-attention-924lJjbmlPM
Mong mọi người giúp mình giải đáp thắc mắc này với ạ. Mình cảm ơn.",Mình đang tìm hiểu về cơ chế Attention trong Deep Learning và đang có một thắc mắc là cơ chế embedding có phải là một bước của Attention không? Hay là 2 cơ chế này có liên quan gì đến nhau không ạ? Theo mình hiểu thì Embedding: giảm dimensions của input --> mục đích giảm nhẹ khối lượng input (?) - mình không chắc lắm Attention: tập trung vào và biểu diễn chỉ những điểm quan trọng của input để đưa qua các layer tiếp theo (?) - Link tham khảo: https://viblo.asia/p/tim-hieu-ve-co-che-attention-924lJjbmlPM Mong mọi người giúp mình giải đáp thắc mắc này với ạ. Mình cảm ơn.,,,,,
"Let's build GPT: from scratch, in code, spelled out.
Author:Andrej Karpathy - Ex Director of Tesla AI
We build a Generatively Pretrained Transformer (GPT), following the paper ""Attention is All You Need"" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.
https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathyLinks:
- Google colab for the video: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing
- GitHub repo for the video: https://github.com/karpathy/ng-video-lecture
#deeplearning #chatgpt ","Let's build GPT: from scratch, in code, spelled out. Author:Andrej Karpathy - Ex Director of Tesla AI We build a Generatively Pretrained Transformer (GPT), following the paper ""Attention is All You Need"" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video. https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathyLinks: - Google colab for the video: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing - GitHub repo for the video: https://github.com/karpathy/ng-video-lecture",#deeplearning	#chatgpt,,,,
"Câu hỏi về RNN và BPTT - Backpropagation through time.
Cả nhà cho em hỏi tại sao trong RNN đạo hàm của Loss Function so với so với các nodes ngay phía trước lại bằng 1 ạ?
Em cám ơn ạ",Câu hỏi về RNN và BPTT - Backpropagation through time. Cả nhà cho em hỏi tại sao trong RNN đạo hàm của Loss Function so với so với các nodes ngay phía trước lại bằng 1 ạ? Em cám ơn ạ,,,,,
"Mình đang đọc paper về sales forecasting cho sản phẩm mới thông qua hình ảnh + text data + temporal data + ggtrends.
Mọi người cho mình hỏi đoạn highlight này ý muốn nói dự đoán doanh số của sản phẩm mới chỉ bằng cách sử dụng image của sp mới này và image của sp trước đó, kết hợp với temporal data + ggtrends hiện tại thôi đúng không ạ? Không sử dụng các features khác của những product trước (ngoại trừ image).
Link paper: https://arxiv.org/pdf/2204.06972v2.pdf","Mình đang đọc paper về sales forecasting cho sản phẩm mới thông qua hình ảnh + text data + temporal data + ggtrends. Mọi người cho mình hỏi đoạn highlight này ý muốn nói dự đoán doanh số của sản phẩm mới chỉ bằng cách sử dụng image của sp mới này và image của sp trước đó, kết hợp với temporal data + ggtrends hiện tại thôi đúng không ạ? Không sử dụng các features khác của những product trước (ngoại trừ image). Link paper: https://arxiv.org/pdf/2204.06972v2.pdf",,,,,
"chào mn ạ
Hiện tại trong nhóm có ai còn có sách này bản mềm k dùng có thế pass lại cho e đc không ạ
Thanks mn !!!!",chào mn ạ Hiện tại trong nhóm có ai còn có sách này bản mềm k dùng có thế pass lại cho e đc không ạ Thanks mn !!!!,,,,,
"Hello mọi người, em đang bị lỗi như này không thể active nvpmodel.service được, vào thư mục thì cũng không có những file trên. Các cao nhân có cao kiến gì giúp em ko ạ?","Hello mọi người, em đang bị lỗi như này không thể active nvpmodel.service được, vào thư mục thì cũng không có những file trên. Các cao nhân có cao kiến gì giúp em ko ạ?",,,,,
"YOLOv8 demo
Chia sẻ với anh em video đầu tiên trong tutorials hướng dẫn Yolov8. Đây là kết quả demo cho bài toán object detection và instance segmentation với video đường phố Hà Nội cuối năm.
Cảm ơn admin đã duyệt bài.
https://youtu.be/ZyKK4o4HaAM",YOLOv8 demo Chia sẻ với anh em video đầu tiên trong tutorials hướng dẫn Yolov8. Đây là kết quả demo cho bài toán object detection và instance segmentation với video đường phố Hà Nội cuối năm. Cảm ơn admin đã duyệt bài. https://youtu.be/ZyKK4o4HaAM,,,,,
"Kính chào các bác!
Nhiều bạn hỏi về việc làm sao để chọn các siêu tham số như: số lớp ẩn, số unit trong lớp ẩn, learning rate... cho tối ưu. Hôm nay mình chia sẻ bài này hi vọng giúp được các bạn nhé!
Chúc các bạn thành công!
Ps: Bài này em chia sẻ dựa trên hiểu biết cá nhân và hi vọng giúp được các bạn newbie mới học ạ!","Kính chào các bác! Nhiều bạn hỏi về việc làm sao để chọn các siêu tham số như: số lớp ẩn, số unit trong lớp ẩn, learning rate... cho tối ưu. Hôm nay mình chia sẻ bài này hi vọng giúp được các bạn nhé! Chúc các bạn thành công! Ps: Bài này em chia sẻ dựa trên hiểu biết cá nhân và hi vọng giúp được các bạn newbie mới học ạ!",,,,,
,nan,,,,,
Sinh viên viết ứng dụng GPTZero chống đạo văn AI thành công,Sinh viên viết ứng dụng GPTZero chống đạo văn AI thành công,,,,,
"Em chào mọi người ạ. Hiện tại em đang tìm hiểu về applications của thuật toán k-means, em có code theo code mẫu của anh Tiệp thì bị lỗi ở phần gạch màu vàng ạ (hình 1) thì được báo lỗi (hình 2)
Em đã download database của MNIST về máy và đã giải nén các file cần thiết nhưng lại bị báo lỗi không tìm thấy file ạ. Mọi người nếu đã từng bị lỗi này có thể cho em xin hướng giải quyết được không ạ?","Em chào mọi người ạ. Hiện tại em đang tìm hiểu về applications của thuật toán k-means, em có code theo code mẫu của anh Tiệp thì bị lỗi ở phần gạch màu vàng ạ (hình 1) thì được báo lỗi (hình 2) Em đã download database của MNIST về máy và đã giải nén các file cần thiết nhưng lại bị báo lỗi không tìm thấy file ạ. Mọi người nếu đã từng bị lỗi này có thể cho em xin hướng giải quyết được không ạ?",,,,,
"Mn cho em hỏi data normalization( đưa về số nhỏ hơn) khác data transformation( đưa data về normal distribution) ở chỗ nào ạ vì em thấy normalization cũng có thể đưa data về dạng normal distrinution đc. Vậy thì data transformation có tác dụng gì khi normalization đã đưa data về normal distribution mà không làm mất đi ý nghĩa của data( khi transfrom thì data không còn ý nghĩa như ban đầu nữa, còn normaklization thì vẫn còn)? Em thấy 1 số bài machine learning nó vừa normalization vừa transformation ạ.","Mn cho em hỏi data normalization( đưa về số nhỏ hơn) khác data transformation( đưa data về normal distribution) ở chỗ nào ạ vì em thấy normalization cũng có thể đưa data về dạng normal distrinution đc. Vậy thì data transformation có tác dụng gì khi normalization đã đưa data về normal distribution mà không làm mất đi ý nghĩa của data( khi transfrom thì data không còn ý nghĩa như ban đầu nữa, còn normaklization thì vẫn còn)? Em thấy 1 số bài machine learning nó vừa normalization vừa transformation ạ.",,,,,
"Mọi người cho mình hỏi lỗi này có thể do đâu vậy nhỉ: 
-""failed to start local kernel module""

    2.- CUDA Error: no kernel image is available for execution on the device
polarisnnet: ./src/cuda.c:36: check_error: Assertion `0' failed.
/home/nvidia/test.sh: line 4: 10345 Aborted                 (core dumped) ./polarisnnet detector line data/test/t.data data/test/t.cfg data/test/t.weights data/test/t.mp4
Mình đang lắp đặt Nvidia AGX Xavier cài Jetpack 4.2, khi flash xong thì load screen hiện lỗi ""failed to start local kernel module"" và mất luôn hai cổng USB không thể sử dụng. Mình thử flash như vậy trên Nvidia TX2 thì vẫn báo lỗi như trên nhưng khi chạy thử file test thì hiện lỗi như mục (2).
Các bác có cao kiến gì chỉ mình không?","Mọi người cho mình hỏi lỗi này có thể do đâu vậy nhỉ: -""failed to start local kernel module"" 2.- CUDA Error: no kernel image is available for execution on the device polarisnnet: ./src/cuda.c:36: check_error: Assertion `0' failed. /home/nvidia/test.sh: line 4: 10345 Aborted (core dumped) ./polarisnnet detector line data/test/t.data data/test/t.cfg data/test/t.weights data/test/t.mp4 Mình đang lắp đặt Nvidia AGX Xavier cài Jetpack 4.2, khi flash xong thì load screen hiện lỗi ""failed to start local kernel module"" và mất luôn hai cổng USB không thể sử dụng. Mình thử flash như vậy trên Nvidia TX2 thì vẫn báo lỗi như trên nhưng khi chạy thử file test thì hiện lỗi như mục (2). Các bác có cao kiến gì chỉ mình không?",,,,,
"#jetsonnano
Chào mọi người. Hiện tại em đang học CV với jetson nano. Em có vấn đề là lúc mở cam bằng opencv thì nó báo lỗi như dưới ảnh. Em đã test thử theo hướng dẫn của bên bán hàng thì vẫn mở được cam, không có vấn đề gì.
Mong các bác chỉ giáo ạ.","Chào mọi người. Hiện tại em đang học CV với jetson nano. Em có vấn đề là lúc mở cam bằng opencv thì nó báo lỗi như dưới ảnh. Em đã test thử theo hướng dẫn của bên bán hàng thì vẫn mở được cam, không có vấn đề gì. Mong các bác chỉ giáo ạ.",#jetsonnano,,,,
"#python #Kmeans #MNIST
Em có đang làm phần nhận diện chữ số với data MNIST ạ. Mọi người cho em hỏi error này là sao ạ và fix thế nào ah. Em xin cảm ơn ạ",Em có đang làm phần nhận diện chữ số với data MNIST ạ. Mọi người cho em hỏi error này là sao ạ và fix thế nào ah. Em xin cảm ơn ạ,#python	#Kmeans	#MNIST,,,,
"Một số ghi chép về vài câu hỏi trong lý thuyết học máy hiện đại:
MSE hay CE cho bài toán phân lớp
Bias-Variance, Double descent - kim chỉ nam cho tư duy giải quyết vấn đề mới trong học máy
Tính chất hàm mục tiêu các mô hình overparameterized
Mô hình lớn bao nhiêu là đủ? Dữ liệu cần bao nhiêu?","Một số ghi chép về vài câu hỏi trong lý thuyết học máy hiện đại: MSE hay CE cho bài toán phân lớp Bias-Variance, Double descent - kim chỉ nam cho tư duy giải quyết vấn đề mới trong học máy Tính chất hàm mục tiêu các mô hình overparameterized Mô hình lớn bao nhiêu là đủ? Dữ liệu cần bao nhiêu?",,,,,
"Hi mọi người, em đang tìm hiểu về KNN và đang không hiểu là vote ở đây là tổng vote của 3 class 1 gần nhất hay sao ạ.
Mong mọi người giải đáp giúp em.
Em cảm ơn ạ.","Hi mọi người, em đang tìm hiểu về KNN và đang không hiểu là vote ở đây là tổng vote của 3 class 1 gần nhất hay sao ạ. Mong mọi người giải đáp giúp em. Em cảm ơn ạ.",,,,,
"Xin chào cả nhà,
Mình xin phép được chia sẻ lại khoá học MLOps Crash Course bằng Tiếng Việt (FREE) do MLOpsVN team tạo ra ở đây, khoá học hiện đã được gần 1000 người đăng ký và may mắn nhận được nhiều phản hồi tích cực.
Khoá học hướng dẫn mọi người step by step từ bước phân tích yêu cầu bài toán cho tới việc thực hiện POC, xây dựng feature store, data pipeline, model serving API cho tới viết các CI/CD pipeline để giảm thiểu tối đa các công việc thủ công.
Mọi người có thể đăng ký khoá học tại đây https://mlops.vn/#registration. Để lấy ACCESS CODE, phiền mọi người điển giúp bọn mình khảo sát nhỏ về mức độ quan tâm của mọi người đối cuộc thi MLOps Marathon sắp tới tại link này https://bit.ly/mlops-marathon-survey-2023.
Cảm ơn cả nhà nhiều!
#mlops","Xin chào cả nhà, Mình xin phép được chia sẻ lại khoá học MLOps Crash Course bằng Tiếng Việt (FREE) do MLOpsVN team tạo ra ở đây, khoá học hiện đã được gần 1000 người đăng ký và may mắn nhận được nhiều phản hồi tích cực. Khoá học hướng dẫn mọi người step by step từ bước phân tích yêu cầu bài toán cho tới việc thực hiện POC, xây dựng feature store, data pipeline, model serving API cho tới viết các CI/CD pipeline để giảm thiểu tối đa các công việc thủ công. Mọi người có thể đăng ký khoá học tại đây https://mlops.vn/#registration. Để lấy ACCESS CODE, phiền mọi người điển giúp bọn mình khảo sát nhỏ về mức độ quan tâm của mọi người đối cuộc thi MLOps Marathon sắp tới tại link này https://bit.ly/mlops-marathon-survey-2023. Cảm ơn cả nhà nhiều!",#mlops,,,,
YOLOv8 vừa ra lò nóng hổi cho anh em vừa thổi vừa ….,YOLOv8 vừa ra lò nóng hổi cho anh em vừa thổi vừa ….,,,,,
YOLOv8 vừa ra lò nóng hổi cho anh em vừa thổi vừa ….,YOLOv8 vừa ra lò nóng hổi cho anh em vừa thổi vừa ….,,,,,
"Chào mọi người,
Em đang viết model dectect biển số xe Việt Nam bằng haar cascade. Hiện tại e đang k kiếm đc file CascadeClassifier.txt cho biển VN. Trên gg hiện có cho biển của Ấn Độ với Nga. Size biển số khác nhau. Khi chạy file của Ấn cho khung bị nhỏ còn file của Nga thì khung lại to quá. Không biết có bác nào có file của VN với size thích hợp cho em xin với ạ.
Em cảm ơn","Chào mọi người, Em đang viết model dectect biển số xe Việt Nam bằng haar cascade. Hiện tại e đang k kiếm đc file CascadeClassifier.txt cho biển VN. Trên gg hiện có cho biển của Ấn Độ với Nga. Size biển số khác nhau. Khi chạy file của Ấn cho khung bị nhỏ còn file của Nga thì khung lại to quá. Không biết có bác nào có file của VN với size thích hợp cho em xin với ạ. Em cảm ơn",,,,,
"Chào mọi người,
Mình đang tìm hiểu về Hidden Markov Model. Cụ thể là ứng dụng nó trong những task cụ thể. Có bạn nào biết cuốn sách hoặc video courses nào nói về việc ứng dụng của HMM có thể giới thiệu mình không? Có thêm phần code/package nào hay thì càng tốt?
Cảm ơn mọi người","Chào mọi người, Mình đang tìm hiểu về Hidden Markov Model. Cụ thể là ứng dụng nó trong những task cụ thể. Có bạn nào biết cuốn sách hoặc video courses nào nói về việc ứng dụng của HMM có thể giới thiệu mình không? Có thêm phần code/package nào hay thì càng tốt? Cảm ơn mọi người",,,,,
"Em hiện tại đang tìm hiểu về deep learning trong việc xử lý hình ảnh (cụ thể là neural network). Mọi người cho em thắc mắc dòng code từ 82-85 trong hình bên dưới mang ý nghĩa gì vậy ạ?
Theo em hiểu thì lấy 5 layers cuối cùng trong cnn, trong mỗi layer này thì sẽ có p tham số (chưa biết tham số về cái gì?), và xét requires_grad là true cho từng tham số p.
Em hiểu như vậy có sai gì không mọi người? Với lại requires_grad là False thì mang đến tác dụng gì ạ, em vẫn chưa hiểu lắm. Mong mọi người hướng dẫn cho em thêm với ạ. Em cảm ơn.","Em hiện tại đang tìm hiểu về deep learning trong việc xử lý hình ảnh (cụ thể là neural network). Mọi người cho em thắc mắc dòng code từ 82-85 trong hình bên dưới mang ý nghĩa gì vậy ạ? Theo em hiểu thì lấy 5 layers cuối cùng trong cnn, trong mỗi layer này thì sẽ có p tham số (chưa biết tham số về cái gì?), và xét requires_grad là true cho từng tham số p. Em hiểu như vậy có sai gì không mọi người? Với lại requires_grad là False thì mang đến tác dụng gì ạ, em vẫn chưa hiểu lắm. Mong mọi người hướng dẫn cho em thêm với ạ. Em cảm ơn.",,,,,
"Chào anh chị trong nhóm. Hiện em đang làm bài toán trích xuất thông tin trên văn bản có cấu trúc nên em có 1 số thắc mắc như sau:
1. Nếu so sánh hiệu quả thì detector thì Segmentation-Based hay Regression-Based sẽ hiệu quả hơn (có kết hợp các bước tiền xứ lý). Yếu tố xem xét ở đây là cả về tài nguyên và thời gian ạ.
2. Việc sử dụng bộ detector train trên dữ liệu phi cấu trúc như DBNet++ cho dữ liệu có cấu trúc thì có ổn không (em có thử thì acc tầm > 90%).
3. Hiện tại em đang tìm hiểu về kiến trúc LayoutLMv3. Anh/ chị có thể cho em cái nhìn và một số đánh giá liên quan đến kiến trúc này không? Về tính hiệu quả, dễ hiểu, dễ tinh chỉnh.
Em xin cảm ơn","Chào anh chị trong nhóm. Hiện em đang làm bài toán trích xuất thông tin trên văn bản có cấu trúc nên em có 1 số thắc mắc như sau: 1. Nếu so sánh hiệu quả thì detector thì Segmentation-Based hay Regression-Based sẽ hiệu quả hơn (có kết hợp các bước tiền xứ lý). Yếu tố xem xét ở đây là cả về tài nguyên và thời gian ạ. 2. Việc sử dụng bộ detector train trên dữ liệu phi cấu trúc như DBNet++ cho dữ liệu có cấu trúc thì có ổn không (em có thử thì acc tầm > 90%). 3. Hiện tại em đang tìm hiểu về kiến trúc LayoutLMv3. Anh/ chị có thể cho em cái nhìn và một số đánh giá liên quan đến kiến trúc này không? Về tính hiệu quả, dễ hiểu, dễ tinh chỉnh. Em xin cảm ơn",,,,,
"Em chào mọi người ạ, em mới tìm hiểu về machine learning và đang đọc blog của thầy Tiệp. Có một chỗ em đọc qua và ngẫm nhiều lần rồi mà vẫn chưa thực sự hiểu, mong mọi người giúp em :
Cụ thể là ở phần 2 của bài viết này :
https://machinelearningcoban.com/2017/04/09/smv/
Tác giả có một giả sử là luôn tồn tại số k (như đã trình bày trong hình bên dưới), tuy nhiên em cho rằng ràng buộc ở (2) phải có thêm điều kiện tồn tại ít nhất 1 điểm x thỏa mãn * (cái kí hiệu dấu sao màu đỏ em đã đánh dấu) thì giả sử của ta mới có ý nghĩa. Nếu không thì ta có thể tùy chọn một số delta đủ nhỏ để thay vào vế phải của * có thể thay bằng delta (tức là thay số 1 bằng số delta đó) , nếu delta đủ nhỏ thì ràng buộc ở (2) hoàn toàn có thể bỏ đi. Không biết em đã hiểu sai ở chỗ nào vậy ạ?
Edit : Em xin lỗi vì chưa trình bày vấn đề rõ ràng được vì hạn chế là em không viết được biểu thức toán học ở đây, chỉ có thể chụp lại phần trình bày của tác giả và dựa vào để đặt câu hỏi, rất mong mọi người thông cảm cho em.","Em chào mọi người ạ, em mới tìm hiểu về machine learning và đang đọc blog của thầy Tiệp. Có một chỗ em đọc qua và ngẫm nhiều lần rồi mà vẫn chưa thực sự hiểu, mong mọi người giúp em : Cụ thể là ở phần 2 của bài viết này : https://machinelearningcoban.com/2017/04/09/smv/ Tác giả có một giả sử là luôn tồn tại số k (như đã trình bày trong hình bên dưới), tuy nhiên em cho rằng ràng buộc ở (2) phải có thêm điều kiện tồn tại ít nhất 1 điểm x thỏa mãn * (cái kí hiệu dấu sao màu đỏ em đã đánh dấu) thì giả sử của ta mới có ý nghĩa. Nếu không thì ta có thể tùy chọn một số delta đủ nhỏ để thay vào vế phải của * có thể thay bằng delta (tức là thay số 1 bằng số delta đó) , nếu delta đủ nhỏ thì ràng buộc ở (2) hoàn toàn có thể bỏ đi. Không biết em đã hiểu sai ở chỗ nào vậy ạ? Edit : Em xin lỗi vì chưa trình bày vấn đề rõ ràng được vì hạn chế là em không viết được biểu thức toán học ở đây, chỉ có thể chụp lại phần trình bày của tác giả và dựa vào để đặt câu hỏi, rất mong mọi người thông cảm cho em.",,,,,
"Hi mọi người,
Hiện tại em đang định hướng học graph neural network (GNN), mọi người cho em hỏi có sách nào hoặc github repository nào tổng hợp về GNN k ạ? Em cảm ơn ạ.","Hi mọi người, Hiện tại em đang định hướng học graph neural network (GNN), mọi người cho em hỏi có sách nào hoặc github repository nào tổng hợp về GNN k ạ? Em cảm ơn ạ.",,,,,
"Dạ chào mọi người, lại là em đây.
Hôm nay em lại lên đây với một thắc mắc như sau:
📑Liệu sử dụng VNtokenizers có thực sự đem lại hiệu quả so với sử dụng tokenizers trong BERT nguyên bản không? Nếu có thì lý do là vì sao?
🖌Cá nhân em thì có một lập luận là do các tiền tố và hậu tố trong tiếng anh đã được chuyển ra thành các từ trong tiếng Việt nên việc khai thác các tiền tố và hậu tố này không còn hiệu quả như trong tiếng Anh.
Ví dụ như từ ""lowest"" tách ra là ""low-est"" nhưng tiếng Việt thì không có 1 hậu tố nào có tác dụng như est mà thay vào đó là từ ""nhất"" kèm theo ngữ cảnh câu.
Em xin cảm ơn.","Dạ chào mọi người, lại là em đây. Hôm nay em lại lên đây với một thắc mắc như sau: Liệu sử dụng VNtokenizers có thực sự đem lại hiệu quả so với sử dụng tokenizers trong BERT nguyên bản không? Nếu có thì lý do là vì sao? Cá nhân em thì có một lập luận là do các tiền tố và hậu tố trong tiếng anh đã được chuyển ra thành các từ trong tiếng Việt nên việc khai thác các tiền tố và hậu tố này không còn hiệu quả như trong tiếng Anh. Ví dụ như từ ""lowest"" tách ra là ""low-est"" nhưng tiếng Việt thì không có 1 hậu tố nào có tác dụng như est mà thay vào đó là từ ""nhất"" kèm theo ngữ cảnh câu. Em xin cảm ơn.",,,,,
"Em chào mọi người ạ, em đang làm đồ án về OCR đọc CCCD từ ảnh chụp, em có thắc mắc là với những ảnh bị xoay nghiêng , chụp méo hay chụp ngược 180 độ , vậy mình có cách nào để model có thể tự nhận ra được và align về ảnh thẳng được không ạ? Em có tìm hiểu về phương pháp perspective transform thì không biết là khi gán nhãn 4 góc của cccd rồi đi train model thì có được không ạ ? Em xin cảm ơn ạ.","Em chào mọi người ạ, em đang làm đồ án về OCR đọc CCCD từ ảnh chụp, em có thắc mắc là với những ảnh bị xoay nghiêng , chụp méo hay chụp ngược 180 độ , vậy mình có cách nào để model có thể tự nhận ra được và align về ảnh thẳng được không ạ? Em có tìm hiểu về phương pháp perspective transform thì không biết là khi gán nhãn 4 góc của cccd rồi đi train model thì có được không ạ ? Em xin cảm ơn ạ.",,,,,
"[ChatBCG: AI làm slide]

Sau sự thành công của DALL-E 2 cho việc sinh ảnh, ChatGPT cho việc sinh chữ, gần đây mô hình ChatBCG được giới thiệu để sinh ra slide, các bạn chỉ cần nhập keyword, title hoặc nội dung chính, slide gồm cả ảnh và text sinh động sẽ được sinh ra.

Hiện tại thì vẫn đang trong giai đoạn demo nên số lượng slide sinh ra vẫn giới hạn và chưa đủ tính năng, nhưng triển vọng và chất lượng của slide thì đáng mong đợi.

Link demo: https://www.chatbcg.com/","[ChatBCG: AI làm slide] Sau sự thành công của DALL-E 2 cho việc sinh ảnh, ChatGPT cho việc sinh chữ, gần đây mô hình ChatBCG được giới thiệu để sinh ra slide, các bạn chỉ cần nhập keyword, title hoặc nội dung chính, slide gồm cả ảnh và text sinh động sẽ được sinh ra. Hiện tại thì vẫn đang trong giai đoạn demo nên số lượng slide sinh ra vẫn giới hạn và chưa đủ tính năng, nhưng triển vọng và chất lượng của slide thì đáng mong đợi. Link demo: https://www.chatbcg.com/",,,,,
"Các bác cho e hỏi
1. E dùng hàm kmean trong sklearn với k=5 mà nó chạy từ 0 đến 4. Em muốn nó chạy từ 1-5 thì làm như thế nào ạ.
2. Cái random-state có ảnh hưởng ntn đến mô hình ạ. Tại e thay đổi cái đó thì phân loại cũng thay đổi khá nhiều
Thankss ạ",Các bác cho e hỏi 1. E dùng hàm kmean trong sklearn với k=5 mà nó chạy từ 0 đến 4. Em muốn nó chạy từ 1-5 thì làm như thế nào ạ. 2. Cái random-state có ảnh hưởng ntn đến mô hình ạ. Tại e thay đổi cái đó thì phân loại cũng thay đổi khá nhiều Thankss ạ,,,,,
"Dạ em xin chào anh/chị và các bạn! Hiện tại em đang làm 1 project cuối khóa có sử dụng Mô hình CNN để nhận phân loại ảnh và có một số thắc mắc mong được mọi người giải đáp ạ! Project của em sử dụng Nucleo F446RE để đọc giá trị các cảm biến và em định dùng con Nucleo này kết hợp với 1 module Camera để chụp và phân loại ảnh dùng CNN, em có tham khảo các đề tài trên mạng và thấy đa số các tác giả dùng Raspberry Pi để thực hiện đề tài này, vậy mọi người cho em hỏi con Nucleo F446RE có thực hiện được không ạ, và nếu được thì nên dùng kết hợp module Camera nào là tối ưu nhất? Em cảm ơn anh/chị và các bạn đã xem câu hỏi ạ!","Dạ em xin chào anh/chị và các bạn! Hiện tại em đang làm 1 project cuối khóa có sử dụng Mô hình CNN để nhận phân loại ảnh và có một số thắc mắc mong được mọi người giải đáp ạ! Project của em sử dụng Nucleo F446RE để đọc giá trị các cảm biến và em định dùng con Nucleo này kết hợp với 1 module Camera để chụp và phân loại ảnh dùng CNN, em có tham khảo các đề tài trên mạng và thấy đa số các tác giả dùng Raspberry Pi để thực hiện đề tài này, vậy mọi người cho em hỏi con Nucleo F446RE có thực hiện được không ạ, và nếu được thì nên dùng kết hợp module Camera nào là tối ưu nhất? Em cảm ơn anh/chị và các bạn đã xem câu hỏi ạ!",,,,,
"#Góc nhờ vả. Chào ace trong group, m đang làm quen với nature language processing, m có 1 data frame trong đó có cột Job tiltle, m dự định tách các công việc sau mỗi dấu "" , "" ( tên cv có từ đơn và từ ghép để đảm bảo đúng ý nghĩa của tên cv), sau đó m sẽ label theo nhóm mà m đã định sẵn như ảnh đính kèm. Ace có thể tư vấn giúp m nên dùng code gì trong python để xử lý tác vụ này đc ko ạ? Cám ơn ace trước, chúc mn 1 năm mới đầy nhiệt huyết và đam mê với ngành.","nhờ vả. Chào ace trong group, m đang làm quen với nature language processing, m có 1 data frame trong đó có cột Job tiltle, m dự định tách các công việc sau mỗi dấu "" , "" ( tên cv có từ đơn và từ ghép để đảm bảo đúng ý nghĩa của tên cv), sau đó m sẽ label theo nhóm mà m đã định sẵn như ảnh đính kèm. Ace có thể tư vấn giúp m nên dùng code gì trong python để xử lý tác vụ này đc ko ạ? Cám ơn ace trước, chúc mn 1 năm mới đầy nhiệt huyết và đam mê với ngành.",#Góc,,,,
"Chào mọi người, chúng mình là Team Lightning tham gia  ZaloAIChallenge2022. Nhóm mình chỉ đạt kết quả khiêm tốn top 5 trên private leaderboard cuộc thi, tuy nhiên do nhóm rất tâm đắc với một giải pháp mới (so với các phương pháp đã công bố), nên xin phép chia sẻ cho mọi người cùng tham khảo. Nhóm mình top thấp nhưng được cái bài viết này bôi dài, cũng là mang tính phân tích, mong sẽ có ích cho mọi người theo cách nào đó :D
Related Solutions:
Vietnamese end-to-end speech recognition using wav2vec 2.0 (https://github.com/vietai/ASR)
Đây là giải pháp của anh Bình (team New Dad) công bố từ trước và đã sử dụng trong cuộc thi, sẽ rất được ưa chuộng với việc pre-train một mô hình ASR với CTC-loss. Giải pháp này train với sub-word, không phải phoneme-based, nên không cần từ điển Grapheme2Phoneme. Đặc biệt với dữ liệu khá noisy, việc pre-train ASR trên các tập dataset ASR sẽ tạo ra một mô hình học được Forced Alignment đủ tốt trên một dataset đủ lớn. Sau đó chúng ta có thể đưa ra quyết định có finetune trên tập Train Dataset BTC hay không.
Nhóm mình đã thử nghiệm phương pháp này, tuy nhiên với kích thước mô hình lớn, việc train lại / finetune khá lâu, đặc biệt với việc chúng mình gặp một chút limit về GPU cũng như muốn mô hình phải có tốc độ inference nhanh chỉ trên CPU (muốn sure kèo vào vòng trong outperform tốc độ inference tất cả, nhưng lại tạch ở top 5 :D). Thêm nữa, kết quả giải pháp này không quá cao sau khi finetune nên nhóm đã dừng giải pháp này, coi nó là Baseline1
Improving Lyrics Alignment through Joint Pitch Detection (https://arxiv.org/abs/2202.01646 - https://github.com/jhuang448/LyricsAlignment-MTL)
Giải pháp này được chúng mình dựng làm Baseline 2. Mình cũng đọc được một số nhóm sử dụng phương pháp này và có kết quả khá cao (cao hơn cả nhóm mình). Điểm mạnh của phương pháp là ngoài việc train không cần nhãn thời gian, chỉ cần nhãn text, và train Multitask với nhãn pitch note (D2-B5 không phải D2-C6 như trong paper). Bài toán đưa về vừa train forced alignment cho lyrics, kết hợp train phân loại note cho từng câu từ một, giúp kết quả tốt (đặc biệt với thể loại nhạc không phải nhạc Rap).
Nhóm mình đã chạy thử nghiệm khá nhiều với pretrained model của phương pháp này, tuy nhiên có nhược điểm là sẽ mất thời gian đáng kể cho việc chạy Spleeter/Demucs để lấy vocal. Bên cạnh đó, phương pháp này là phoneme-based, vậy nên pretrained cũng đã train với G2p_en (bản tiếng Anh), khiến phoneme học được của mô hình hoàn toàn là theo Anh ngữ. Muốn finetune phải có Grapheme2Phoneme riêng lại cho tiếng Việt để tránh bị sai âm, cộng thêm thời gian inference mô hình Multitask không nhanh như kỳ vọng, kết quả lại không quá nổi trội tương xứng với thời gian, nên sau một hồi tranh luận gắt gao đã chốt là thôi, train hẳn mô hình mới, không xài nữa và coi nó làm Baseline 2.
PROPOSED MODEL:
Nhóm mình sử dụng kiến trúc Conformer - một kiến trúc khá mạnh trong bài toán ASR. Kết hợp cùng với đó là train Multitask với pitch note tự extract. Hai điểm này của mô hình đã vá lại các nhược điểm kể ở hai baselines trên, hoạt động tốt với cả nhạc Rap lẫn nhạc nhiều thanh điệu. Vì tập train khá noisy, nhãn không consistent và reliable cho lắm, nên chúng mình đã train không dùng nhãn alignment của Zalo, chỉ dùng CTC loss Forced Alignment, và loss phân loại note.  Đặc biệt chúng mình không dùng vocal separation, cứ ném thẳng cả audio vào để train, kết hợp Data Augmentation với SpecAug giúp mô hình predict mà không cần extract vocal.
Về phần dữ liệu train, chúng mình sử dụng tập VinBigData ASR công khai từ vài năm trước, train một cái unsupervised model. Sau đó thử nghiệm train Finetune lại trên tập nhạc của BTC (tuy nhiên việc này thậm chí làm mô hình học tệ đi :( )
Các điểm trên giúp mô hình của chúng mình rất đơn giản và nhỏ với khoảng 2M tham số, tốc độ inference chưa tới 45s cho toàn bộ tập Public test, chỉ chạy trên CPU Core I5.
Results:
Các baseline đầu tiên cho kết quả khá trồi sụt. Đặc biệt với mô hình LyricsAlignmentMTL, chúng mình đã ốp một vài trick như shift nhãn về bên trái tầm 30-50 ms, kéo đầu align từ sau dính vào đuôi align của từ trước. Việc nên ốp trick hay không là khá đau đầu vì kết quả trên các bảng xếp hạng cũng không hề đáng tin.
Cuối cùng, nhóm mình chốt lại là: “Kỹ Năng Thượng Thừa Không Bằng Sức Mạnh Tuyệt Đối” 😀. Submit cuối cùng sẽ nộp mô hình tự propose và không post-process (vì sau khi check nhãn thấy mô hình đã align khá chuẩn, chỉnh chỉ làm tệ thêm). Các đoạn ngân cuối bài cũng đã được detect khá chuẩn. Check lại nhãn cũng thấy mô hình có khả năng cắt chuẩn phoneme mở đầu thực sự được phát ra. Rất tiếc đến lúc nộp submission quyết định (ở lần submit số 2), nhóm mình đã submit nhầm bản Shifted (ốp các trick linh tinh). Ngay sau đó nhóm mình đã submit lại bản chuẩn như trong docker nộp cho Zalo, kết quả nhảy lên top 2 private leaderboard, nhưng BTC Zalo đã remove hết tất cả các Submission từ số 3 đổ đi để kết thúc việc probing leaderboard của anh em tham dự =)).
Thêm một kinh nghiệm nhỏ nhỏ (hơi đau đớn) là nhóm mình cảm thấy càng finetune, make use of public dataset, kết quả càng kém đi. Có thể do kinh nghiệm / kiến thức của chúng mình còn hạn hẹp chưa biết tận dụng, nhưng kết quả sẽ xuống kha khá đi khi force mô hình học các dataset nhiều noise và sai nhãn. Mình đoán nhiều nhóm sẽ gặp phải trường hợp này, khiến mô hình của các bạn perform tệ đi (không biết có bao gồm giải pháp của anh Bình New Dad không, khi kết quả public test của team New Dad rất đáng nể…)
Conclusion:
Mô hình của chúng mình còn tồn tại nhiều thiếu sót, ví dụ như việc task pitch detection còn chưa thực sự có leverage, chủ yếu kết quả vẫn là phần Conformer ASR gánh. Phần đóng docker cũng khiến chúng mình khá loay hoay khi không phải upload lên Dockerhub mà lại là Drive, với tốc độ rất giới hạn. Lần đầu tiên chúng mình làm việc với một task liên quan không chỉ voice/speech mà là toàn bộ audio, music, giúp nhóm học được rất nhiều kinh nghiệm. Đặc biệt hơn, được tham gia cùng với các top Team siêu giỏi như anh Bình (New Dad), team Telegram, v.v… giúp chúng mình tự tin hơn trước rất nhiều, được truyền cảm hứng cho các công việc liên quan trong tương lai.
Toàn bộ Code và Pretrained model đã có trên link github dưới đây:
littlebeanhp/ZAC2022-LightningConformer: Team Lightning's solution for Zalo AI Challenge 2022 (github.com)
Cuối bài, Team Lightning chúng mình cảm ơn Zalo đã tạo ra sân chơi rất thú vị, giúp kết nối các anh em làm AI chúng mình gần nhau hơn, được thảo luận và tăng độ lớn mạnh của cộng đồng! Mong rằng trong tương lai sẽ còn nhiều sân chơi với giải thưởng ngày càng to hơn để chúng mình có thể học hỏi được từ những đội thi mạnh hơn!","Chào mọi người, chúng mình là Team Lightning tham gia ZaloAIChallenge2022. Nhóm mình chỉ đạt kết quả khiêm tốn top 5 trên private leaderboard cuộc thi, tuy nhiên do nhóm rất tâm đắc với một giải pháp mới (so với các phương pháp đã công bố), nên xin phép chia sẻ cho mọi người cùng tham khảo. Nhóm mình top thấp nhưng được cái bài viết này bôi dài, cũng là mang tính phân tích, mong sẽ có ích cho mọi người theo cách nào đó :D Related Solutions: Vietnamese end-to-end speech recognition using wav2vec 2.0 (https://github.com/vietai/ASR) Đây là giải pháp của anh Bình (team New Dad) công bố từ trước và đã sử dụng trong cuộc thi, sẽ rất được ưa chuộng với việc pre-train một mô hình ASR với CTC-loss. Giải pháp này train với sub-word, không phải phoneme-based, nên không cần từ điển Grapheme2Phoneme. Đặc biệt với dữ liệu khá noisy, việc pre-train ASR trên các tập dataset ASR sẽ tạo ra một mô hình học được Forced Alignment đủ tốt trên một dataset đủ lớn. Sau đó chúng ta có thể đưa ra quyết định có finetune trên tập Train Dataset BTC hay không. Nhóm mình đã thử nghiệm phương pháp này, tuy nhiên với kích thước mô hình lớn, việc train lại / finetune khá lâu, đặc biệt với việc chúng mình gặp một chút limit về GPU cũng như muốn mô hình phải có tốc độ inference nhanh chỉ trên CPU (muốn sure kèo vào vòng trong outperform tốc độ inference tất cả, nhưng lại tạch ở top 5 :D). Thêm nữa, kết quả giải pháp này không quá cao sau khi finetune nên nhóm đã dừng giải pháp này, coi nó là Baseline1 Improving Lyrics Alignment through Joint Pitch Detection (https://arxiv.org/abs/2202.01646 - https://github.com/jhuang448/LyricsAlignment-MTL) Giải pháp này được chúng mình dựng làm Baseline 2. Mình cũng đọc được một số nhóm sử dụng phương pháp này và có kết quả khá cao (cao hơn cả nhóm mình). Điểm mạnh của phương pháp là ngoài việc train không cần nhãn thời gian, chỉ cần nhãn text, và train Multitask với nhãn pitch note (D2-B5 không phải D2-C6 như trong paper). Bài toán đưa về vừa train forced alignment cho lyrics, kết hợp train phân loại note cho từng câu từ một, giúp kết quả tốt (đặc biệt với thể loại nhạc không phải nhạc Rap). Nhóm mình đã chạy thử nghiệm khá nhiều với pretrained model của phương pháp này, tuy nhiên có nhược điểm là sẽ mất thời gian đáng kể cho việc chạy Spleeter/Demucs để lấy vocal. Bên cạnh đó, phương pháp này là phoneme-based, vậy nên pretrained cũng đã train với G2p_en (bản tiếng Anh), khiến phoneme học được của mô hình hoàn toàn là theo Anh ngữ. Muốn finetune phải có Grapheme2Phoneme riêng lại cho tiếng Việt để tránh bị sai âm, cộng thêm thời gian inference mô hình Multitask không nhanh như kỳ vọng, kết quả lại không quá nổi trội tương xứng với thời gian, nên sau một hồi tranh luận gắt gao đã chốt là thôi, train hẳn mô hình mới, không xài nữa và coi nó làm Baseline 2. PROPOSED MODEL: Nhóm mình sử dụng kiến trúc Conformer - một kiến trúc khá mạnh trong bài toán ASR. Kết hợp cùng với đó là train Multitask với pitch note tự extract. Hai điểm này của mô hình đã vá lại các nhược điểm kể ở hai baselines trên, hoạt động tốt với cả nhạc Rap lẫn nhạc nhiều thanh điệu. Vì tập train khá noisy, nhãn không consistent và reliable cho lắm, nên chúng mình đã train không dùng nhãn alignment của Zalo, chỉ dùng CTC loss Forced Alignment, và loss phân loại note. Đặc biệt chúng mình không dùng vocal separation, cứ ném thẳng cả audio vào để train, kết hợp Data Augmentation với SpecAug giúp mô hình predict mà không cần extract vocal. Về phần dữ liệu train, chúng mình sử dụng tập VinBigData ASR công khai từ vài năm trước, train một cái unsupervised model. Sau đó thử nghiệm train Finetune lại trên tập nhạc của BTC (tuy nhiên việc này thậm chí làm mô hình học tệ đi :( ) Các điểm trên giúp mô hình của chúng mình rất đơn giản và nhỏ với khoảng 2M tham số, tốc độ inference chưa tới 45s cho toàn bộ tập Public test, chỉ chạy trên CPU Core I5. Results: Các baseline đầu tiên cho kết quả khá trồi sụt. Đặc biệt với mô hình LyricsAlignmentMTL, chúng mình đã ốp một vài trick như shift nhãn về bên trái tầm 30-50 ms, kéo đầu align từ sau dính vào đuôi align của từ trước. Việc nên ốp trick hay không là khá đau đầu vì kết quả trên các bảng xếp hạng cũng không hề đáng tin. Cuối cùng, nhóm mình chốt lại là: “Kỹ Năng Thượng Thừa Không Bằng Sức Mạnh Tuyệt Đối” . Submit cuối cùng sẽ nộp mô hình tự propose và không post-process (vì sau khi check nhãn thấy mô hình đã align khá chuẩn, chỉnh chỉ làm tệ thêm). Các đoạn ngân cuối bài cũng đã được detect khá chuẩn. Check lại nhãn cũng thấy mô hình có khả năng cắt chuẩn phoneme mở đầu thực sự được phát ra. Rất tiếc đến lúc nộp submission quyết định (ở lần submit số 2), nhóm mình đã submit nhầm bản Shifted (ốp các trick linh tinh). Ngay sau đó nhóm mình đã submit lại bản chuẩn như trong docker nộp cho Zalo, kết quả nhảy lên top 2 private leaderboard, nhưng BTC Zalo đã remove hết tất cả các Submission từ số 3 đổ đi để kết thúc việc probing leaderboard của anh em tham dự =)). Thêm một kinh nghiệm nhỏ nhỏ (hơi đau đớn) là nhóm mình cảm thấy càng finetune, make use of public dataset, kết quả càng kém đi. Có thể do kinh nghiệm / kiến thức của chúng mình còn hạn hẹp chưa biết tận dụng, nhưng kết quả sẽ xuống kha khá đi khi force mô hình học các dataset nhiều noise và sai nhãn. Mình đoán nhiều nhóm sẽ gặp phải trường hợp này, khiến mô hình của các bạn perform tệ đi (không biết có bao gồm giải pháp của anh Bình New Dad không, khi kết quả public test của team New Dad rất đáng nể…) Conclusion: Mô hình của chúng mình còn tồn tại nhiều thiếu sót, ví dụ như việc task pitch detection còn chưa thực sự có leverage, chủ yếu kết quả vẫn là phần Conformer ASR gánh. Phần đóng docker cũng khiến chúng mình khá loay hoay khi không phải upload lên Dockerhub mà lại là Drive, với tốc độ rất giới hạn. Lần đầu tiên chúng mình làm việc với một task liên quan không chỉ voice/speech mà là toàn bộ audio, music, giúp nhóm học được rất nhiều kinh nghiệm. Đặc biệt hơn, được tham gia cùng với các top Team siêu giỏi như anh Bình (New Dad), team Telegram, v.v… giúp chúng mình tự tin hơn trước rất nhiều, được truyền cảm hứng cho các công việc liên quan trong tương lai. Toàn bộ Code và Pretrained model đã có trên link github dưới đây: littlebeanhp/ZAC2022-LightningConformer: Team Lightning's solution for Zalo AI Challenge 2022 (github.com) Cuối bài, Team Lightning chúng mình cảm ơn Zalo đã tạo ra sân chơi rất thú vị, giúp kết nối các anh em làm AI chúng mình gần nhau hơn, được thảo luận và tăng độ lớn mạnh của cộng đồng! Mong rằng trong tương lai sẽ còn nhiều sân chơi với giải thưởng ngày càng to hơn để chúng mình có thể học hỏi được từ những đội thi mạnh hơn!",,,,,
"Xin chào mọi người!
Bác nào có link hay tài liệu nào về inreforcement learning ko (có code thử càng tốt)? Share mình tham khảo với. Thanks!",Xin chào mọi người! Bác nào có link hay tài liệu nào về inreforcement learning ko (có code thử càng tốt)? Share mình tham khảo với. Thanks!,,,,,
"Mọi người cho mình hỏi, có chứng chỉ nào free về machine learning của google hoặc chỗ khác để tăng kinh nghiệm và làm đẹp CV không ạ. Mình cảm ơn","Mọi người cho mình hỏi, có chứng chỉ nào free về machine learning của google hoặc chỗ khác để tăng kinh nghiệm và làm đẹp CV không ạ. Mình cảm ơn",,,,,
"Mình cần tìm một mentor có kinh nghiệm về xây dựng mô hình dự đoán kinh doanh và NLP.
Hình thức Q&A theo giờ phù hợp của hai bên
Chi phí thoả thuận.",Mình cần tìm một mentor có kinh nghiệm về xây dựng mô hình dự đoán kinh doanh và NLP. Hình thức Q&A theo giờ phù hợp của hai bên Chi phí thoả thuận.,,,,,
"Cuộc thi ""Novozymes Enzyme Stability Prediction"" trên kaggle một lần nữa lại thể hiện sự biến động mạnh về xếp hạng giữa public LB và private LB. Tại public LB, anh Chris Deotte dẫn đầu trong thời gian dài với số điểm vượt trội so với phần còn lại, nhưng tại private LB anh chỉ giữ vị trí 968 (xem ảnh chụp). Đây là một cuộc thi khó vì nó yêu cầu kiến thức nền rất khác biệt so với dân IT, và cũng rất vui khi phát hiện ra công dân Đông Lào đạt được thứ hạng cao trong cuộc thi này.
Bài toán phát hiện liệu mô hình có bị overfit hay không vẫn là khó nhằn, kể cả với cao thủ hạng nhất thế giới.","Cuộc thi ""Novozymes Enzyme Stability Prediction"" trên kaggle một lần nữa lại thể hiện sự biến động mạnh về xếp hạng giữa public LB và private LB. Tại public LB, anh Chris Deotte dẫn đầu trong thời gian dài với số điểm vượt trội so với phần còn lại, nhưng tại private LB anh chỉ giữ vị trí 968 (xem ảnh chụp). Đây là một cuộc thi khó vì nó yêu cầu kiến thức nền rất khác biệt so với dân IT, và cũng rất vui khi phát hiện ra công dân Đông Lào đạt được thứ hạng cao trong cuộc thi này. Bài toán phát hiện liệu mô hình có bị overfit hay không vẫn là khó nhằn, kể cả với cao thủ hạng nhất thế giới.",,,,,
"Share event về AI cho bà con thảo luận trước Tết 😀
Được chia sẻ bởi anh Hưng Ngô - Founder/ CEO của CoTAI.
Đăng ký ngay nhé: https://gambaru.io/en/events/te15-ai4dev-landscapes-roadmaps-collabs
⏰ Thời gian: 07/01, sáng thứ 7, 9h30-11h30
💻 Online qua Zoom

Lợi ích khi tham dự:
1) Hiểu rõ về AI cũng như các công cụ, ứng dụng, xu thế của AI lên ngành phần mềm
2) Thấy được bức tranh tổng thể về các lĩnh vực nghiên cứu và ứng dụng sôi động của AI
3) Có được các lộ trình học tập tối ưu để phát triển nghề nghiệp có sử dụng AI
4) Nắm bắt các nguồn tài nguyên, các cộng đồng, và các cơ hội cộng tác học tập cũng như làm dự án sản phẩm AI.","Share event về AI cho bà con thảo luận trước Tết Được chia sẻ bởi anh Hưng Ngô - Founder/ CEO của CoTAI. Đăng ký ngay nhé: https://gambaru.io/en/events/te15-ai4dev-landscapes-roadmaps-collabs ⏰ Thời gian: 07/01, sáng thứ 7, 9h30-11h30 Online qua Zoom Lợi ích khi tham dự: 1) Hiểu rõ về AI cũng như các công cụ, ứng dụng, xu thế của AI lên ngành phần mềm 2) Thấy được bức tranh tổng thể về các lĩnh vực nghiên cứu và ứng dụng sôi động của AI 3) Có được các lộ trình học tập tối ưu để phát triển nghề nghiệp có sử dụng AI 4) Nắm bắt các nguồn tài nguyên, các cộng đồng, và các cơ hội cộng tác học tập cũng như làm dự án sản phẩm AI.",,,,,
"Em chào anh chị ạ,
Em đang cần tìm 1 công cụ xử lý hình ảnh, có thể là dùng bất cứ mô hình ML hoặc DL để xác định vết nứt từ 2 ảnh năm 2009 và 2014 ( vết nứt là cái được khoanh tròn xanh trong hình đính kèm ) và sau đó so sánh độ dài 2 vết nứt với nhau. Anh chị nào đã làm vấn đề tương tự có thể chỉ cho em 1 số công cụ hoặc mô hình có thể giải quyết được vấn đề không ạ? Hoặc anh chị nào đã làm image segmentation cho đối tượng nhỏ có thể gợi ý cho em một số mô hình DL hoặc ML làm image segmentation cho đối tượng
""Nhỏ"" hoặc object detection? Hoặc các công cụ giúp xử lý hình ảnh tốt không sử dụng ML hoặc DL cũng được ạ.
Em cảm ơn ạ.","Em chào anh chị ạ, Em đang cần tìm 1 công cụ xử lý hình ảnh, có thể là dùng bất cứ mô hình ML hoặc DL để xác định vết nứt từ 2 ảnh năm 2009 và 2014 ( vết nứt là cái được khoanh tròn xanh trong hình đính kèm ) và sau đó so sánh độ dài 2 vết nứt với nhau. Anh chị nào đã làm vấn đề tương tự có thể chỉ cho em 1 số công cụ hoặc mô hình có thể giải quyết được vấn đề không ạ? Hoặc anh chị nào đã làm image segmentation cho đối tượng nhỏ có thể gợi ý cho em một số mô hình DL hoặc ML làm image segmentation cho đối tượng ""Nhỏ"" hoặc object detection? Hoặc các công cụ giúp xử lý hình ảnh tốt không sử dụng ML hoặc DL cũng được ạ. Em cảm ơn ạ.",,,,,
"Dạ chào mọi người ạ. Em có một mô hình như vậy và có đầu vào là 1000 điểm x,y của 2 đường R và J và có R0, J0. Em được gợi ý là sử dụng Variational AutoEncoder để tính hệ số a,b,c,d nhưng em không biết sử dụng như thế nào. Anh chị có thể giúp em xây dựng neutral network được không ạ.
Em xin cảm ơn.","Dạ chào mọi người ạ. Em có một mô hình như vậy và có đầu vào là 1000 điểm x,y của 2 đường R và J và có R0, J0. Em được gợi ý là sử dụng Variational AutoEncoder để tính hệ số a,b,c,d nhưng em không biết sử dụng như thế nào. Anh chị có thể giúp em xây dựng neutral network được không ạ. Em xin cảm ơn.",,,,,
"Dạ em xin chào mọi người ạ! Hôm nay ở trên lớp em được học về Naivy Bayes ạ!
Hiện tại em đang gặp khó khăn về bài toán này ạ!
Cho em hỏi là tại sao tính được những giá trị như trong dữ liệu huấn luyện như bên dưới ạ!:
P(a/A) = 4/6 = 0.67
P(b/A) = 2/6 = 0.33
P(a/B) = 0/2 = 0
P(b/B) = 2/2 = 1
Những phần này là em trích ra từ phần em gạch đỏ ạ!. Mong được anh chị giúp đỡ ạ!",Dạ em xin chào mọi người ạ! Hôm nay ở trên lớp em được học về Naivy Bayes ạ! Hiện tại em đang gặp khó khăn về bài toán này ạ! Cho em hỏi là tại sao tính được những giá trị như trong dữ liệu huấn luyện như bên dưới ạ!: P(a/A) = 4/6 = 0.67 P(b/A) = 2/6 = 0.33 P(a/B) = 0/2 = 0 P(b/B) = 2/2 = 1 Những phần này là em trích ra từ phần em gạch đỏ ạ!. Mong được anh chị giúp đỡ ạ!,,,,,
"[Xin trợ giúp về xác suất] Mình đang làm nghiên cứu và gặp bài toán sau đây. Mình xin phép viết tiếng Anh cho dễ.
Consider a multivariate Gaussian random variable X ~ N(m, C) of d variables. Denote Y as the index (among d entries) of X that has the maximum values. In particular Y = argmax_i X_i.
Compute P(Y=i) for a given i in {1, 2,..,d}?
Xin cảm ơn.","[Xin trợ giúp về xác suất] Mình đang làm nghiên cứu và gặp bài toán sau đây. Mình xin phép viết tiếng Anh cho dễ. Consider a multivariate Gaussian random variable X ~ N(m, C) of d variables. Denote Y as the index (among d entries) of X that has the maximum values. In particular Y = argmax_i X_i. Compute P(Y=i) for a given i in {1, 2,..,d}? Xin cảm ơn.",,,,,
"Mình định không viết post này, nhưng thấy nhiều bạn hỏi, xong rồi nhiều bạn vào quăng cho 1 cái link (mình nghĩ bạn này cũng chỉ search google). Những bạn chưa biết gì mà bị rơi vào ma cung kiểu này rất khổ (mình từng là nạn nhân). Nên mình sẽ chia sẽ những gì mình đã học, và thấy có ích.
1) Reinforcement Learning: Bạn search google thì sẽ ra recommend: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html
Book: http://incompleteideas.net/sutton/book/bookdraft2017june.pdf
Nếu bạn chưa biết gì mà học và đọc sách này, bạn trụ được 1 tuần, tôi phục bạn.
Recommend của tôi: bạn nên bắt đầu từ khoá:
https://learning.edx.org/course/course-v1:Microsoft+DAT257x+3T2018
Cần biết thêm kiến thức về Markov Decision Process thì xem phần cuối của khoá stat110 và cuốn sách đi kèm:
https://projects.iq.harvard.edu/stat110/home
2) Data Science:
Bạn muốn bắt đầu mà chưa biết nhiều về Python hay xử lý dữ liệu thì nên bắt đầu với khoá CS109 của Harvard:http://cs109.github.io/2015/ (chú ý tìm lecture của 2015, homework của 2013). Họ sẽ dạy bạn từ collect dữ liệu, làm sạch dữ liệu và xử lý dữ liệu.
Bạn cần thêm kiến thức về xác suất: Học khoá Stat110 như bên trên tôi đề cập. Học xong khoá này bạn sẽ thấy sự diệu kì của xác suất:
https://projects.iq.harvard.edu/stat110/home
Bạn muốn có 1 chứng chỉ free liên quan đến Data Science: học khoá Stat Learning của Stanford ( tiện thể học thêm ít về ngôn ngữ R luôn), sách đi kèm free nhé: https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/courseware/995220423fd14a4588d8e47920f1b5df/99faa3a82fca4fc19adc577ce9f75afd/
3) Game Theory:
Bạn học khoá này của Stanford: https://www.coursera.org/learn/game-theory-1
Nhưng đọc thêm quyển sách Game Theory 101, và giải thích của tác giả trên youtube ( vì quyển đi kèm với khoá Stanford cũng toàn lý thuyết).
4) Deep Learning - Tensorflow
Bạn có thể bắt đầu với khoá của IBM, có chứng chỉ free cho bạn, có code cho bạn chạy: trên trang này có nhiều khoá khác, thích thì bạn có thể học.
https://cognitiveclass.ai/courses/deep-learning-tensorflow/
or https://classroom.udacity.com/courses/ud730
Sau đó nghe thêm video: Tensorflow and deep learning - without a Phd
https://www.youtube.com/watch?v=vq2nnJ4g6N0
Tiếp đó để hiểu sâu hơn, tham khảo khoá CS231 của Stanford:
http://cs231n.stanford.edu/2016/syllabus.html
Nhớ xem github của thánh Andrej Karpathy
5) NLP - Chatbot
Khoá này mới mở trên coursera, tất cả mọi thứ clear đến từng chi tiết:
https://www.coursera.org/learn/nlp-sequence-models
Ngoài ra để chuyên sâu hơn, bạn cần học Natural Language Processing (CS224D), Deep Learning (CS231) trước, sau đó down code của Huyền Chip về tham khảo ( nhớ tìm đúng version của Tensorflow để chạy): Thực ra có thể down code luôn về chạy nhìn kết quả cũng okie.
https://github.com/chiphuyen/stanford-tensorflow-tutorials/tree/master/assignments/chatbot
6) Bigdata
Bạn nào muốn tìm hiểu về Spark, có thể học 3 khoá này, có thể giờ không mở nhưng bạn vẫn xem được video và lấy bài lab về chạy bình thường, cần solution tôi có thể gửi. Học xong bạn sẽ biết Spark, Map Reduce, Machine Learning,...:
https://courses.edx.org/dashboard/programs/a06a1f8b-21e6-49c8-887e-1016d3639de3/
Bạn muốn biết thực sự xử lý Big Data phức tạp như nào, bạn học khoá này, học xong có chứng chỉ nhé, khoá này nặng nhưng hay lắm:
http://online.stanford.edu/course/mining-massive-datasets-self-paced
7) AI chung ( Algorithms, Machine Learning, Reinforcement Learning)
Bạn học khoá này: https://courses.edx.org/courses/course-v1:ColumbiaX+CSMM.101x+2T2017/course/
Và cuốn sách kinh điển đi cùng: Artificial Intelligence A Modern Approach, Third Edition
8) Book để đi phỏng vấn xin việc:
Cracking the Coding Interview 6
Elements of Programming Interviews in Java (Python, C++)
Introduction to Algorithms 3","Mình định không viết post này, nhưng thấy nhiều bạn hỏi, xong rồi nhiều bạn vào quăng cho 1 cái link (mình nghĩ bạn này cũng chỉ search google). Những bạn chưa biết gì mà bị rơi vào ma cung kiểu này rất khổ (mình từng là nạn nhân). Nên mình sẽ chia sẽ những gì mình đã học, và thấy có ích. 1) Reinforcement Learning: Bạn search google thì sẽ ra recommend: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html Book: http://incompleteideas.net/sutton/book/bookdraft2017june.pdf Nếu bạn chưa biết gì mà học và đọc sách này, bạn trụ được 1 tuần, tôi phục bạn. Recommend của tôi: bạn nên bắt đầu từ khoá: https://learning.edx.org/course/course-v1:Microsoft+DAT257x+3T2018 Cần biết thêm kiến thức về Markov Decision Process thì xem phần cuối của khoá stat110 và cuốn sách đi kèm: https://projects.iq.harvard.edu/stat110/home 2) Data Science: Bạn muốn bắt đầu mà chưa biết nhiều về Python hay xử lý dữ liệu thì nên bắt đầu với khoá CS109 của Harvard:http://cs109.github.io/2015/ (chú ý tìm lecture của 2015, homework của 2013). Họ sẽ dạy bạn từ collect dữ liệu, làm sạch dữ liệu và xử lý dữ liệu. Bạn cần thêm kiến thức về xác suất: Học khoá Stat110 như bên trên tôi đề cập. Học xong khoá này bạn sẽ thấy sự diệu kì của xác suất: https://projects.iq.harvard.edu/stat110/home Bạn muốn có 1 chứng chỉ free liên quan đến Data Science: học khoá Stat Learning của Stanford ( tiện thể học thêm ít về ngôn ngữ R luôn), sách đi kèm free nhé: https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/courseware/995220423fd14a4588d8e47920f1b5df/99faa3a82fca4fc19adc577ce9f75afd/ 3) Game Theory: Bạn học khoá này của Stanford: https://www.coursera.org/learn/game-theory-1 Nhưng đọc thêm quyển sách Game Theory 101, và giải thích của tác giả trên youtube ( vì quyển đi kèm với khoá Stanford cũng toàn lý thuyết). 4) Deep Learning - Tensorflow Bạn có thể bắt đầu với khoá của IBM, có chứng chỉ free cho bạn, có code cho bạn chạy: trên trang này có nhiều khoá khác, thích thì bạn có thể học. https://cognitiveclass.ai/courses/deep-learning-tensorflow/ or https://classroom.udacity.com/courses/ud730 Sau đó nghe thêm video: Tensorflow and deep learning - without a Phd https://www.youtube.com/watch?v=vq2nnJ4g6N0 Tiếp đó để hiểu sâu hơn, tham khảo khoá CS231 của Stanford: http://cs231n.stanford.edu/2016/syllabus.html Nhớ xem github của thánh Andrej Karpathy 5) NLP - Chatbot Khoá này mới mở trên coursera, tất cả mọi thứ clear đến từng chi tiết: https://www.coursera.org/learn/nlp-sequence-models Ngoài ra để chuyên sâu hơn, bạn cần học Natural Language Processing (CS224D), Deep Learning (CS231) trước, sau đó down code của Huyền Chip về tham khảo ( nhớ tìm đúng version của Tensorflow để chạy): Thực ra có thể down code luôn về chạy nhìn kết quả cũng okie. https://github.com/chiphuyen/stanford-tensorflow-tutorials/tree/master/assignments/chatbot 6) Bigdata Bạn nào muốn tìm hiểu về Spark, có thể học 3 khoá này, có thể giờ không mở nhưng bạn vẫn xem được video và lấy bài lab về chạy bình thường, cần solution tôi có thể gửi. Học xong bạn sẽ biết Spark, Map Reduce, Machine Learning,...: https://courses.edx.org/dashboard/programs/a06a1f8b-21e6-49c8-887e-1016d3639de3/ Bạn muốn biết thực sự xử lý Big Data phức tạp như nào, bạn học khoá này, học xong có chứng chỉ nhé, khoá này nặng nhưng hay lắm: http://online.stanford.edu/course/mining-massive-datasets-self-paced 7) AI chung ( Algorithms, Machine Learning, Reinforcement Learning) Bạn học khoá này: https://courses.edx.org/courses/course-v1:ColumbiaX+CSMM.101x+2T2017/course/ Và cuốn sách kinh điển đi cùng: Artificial Intelligence A Modern Approach, Third Edition 8) Book để đi phỏng vấn xin việc: Cracking the Coding Interview 6 Elements of Programming Interviews in Java (Python, C++) Introduction to Algorithms 3",,,,,
"Chào mọi người, mình là Hợp, đến từ team NNN đạt top 2 cuộc thi Zalo AI Challenge với track Liveness Detection. Mình muốn thay mặt team chia sẻ solution cho cuộc thi.
Team mình cảm thấy rất may mắn khi đạt được top 2 Final; cũng như đồng top 1 trong LB Public test 1 và top 30 LB Public test 2. Sau đây là solution của team:
1.Xử lý dữ liệu: Team mình chia tập train-val đơn giản theo tỷ lệ 80/20 theo id video và cắt lấy 1 frame trên 1s.
2.Huấn luyện
Public test 1: Giai đoạn đầu tụi mình xây dựng mô hình baseline với mô hình Efficientnet-B4 Noisy Student input size 512, sử dụng một số augmentation đơn giản như H/V Flip, ColorJitter. Mô hình đã fit được tập public test 1 rất tốt. Với một vài lần submit và thử nghiệm tay (với một chút may mắn) metric EER, team mình đã có được ground-truth của tập này.
Public test 2: Với tập dữ liệu mới này, team mình đánh giá dữ liệu có chất lượng video kém hơn so với trước, cảm giác như video bị cắt nhỏ, làm mờ, … Nên tụi mình đưa ra 1 số hướng để giải quyết.
Phương pháp 1 (PP1): Bổ sung vào tập Val hiện tại các augs của chính nó, cố định thành một tập Val-aug mới (số lượng x2 Val cũ).
Phương pháp 2 (PP2): Các aug cho tập Val được tùy biến mỗi khi load dữ liệu (tương tự cách aug cho tập Train).
Mục tiêu của 2 hướng trên là tìm ra một không gian Augmentation của tập Val miêu tả chính xác nhất, team mình sử dụng mô hình Swin Transformer để huấn luyện cho cả 2 phương pháp trên.
Mô hình Swin_PP1:
Kiến trúc: swin_large_patch4_window12_384 với drop_path_rate: 0.3
Optimizer: LR: 1e-5, Weight decay: 3e-5
LR Scheduler: CosineAnnealingWarmRestarts với T0: 600, T_mult: 1, Eta_min: 1e-7
Train Augmentation: RandomResizedCrop 0.49-1.0, RandomHorizontalFlip 0.5, RandomVerticalFlip 0.2, GaussianBlur kernel_size 3 sigma 0.2-2.0, ColorJitter contrast 0.2.
Offline-Augmentation Val: Downsize Image: CenterCrop 0.75 * height - 0.75 * width, Resize 0.375 * height - 0.375 * width.
Chuẩn bị dữ liệu Val-aug = tập val gốc + offline-augmentation val.
Trainer: Mixed Precision với FP16, Batch size 16, Max 12000 step, validation mỗi epoch, lưu 2 mô hình val loss thấp nhất, lấy mô hình với val accuracy cao nhất trong 2.
Mô hình Swin_PP2:
Kiến trúc: swin_large_patch4_window12_384
Optimizer: LR: 3e-5, Weight decay: 1e-6
LR Scheduler: StepLR với step_size: 5, gamma: 0.2
Train / Val Augmentation: RandomResizedCrop 0.49-1.0, RandomVerticalFlip 0.2, GaussianBlur kernel_size 3 sigma 0.2-2.0.
Trainer: Mixed Precision với FP16, Batch size 16, Max 20 epoch, validation mỗi epoch, lưu 3 mô hình val loss thấp nhất và last checkpoint. Qua thử nghiệm tụi mình chọn last checkpoint.
Tụi mình đánh giá 2 mô hình Swin_PP1 và Swin_PP2 nhận thấy mô hình PP1 tốt trên public test 1, mô hình PP2 lại tốt trên public test 2 nên team đã ensemble 2 mô hình lại và thấy mô hình Ensemble này cho kết quả (EER và TEER) tốt hơn.
Một số điểm tốt mà tụi mình đánh giá đem lại thành công cho mô hình:
Training với Mixed Precision. Team mình từ đầu cuộc thi cũng cân nhắc về time constraint và chú ý training với mixed precision, nó giúp các mô hình tụi mình inference nhanh nhưng hiệu quả không kém mô hình FP32.
Ensemble: tụi mình chỉ muốn ensemble 2 mô hình tốt nhất trên các LB, nhưng có lẽ nên dùng nhiều hơn.
Augmentation offline và online: team xác định các loại augs phù hợp và tuning :)
May mắn: team khá gặp may mắn khi hoàn thành được codebase và baseline khá sớm, cũng như một số may mắn ở trên, tụi mình chỉ phân tích lỗi và để nó tuning liên tục thôi :’)
Training code và inference đều nằm trong repo. Mình cảm ơn mọi người đã quan tâm.","Chào mọi người, mình là Hợp, đến từ team NNN đạt top 2 cuộc thi Zalo AI Challenge với track Liveness Detection. Mình muốn thay mặt team chia sẻ solution cho cuộc thi. Team mình cảm thấy rất may mắn khi đạt được top 2 Final; cũng như đồng top 1 trong LB Public test 1 và top 30 LB Public test 2. Sau đây là solution của team: 1.Xử lý dữ liệu: Team mình chia tập train-val đơn giản theo tỷ lệ 80/20 theo id video và cắt lấy 1 frame trên 1s. 2.Huấn luyện Public test 1: Giai đoạn đầu tụi mình xây dựng mô hình baseline với mô hình Efficientnet-B4 Noisy Student input size 512, sử dụng một số augmentation đơn giản như H/V Flip, ColorJitter. Mô hình đã fit được tập public test 1 rất tốt. Với một vài lần submit và thử nghiệm tay (với một chút may mắn) metric EER, team mình đã có được ground-truth của tập này. Public test 2: Với tập dữ liệu mới này, team mình đánh giá dữ liệu có chất lượng video kém hơn so với trước, cảm giác như video bị cắt nhỏ, làm mờ, … Nên tụi mình đưa ra 1 số hướng để giải quyết. Phương pháp 1 (PP1): Bổ sung vào tập Val hiện tại các augs của chính nó, cố định thành một tập Val-aug mới (số lượng x2 Val cũ). Phương pháp 2 (PP2): Các aug cho tập Val được tùy biến mỗi khi load dữ liệu (tương tự cách aug cho tập Train). Mục tiêu của 2 hướng trên là tìm ra một không gian Augmentation của tập Val miêu tả chính xác nhất, team mình sử dụng mô hình Swin Transformer để huấn luyện cho cả 2 phương pháp trên. Mô hình Swin_PP1: Kiến trúc: swin_large_patch4_window12_384 với drop_path_rate: 0.3 Optimizer: LR: 1e-5, Weight decay: 3e-5 LR Scheduler: CosineAnnealingWarmRestarts với T0: 600, T_mult: 1, Eta_min: 1e-7 Train Augmentation: RandomResizedCrop 0.49-1.0, RandomHorizontalFlip 0.5, RandomVerticalFlip 0.2, GaussianBlur kernel_size 3 sigma 0.2-2.0, ColorJitter contrast 0.2. Offline-Augmentation Val: Downsize Image: CenterCrop 0.75 * height - 0.75 * width, Resize 0.375 * height - 0.375 * width. Chuẩn bị dữ liệu Val-aug = tập val gốc + offline-augmentation val. Trainer: Mixed Precision với FP16, Batch size 16, Max 12000 step, validation mỗi epoch, lưu 2 mô hình val loss thấp nhất, lấy mô hình với val accuracy cao nhất trong 2. Mô hình Swin_PP2: Kiến trúc: swin_large_patch4_window12_384 Optimizer: LR: 3e-5, Weight decay: 1e-6 LR Scheduler: StepLR với step_size: 5, gamma: 0.2 Train / Val Augmentation: RandomResizedCrop 0.49-1.0, RandomVerticalFlip 0.2, GaussianBlur kernel_size 3 sigma 0.2-2.0. Trainer: Mixed Precision với FP16, Batch size 16, Max 20 epoch, validation mỗi epoch, lưu 3 mô hình val loss thấp nhất và last checkpoint. Qua thử nghiệm tụi mình chọn last checkpoint. Tụi mình đánh giá 2 mô hình Swin_PP1 và Swin_PP2 nhận thấy mô hình PP1 tốt trên public test 1, mô hình PP2 lại tốt trên public test 2 nên team đã ensemble 2 mô hình lại và thấy mô hình Ensemble này cho kết quả (EER và TEER) tốt hơn. Một số điểm tốt mà tụi mình đánh giá đem lại thành công cho mô hình: Training với Mixed Precision. Team mình từ đầu cuộc thi cũng cân nhắc về time constraint và chú ý training với mixed precision, nó giúp các mô hình tụi mình inference nhanh nhưng hiệu quả không kém mô hình FP32. Ensemble: tụi mình chỉ muốn ensemble 2 mô hình tốt nhất trên các LB, nhưng có lẽ nên dùng nhiều hơn. Augmentation offline và online: team xác định các loại augs phù hợp và tuning :) May mắn: team khá gặp may mắn khi hoàn thành được codebase và baseline khá sớm, cũng như một số may mắn ở trên, tụi mình chỉ phân tích lỗi và để nó tuning liên tục thôi :’) Training code và inference đều nằm trong repo. Mình cảm ơn mọi người đã quan tâm.",,,,,
"Dạ em chào các anh chị,
Em mới tập tành học về machine learning nên có nhiều vấn đề em chưa rõ. Cụ thể là em gặp khó khăn trong bài toán tìm SVM.
Trong bài toán SVM, để tìm nghiệm tối ưu, ta thường đưa về bài toán đối ngẫu (dual problem). Tuy nhiên, vấn đề là bài toán đối ngẫu mà giải bằng hệ KKT cũng sẽ gặp khó khăn (khi số ẩn lớn). Trong quyển machine learning cơ bản của anh Tiệp cũng có nói về khó khăn này và đưa ra hướng giải quyết là sử dụng thuật toán SMO để giải. Em thấy trong bài giảng thầy Andrew cũng nhắc về thuật toán này.
Em cũng có tìm hiểu về nó (đọc bài báo gốc của tác giả John Platt, xem các video trên youtube) nhưng mà em vẫn chưa hình dung được nó hoạt động cụ thể ra sao. Hai video trên youtube mà em đã xem qua đó là
Video 1: https://www.youtube.com/watch?v=Mfp7HQKLSAo&t=909s
Video 2: https://www.youtube.com/watch?v=lHaFpRCKHF8&t=4s.
Em hiểu ý tưởng chung của thuật toán này là (không biết em hiểu đúng không, nhờ mọi người confirm giúp em nhé)
+ Bước 1: Mình sẽ chọn initial guess: Chọn một bộ alpha thỏa mãn các ràng buộc.
+ Bước 2: Chọn 2 phần tử trong đó và tiến hình maximize nó.
Repeat cho tới khi thuật tối hội tụ (thỏa điều kiện KKT)
Vấn đề là: Chọn 2 phần tử như thế nào để bài toán nhanh chóng hội tụ? Hai video em xem cũng không có đề cập tới. Họ nhắc chung chung một câu là ""heuristics to choose these 2 variables"" nhưng em không hiểu câu này lắm :(
Ví dụ xét bài toán SVM trong không gian 2 chiều, với bộ dữ liệu có 5 điểm thôi, thì số trường hợp nếu giải bằng hệ KKT sẽ là 2^5=32 trường hợp.
Nếu giải SMO thì chọn 2 điểm trong 5 điểm sẽ có 10 cách chọn. Nhưng đó chỉ là 1 step, vẫn chưa biết khi nào nó hội tụ. Em chưa thấy được việc thuật toán SMO sẽ nhanh hơn việc giải hệ KKT chỗ nào? (Có lẽ vì em chưa hiểu câu ""heuristics to choose these 2 variables"").
Em có cho thử một bài toán như thế này (SVM with non separable case): Consider two classes of data, where three points $(0.5,0),(2,1),(1,1)$ have the label $+1$ and two points $(1,0),(0,1)$ have the label $-1$. Let's choose $C=5$. Compute the maximum margin hyperplane that separates the two classes of data.
Để cho tiện, em có ghi lại bài toán đối ngẫu của nó (em đính kèm ở hình vẽ)
Mọi người có thể giúp em chạy thử bằng cơm thuật toán SMO để giải quyết bài toán đối ngẫu cho bài toán này không ạ?
Em cảm ơn anh chị đã quan tâm.","Dạ em chào các anh chị, Em mới tập tành học về machine learning nên có nhiều vấn đề em chưa rõ. Cụ thể là em gặp khó khăn trong bài toán tìm SVM. Trong bài toán SVM, để tìm nghiệm tối ưu, ta thường đưa về bài toán đối ngẫu (dual problem). Tuy nhiên, vấn đề là bài toán đối ngẫu mà giải bằng hệ KKT cũng sẽ gặp khó khăn (khi số ẩn lớn). Trong quyển machine learning cơ bản của anh Tiệp cũng có nói về khó khăn này và đưa ra hướng giải quyết là sử dụng thuật toán SMO để giải. Em thấy trong bài giảng thầy Andrew cũng nhắc về thuật toán này. Em cũng có tìm hiểu về nó (đọc bài báo gốc của tác giả John Platt, xem các video trên youtube) nhưng mà em vẫn chưa hình dung được nó hoạt động cụ thể ra sao. Hai video trên youtube mà em đã xem qua đó là Video 1: https://www.youtube.com/watch?v=Mfp7HQKLSAo&t=909s Video 2: https://www.youtube.com/watch?v=lHaFpRCKHF8&t=4s. Em hiểu ý tưởng chung của thuật toán này là (không biết em hiểu đúng không, nhờ mọi người confirm giúp em nhé) + Bước 1: Mình sẽ chọn initial guess: Chọn một bộ alpha thỏa mãn các ràng buộc. + Bước 2: Chọn 2 phần tử trong đó và tiến hình maximize nó. Repeat cho tới khi thuật tối hội tụ (thỏa điều kiện KKT) Vấn đề là: Chọn 2 phần tử như thế nào để bài toán nhanh chóng hội tụ? Hai video em xem cũng không có đề cập tới. Họ nhắc chung chung một câu là ""heuristics to choose these 2 variables"" nhưng em không hiểu câu này lắm :( Ví dụ xét bài toán SVM trong không gian 2 chiều, với bộ dữ liệu có 5 điểm thôi, thì số trường hợp nếu giải bằng hệ KKT sẽ là 2^5=32 trường hợp. Nếu giải SMO thì chọn 2 điểm trong 5 điểm sẽ có 10 cách chọn. Nhưng đó chỉ là 1 step, vẫn chưa biết khi nào nó hội tụ. Em chưa thấy được việc thuật toán SMO sẽ nhanh hơn việc giải hệ KKT chỗ nào? (Có lẽ vì em chưa hiểu câu ""heuristics to choose these 2 variables""). Em có cho thử một bài toán như thế này (SVM with non separable case): Consider two classes of data, where three points $(0.5,0),(2,1),(1,1)$ have the label $+1$ and two points $(1,0),(0,1)$ have the label $-1$. Let's choose $C=5$. Compute the maximum margin hyperplane that separates the two classes of data. Để cho tiện, em có ghi lại bài toán đối ngẫu của nó (em đính kèm ở hình vẽ) Mọi người có thể giúp em chạy thử bằng cơm thuật toán SMO để giải quyết bài toán đối ngẫu cho bài toán này không ạ? Em cảm ơn anh chị đã quan tâm.",,,,,
"Hi các bác, em đang muốn dùng colab để predict model sau đó gửi kết quả về máy local thì có cách nào khả thi ko.
Em cảm ơn ạ Em đã thử pydrive nhưng pydrive chỉ cho em đc kết quả của lần đầu chạy thôi (tức là ko lấy realtime đc)","Hi các bác, em đang muốn dùng colab để predict model sau đó gửi kết quả về máy local thì có cách nào khả thi ko. Em cảm ơn ạ Em đã thử pydrive nhưng pydrive chỉ cho em đc kết quả của lần đầu chạy thôi (tức là ko lấy realtime đc)",,,,,
"This comment and its author have been removed from this group. Vietnamese and English are allowed here, hate speech is not!
This forum is where members are free to share their ideas/opinions about AI/ML topics in either language as long as the original posters are comfortable with the one they choose. A large portion of members in this forum study and work in an English environment, that makes them more fluent in this language when discussing technical topics.","This comment and its author have been removed from this group. Vietnamese and English are allowed here, hate speech is not! This forum is where members are free to share their ideas/opinions about AI/ML topics in either language as long as the original posters are comfortable with the one they choose. A large portion of members in this forum study and work in an English environment, that makes them more fluent in this language when discussing technical topics.",,,,,
"For those who want to try a better grounded version of ChatGPT in Vietnam, this is YouChat - a conversational search feature inside You Search Engine","For those who want to try a better grounded version of ChatGPT in Vietnam, this is YouChat - a conversational search feature inside You Search Engine",,,,,
"Chào mọi người ạ, em là newbie mới học về Deep Learning (cụ thể là RNN).
Em có xem và làm theo video trên youtube bài hướng dẫn về dự đoán doanh số cơ bản sử dụng RNN, nhưng em có gặp tình trạng là khi run model ở các lần khác nhau thì totalLoss càng giảm (như hình), không lần nào giống với lần trước. Các anh chị cho em hỏi tại sao lại xuất hiện tình trạng này vậy ạ?","Chào mọi người ạ, em là newbie mới học về Deep Learning (cụ thể là RNN). Em có xem và làm theo video trên youtube bài hướng dẫn về dự đoán doanh số cơ bản sử dụng RNN, nhưng em có gặp tình trạng là khi run model ở các lần khác nhau thì totalLoss càng giảm (như hình), không lần nào giống với lần trước. Các anh chị cho em hỏi tại sao lại xuất hiện tình trạng này vậy ạ?",,,,,
"Mình thấy có 1 repo nhiều sao (*) về lập trình cho Robot viết bằng ngôn ngữ Python, tại đây https://github.com/AtsushiSakai/PythonRobotics.
Hi vọng nó hữu ích với nhiều bạn.","Mình thấy có 1 repo nhiều sao (*) về lập trình cho Robot viết bằng ngôn ngữ Python, tại đây https://github.com/AtsushiSakai/PythonRobotics. Hi vọng nó hữu ích với nhiều bạn.",,,,,
"em xin chào mọi người, em đang làm đồ án và định làm về bài toán detect người và đếm số lượng để đưa ra cảnh báo. tuy nhiên khi tìm data có 2 bộ dữ liệu: CityPersons https://www.v7labs.com/open-datasets/citypersons và EurocityPersons https://eurocity-dataset.tudelft.nl/ mà em vẫn chưa thể download được. không biết trong group mình đã ai tải 2 tập này chưa có thể share cho em được không ạ.
em cảm ơn mọi người!","em xin chào mọi người, em đang làm đồ án và định làm về bài toán detect người và đếm số lượng để đưa ra cảnh báo. tuy nhiên khi tìm data có 2 bộ dữ liệu: CityPersons https://www.v7labs.com/open-datasets/citypersons và EurocityPersons https://eurocity-dataset.tudelft.nl/ mà em vẫn chưa thể download được. không biết trong group mình đã ai tải 2 tập này chưa có thể share cho em được không ạ. em cảm ơn mọi người!",,,,,
"Hướng dẫn đăng ký, trải nghiệm ChatGPT - AI ChatBot hot nhất hiện nay
ChatGPT của OpenAI đang rất hot trên khắp các diễn đàn. Trong video này mình sẽ chia sẻ với anh em một số thông tin về ChatGPT - một language model cực xịn của OpenAI được fine tune từ GPT3.5 với học giám sát và tăng cường, cách đăng ký để có thể sử dụng từ Việt Nam và trải nghiệm với nó.
Chú ý: Nếu anh em không có thẻ hoặc không tự đăng ký để trải nghiệm được thì có thể ib mình
https://youtu.be/pmEkdkJ0hmg","Hướng dẫn đăng ký, trải nghiệm ChatGPT - AI ChatBot hot nhất hiện nay ChatGPT của OpenAI đang rất hot trên khắp các diễn đàn. Trong video này mình sẽ chia sẻ với anh em một số thông tin về ChatGPT - một language model cực xịn của OpenAI được fine tune từ GPT3.5 với học giám sát và tăng cường, cách đăng ký để có thể sử dụng từ Việt Nam và trải nghiệm với nó. Chú ý: Nếu anh em không có thẻ hoặc không tự đăng ký để trải nghiệm được thì có thể ib mình https://youtu.be/pmEkdkJ0hmg",,,,,
"Các bạn cho mình hỏi về deploy một sklearn model.
Giả sử ta có một pipeline bao gồm feature engineering theo sau bởi một sklearn model. Thường thì với các built-in transformers và built-in sklearn model thì có thể dump rồi load pickle rồi chạy độc lập không cần code ban đầu. Tuy nhiên trong phần feature engineering có rất nhiều custom code khiến cho việc chỉ lưu file pickle thì không đủ mà còn phải lưu cả code. Vấn đề xảy ra là khi code thay đổi một chút thì input vào model cũng có khẳ năng thay đổi, dẫn đến việc phải lưu cả version cua code cùng với file pickle kia. Một giải pháp mình đang dùng là lưu cả file pickle và git commit hash tại thời điểm file pickle được tạo ra, ở serving thì git checkout commit đó rồi inference.
Mình nghĩ cách này không được tối ưu, muốn tham khảo các bạn xem có giải pháp nào tốt hơn không.
Trong quá trình tìm kiếm các giải pháp, mình thấy dill (link trong comment) được cho là có khả năng lưu cả code nhưng dill là third-party nên mình không biết có phải cách tốt nhất không.
Cảm ơn các bạn","Các bạn cho mình hỏi về deploy một sklearn model. Giả sử ta có một pipeline bao gồm feature engineering theo sau bởi một sklearn model. Thường thì với các built-in transformers và built-in sklearn model thì có thể dump rồi load pickle rồi chạy độc lập không cần code ban đầu. Tuy nhiên trong phần feature engineering có rất nhiều custom code khiến cho việc chỉ lưu file pickle thì không đủ mà còn phải lưu cả code. Vấn đề xảy ra là khi code thay đổi một chút thì input vào model cũng có khẳ năng thay đổi, dẫn đến việc phải lưu cả version cua code cùng với file pickle kia. Một giải pháp mình đang dùng là lưu cả file pickle và git commit hash tại thời điểm file pickle được tạo ra, ở serving thì git checkout commit đó rồi inference. Mình nghĩ cách này không được tối ưu, muốn tham khảo các bạn xem có giải pháp nào tốt hơn không. Trong quá trình tìm kiếm các giải pháp, mình thấy dill (link trong comment) được cho là có khả năng lưu cả code nhưng dill là third-party nên mình không biết có phải cách tốt nhất không. Cảm ơn các bạn",,,,,
"Giới thiệu thư viện mới thay thế Pandas có tên là Polars (https://github.com/pola-rs/polars). Thư viện này được viết bằng ngôn ngữ Rust, nhưng được đóng gói cả bằng Python. Mình test thử quả thật nó rất nhanh, nhanh hơn nhiều lần so với Pandas và nhiều thư viện phổ biến khác nữa. Về benchmark của Polars (trên dữ liệu 10^9 dòng và 9 cột) so với các thư viện khác, các bạn có thể xem thêm ở đây (https://h2oai.github.io/db-benchmark/; https://www.pola.rs/benchmarks.html). Nhóm tác giả có viết hướng dẫn cho thư viện bằng cả ngôn ngữ Rust và Python tại đây (https://pola-rs.github.io/polars-book/user-guide/introduction.html).","Giới thiệu thư viện mới thay thế Pandas có tên là Polars (https://github.com/pola-rs/polars). Thư viện này được viết bằng ngôn ngữ Rust, nhưng được đóng gói cả bằng Python. Mình test thử quả thật nó rất nhanh, nhanh hơn nhiều lần so với Pandas và nhiều thư viện phổ biến khác nữa. Về benchmark của Polars (trên dữ liệu 10^9 dòng và 9 cột) so với các thư viện khác, các bạn có thể xem thêm ở đây (https://h2oai.github.io/db-benchmark/; https://www.pola.rs/benchmarks.html). Nhóm tác giả có viết hướng dẫn cho thư viện bằng cả ngôn ngữ Rust và Python tại đây (https://pola-rs.github.io/polars-book/user-guide/introduction.html).",,,,,
"VinAI Seminar - ""Fairness in Natural Language Processing""
Register here [https://lnkd.in/gJnhj2rm] to access seminar via Ms. Teams.
Speaker: Tim Baldwin, Assoc. Provost at MBZUAI and Melbourne Laureate Prof. at The University of Melbourne
Time: 2:00 pm - 3:00 pm (GMT+7), Tue, Dec 20, 2022","VinAI Seminar - ""Fairness in Natural Language Processing"" Register here [https://lnkd.in/gJnhj2rm] to access seminar via Ms. Teams. Speaker: Tim Baldwin, Assoc. Provost at MBZUAI and Melbourne Laureate Prof. at The University of Melbourne Time: 2:00 pm - 3:00 pm (GMT+7), Tue, Dec 20, 2022",,,,,
"Các anh chị có hiểu biết về machine learning, đại khái là về K- Means Clustering (Đề tài: Phân Cụm Màu Sắc )
hiện tại trong quá trình tìm hiểu và học thì em có gặp một chút khó khăn, mong anh chị nào đi ngang qua, hiểu rõ có thể để lại 1 cmt để e có thể hoàn thiện về cái project này với ạ !! Sau đây là những khó khăn em gặp phải ạ.
**Khó Khăn:
0) Làm sao để tìm ra K ?
- Cái này thì em vẫn chưa hiểu ra cách để tìm K ạ.
1) Trục tung, trục hoành làm sao để hiểu nó trong elbow graph ?
- như em tìm hiểu thì phần nằm ngang nó là giá trị của K) vậy thì còn trục nằm dọc thì làm sao để kiếm được giá trị của nó ạ(Distortion)
2) Thế nào là cụm dữ liệu không thay đổi ?
- cái này thì e kiếm tài liệu có tựa tựa là khi “trọng tâm không thay đổi” e không biết là có đúng không, nếu sai thì các anh chị cho e xin định nghĩa đúng về nó với ạ
3) Hàm gì không thay đổi ?
-như e tìm hiểu thì là hàm mất mát không thay đổi, nhưng e nghe các anh chị khac bảo là hàm này rất khó hiểu, vậy thì còn hàm nào không thay đổi nữa k ạ
-4) Với trọng tâm khởi tạo khác nhau, thì trọng tâm có thay đổi không
- thì em nghĩ là có, nhưng tại vì sao thì e chưa rõ, mong được giải đáp ạ.
5) điều kiện dừng là gì ??
- Theo em thì khi trọng tâm không thay đổi nữa thì nó sẽ dừng
***
Ở trên là những khó khăn khi e tìm hiểu về K means, và đưa ra những đáp án mà e tìm được thông qua tài liệu, nên có thể sai lệch, mong anh chị có kiến thức về K mean Clustering có thể hỗ trợ em giải đáp về các hướng đi cũng như các khó khăn của e khi tìm hiểu ở phía trên với ạ.
EM XIN CẢM ƠN !!!!","Các anh chị có hiểu biết về machine learning, đại khái là về K- Means Clustering (Đề tài: Phân Cụm Màu Sắc ) hiện tại trong quá trình tìm hiểu và học thì em có gặp một chút khó khăn, mong anh chị nào đi ngang qua, hiểu rõ có thể để lại 1 cmt để e có thể hoàn thiện về cái project này với ạ !! Sau đây là những khó khăn em gặp phải ạ. **Khó Khăn: 0) Làm sao để tìm ra K ? - Cái này thì em vẫn chưa hiểu ra cách để tìm K ạ. 1) Trục tung, trục hoành làm sao để hiểu nó trong elbow graph ? - như em tìm hiểu thì phần nằm ngang nó là giá trị của K) vậy thì còn trục nằm dọc thì làm sao để kiếm được giá trị của nó ạ(Distortion) 2) Thế nào là cụm dữ liệu không thay đổi ? - cái này thì e kiếm tài liệu có tựa tựa là khi “trọng tâm không thay đổi” e không biết là có đúng không, nếu sai thì các anh chị cho e xin định nghĩa đúng về nó với ạ 3) Hàm gì không thay đổi ? -như e tìm hiểu thì là hàm mất mát không thay đổi, nhưng e nghe các anh chị khac bảo là hàm này rất khó hiểu, vậy thì còn hàm nào không thay đổi nữa k ạ -4) Với trọng tâm khởi tạo khác nhau, thì trọng tâm có thay đổi không - thì em nghĩ là có, nhưng tại vì sao thì e chưa rõ, mong được giải đáp ạ. 5) điều kiện dừng là gì ?? - Theo em thì khi trọng tâm không thay đổi nữa thì nó sẽ dừng *** Ở trên là những khó khăn khi e tìm hiểu về K means, và đưa ra những đáp án mà e tìm được thông qua tài liệu, nên có thể sai lệch, mong anh chị có kiến thức về K mean Clustering có thể hỗ trợ em giải đáp về các hướng đi cũng như các khó khăn của e khi tìm hiểu ở phía trên với ạ. EM XIN CẢM ƠN !!!!",,,,,
"mn cho em hỏi cách hoặc trang web áp dụng Reinforcement Learning vào bài toán controlling traffic light với ạ
Em cám ơn ạ",mn cho em hỏi cách hoặc trang web áp dụng Reinforcement Learning vào bài toán controlling traffic light với ạ Em cám ơn ạ,,,,,
"Hi các bác, em đang tìm hiểu bài toán table structure recognition.
Em không chuyên CV nên không biết có heuristic nào từ các đường phân cách (12 grids) thành 9 cells (relative cells) không ạ ?
Các bác cứ tự do cho em keywords, hướng của các bác, em vô cùng cảm kích.
Em cảm ơn các bác nhiều ạ.
#tabledetection #tablestructurerecognition","Hi các bác, em đang tìm hiểu bài toán table structure recognition. Em không chuyên CV nên không biết có heuristic nào từ các đường phân cách (12 grids) thành 9 cells (relative cells) không ạ ? Các bác cứ tự do cho em keywords, hướng của các bác, em vô cùng cảm kích. Em cảm ơn các bác nhiều ạ.",#tabledetection	#tablestructurerecognition,,,,
"Chào mọi người, em là newbie, em đang tìm thuật toán đúng nhất để xây dựng một recommendation model (gợi ý một list category khi người dùng thao tác với một hoặc nhiều items), cho một bộ dữ liệu(datasets) với format CSV như sau:
+) ~1500 items, ~2000 categories. Một product có nhiều categories và ngược lại.
+) 1 triệu data interactions (dữ liệu users mua item, không mua ko tính vào interaction) - user nào, mua item gì, ở thời gian nào
+) 300k users với thông tin về nhóm tuổi, giới tính
##############################
Input: 1 list unique ItemIDs
Output: 1 list unique Categories
Kết quả phải dựa trên những thông tin sau:
categories của item + thông tin tuổi + giới tính của user
##############################
Do không có kinh nghiệm và thời gian quá gấp nên dự án phải dùng AWS Sagemaker để build. Em cũng đã tìm hiểu về supervised, unsupervised và RL, one-hot-encode. Em cũng đã đọc và làm sample với nhiều thuật toán khác nhau(Factorization Machine) nhưng vẫn chưa tìm được thuật toán đúng.
Mong mọi người chỉ giáo và suggest thuật toán Machine Learning dùng được cho bài toán này ạ. Hiện tại em đang đọc về thằng Linear Learner. Em cảm ơn ạ!","Chào mọi người, em là newbie, em đang tìm thuật toán đúng nhất để xây dựng một recommendation model (gợi ý một list category khi người dùng thao tác với một hoặc nhiều items), cho một bộ dữ liệu(datasets) với format CSV như sau: +) ~1500 items, ~2000 categories. Một product có nhiều categories và ngược lại. +) 1 triệu data interactions (dữ liệu users mua item, không mua ko tính vào interaction) - user nào, mua item gì, ở thời gian nào +) 300k users với thông tin về nhóm tuổi, giới tính ############################## Input: 1 list unique ItemIDs Output: 1 list unique Categories Kết quả phải dựa trên những thông tin sau: categories của item + thông tin tuổi + giới tính của user ############################## Do không có kinh nghiệm và thời gian quá gấp nên dự án phải dùng AWS Sagemaker để build. Em cũng đã tìm hiểu về supervised, unsupervised và RL, one-hot-encode. Em cũng đã đọc và làm sample với nhiều thuật toán khác nhau(Factorization Machine) nhưng vẫn chưa tìm được thuật toán đúng. Mong mọi người chỉ giáo và suggest thuật toán Machine Learning dùng được cho bài toán này ạ. Hiện tại em đang đọc về thằng Linear Learner. Em cảm ơn ạ!",,,,,
"Dạ chào anh chị, lại là em đây. Hôm nay em trở lại với một câu hỏi về Transformer.
Vì việc nạp tất cả các từ trong câu vào trong mạng sẽ dẫn đến việc không xác định được thứ tự của từ nên tác giả có đề xuất thêm phần ""Positional Encoding"" vào ma trận biểu diễn input. Tác giả cũng đưa ra giả thuyết rằng việc lựa chọn hàm sin, cos đem lại hiệu quả. Vậy nếu em dùng 1 hàm tăng giá trị theo tuyến tính thay vì hàm đề xuất thì có ảnh hưởng nhiều đến hiệu quả không? (giả sử độ dài input là 512 thì em sẽ cộng thêm 1 ma trận nó có dạng [1, 2, 3, ..., 512] vào mỗi hàng cảu ma trận input)
Em xin cảm ơn
#NLP","Dạ chào anh chị, lại là em đây. Hôm nay em trở lại với một câu hỏi về Transformer. Vì việc nạp tất cả các từ trong câu vào trong mạng sẽ dẫn đến việc không xác định được thứ tự của từ nên tác giả có đề xuất thêm phần ""Positional Encoding"" vào ma trận biểu diễn input. Tác giả cũng đưa ra giả thuyết rằng việc lựa chọn hàm sin, cos đem lại hiệu quả. Vậy nếu em dùng 1 hàm tăng giá trị theo tuyến tính thay vì hàm đề xuất thì có ảnh hưởng nhiều đến hiệu quả không? (giả sử độ dài input là 512 thì em sẽ cộng thêm 1 ma trận nó có dạng [1, 2, 3, ..., 512] vào mỗi hàng cảu ma trận input) Em xin cảm ơn",#NLP,,,,
"Xin chào mọi người, mình đang thực hiện dự án web nghe nhạc giờ muốn nghiên cứu tìm kiếm theo giai điệu nhưng không biết tìm đọc tài liệu gì .xin mọi người chỉ giúp","Xin chào mọi người, mình đang thực hiện dự án web nghe nhạc giờ muốn nghiên cứu tìm kiếm theo giai điệu nhưng không biết tìm đọc tài liệu gì .xin mọi người chỉ giúp",,,,,
"Xin chào mọi người. Mình đại diện cho nhóm Cool Water hôm nay xin chia sẻ solution cho đề bài Liveness Detection trong cuộc thi Zalo AI Challenge 2022.
Chung cuộc thì nhóm mình may mắn đứng thứ nhất về điểm T-EER (Time + EER) công bố ở Zalo AI Summit ngày 17/12/2022.
Solution cho đề bài này nhóm mình sử dụng kỹ thuật ensemble các mô hình ở các resolution khác nhau để tránh bị overfit trên bộ private và có sử dụng thêm một số kỹ thuật nhỏ như heavy augmentation, label smoothing, training EMA để cải thiện điểm CV lẫn LB. Tuy nhiên do ensemble khá nhiều mô hình nên tốc độ nhóm mình gần như chậm gấp đôi so với các đội khác 😅
Code và một số kết quả thử nghiệm của nhóm mọi người có thể tham khảo tại đây:
https://github.com/hungk64it1x/zac-2022
Lời cuối mình xin chân thành cảm ơn BTC Zalo AI đã tổ chức một cuộc thi về AI rất thú vị, thử thách và đầy ý nghĩa thực tiễn. Chúc cho nhiều năm tới BTC sẽ tổ chức được nhiều cuộc thi về AI thật thành công hơn nữa.
 — với Biên Dương Văn và 2 người khác.","Xin chào mọi người. Mình đại diện cho nhóm Cool Water hôm nay xin chia sẻ solution cho đề bài Liveness Detection trong cuộc thi Zalo AI Challenge 2022. Chung cuộc thì nhóm mình may mắn đứng thứ nhất về điểm T-EER (Time + EER) công bố ở Zalo AI Summit ngày 17/12/2022. Solution cho đề bài này nhóm mình sử dụng kỹ thuật ensemble các mô hình ở các resolution khác nhau để tránh bị overfit trên bộ private và có sử dụng thêm một số kỹ thuật nhỏ như heavy augmentation, label smoothing, training EMA để cải thiện điểm CV lẫn LB. Tuy nhiên do ensemble khá nhiều mô hình nên tốc độ nhóm mình gần như chậm gấp đôi so với các đội khác Code và một số kết quả thử nghiệm của nhóm mọi người có thể tham khảo tại đây: https://github.com/hungk64it1x/zac-2022 Lời cuối mình xin chân thành cảm ơn BTC Zalo AI đã tổ chức một cuộc thi về AI rất thú vị, thử thách và đầy ý nghĩa thực tiễn. Chúc cho nhiều năm tới BTC sẽ tổ chức được nhiều cuộc thi về AI thật thành công hơn nữa. — với Biên Dương Văn và 2 người khác.",,,,,
"Xin chào mọi người, mình xin đại diện cho team VTS-HTML chia sẻ solution cho track Lyric Alignment trong cuộc thi Zalo AI Challenge 2022 (Top 3 Private test).
*Tổng quan về solution của nhóm:
- Tách vocal từ audio gốc
- Xử lý noise trong datasets
- Xử lý misalignment của GT: sử dụng pseudo-labels với những samples audio có IoU thấp < 0.7 (giữa pseudo-label từ pretrained và GT của BTC)
- Fine-tune Acoustics models với dataset ở bước 2 và align bằng thuật toán Viterbi forced alignment
- Post-process blank giữa dự đoán của 2 từ liên kề
Chi tiết kỹ thuật được trình bày ở repo: https://github.com/vieduy/zac2022-lyric-alignment
Qua đây xin cảm ơn BTC Zalo AI Challenge đã tổ chức một cuộc thi vô cùng bổ ích. Hy vọng chúng ta sẽ có thêm nhiều cuộc thi AI mang tính thực tiễn hơn trong tương lai.","Xin chào mọi người, mình xin đại diện cho team VTS-HTML chia sẻ solution cho track Lyric Alignment trong cuộc thi Zalo AI Challenge 2022 (Top 3 Private test). *Tổng quan về solution của nhóm: - Tách vocal từ audio gốc - Xử lý noise trong datasets - Xử lý misalignment của GT: sử dụng pseudo-labels với những samples audio có IoU thấp < 0.7 (giữa pseudo-label từ pretrained và GT của BTC) - Fine-tune Acoustics models với dataset ở bước 2 và align bằng thuật toán Viterbi forced alignment - Post-process blank giữa dự đoán của 2 từ liên kề Chi tiết kỹ thuật được trình bày ở repo: https://github.com/vieduy/zac2022-lyric-alignment Qua đây xin cảm ơn BTC Zalo AI Challenge đã tổ chức một cuộc thi vô cùng bổ ích. Hy vọng chúng ta sẽ có thêm nhiều cuộc thi AI mang tính thực tiễn hơn trong tương lai.",,,,,
"Em chào anh chị. Em có một mô hình đơn giản như này để nhận diện một hệ thống.
Giả sử em cho nhận diện hàm y=3x, vậy kết quả dy/dx tính theo mạng neuron có cho bằng 3 không ạ?","Em chào anh chị. Em có một mô hình đơn giản như này để nhận diện một hệ thống. Giả sử em cho nhận diện hàm y=3x, vậy kết quả dy/dx tính theo mạng neuron có cho bằng 3 không ạ?",,,,,
"Xin chào mọi người hôm nay mình đại diện Team Telegram chia sẻ solution cho các task E2E Question Answering và task Lyric Alignment trong chuỗi các cuộc thi Zalo AI Challenge 2022.
Team mình tập trung hơn vào task NLP - E2E Question Answering, solution của nhóm xoay quanh việc search candidates (2 stage) và re-ranking (2 stage) cùng với đó là finetuning MRCQA model, pseudo labeling, chi tiết hơn về pipeline các bạn có thể xem ở repo (link ở bên dưới), còn về task Lyric Alignment nhận thấy dữ liệu training quá noise và tracking điểm local, visualize sample data, public leaderboard không consistent nên team quyết định sử dụng solution đơn giản và finish trong tuần đầu của challenge.
UPDATE: Kết quả chung cuộc công bố tại Zalo AI Submit (17/12/2022) thì bài E2E Question Answering được Top 1 Private Leaderboard (time and accuracy) và bài Lyric Alignment được Top 2 Private Leaderboard (time and iou metric).
Code cho Solution của các Task mọi người có thể đọc và tham khảo ở đây (mọi người ai có câu hỏi gì có thể hỏi ở Issues của repo hoặc bình luận ở bài viết này, nếu mọi người thấy hữu ích có thể cho nhóm một Star 😀
Task Lyric Alignment: https://github.com/Telegram-Zalo/zac2022-lyric-alignment
Task E2E Question Answering: https://github.com/Telegram-Zalo/zac2022-e2e-qa
Cuối cùng mình xin được cảm ơn BTC cuộc thi Zalo AI Challenge đã tổ chức những cuộc thi vô cùng thú vị và đầy thử thách, hy vọng những năm tới cuộc thi sẽ được tổ chức thành công hơn nữa. Cảm ơn tất cả các đội thi cùng nhau thi đấu, nỗ lực cải thiện giải pháp, BXH thay đổi liên tục cũng là động lực cho team mình cố gắng hơn nữa. Đặc biệt mình xin được cảm ơn đến 2 anh mentor của mình đó là anh Khôi Tuấn Nguyễn và anh Nguyễn Quán Anh Minh trong suốt thời gian tham gia cuộc thi.
P/s: Lưu ý Code và README đang ở trong giai đoạn refactor và cleaning.
README sẽ được cập nhật chi tiết hơn.
 — với Nguyễn Quán Anh Minh và Khôi Tuấn Nguyễn.","Xin chào mọi người hôm nay mình đại diện Team Telegram chia sẻ solution cho các task E2E Question Answering và task Lyric Alignment trong chuỗi các cuộc thi Zalo AI Challenge 2022. Team mình tập trung hơn vào task NLP - E2E Question Answering, solution của nhóm xoay quanh việc search candidates (2 stage) và re-ranking (2 stage) cùng với đó là finetuning MRCQA model, pseudo labeling, chi tiết hơn về pipeline các bạn có thể xem ở repo (link ở bên dưới), còn về task Lyric Alignment nhận thấy dữ liệu training quá noise và tracking điểm local, visualize sample data, public leaderboard không consistent nên team quyết định sử dụng solution đơn giản và finish trong tuần đầu của challenge. UPDATE: Kết quả chung cuộc công bố tại Zalo AI Submit (17/12/2022) thì bài E2E Question Answering được Top 1 Private Leaderboard (time and accuracy) và bài Lyric Alignment được Top 2 Private Leaderboard (time and iou metric). Code cho Solution của các Task mọi người có thể đọc và tham khảo ở đây (mọi người ai có câu hỏi gì có thể hỏi ở Issues của repo hoặc bình luận ở bài viết này, nếu mọi người thấy hữu ích có thể cho nhóm một Star Task Lyric Alignment: https://github.com/Telegram-Zalo/zac2022-lyric-alignment Task E2E Question Answering: https://github.com/Telegram-Zalo/zac2022-e2e-qa Cuối cùng mình xin được cảm ơn BTC cuộc thi Zalo AI Challenge đã tổ chức những cuộc thi vô cùng thú vị và đầy thử thách, hy vọng những năm tới cuộc thi sẽ được tổ chức thành công hơn nữa. Cảm ơn tất cả các đội thi cùng nhau thi đấu, nỗ lực cải thiện giải pháp, BXH thay đổi liên tục cũng là động lực cho team mình cố gắng hơn nữa. Đặc biệt mình xin được cảm ơn đến 2 anh mentor của mình đó là anh Khôi Tuấn Nguyễn và anh Nguyễn Quán Anh Minh trong suốt thời gian tham gia cuộc thi. P/s: Lưu ý Code và README đang ở trong giai đoạn refactor và cleaning. README sẽ được cập nhật chi tiết hơn. — với Nguyễn Quán Anh Minh và Khôi Tuấn Nguyễn.",,,,,
"Dạ chào anh chị trong gr, nay em lại có thắc mắc như sau ạ. Khi tìm hiểu thì em được biết vấn đề của các DNNs(Deep neural networks) ở thời gian đầu là gradient có thể bị vanishing/exploding. Với Exploding thì ta có thể dùng Gradient Clipping hoặc Gradient Scaling. Nhưng với Vanishing thì lại chỉ nhận được hướng dẫn là sử dụng các hàm có dạng ReLU để tránh đưa đạo hàm về 1 đoạn giá trị cố định như hàm Tanh hoặc Sigmod. Vậy em có câu hỏi sau:
1. Liệu các hàm dạng ReLU có thực sự giải quyết triệt để vấn đề Vanishing không?
2. Ngoài việc chọn activity function thì ta còn có những hướng giải quyết nào cho vấn đề này?
Em xin cảm ơn
#RNN #ML","Dạ chào anh chị trong gr, nay em lại có thắc mắc như sau ạ. Khi tìm hiểu thì em được biết vấn đề của các DNNs(Deep neural networks) ở thời gian đầu là gradient có thể bị vanishing/exploding. Với Exploding thì ta có thể dùng Gradient Clipping hoặc Gradient Scaling. Nhưng với Vanishing thì lại chỉ nhận được hướng dẫn là sử dụng các hàm có dạng ReLU để tránh đưa đạo hàm về 1 đoạn giá trị cố định như hàm Tanh hoặc Sigmod. Vậy em có câu hỏi sau: 1. Liệu các hàm dạng ReLU có thực sự giải quyết triệt để vấn đề Vanishing không? 2. Ngoài việc chọn activity function thì ta còn có những hướng giải quyết nào cho vấn đề này? Em xin cảm ơn",#RNN	#ML,,,,
"GIỚI THIỆU TRANG WEB HAY ĐỂ ĐỌC VÀ TỔNG HỢP CÁC BÀI BÁO
Chào anh/chị/các bạn,
Gần đây em/mình được giới thiệu về Research Rabbit (https://www.researchrabbit.ai/), một công cụ tuyệt vời để đọc và tổng hợp các tài liệu tham khảo cho phần Literature review. Công cụ này cho phép tìm kiếm các bài báo (bao gồm các bài báo tương tự với những bài đã lưu), hiển thị bản tóm tắt (abstract) các bài báo đó và đề xuất công trình nghiên cứu liên quan.
Không chỉ vậy Research Rabbit còn cho phép người dùng chia sẻ các bộ sưu tập (collections) cho người khác và hỗ trợ việc trích xuất các tài liệu tham khảo một cách dễ dàng thông qua Shareable link và BibTeX (hình minh họa).
Hy vọng bài này sẽ giúp ích cho các nhà nghiên cứu tìm kiếm, quản lý tài liệu tham khảo nhanh và hiệu quả hơn so với các phương pháp truyền thống.
Nguồn tham khảo: https://twitter.com/MushtaqBilalPhD/status/1583392303183712256","GIỚI THIỆU TRANG WEB HAY ĐỂ ĐỌC VÀ TỔNG HỢP CÁC BÀI BÁO Chào anh/chị/các bạn, Gần đây em/mình được giới thiệu về Research Rabbit (https://www.researchrabbit.ai/), một công cụ tuyệt vời để đọc và tổng hợp các tài liệu tham khảo cho phần Literature review. Công cụ này cho phép tìm kiếm các bài báo (bao gồm các bài báo tương tự với những bài đã lưu), hiển thị bản tóm tắt (abstract) các bài báo đó và đề xuất công trình nghiên cứu liên quan. Không chỉ vậy Research Rabbit còn cho phép người dùng chia sẻ các bộ sưu tập (collections) cho người khác và hỗ trợ việc trích xuất các tài liệu tham khảo một cách dễ dàng thông qua Shareable link và BibTeX (hình minh họa). Hy vọng bài này sẽ giúp ích cho các nhà nghiên cứu tìm kiếm, quản lý tài liệu tham khảo nhanh và hiệu quả hơn so với các phương pháp truyền thống. Nguồn tham khảo: https://twitter.com/MushtaqBilalPhD/status/1583392303183712256",,,,,
"Hiện tại em/mình đang cần tối ưu hoá cho việc inference model trên CPU. Mọi người có kinh nghiệm có thể share 1 số phương pháp được không ạ. Sử dụng static, dynamic quantize và pruning thì bị trade off acc nên e đã bỏ ra khỏi các option. E có ý convert sang jax dùng framework flax, tuy nhiên e có thử vs 1 mạng neural chỉ gồm cnn nhỏ và thấy trên cpu thậm chí jax chậm hơn gấp đôi torch. Đưa về C++ không biết có thể giảm đc nhiều thời gian hơn không hay còn có các cách khác, mong mọi người cho thêm cao kiến ạ.","Hiện tại em/mình đang cần tối ưu hoá cho việc inference model trên CPU. Mọi người có kinh nghiệm có thể share 1 số phương pháp được không ạ. Sử dụng static, dynamic quantize và pruning thì bị trade off acc nên e đã bỏ ra khỏi các option. E có ý convert sang jax dùng framework flax, tuy nhiên e có thử vs 1 mạng neural chỉ gồm cnn nhỏ và thấy trên cpu thậm chí jax chậm hơn gấp đôi torch. Đưa về C++ không biết có thể giảm đc nhiều thời gian hơn không hay còn có các cách khác, mong mọi người cho thêm cao kiến ạ.",,,,,
"Xin chào mọi người, kết quả 2 teams lọt vào top2 tạm thời đã có của các cuộc thi Zalo Challenge 2022. Tuy chưa biết score cuối cùng cũng như team nào sẽ là team chiến thắng chung cuộc, hôm nay, mình đại diện cho team Telegram, KHIÊU CHIẾN team Famers về performance của giải pháp hai đội side by side trên tập private test bao gồm 465 samples.
Dưới đây là kết quả của mô hình hai đội mời các bạn xem qua ạ. (https://www.youtube.com/watch?v=wcwMFT9QiTc).
Xin cảm ơn Zalo vì đã đem đến một cuộc thi thú vị và rất thực tiễn. Với kết quả trên của 2 giải pháp, mình tin rằng cả 2 giải pháp đều đã đủ tốt để áp dụng cho các bài toán thực tế.
Nếu các bạn có câu hỏi hay nhận xét gì về performance của cả hai giải pháp thì đừng ngần ngại comment phía dưới ạ.
Cảm ơn mọi người 🥲🥲🥲, chúc mọi người có ngày cuối tuần vui vẻ 😊😊😊.
 — đang cảm thấy trống vắng cùng với Khánh Vũ Duy và 3 người khác.","Xin chào mọi người, kết quả 2 teams lọt vào top2 tạm thời đã có của các cuộc thi Zalo Challenge 2022. Tuy chưa biết score cuối cùng cũng như team nào sẽ là team chiến thắng chung cuộc, hôm nay, mình đại diện cho team Telegram, KHIÊU CHIẾN team Famers về performance của giải pháp hai đội side by side trên tập private test bao gồm 465 samples. Dưới đây là kết quả của mô hình hai đội mời các bạn xem qua ạ. (https://www.youtube.com/watch?v=wcwMFT9QiTc). Xin cảm ơn Zalo vì đã đem đến một cuộc thi thú vị và rất thực tiễn. Với kết quả trên của 2 giải pháp, mình tin rằng cả 2 giải pháp đều đã đủ tốt để áp dụng cho các bài toán thực tế. Nếu các bạn có câu hỏi hay nhận xét gì về performance của cả hai giải pháp thì đừng ngần ngại comment phía dưới ạ. Cảm ơn mọi người , chúc mọi người có ngày cuối tuần vui vẻ . — đang cảm thấy trống vắng cùng với Khánh Vũ Duy và 3 người khác.",,,,,
Mình đang đọc về PhoBert và có một thắc mắc nhỏ. Khi PhoBert làm việc với Tiếng Việt có dấu và không dấu thì hiệu quả có khác nhau nhiều không? Mình đọc paper của PhoBert thì không thấy nhắc đến tập dữ liệu TV không dấu. Không biết bạn nào review vấn đề này chưa.,Mình đang đọc về PhoBert và có một thắc mắc nhỏ. Khi PhoBert làm việc với Tiếng Việt có dấu và không dấu thì hiệu quả có khác nhau nhiều không? Mình đọc paper của PhoBert thì không thấy nhắc đến tập dữ liệu TV không dấu. Không biết bạn nào review vấn đề này chưa.,,,,,
"Hi các bạn,
Lần này, thay vì dùng AI để hỗ trợ chơi game qua cử chỉ tay hoặc khuôn mặt, mình đã huấn luyện 1 mô hình AI để có thể tự chơi game mà không cần con người can thiệp. Cụ thể, mình đã huấn luyện AI với thuật toán Deep Q-learning trong 2 triệu vòng lặp với tựa game Flappy Bird, và kết quả là AI của mình có thể chơi game mãi mãi mà không chết bao giờ. Các bạn có thể tự kiểm chứng với source code của mình. Chạy chương trình, xong đi ngủ, sáng hôm sau dậy bạn vẫn sẽ thấy AI đang chơi game 😎😎😎
Qua video này, mình muốn hướng dẫn các bạn những khái niệm cơ bản nhất về Reinforcement learning (học tăng cường) cùng với 1 trong những thuật toán cơ bản nhất của mảng này - Deep Q-learning. Học tăng cường ít phổ biến hơn 2 mảng còn lại của học máy, nhưng những ứng dụng của mảng này thú vị không hề kém cạnh đâu nhé 🤩

I hope you enjoy it!
Full video: https://youtu.be/hGO0ztpA19g

#ReinforcementLearning #flappybird","Hi các bạn, Lần này, thay vì dùng AI để hỗ trợ chơi game qua cử chỉ tay hoặc khuôn mặt, mình đã huấn luyện 1 mô hình AI để có thể tự chơi game mà không cần con người can thiệp. Cụ thể, mình đã huấn luyện AI với thuật toán Deep Q-learning trong 2 triệu vòng lặp với tựa game Flappy Bird, và kết quả là AI của mình có thể chơi game mãi mãi mà không chết bao giờ. Các bạn có thể tự kiểm chứng với source code của mình. Chạy chương trình, xong đi ngủ, sáng hôm sau dậy bạn vẫn sẽ thấy AI đang chơi game Qua video này, mình muốn hướng dẫn các bạn những khái niệm cơ bản nhất về Reinforcement learning (học tăng cường) cùng với 1 trong những thuật toán cơ bản nhất của mảng này - Deep Q-learning. Học tăng cường ít phổ biến hơn 2 mảng còn lại của học máy, nhưng những ứng dụng của mảng này thú vị không hề kém cạnh đâu nhé I hope you enjoy it! Full video: https://youtu.be/hGO0ztpA19g",#ReinforcementLearning	#flappybird,,,,
"Khai phá dữ liệu quyết định chi ăn nhà hàng. Với bài này nên dùng hồi quy, phân lớp hay cây quyết định ạ.","Khai phá dữ liệu quyết định chi ăn nhà hàng. Với bài này nên dùng hồi quy, phân lớp hay cây quyết định ạ.",,,,,
"[ChatGPT: Optimizing Language Models for Dialogue]

Gần đây OpenAI cho ra mắt mô hình ChatGPT, giúp người dùng có thể tương tác AI theo hình thức Chatbot. ChatGPT có thể hiểu nội dung cuộc nói chuyện và trả lời các câu hỏi của người dùng. Khả năng giao tiếp và trả lời của ChatGPT có thể sẽ khiến mọi người ngạc nhiên: từ viết code, viết latex, đến viết truyện, viết văn, làm assignment,....

Chi tiết ở đây: https://openai.com/blog/chatgpt/
Demo: https://chat.openai.com/","[ChatGPT: Optimizing Language Models for Dialogue] Gần đây OpenAI cho ra mắt mô hình ChatGPT, giúp người dùng có thể tương tác AI theo hình thức Chatbot. ChatGPT có thể hiểu nội dung cuộc nói chuyện và trả lời các câu hỏi của người dùng. Khả năng giao tiếp và trả lời của ChatGPT có thể sẽ khiến mọi người ngạc nhiên: từ viết code, viết latex, đến viết truyện, viết văn, làm assignment,.... Chi tiết ở đây: https://openai.com/blog/chatgpt/ Demo: https://chat.openai.com/",,,,,
"Chào mọi người, đại diện nhóm Shadow chicken sua gau gau (Liveness detection track, Zalo AI Challenge 2022, top 6 private test) mình xin phép chia sẻ solution của nhóm:
-  Sau quá trình experiment, nhóm mình phát hiện ra RandomResizedCrop tốt cho tập dữ liệu public test 2, kích thước ảnh nhỏ hơn thì bớt overfit hơn trên tập public test 2, kích thước ảnh to hơn thì đạt điểm CV cao hơn, nên nhóm mình quyết định ensemble đa dạng kích thước ảnh. Ngoài ra nhóm mình sử dụng thêm cutout và mixup data augmentation. Optimizer sử dụng là ADAN. Lựa chọn model dựa vào accuracy metric. Tập pub 1 thì còn thấy nhiễu vân, tập pub 2 thì nhìn mỏi mắt chẳng thấy cái nào. nếu ban tổ chức cho phép pseudo label thì tốt, kết quả sử dụng model train bằng pseudo pub1 đạt điểm cao hơn trên pub 2. Sử dụng toàn bộ bức ảnh để training thay vì chỉ crop khuôn mặt. Mỗi epoch, mỗi video lấy ra ngẫu nhiên 1 frame để training. Eval bằng cách mỗi video lấy ra 5 frames cách đều nhau, sau đó voting.
-  Kết quả trên leaderboard2 (0.00855) là ensemble của 5 loại  model, mỗi loại 5 fold, bao gồm:
convnext_base_384_in22ft1k, train image size   384, test image size 448, sử dụng ảnh  YCBCB để training
convnext_base_in22ft1k, train image size   224, test image size 288, ảnh RGB
swin_large_patch4_window12_384_in22k, train   image size 384, test image size 384, ảnh RGB, model này đạt   CV cao nhất.
swin_large_patch4_window7_224_in22k,  train   image size 224, test image size 224, ảnh RGB. Sử dụng yolov7   để detect mặt, sau đó sử dụng box khuôn mặt để   random che đi khuôn mặt khi training, vì có nhiều diễn   viên xuất hiện trong cả tập training và public test 1,   public test 2, và xuất hiện ở cả trong nhiều video, nên   muốn che đi khuôn mặt để model không nhớ khuôn mặt   của diễn viên.
efficientnet B5, train image size 512, test   image size 618, ảnh BGR. Model này dự đoán ảnh real rất   tốt.
TTA 3 lần, bao gồm ảnh gốc, ảnh   hlip, ảnh vlip. Lựa chọn ra 11 frame cách đều nhau từ   video để voting, do nhóm mình quan sát thấy có một số   video kết quả inference xuất hiện cả đoạn real và   fake xen kẽ nhau.
- Kết quả trên private test (0.06373), do  ban đầu không để ý đến time constraint “The minimum  accepted running speed is real time. If the total inference time is  longer than the total length of videos, the result will not be  evaluated.” , nên nhóm mình chỉ tập trung vào tối ưu  hóa metric eer, đến thứ 7 sát ngày nộp mới biết về  time constraint. Vì vậy, kết quả submit chỉ còn là ensemble của 4  model (Trừ đi efficientnet B5), tổng cộng 20 weights, lấy  ra 8 frame từ mỗi video sau đó vlip 4 frame, tổng cộng mỗi  video phải inference 12 ảnh. Quantize tất cả weight model  xuống float16. Ensemble của 4 loại model (20 weights) đạt  Public 2 test eer score  = 0.019, sau khi quantize thì eer giảm  xuống còn 0.02137
- Bất ngờ khi tải xuống tập private  test, nhóm mình nhận ra nhiều diễn viên xuất hiện trong  private test xuất hiện trong cả training dataset và pub 1,  pub 2. Nếu muốn đánh giá tốt hơn, nhóm mình nghĩ nên  lưu lại id của diễn viên và tránh việc một diễn viên  xuất hiện trong cả tập train và tập test.
- Một số hướng đang thử nhưng chưa thành công, DeePixBiS, arcface, stack các frame trong video theo chiều channel thành ảnh 2.5D nhưng train không hội tụ, sử dụng pyslowfast kết quả đạt accuracy khả quan (Hướng này có nhiều tiềm năng, do chưa tunning mà accuracy đã xấp xỉ 96, 97%, model sử dụng là X3D) nhưng nhóm không theo hướng giải pháp này do không có thời gian để tunning mô hình.
- Training source code public tại:  https://github.com/VTCC-uTVM/Zalo-AI-Challenge-2022-Liveness-Detection
 — với Trần Mạnh Tùng và Nguyễn Văn Phúc.","Chào mọi người, đại diện nhóm Shadow chicken sua gau gau (Liveness detection track, Zalo AI Challenge 2022, top 6 private test) mình xin phép chia sẻ solution của nhóm: - Sau quá trình experiment, nhóm mình phát hiện ra RandomResizedCrop tốt cho tập dữ liệu public test 2, kích thước ảnh nhỏ hơn thì bớt overfit hơn trên tập public test 2, kích thước ảnh to hơn thì đạt điểm CV cao hơn, nên nhóm mình quyết định ensemble đa dạng kích thước ảnh. Ngoài ra nhóm mình sử dụng thêm cutout và mixup data augmentation. Optimizer sử dụng là ADAN. Lựa chọn model dựa vào accuracy metric. Tập pub 1 thì còn thấy nhiễu vân, tập pub 2 thì nhìn mỏi mắt chẳng thấy cái nào. nếu ban tổ chức cho phép pseudo label thì tốt, kết quả sử dụng model train bằng pseudo pub1 đạt điểm cao hơn trên pub 2. Sử dụng toàn bộ bức ảnh để training thay vì chỉ crop khuôn mặt. Mỗi epoch, mỗi video lấy ra ngẫu nhiên 1 frame để training. Eval bằng cách mỗi video lấy ra 5 frames cách đều nhau, sau đó voting. - Kết quả trên leaderboard2 (0.00855) là ensemble của 5 loại model, mỗi loại 5 fold, bao gồm: convnext_base_384_in22ft1k, train image size 384, test image size 448, sử dụng ảnh YCBCB để training convnext_base_in22ft1k, train image size 224, test image size 288, ảnh RGB swin_large_patch4_window12_384_in22k, train image size 384, test image size 384, ảnh RGB, model này đạt CV cao nhất. swin_large_patch4_window7_224_in22k, train image size 224, test image size 224, ảnh RGB. Sử dụng yolov7 để detect mặt, sau đó sử dụng box khuôn mặt để random che đi khuôn mặt khi training, vì có nhiều diễn viên xuất hiện trong cả tập training và public test 1, public test 2, và xuất hiện ở cả trong nhiều video, nên muốn che đi khuôn mặt để model không nhớ khuôn mặt của diễn viên. efficientnet B5, train image size 512, test image size 618, ảnh BGR. Model này dự đoán ảnh real rất tốt. TTA 3 lần, bao gồm ảnh gốc, ảnh hlip, ảnh vlip. Lựa chọn ra 11 frame cách đều nhau từ video để voting, do nhóm mình quan sát thấy có một số video kết quả inference xuất hiện cả đoạn real và fake xen kẽ nhau. - Kết quả trên private test (0.06373), do ban đầu không để ý đến time constraint “The minimum accepted running speed is real time. If the total inference time is longer than the total length of videos, the result will not be evaluated.” , nên nhóm mình chỉ tập trung vào tối ưu hóa metric eer, đến thứ 7 sát ngày nộp mới biết về time constraint. Vì vậy, kết quả submit chỉ còn là ensemble của 4 model (Trừ đi efficientnet B5), tổng cộng 20 weights, lấy ra 8 frame từ mỗi video sau đó vlip 4 frame, tổng cộng mỗi video phải inference 12 ảnh. Quantize tất cả weight model xuống float16. Ensemble của 4 loại model (20 weights) đạt Public 2 test eer score = 0.019, sau khi quantize thì eer giảm xuống còn 0.02137 - Bất ngờ khi tải xuống tập private test, nhóm mình nhận ra nhiều diễn viên xuất hiện trong private test xuất hiện trong cả training dataset và pub 1, pub 2. Nếu muốn đánh giá tốt hơn, nhóm mình nghĩ nên lưu lại id của diễn viên và tránh việc một diễn viên xuất hiện trong cả tập train và tập test. - Một số hướng đang thử nhưng chưa thành công, DeePixBiS, arcface, stack các frame trong video theo chiều channel thành ảnh 2.5D nhưng train không hội tụ, sử dụng pyslowfast kết quả đạt accuracy khả quan (Hướng này có nhiều tiềm năng, do chưa tunning mà accuracy đã xấp xỉ 96, 97%, model sử dụng là X3D) nhưng nhóm không theo hướng giải pháp này do không có thời gian để tunning mô hình. - Training source code public tại: https://github.com/VTCC-uTVM/Zalo-AI-Challenge-2022-Liveness-Detection — với Trần Mạnh Tùng và Nguyễn Văn Phúc.",,,,,
"Một thư viện khá hay, x400 nhanh hơn weka (: :)
https://github.com/mikeizbicki/HLearn","Một thư viện khá hay, x400 nhanh hơn weka (: :) https://github.com/mikeizbicki/HLearn",,,,,
"Mọi người cho em hỏi làm thế nào để có thể sử dụng gpu với mediapipe vậy ạ. Mặc dù máy tính của em có sử dụng gpu rtx 2070 và đã cài đầy đủ CUDA CUDnn tuy nhiên khi em sử dụng mediapipe và kiểm tra gpu thì chỉ thấy gpu hoạt động 5% đồng thời nó hiện lên thông báo"" INFO: Created TensorFlow Lite XNNPACK delegate for CPU"" trong quá trình nhận dạng. Mong mọi người có thể giúp em khắc phục nó, em cảm ơn ạ.","Mọi người cho em hỏi làm thế nào để có thể sử dụng gpu với mediapipe vậy ạ. Mặc dù máy tính của em có sử dụng gpu rtx 2070 và đã cài đầy đủ CUDA CUDnn tuy nhiên khi em sử dụng mediapipe và kiểm tra gpu thì chỉ thấy gpu hoạt động 5% đồng thời nó hiện lên thông báo"" INFO: Created TensorFlow Lite XNNPACK delegate for CPU"" trong quá trình nhận dạng. Mong mọi người có thể giúp em khắc phục nó, em cảm ơn ạ.",,,,,
"Nếu mọi người quan tâm đến AI và tech thì không nên bỏ qua sự kiện Zalo AI Summit 2022 - diễn đàn công nghệ do Zalo tổ chức mỗi năm.
Đây là dịp để mọi người có thể nghe những chuyên gia chia sẻ về những sản phẩm, bài toán công nghệ mới; chiêm ngưỡng những sản phẩm lần đầu trình làng; giao lưu với cộng đồng đam mê AI và tech
Mọi năm, Zalo AiI Summit chỉ dành cho các kỹ sư Zalo, nhưng năm nay thì mở cửa cho những ai quan tâm đến lĩnh vực này. Vậy nên mọi người đăng kí tham gia thông qua website nếu có hứng thú nhé!
https://summit.zalo.ai/","Nếu mọi người quan tâm đến AI và tech thì không nên bỏ qua sự kiện Zalo AI Summit 2022 - diễn đàn công nghệ do Zalo tổ chức mỗi năm. Đây là dịp để mọi người có thể nghe những chuyên gia chia sẻ về những sản phẩm, bài toán công nghệ mới; chiêm ngưỡng những sản phẩm lần đầu trình làng; giao lưu với cộng đồng đam mê AI và tech Mọi năm, Zalo AiI Summit chỉ dành cho các kỹ sư Zalo, nhưng năm nay thì mở cửa cho những ai quan tâm đến lĩnh vực này. Vậy nên mọi người đăng kí tham gia thông qua website nếu có hứng thú nhé! https://summit.zalo.ai/",,,,,
"Các bạn cho hỏi (theo kinh nghiệm cá nhân), kiến trúc model (cho chủ đề #objectdetection) nào có khả năng phát hiện chính xác nhiều vật thể nhỏ nhất trong mỗi khung hình? Xin cảm ơn các bạn trước nhé!","Các bạn cho hỏi (theo kinh nghiệm cá nhân), kiến trúc model (cho chủ đề nào có khả năng phát hiện chính xác nhiều vật thể nhỏ nhất trong mỗi khung hình? Xin cảm ơn các bạn trước nhé!",#objectdetection),,,,
"Chào mọi người, trước tiên mình muốn chúc mừng các team của cuộc thi Zalo AI 2022 đã hoàn thành cuộc thi và có những kết quả hết sức ấn tượng. Năm nay mình tham gia cuộc thi dưới danh nghĩa team New Dad tranh tài ở task Lyric Alignment. New Dad có may mắn đứng nhất ở public leaderboard tuy nhiên ở private leaderboard thì chỉ đứng ở vị trí thứ 7. Tuy vậy mình vẫn muốn chia sẻ giải pháp mình đã sử dụng và public source code cùng data, hi vọng có thể giúp ích cho mọi người. Tóm tắt thì hướng tiếp cận của mình bao gồm:
- Xử lý lyric dạng viết sang dạng nói (số, ngày tháng, từ ngoại lai).
- Crawl data 30.000 bài hát ~ 1500h audio music (nhìn chung mình không sử dụng data của BTC để train model).
- Finetune ASR model cho music audio.
- Audio & lyric align sử dụng phương pháp CTC force alignment.
Chi tiết code finetune, infer và align mọi người xem theo link github: https://github.com/nguyenvulebinh/lyric-alignment
1500h data music & lyric: https://huggingface.co/datasets/nguyenvulebinh/song_dataset","Chào mọi người, trước tiên mình muốn chúc mừng các team của cuộc thi Zalo AI 2022 đã hoàn thành cuộc thi và có những kết quả hết sức ấn tượng. Năm nay mình tham gia cuộc thi dưới danh nghĩa team New Dad tranh tài ở task Lyric Alignment. New Dad có may mắn đứng nhất ở public leaderboard tuy nhiên ở private leaderboard thì chỉ đứng ở vị trí thứ 7. Tuy vậy mình vẫn muốn chia sẻ giải pháp mình đã sử dụng và public source code cùng data, hi vọng có thể giúp ích cho mọi người. Tóm tắt thì hướng tiếp cận của mình bao gồm: - Xử lý lyric dạng viết sang dạng nói (số, ngày tháng, từ ngoại lai). - Crawl data 30.000 bài hát ~ 1500h audio music (nhìn chung mình không sử dụng data của BTC để train model). - Finetune ASR model cho music audio. - Audio & lyric align sử dụng phương pháp CTC force alignment. Chi tiết code finetune, infer và align mọi người xem theo link github: https://github.com/nguyenvulebinh/lyric-alignment 1500h data music & lyric: https://huggingface.co/datasets/nguyenvulebinh/song_dataset",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2022 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2022 vào comment của post này.",,,,,
Tách nhanh mp3 từ video với Python 😊,Tách nhanh mp3 từ video với Python,,,,,
"Xin giới thiệu với mọi người một bài báo xuất sắc từ Dr. Michael A. Lones, được viết rất dễ hiểu về cách để tránh những cạm bẫy trong Machine Learning. Bài báo được trình bày rất trực quan dưới dạng Do and Don't.
Đối với các tiền bối thì những điều trong này chắc chắn là đã nằm lòng, nhưng đối với các bạn mới tiếp cận ML hay những nhà nghiên cứu ở lĩnh vực khác thì bài báo này rất hữu ích, giúp cho các bạn lãnh hội ngay ít nhất 6 tháng-1 năm kinh nghiệm và tránh lãng phí vài trăm giờ train model không hiệu quả.
Link bài báo:
https://arxiv.org/abs/2108.02497","Xin giới thiệu với mọi người một bài báo xuất sắc từ Dr. Michael A. Lones, được viết rất dễ hiểu về cách để tránh những cạm bẫy trong Machine Learning. Bài báo được trình bày rất trực quan dưới dạng Do and Don't. Đối với các tiền bối thì những điều trong này chắc chắn là đã nằm lòng, nhưng đối với các bạn mới tiếp cận ML hay những nhà nghiên cứu ở lĩnh vực khác thì bài báo này rất hữu ích, giúp cho các bạn lãnh hội ngay ít nhất 6 tháng-1 năm kinh nghiệm và tránh lãng phí vài trăm giờ train model không hiệu quả. Link bài báo: https://arxiv.org/abs/2108.02497",,,,,
"Kính chào các bác!
Đúng lời hứa chia sẻ cùng anh em một bài về Định giá SIM số. Dạy máy tính làm thầy SIM SỐ nào :D
Bài này áp dụng nhiều về LSTM và xử lý dữ liệu đầu vào nhé!
Hi vọng giúp được anh em mới học!",Kính chào các bác! Đúng lời hứa chia sẻ cùng anh em một bài về Định giá SIM số. Dạy máy tính làm thầy SIM SỐ nào :D Bài này áp dụng nhiều về LSTM và xử lý dữ liệu đầu vào nhé! Hi vọng giúp được anh em mới học!,,,,,
"Chào mọi người, nhóm mình cũng mới tham gia Zalo AI Challenge 2022 (Lyric Track, top 2 private test) cũng muốn share code solution của nhóm 😀
Training/model definition mọi người có thể tham khảo ở đây [https://github.com/anhvth/WKaraokeMaker]
Ngoài ra mình có thêm  1 scripts để mọi người có thể visualize kết quả như karaoke ( demo video karaoke https://www.youtube.com/watch?v=16RjtOygs7o) và có demo colab, pretrained cho bạn nào muốn thử thêm 😀, nếu bạn thấy repo hữu ích thì cho nhóm xin 1 ngôi sao hy vọng ^^
Có câu hỏi hay ý kiến đóng góp gì các bạn cứ open issue hoặc comment bên dưới cũng được
Cảm ơn mọi người 😀!
 — với Nguyễn Văn Bảo Khánh.","Chào mọi người, nhóm mình cũng mới tham gia Zalo AI Challenge 2022 (Lyric Track, top 2 private test) cũng muốn share code solution của nhóm Training/model definition mọi người có thể tham khảo ở đây [https://github.com/anhvth/WKaraokeMaker] Ngoài ra mình có thêm 1 scripts để mọi người có thể visualize kết quả như karaoke ( demo video karaoke https://www.youtube.com/watch?v=16RjtOygs7o) và có demo colab, pretrained cho bạn nào muốn thử thêm , nếu bạn thấy repo hữu ích thì cho nhóm xin 1 ngôi sao hy vọng ^^ Có câu hỏi hay ý kiến đóng góp gì các bạn cứ open issue hoặc comment bên dưới cũng được Cảm ơn mọi người ! — với Nguyễn Văn Bảo Khánh.",,,,,
"Xin chào mọi người hôm nay em đại diện nhóm CTA Zero9 (Trùng hợp là team về thứ 9 🙄) chia sẻ solution cho challenge Lyric Alignment trong cuộc thi Zalo AI Challenge 2022. Nhóm em cũng đã học được kha khá kiến thức về xử lí audio thông qua challenge này luôn. 🤣

Source Lyric Alignment: https://github.com/vnk8071/CTA-Zero9-ZAIC2022-Lyric-Alignment

Sơ lược về solution, nhóm em chạy cả 2 solution song song bao gồm:
Wav2Vec: Rất tiềm năng nhưng nhóm không thể tìm ra cách tối ưu solution. 
MFA (Montreal Forced Aligner): Model này dùng làm baseline nhưng về sau lại được cải thiện nhiều và dùng để làm solution chính.
Team tụi em xin được cảm ơn BTC cuộc thi Zalo AI Challenge đã tổ chức cuộc thi thường niên với những challenges vô cùng thú vị và thử thách. Mong những năm tới cuộc thi sẽ càng có nhiều bạn tham gia hơn nữa để leaderboard thêm phần sôi động ạ. 
Tụi em rất mong nhận được góp ý của cách anh chị để cải thiện solution ạ. 
 — với Khôi VN.","Xin chào mọi người hôm nay em đại diện nhóm CTA Zero9 (Trùng hợp là team về thứ 9 ) chia sẻ solution cho challenge Lyric Alignment trong cuộc thi Zalo AI Challenge 2022. Nhóm em cũng đã học được kha khá kiến thức về xử lí audio thông qua challenge này luôn. Source Lyric Alignment: https://github.com/vnk8071/CTA-Zero9-ZAIC2022-Lyric-Alignment Sơ lược về solution, nhóm em chạy cả 2 solution song song bao gồm: Wav2Vec: Rất tiềm năng nhưng nhóm không thể tìm ra cách tối ưu solution. MFA (Montreal Forced Aligner): Model này dùng làm baseline nhưng về sau lại được cải thiện nhiều và dùng để làm solution chính. Team tụi em xin được cảm ơn BTC cuộc thi Zalo AI Challenge đã tổ chức cuộc thi thường niên với những challenges vô cùng thú vị và thử thách. Mong những năm tới cuộc thi sẽ càng có nhiều bạn tham gia hơn nữa để leaderboard thêm phần sôi động ạ. Tụi em rất mong nhận được góp ý của cách anh chị để cải thiện solution ạ. — với Khôi VN.",,,,,
"MÌnh có một đề bài dự đoán giá sim, không biết nên chia thành các features gì và hướng đi ntn cho hợp lý mong mn giải đáp","MÌnh có một đề bài dự đoán giá sim, không biết nên chia thành các features gì và hướng đi ntn cho hợp lý mong mn giải đáp",,,,,
"Can ChatGPT pass the MLE interview?
Here is my git repo to validate the hypothesis by sampling questions from HuyenChip and Data Science. 🥸",Can ChatGPT pass the MLE interview? Here is my git repo to validate the hypothesis by sampling questions from HuyenChip and Data Science.,,,,,
"Chào các anh chị trong nhóm, hiện tại em đang làm về bài toán Information Retrieval thì em có tìm hiểu và thấy thuật toán BIRCH và các paper phát triển từ phương pháp này đạt hiệu quả cao (xem tại https://aclanthology.org/D19-3004/). Em có thắc mắc là:
1. Phương pháp này có nhược điểm gì? 
2. Có định hướng giải quyết nào cho các nhược điểm trên không? 
3. Ngoài phương pháp này thì còn những phương pháp nào cho hiệu quả cao hơn với bài toán này không?
Em xin cảm ơn
#nlp #IR","Chào các anh chị trong nhóm, hiện tại em đang làm về bài toán Information Retrieval thì em có tìm hiểu và thấy thuật toán BIRCH và các paper phát triển từ phương pháp này đạt hiệu quả cao (xem tại https://aclanthology.org/D19-3004/). Em có thắc mắc là: 1. Phương pháp này có nhược điểm gì? 2. Có định hướng giải quyết nào cho các nhược điểm trên không? 3. Ngoài phương pháp này thì còn những phương pháp nào cho hiệu quả cao hơn với bài toán này không? Em xin cảm ơn",#nlp	#IR,,,,
"[Data Engine - Tesla]
""Data scientists spend 80% of their time cleaning data"" - Nay mình chia sẻ về Data Engine của Tesla để mọi người thấy tầm quan trọng của dữ liệu khi xây dựng mô hình.","[Data Engine - Tesla] ""Data scientists spend 80% of their time cleaning data"" - Nay mình chia sẻ về Data Engine của Tesla để mọi người thấy tầm quan trọng của dữ liệu khi xây dựng mô hình.",,,,,
"Chào mn ạ. Em đang gặp vấn đề này mong mn giúp em. Em đang tự mày mò về machine learning để build ra một model dự đoán giá của cổ phiếu bằng mô hình ARIMA. Em làm theo hướng dẫn của blog này (https://phamdinhkhanh.github.io/2019/12/12/ARIMAmodel.html).
Cụ thể thì em muốn dự đoán giá đóng cửa của mã cố phiếu ASG. Em đã dùng package vnquant để thu thập dữ liệu rồi tính chuỗi lợi suất. Em cũng đã dùng kiểm định Argument dickey fuller trên chuỗi để đảm bảo đây là chuỗi dừng. Em cũng vẽ biểu đồ ACF và PACF của chuỗi. Dựa vào biểu đồ này, em sẽ chọn mô hình ARIMA(3,0,3).
Tập training của e chính là chuỗi này trong khoảng thời gian 11 tháng đầu tiên của năm 2021(244 điểm dữ liệu) còn tập test là tháng 12/2021(23 điểm dữ liệu). Em đã làm hết những gì mình biết nhưng mô hình của e vẫn bị underfitting nghiêm trọng trên tập test. Mn cho e hỏi là giờ e phải làm gì để cải thiện độ chính xác của mô hình không ạ?","Chào mn ạ. Em đang gặp vấn đề này mong mn giúp em. Em đang tự mày mò về machine learning để build ra một model dự đoán giá của cổ phiếu bằng mô hình ARIMA. Em làm theo hướng dẫn của blog này (https://phamdinhkhanh.github.io/2019/12/12/ARIMAmodel.html). Cụ thể thì em muốn dự đoán giá đóng cửa của mã cố phiếu ASG. Em đã dùng package vnquant để thu thập dữ liệu rồi tính chuỗi lợi suất. Em cũng đã dùng kiểm định Argument dickey fuller trên chuỗi để đảm bảo đây là chuỗi dừng. Em cũng vẽ biểu đồ ACF và PACF của chuỗi. Dựa vào biểu đồ này, em sẽ chọn mô hình ARIMA(3,0,3). Tập training của e chính là chuỗi này trong khoảng thời gian 11 tháng đầu tiên của năm 2021(244 điểm dữ liệu) còn tập test là tháng 12/2021(23 điểm dữ liệu). Em đã làm hết những gì mình biết nhưng mô hình của e vẫn bị underfitting nghiêm trọng trên tập test. Mn cho e hỏi là giờ e phải làm gì để cải thiện độ chính xác của mô hình không ạ?",,,,,
"Chào mọi người ạ, em đang thực hiện fine tune các mô hình bert cho các task tiếng Việt. Các mô hình nhìn chung là đều chạy, tuy nhiên khi em sử dụng đến mô hình phobert-base của VinAI thì colab báo lỗi như thế này ạ, em cũng thử trên kaggle thì lỗi tương tự. Hy vọng mọi người cho em hướng giải quyết ạ, em cảm ơn ạ!","Chào mọi người ạ, em đang thực hiện fine tune các mô hình bert cho các task tiếng Việt. Các mô hình nhìn chung là đều chạy, tuy nhiên khi em sử dụng đến mô hình phobert-base của VinAI thì colab báo lỗi như thế này ạ, em cũng thử trên kaggle thì lỗi tương tự. Hy vọng mọi người cho em hướng giải quyết ạ, em cảm ơn ạ!",,,,,
"Không biết ở đây có bác nào làm về dự đoán tỉ số các trận đấu thuộc các giải vòng tròn Round-Robin chưa, nếu có thì em có 1 thắc mắc là em dùng KNN, Bayes cũng như XGBoost, khi log_loss càng thấp thì tỉ lệ dự đoán hòa càng thấp, thậm chí là không có luôn vậy ạ?","Không biết ở đây có bác nào làm về dự đoán tỉ số các trận đấu thuộc các giải vòng tròn Round-Robin chưa, nếu có thì em có 1 thắc mắc là em dùng KNN, Bayes cũng như XGBoost, khi log_loss càng thấp thì tỉ lệ dự đoán hòa càng thấp, thậm chí là không có luôn vậy ạ?",,,,,
Chào mọi người ạ. Em đang cần làm 1 bài toán về thay đổi giọng nói từ người này thành người khác. Khi em search thì toàn ra thay đổi voice từ người thành giọng robot hoặc có tiếng vang. Không biết anh chị nào từng làm rồi có thể cho em keyword để search được không ạ. Em cảm ơn ạ,Chào mọi người ạ. Em đang cần làm 1 bài toán về thay đổi giọng nói từ người này thành người khác. Khi em search thì toàn ra thay đổi voice từ người thành giọng robot hoặc có tiếng vang. Không biết anh chị nào từng làm rồi có thể cho em keyword để search được không ạ. Em cảm ơn ạ,,,,,
Chào mọi người! Em đang build một cái model như hình bên dưới ạ. Lúc training thì mô hình của em tương tự như Siamese Neural Network. Nhưng lúc em sử dụng mô hình thì em chỉ muốn đưa vào 1 input nhận 1 output. Hiện tại ý tưởng của em là nhân đôi cái input lên nhưng như vậy thời gian predict sẽ tăng x2. Mọi người có ý tưởng nào tốt hơn thì góp ý giúp em với ạ!,Chào mọi người! Em đang build một cái model như hình bên dưới ạ. Lúc training thì mô hình của em tương tự như Siamese Neural Network. Nhưng lúc em sử dụng mô hình thì em chỉ muốn đưa vào 1 input nhận 1 output. Hiện tại ý tưởng của em là nhân đôi cái input lên nhưng như vậy thời gian predict sẽ tăng x2. Mọi người có ý tưởng nào tốt hơn thì góp ý giúp em với ạ!,,,,,
"Hiện tại e đang tham gia cuộc thi RSNA kaggle tổng giải thưởng $50,000. Cuộc thi bắt đầu được 2 ngày và kết thúc vào 28/2. Nhóm e được 3tv và mới mong muốn tìm thêm 1 mentor join team.Mn hứng thú với cuộc thi này thì ib e để trao đổi","Hiện tại e đang tham gia cuộc thi RSNA kaggle tổng giải thưởng $50,000. Cuộc thi bắt đầu được 2 ngày và kết thúc vào 28/2. Nhóm e được 3tv và mới mong muốn tìm thêm 1 mentor join team.Mn hứng thú với cuộc thi này thì ib e để trao đổi",,,,,
"Chào a chị, cho e hỏi về svm,làm sao tính được w và b,mỗi bài toán ạ. E đang cần nhận dạng ảnh, ảnh đó có dấu hiệu (m nào đó),e bắt đầu từ ảnh [3x3].Mong ad duyệt bài sớm ạ.","Chào a chị, cho e hỏi về svm,làm sao tính được w và b,mỗi bài toán ạ. E đang cần nhận dạng ảnh, ảnh đó có dấu hiệu (m nào đó),e bắt đầu từ ảnh [3x3].Mong ad duyệt bài sớm ạ.",,,,,
"Em chào các anh chị ạ.
Hiện nay em đang cố giải bài toán POS tagging bằng Hidden Markov Model và rất mong các anh chị có thể góp ý ạ. Cụ thể hướng đi hiện nay của em là maximize likelihood của chuỗi các từ trong câu trong sử dụng Viterbi algorithm và đang đạt khoảng 83-84% accuracy on test (mix cả những từ chưa gặp và đã gặp). Em vẫn muốn cài thiện thêm và hiện đây là những vấn đề em nghĩ có thể giải quyết và mong được nghe ý kiến của mọi người ạ:
1. Ambiguous tag: Trong train và test set có 1 số những từ có ambiguous tag khi vai trò của từ chưa rõ ràng (ví dụ như ""developed"" : VVD-VVN). Em không rõ cách tốt nhất để disambiguate những trường hợp như thế này ạ? Và có nên tập trung vào việc này ko ạ?
2. Unseen word: Hiện tại em dùng Laplace smoothing để tránh probability = 0 cho những từ ko có trong training. Tuy nhiên em ko chắc đây là cách tốt nhất ạ.
Em cảm ơn group nhiều ạ!
https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4
#NLP","Em chào các anh chị ạ. Hiện nay em đang cố giải bài toán POS tagging bằng Hidden Markov Model và rất mong các anh chị có thể góp ý ạ. Cụ thể hướng đi hiện nay của em là maximize likelihood của chuỗi các từ trong câu trong sử dụng Viterbi algorithm và đang đạt khoảng 83-84% accuracy on test (mix cả những từ chưa gặp và đã gặp). Em vẫn muốn cài thiện thêm và hiện đây là những vấn đề em nghĩ có thể giải quyết và mong được nghe ý kiến của mọi người ạ: 1. Ambiguous tag: Trong train và test set có 1 số những từ có ambiguous tag khi vai trò của từ chưa rõ ràng (ví dụ như ""developed"" : VVD-VVN). Em không rõ cách tốt nhất để disambiguate những trường hợp như thế này ạ? Và có nên tập trung vào việc này ko ạ? 2. Unseen word: Hiện tại em dùng Laplace smoothing để tránh probability = 0 cho những từ ko có trong training. Tuy nhiên em ko chắc đây là cách tốt nhất ạ. Em cảm ơn group nhiều ạ! https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4",#NLP,,,,
"Xin chào mọi người ạ.
Em là beginner vừa học Machine Learning được vài tháng thôi ạ. Em muốn tìm một bạn/anh chị nào đó học chung.
Em và 1 bạn Trung Quốc nữa đang làm contest này, anh chị nào vừa học ML ở mức cơ bản nếu có thời gian hãy tham gia cùng bọn em và chia sẻ kiến thức .
Cám ơn mọi người nhiều.","Xin chào mọi người ạ. Em là beginner vừa học Machine Learning được vài tháng thôi ạ. Em muốn tìm một bạn/anh chị nào đó học chung. Em và 1 bạn Trung Quốc nữa đang làm contest này, anh chị nào vừa học ML ở mức cơ bản nếu có thời gian hãy tham gia cùng bọn em và chia sẻ kiến thức . Cám ơn mọi người nhiều.",,,,,
"Em có model như này và em có set 1000 điểm (x,y)
Anh chị có thể chỉ cho em cách tạo neural network cũng như train đc ko ạ :(","Em có model như này và em có set 1000 điểm (x,y) Anh chị có thể chỉ cho em cách tạo neural network cũng như train đc ko ạ :(",,,,,
"Chào mọi người, hiện tại em đang giải quyết 1 bài tập lớn liên quan đến mạng neural kết hợp cùng hệ mờ, hệ thống đang được đưa về dạng gần giống với ANFIS(Adaptive neuro fuzzy inference system), trong quá trình tìm hiểu, em có 1 số thắc mắc:
Layer đóng vai trò là hàm thành viên với tham số là a và b của neuron(Gỉa sử chọn hàm thành viên là hàm tam giác), quá trình học cho mạng neuron chính là để tìm ra 2 số a và b tối ưu cho hệ thống. Như ở hình 2 thì em đã xác định được cách tính a và b cho mỗi luật tạo thành với trọng số của luật tương ứng với Wi. Câu hỏi em muốn đặt ra là ví dụ với hình 1, A1 liên kết với cả R1,R2,R3 thì quá trình back-propagation sẽ diễn ra như thế nào để điều chỉnh a và b của A1. Vì trong công thức trong hình 2, a và b đều đang chỉ điều chỉnh dựa trên trọng số của 1 luật Wi chứ không phải là cả 3 luật như hình.
Em cảm ơn mọi người.","Chào mọi người, hiện tại em đang giải quyết 1 bài tập lớn liên quan đến mạng neural kết hợp cùng hệ mờ, hệ thống đang được đưa về dạng gần giống với ANFIS(Adaptive neuro fuzzy inference system), trong quá trình tìm hiểu, em có 1 số thắc mắc: Layer đóng vai trò là hàm thành viên với tham số là a và b của neuron(Gỉa sử chọn hàm thành viên là hàm tam giác), quá trình học cho mạng neuron chính là để tìm ra 2 số a và b tối ưu cho hệ thống. Như ở hình 2 thì em đã xác định được cách tính a và b cho mỗi luật tạo thành với trọng số của luật tương ứng với Wi. Câu hỏi em muốn đặt ra là ví dụ với hình 1, A1 liên kết với cả R1,R2,R3 thì quá trình back-propagation sẽ diễn ra như thế nào để điều chỉnh a và b của A1. Vì trong công thức trong hình 2, a và b đều đang chỉ điều chỉnh dựa trên trọng số của 1 luật Wi chứ không phải là cả 3 luật như hình. Em cảm ơn mọi người.",,,,,
"Em chào mọi người ạ, em đang là sinh viên và có mong muốn học Machine Learning bằng cách tự học để tham gia các cuộc thi và dự án nghiên cứu. Hiện tại em đã có tài liệu lý thuyết về Machine Learning, có nền tảng lập trình và có lộ trình tự học như sau : 
Đọc tài liệu để hiểu cơ sở. 
Làm bài tập nhỏ đề rà soát lại kiến thức và hiểu sâu hơn về những vấn đề được học.
Kết hợp những kĩ năng để tham gia vào các project từ nhỏ tới lớn để tích lũy thêm kinh nghiệm và kiến thức.
Em cũng đã tham khảo các kênh, trang nhưng hầu hết đều chỉ dừng lại mức lý thuyết hoặc là đi vào thực hành project luôn (có thể do em chưa tham gia bất cứ online course nào). Do đó em mong được các bậc tiền bối ở đây có thể chia sẻ giúp em những trang training và góp ý xây dựng để em có thể tiến bộ hơn trong tương lai. Em xin cảm ơn ạ. ","Em chào mọi người ạ, em đang là sinh viên và có mong muốn học Machine Learning bằng cách tự học để tham gia các cuộc thi và dự án nghiên cứu. Hiện tại em đã có tài liệu lý thuyết về Machine Learning, có nền tảng lập trình và có lộ trình tự học như sau : Đọc tài liệu để hiểu cơ sở. Làm bài tập nhỏ đề rà soát lại kiến thức và hiểu sâu hơn về những vấn đề được học. Kết hợp những kĩ năng để tham gia vào các project từ nhỏ tới lớn để tích lũy thêm kinh nghiệm và kiến thức. Em cũng đã tham khảo các kênh, trang nhưng hầu hết đều chỉ dừng lại mức lý thuyết hoặc là đi vào thực hành project luôn (có thể do em chưa tham gia bất cứ online course nào). Do đó em mong được các bậc tiền bối ở đây có thể chia sẻ giúp em những trang training và góp ý xây dựng để em có thể tiến bộ hơn trong tương lai. Em xin cảm ơn ạ.",,,,,
"Trong Status trước của mình hỏi về việc #objectdetection với vật thể nhỏ và rất nhỏ ở đây (shorturl.at/dmpvY). Mình đang làm thí nghiệm theo các gợi ý của các bạn. Tuy nhiên, cũng dạng dataset này, nhưng với #Semanticsegmentation mình thử nghiệm với Segformer (bài báo tại đây https://arxiv.org/abs/2105.15203) thì thấy kết quả khá khả quan. Câu hỏi đặt ra, có bạn nào biết có ai đó đã hack Segformer cho chủ đề #objectdetection hay chưa?","Trong Status trước của mình hỏi về việc với vật thể nhỏ và rất nhỏ ở đây (shorturl.at/dmpvY). Mình đang làm thí nghiệm theo các gợi ý của các bạn. Tuy nhiên, cũng dạng dataset này, nhưng với mình thử nghiệm với Segformer (bài báo tại đây https://arxiv.org/abs/2105.15203) thì thấy kết quả khá khả quan. Câu hỏi đặt ra, có bạn nào biết có ai đó đã hack Segformer cho chủ đề hay chưa?",#objectdetection	#Semanticsegmentation	#objectdetection,,,,
Em đang có hứng thú với Reinforcement Learning và đã học một vài course ở trên Coursera. Em có vào vài homepage của các giáo sư nổi tiếng trên thế giới thì chỉ thấy họ làm về lí thuyết. Mọi người cho em xin những topic thiên về ứng dụng của RL hay Deep RL để có thể nghiên cứu ạ.,Em đang có hứng thú với Reinforcement Learning và đã học một vài course ở trên Coursera. Em có vào vài homepage của các giáo sư nổi tiếng trên thế giới thì chỉ thấy họ làm về lí thuyết. Mọi người cho em xin những topic thiên về ứng dụng của RL hay Deep RL để có thể nghiên cứu ạ.,,,,,
"mọi người có thể giải thích giúp mình bug này là gì ko ạ? Mình đang sử dụng VGG để tính toán loss cho GAN. Nhưng không hiểu sao input phải có size như yêu cầu ạ.
Source project của mình: https://github.com/phuocnguyenbk/pytorch
Mình đang reseach AI nên ae có gì thông cảm cho những câu hỏi basic này nhé.",mọi người có thể giải thích giúp mình bug này là gì ko ạ? Mình đang sử dụng VGG để tính toán loss cho GAN. Nhưng không hiểu sao input phải có size như yêu cầu ạ. Source project của mình: https://github.com/phuocnguyenbk/pytorch Mình đang reseach AI nên ae có gì thông cảm cho những câu hỏi basic này nhé.,,,,,
"Em chào mọi người. Em đang research về Machine Learning cho hệ thống tính điểm Credit Scoring. em đã rs được Algorithmic Credit Scoring (AdaBoost, XGBoost, Random Forest...), hiện tại em cần rs sâu hơn về fields của của các thuật toán này (như tên field, kiểu dữ liệu, nguồn lấy). Mọi người cho em xin keyword hoặc các papers nghiên cứu nhé. Em cảm ơn nhiều ạ!!!","Em chào mọi người. Em đang research về Machine Learning cho hệ thống tính điểm Credit Scoring. em đã rs được Algorithmic Credit Scoring (AdaBoost, XGBoost, Random Forest...), hiện tại em cần rs sâu hơn về fields của của các thuật toán này (như tên field, kiểu dữ liệu, nguồn lấy). Mọi người cho em xin keyword hoặc các papers nghiên cứu nhé. Em cảm ơn nhiều ạ!!!",,,,,
Em đang có bài toán binary classification nhưng dữ liệu của em quá imblance chỉ có 10% là lớp 1 còn lại là lớp 0. Em cũng đã thử cả Oversampling và Undersampling với các model họ nhà tree như random forest hay Xgboost thì đều cho ra kết quả f1-score của lớp 1 rất thấp gần 20%. Anh chị có thể có gợi ý cho em một số phương pháp để cải thiện f1 của lớp 1 không ạ? Em cảm ơn.,Em đang có bài toán binary classification nhưng dữ liệu của em quá imblance chỉ có 10% là lớp 1 còn lại là lớp 0. Em cũng đã thử cả Oversampling và Undersampling với các model họ nhà tree như random forest hay Xgboost thì đều cho ra kết quả f1-score của lớp 1 rất thấp gần 20%. Anh chị có thể có gợi ý cho em một số phương pháp để cải thiện f1 của lớp 1 không ạ? Em cảm ơn.,,,,,
"Chào mn ạ, em đang thực hiện Q&A task với sự hướng dẫn của Hugging Face. Ý định của em là muốn hiển thị Accuracy, F1 và Validation Los (đang bị no log) trong quá trình train ạ. Hi vọng được mn giúp đỡ ạ. Em xin cảm ơn ạ!","Chào mn ạ, em đang thực hiện Q&A task với sự hướng dẫn của Hugging Face. Ý định của em là muốn hiển thị Accuracy, F1 và Validation Los (đang bị no log) trong quá trình train ạ. Hi vọng được mn giúp đỡ ạ. Em xin cảm ơn ạ!",,,,,
"Hi all,
Mình tên là Việt, Hiện tại mình là senior AI engineer đang làm việc cho https://sporttotal.tv/ - công ty về sport broadcasting ở Berlin, Đức. Sau khi chia sẻ video hướng dẫn về AirGesture - chơi game không cần dùng bàn phím tuần trước, mình đọc được 1 vài comment các bạn nói rằng chơi game kiểu này thì mỏi tay quá. Vì thế nên trong video này, mình sẽ hướng dẫn các bạn 1 phiên bản cao cấp hơn: chơi game mà thậm chí các bạn không cần phải dùng đến tay. Thay vì Mediapipe Hands, lần này chúng ta sẽ thử nghiệm Mediapipe Face Mesh - giải pháp dành cho khuôn mặt. 

Cách chơi này chống chỉ định cho những bạn vừa nhổ răng khôn hay đang bị đau mắt nhé. Chúc các bạn có 1 trải nghiệm chơi game vui vẻ
Full demo + hướng dẫn + source code: https://youtu.be/Hcl-cREVo9Q","Hi all, Mình tên là Việt, Hiện tại mình là senior AI engineer đang làm việc cho https://sporttotal.tv/ - công ty về sport broadcasting ở Berlin, Đức. Sau khi chia sẻ video hướng dẫn về AirGesture - chơi game không cần dùng bàn phím tuần trước, mình đọc được 1 vài comment các bạn nói rằng chơi game kiểu này thì mỏi tay quá. Vì thế nên trong video này, mình sẽ hướng dẫn các bạn 1 phiên bản cao cấp hơn: chơi game mà thậm chí các bạn không cần phải dùng đến tay. Thay vì Mediapipe Hands, lần này chúng ta sẽ thử nghiệm Mediapipe Face Mesh - giải pháp dành cho khuôn mặt. Cách chơi này chống chỉ định cho những bạn vừa nhổ răng khôn hay đang bị đau mắt nhé. Chúc các bạn có 1 trải nghiệm chơi game vui vẻ Full demo + hướng dẫn + source code: https://youtu.be/Hcl-cREVo9Q",,,,,
"Hi các bạn,
Đức là 1 trong số ít các quốc gia miễn hoàn toàn học phí cho tất cả các cấp học, từ mẫu giáo cho đến cao học (Đây cũng là lý do chính mình chọn Đức để học master 🥲). Tuy miễn học phí nhưng chất lượng giáo dục của Đức là rất cao (Có rất nhiều trường của Đức nằm trong top ranking thế giới). Ngoài ra cơ hội việc làm ở Đức sau khi tốt nghiệp cũng là rất lớn, đặc biệt cho các bạn học IT (Mình xin được việc ngay sau khi ra trường với tiếng Đức bằng 0 🥲). Do vậy mình làm video này để chia sẻ với những bạn đang có ý định tìm hiểu du học:
Những thông tin cơ bản nhất về hệ thống giáo dục Đức
Cách để tra world rank (xếp hạng thế giới) của 1 trường bất kì trên thế giới
Quá trình 2.5 năm mình học master về AI ở Munich, Đức
Phần cuối cùng tuy là phần dài nhất nhưng chỉ là chia sẻ trải nghiệm của bản thân mình, nên các bạn có thể hoàn toàn bỏ qua nhé.
Mình hy vọng video này sẽ giúp ích được cho các bạn 😊.
Video: https://youtu.be/eXAxTmCyuEw","Hi các bạn, Đức là 1 trong số ít các quốc gia miễn hoàn toàn học phí cho tất cả các cấp học, từ mẫu giáo cho đến cao học (Đây cũng là lý do chính mình chọn Đức để học master ). Tuy miễn học phí nhưng chất lượng giáo dục của Đức là rất cao (Có rất nhiều trường của Đức nằm trong top ranking thế giới). Ngoài ra cơ hội việc làm ở Đức sau khi tốt nghiệp cũng là rất lớn, đặc biệt cho các bạn học IT (Mình xin được việc ngay sau khi ra trường với tiếng Đức bằng 0 ). Do vậy mình làm video này để chia sẻ với những bạn đang có ý định tìm hiểu du học: Những thông tin cơ bản nhất về hệ thống giáo dục Đức Cách để tra world rank (xếp hạng thế giới) của 1 trường bất kì trên thế giới Quá trình 2.5 năm mình học master về AI ở Munich, Đức Phần cuối cùng tuy là phần dài nhất nhưng chỉ là chia sẻ trải nghiệm của bản thân mình, nên các bạn có thể hoàn toàn bỏ qua nhé. Mình hy vọng video này sẽ giúp ích được cho các bạn . Video: https://youtu.be/eXAxTmCyuEw",,,,,
"Hi các bạn,
2 video dưới đây, 1 là sản phẩm của digital human, 1 là sản phẩm của Deepfake. Đây là 2 khái niệm rất hay bị nhầm lẫn với nhau. Trong video này mình sẽ giải thích ngắn gọn cho các bạn về 2 khái niệm này cũng như sự khác nhau cơ bản của chúng. Mình hy vọng video này sẽ hữu ích với các bạn, đặc biệt là những bạn đang học cũng như đang đi làm trong mảng AI/Computer Vision. Video chỉ dài hơn 5 phút thôi nên các bạn đừng skip nhé.
Full videos + giải thích: https://youtu.be/Zs3hzOZTPaw","Hi các bạn, 2 video dưới đây, 1 là sản phẩm của digital human, 1 là sản phẩm của Deepfake. Đây là 2 khái niệm rất hay bị nhầm lẫn với nhau. Trong video này mình sẽ giải thích ngắn gọn cho các bạn về 2 khái niệm này cũng như sự khác nhau cơ bản của chúng. Mình hy vọng video này sẽ hữu ích với các bạn, đặc biệt là những bạn đang học cũng như đang đi làm trong mảng AI/Computer Vision. Video chỉ dài hơn 5 phút thôi nên các bạn đừng skip nhé. Full videos + giải thích: https://youtu.be/Zs3hzOZTPaw",,,,,
"Hi all,

Mình  đã học và làm việc trong lĩnh vực AI ở Đức được 7 năm rồi. Trong thời  gian này mình đã trải qua các vị trí từ Data scientist, ML engineer cho  đến AI engineer như hiện tại, trên cả 2 mảng lớn là NLP (xử lý ngôn ngữ  tự nhiên) cũng như Computer Vision (thị giác máy tính). Trong video này  mình muốn chia sẻ với các bạn Roadmap để trở thành 1 Data scientist/ML  engineer/AI engineer, dựa trên những gì bản thân mình đã trải qua, từ  lúc bắt đầu học về AI cho đến giờ. Mình hi vọng những chia sẻ này của  mình sẽ giúp được phần nào các bạn trong quá trình các bạn theo đuổi  lĩnh vực chông gai nhưng cũng vô cùng thú vị này

Link to video: https://youtu.be/B6N44PkQG6o","Hi all, Mình đã học và làm việc trong lĩnh vực AI ở Đức được 7 năm rồi. Trong thời gian này mình đã trải qua các vị trí từ Data scientist, ML engineer cho đến AI engineer như hiện tại, trên cả 2 mảng lớn là NLP (xử lý ngôn ngữ tự nhiên) cũng như Computer Vision (thị giác máy tính). Trong video này mình muốn chia sẻ với các bạn Roadmap để trở thành 1 Data scientist/ML engineer/AI engineer, dựa trên những gì bản thân mình đã trải qua, từ lúc bắt đầu học về AI cho đến giờ. Mình hi vọng những chia sẻ này của mình sẽ giúp được phần nào các bạn trong quá trình các bạn theo đuổi lĩnh vực chông gai nhưng cũng vô cùng thú vị này Link to video: https://youtu.be/B6N44PkQG6o",,,,,
"Xin chào mọi người trong group, em đang tự học data analysis cơ bản, cụ thể là phân tích giá cổ phiếu của nước mình. Bài toán cụ thể em đang muốn giải quyết đó là tìm ra được 3 mã cố phiếu có giá tăng trưởng nhanh nhất trong danh mục cổ phiếu của chỉ số VNINDEX trong một giai đoạn thời gian cụ thể( theo công thức growth rate = (giá ngày cuối - giá ngày đầu)/giá ngày đầu). Việc em cần làm là phải tính được growth rate của tất cả các mã cố phiếu thuộc danh mục VNINDEX và chọn ra giá trị lớn nhất. Mọi người cho em hỏi là có hàm nào trong pandas hay vnquant có thể làm đc công việc này không ạ? Hoặc nếu phải tự viết thì có cách nào để clone data của tất cả các mã mà không phải làm thủ công hay không ạ? Em xin cảm ơn.","Xin chào mọi người trong group, em đang tự học data analysis cơ bản, cụ thể là phân tích giá cổ phiếu của nước mình. Bài toán cụ thể em đang muốn giải quyết đó là tìm ra được 3 mã cố phiếu có giá tăng trưởng nhanh nhất trong danh mục cổ phiếu của chỉ số VNINDEX trong một giai đoạn thời gian cụ thể( theo công thức growth rate = (giá ngày cuối - giá ngày đầu)/giá ngày đầu). Việc em cần làm là phải tính được growth rate của tất cả các mã cố phiếu thuộc danh mục VNINDEX và chọn ra giá trị lớn nhất. Mọi người cho em hỏi là có hàm nào trong pandas hay vnquant có thể làm đc công việc này không ạ? Hoặc nếu phải tự viết thì có cách nào để clone data của tất cả các mã mà không phải làm thủ công hay không ạ? Em xin cảm ơn.",,,,,
"Xin chào mọi người, em là sinh viên năm cuối hiện đang có bài tập như hình. Qua quá trình tìm hiểu em có 1 thắc mắc đến giờ vẫn chưa tìm được lời giải:
Hệ thống này sẽ học bằng cách nào? How to learning algorithm? Vì quá trình học diễn ra ở Hệ mờ nên ko biết được target output thì training Neural network bằng cách nào dựa trên data có sẵn, lsao để đánh lại weight lẫn bias?","Xin chào mọi người, em là sinh viên năm cuối hiện đang có bài tập như hình. Qua quá trình tìm hiểu em có 1 thắc mắc đến giờ vẫn chưa tìm được lời giải: Hệ thống này sẽ học bằng cách nào? How to learning algorithm? Vì quá trình học diễn ra ở Hệ mờ nên ko biết được target output thì training Neural network bằng cách nào dựa trên data có sẵn, lsao để đánh lại weight lẫn bias?",,,,,
"Xin nhờ các cao nhân hướng dẫn em cách cài package linear_assignment. Em search trên mạng không thấy cách cài để tránh lỗi không định danh được
from sklearn.utils.linear_assignment_ import linear_assignment",Xin nhờ các cao nhân hướng dẫn em cách cài package linear_assignment. Em search trên mạng không thấy cách cài để tránh lỗi không định danh được from sklearn.utils.linear_assignment_ import linear_assignment,,,,,
"Chào mọi người. Hiện tại em đã train xong model yolov7 để detect được biển số, tuy nhiên custom model của em lại ko detect được xe. Vậy mọi người cho em hỏi có cách nào để sử dụng được cả weights của pretrained model và custom weights để detect được cả xe và biển số hay không, hay lại phải quay về custom dataset để đánh nhãn cả xe và biển rồi quay lại train ạ?","Chào mọi người. Hiện tại em đã train xong model yolov7 để detect được biển số, tuy nhiên custom model của em lại ko detect được xe. Vậy mọi người cho em hỏi có cách nào để sử dụng được cả weights của pretrained model và custom weights để detect được cả xe và biển số hay không, hay lại phải quay về custom dataset để đánh nhãn cả xe và biển rồi quay lại train ạ?",,,,,
"Cho em hỏi các dự án thực tế có ứng dụng computer vision người ta code cái phần gì bằng c++ ạ?
Em theo computer vision thì ko rõ, yêu cầu c++ có phải bắt buộc cần biết để đi làm ko.
Em cảm ơn ạ","Cho em hỏi các dự án thực tế có ứng dụng computer vision người ta code cái phần gì bằng c++ ạ? Em theo computer vision thì ko rõ, yêu cầu c++ có phải bắt buộc cần biết để đi làm ko. Em cảm ơn ạ",,,,,
"Dạ em chào mọi người trong gr, em đang thực hiện một bài toán cần dùng wikiextractor để extract từ các file dump wiki nhưng output chỉ gồm 4 field id, url, title, text. Em muốn có thêm trường charoffset hoặc text_with_links như bên hotpotQA thì phải xử lý ntn ạ!
https://hotpotqa.github.io/wiki-readme.html","Dạ em chào mọi người trong gr, em đang thực hiện một bài toán cần dùng wikiextractor để extract từ các file dump wiki nhưng output chỉ gồm 4 field id, url, title, text. Em muốn có thêm trường charoffset hoặc text_with_links như bên hotpotQA thì phải xử lý ntn ạ! https://hotpotqa.github.io/wiki-readme.html",,,,,
"Đóng góp tiếp theo đến từ VietAI research team cho thư viện huggingface/diffusers: Latent Diffusion Super Resolution pipeline, chuyên trị ảnh out nét cho các “thánh sống ảo” 😝.

Trải nghiệm tại: https://bom.so/nCZW9c","Đóng góp tiếp theo đến từ VietAI research team cho thư viện huggingface/diffusers: Latent Diffusion Super Resolution pipeline, chuyên trị ảnh out nét cho các “thánh sống ảo” . Trải nghiệm tại: https://bom.so/nCZW9c",,,,,
"Chào mọi người, hiện tại em đang có 1 dự án sinh viên, có 1 chức năng đó là dùng camera gặp ở các kệ trưng bày trong triển lãm, bảo tàng để đo số lượng người đứng trước sản phẩm. Vì cam đặt tầm ngang người nên em không dùng được bài toán in-store heatmap được. Em mong mọi người có thể cho em gợi ý ạ","Chào mọi người, hiện tại em đang có 1 dự án sinh viên, có 1 chức năng đó là dùng camera gặp ở các kệ trưng bày trong triển lãm, bảo tàng để đo số lượng người đứng trước sản phẩm. Vì cam đặt tầm ngang người nên em không dùng được bài toán in-store heatmap được. Em mong mọi người có thể cho em gợi ý ạ",,,,,
"Hiện nay e đang học năm thứ 3 chuyên ngành AI và đã đến kỳ thực tập, e search trên google thì thấy rất ít công ty đang tuyển thực tập sinh, mọi người có thông tin tuyển dụng công ty nào giúp e với ạ, e cảm ơn.","Hiện nay e đang học năm thứ 3 chuyên ngành AI và đã đến kỳ thực tập, e search trên google thì thấy rất ít công ty đang tuyển thực tập sinh, mọi người có thông tin tuyển dụng công ty nào giúp e với ạ, e cảm ơn.",,,,,
"Kính chào các bác.
Cuối tuần em xin phép share cùng anh em một game vui vẻ với Mediapipe và OpenCV. Chúc anh em thành công!",Kính chào các bác. Cuối tuần em xin phép share cùng anh em một game vui vẻ với Mediapipe và OpenCV. Chúc anh em thành công!,,,,,
"Chào mọi người, hiện tại em đang nghiên cứu về bài toán viết lại nội dung 1 bài báo tiếng Việt. 
Mọi người ai đã làm rồi cho em xin giải pháp. Nếu có opensource nào cho em xin để học hỏi với ạ.
Em cảm ơn!","Chào mọi người, hiện tại em đang nghiên cứu về bài toán viết lại nội dung 1 bài báo tiếng Việt. Mọi người ai đã làm rồi cho em xin giải pháp. Nếu có opensource nào cho em xin để học hỏi với ạ. Em cảm ơn!",,,,,
"Hi all. 
Mình tên là Việt, Hiện tại mình là senior AI engineer đang làm việc cho https://sporttotal.tv/ - công ty về sport broadcasting ở Berlin, Đức. Trong lúc rảnh rỗi mình hay làm 1 vài mini project liên quan đến AI/Computer Vision. Trong video này mình xin hướng dẫn các bạn cách sử dụng framework rất nổi tiếng Mediapipe của Google để có thể tạo ra những ứng dụng chơi game không cần bàn phím của riêng bạn. Have fun!
Full demo + hướng dẫn: https://youtu.be/iJIDLYQbGDI","Hi all. Mình tên là Việt, Hiện tại mình là senior AI engineer đang làm việc cho https://sporttotal.tv/ - công ty về sport broadcasting ở Berlin, Đức. Trong lúc rảnh rỗi mình hay làm 1 vài mini project liên quan đến AI/Computer Vision. Trong video này mình xin hướng dẫn các bạn cách sử dụng framework rất nổi tiếng Mediapipe của Google để có thể tạo ra những ứng dụng chơi game không cần bàn phím của riêng bạn. Have fun! Full demo + hướng dẫn: https://youtu.be/iJIDLYQbGDI",,,,,
"Register now for VinAI Winter Workshop 2022 (Sat, 19 Nov): http://bit.ly/3UDu7gV
We will discuss ""How to get your paper accepted to top-tier conferences?"" & showcase our latest R&D results.","Register now for VinAI Winter Workshop 2022 (Sat, 19 Nov): http://bit.ly/3UDu7gV We will discuss ""How to get your paper accepted to top-tier conferences?"" & showcase our latest R&D results.",,,,,
"Hi mọi người, hiện tại em có đang làm một dự án về image retrieval cho Android app. Như là em có một data set về tất cả ảnh chụp màn hình của một Android app bất kì nào đó, rồi khi em có một cái ảnh truy vấn, em muốn lấy ra cái ảnh mà giống với cái ảnh truy vấn nhất (ảnh màn hình tại cùng trang của app với query image). Em có thử dùng hash-code để so sánh giữa ảnh truy vấn và từng ảnh trong kho dữ liệu, nhưng không hiệu quá cho lắm. Em muốn hỏi là có Machine Learning hoặc Deep Learning model nào mà em có thể giúp mình trong việc so sánh ảnh truy vấn với ảnh trong dữ liệu rồi lấy ra ảnh giống nhất k ạ? Em cảm ơn ạ","Hi mọi người, hiện tại em có đang làm một dự án về image retrieval cho Android app. Như là em có một data set về tất cả ảnh chụp màn hình của một Android app bất kì nào đó, rồi khi em có một cái ảnh truy vấn, em muốn lấy ra cái ảnh mà giống với cái ảnh truy vấn nhất (ảnh màn hình tại cùng trang của app với query image). Em có thử dùng hash-code để so sánh giữa ảnh truy vấn và từng ảnh trong kho dữ liệu, nhưng không hiệu quá cho lắm. Em muốn hỏi là có Machine Learning hoặc Deep Learning model nào mà em có thể giúp mình trong việc so sánh ảnh truy vấn với ảnh trong dữ liệu rồi lấy ra ảnh giống nhất k ạ? Em cảm ơn ạ",,,,,
"Chào cả nhà mình đang train model. Khi train với tập dữ liệu nhỏ (30 ảnh) thì ra kết quả, nhưng khi tăng cường tập dữ liệu train (từ 30 ảnh thành 1620 ảnh) lại thì xuất hiện lỗi này. Nhờ Mọi người xem giúp lỗi này là thế nào? Mình đã cố gắng fix mà chưa được. Thanks All!","Chào cả nhà mình đang train model. Khi train với tập dữ liệu nhỏ (30 ảnh) thì ra kết quả, nhưng khi tăng cường tập dữ liệu train (từ 30 ảnh thành 1620 ảnh) lại thì xuất hiện lỗi này. Nhờ Mọi người xem giúp lỗi này là thế nào? Mình đã cố gắng fix mà chưa được. Thanks All!",,,,,
Em đang là hs muốn tìm hiểu và học hỏi kinh nghiệm cần tìm người chung đội hỗ trợ cho cuộc thi ZALO AI. Ai chưa có đội ib,Em đang là hs muốn tìm hiểu và học hỏi kinh nghiệm cần tìm người chung đội hỗ trợ cho cuộc thi ZALO AI. Ai chưa có đội ib,,,,,
"Mặc dù máy tính ngày càng trở nên hiện đại, AI ngày càng trở nên thông minh và được áp dụng rộng rãi trên mọi lĩnh vực, thì về mặt bản chất, các mô hình AI nói riêng hay các (siêu) máy tính ngày nay nói chung cũng không khác gì với những chiếc máy tính cổ từ thời nhà Tống ở 1 điểm cốt lõi: Chỉ hiểu được 2 con số 0 và 1! Đúng vậy, bất kể dữ liệu các bạn đưa vào là 1 dãy số có n chữ số, văn bản, âm thanh, hình ảnh, …, máy tính sẽ không bao giờ trực tiếp làm việc với những dữ liệu ấy, vì đơn giản là chúng hoàn toàn không hiểu những dữ liệu này. Trong AI/Machine Learning cũng vậy, hầu hết dữ liệu đều cần phải được mã hóa (encoding) để có thể được sử dụng để huấn luyện mô hình. Trong video này, mình sẽ hướng dẫn các bạn các kỹ thuật encoding cơ bản nhất, mà bất cứ ai muốn tìm hiểu hay làm việc trong lĩnh vực này đều cần phải nắm được. Như mọi video trước, mình đã cố gắng trình bày đơn giản nhất có thể. Mình mong nhận được sự ủng hộ cũng như góp ý của các bạn.
Video: https://youtu.be/2iO6vU2BCh4
#tutorial","Mặc dù máy tính ngày càng trở nên hiện đại, AI ngày càng trở nên thông minh và được áp dụng rộng rãi trên mọi lĩnh vực, thì về mặt bản chất, các mô hình AI nói riêng hay các (siêu) máy tính ngày nay nói chung cũng không khác gì với những chiếc máy tính cổ từ thời nhà Tống ở 1 điểm cốt lõi: Chỉ hiểu được 2 con số 0 và 1! Đúng vậy, bất kể dữ liệu các bạn đưa vào là 1 dãy số có n chữ số, văn bản, âm thanh, hình ảnh, …, máy tính sẽ không bao giờ trực tiếp làm việc với những dữ liệu ấy, vì đơn giản là chúng hoàn toàn không hiểu những dữ liệu này. Trong AI/Machine Learning cũng vậy, hầu hết dữ liệu đều cần phải được mã hóa (encoding) để có thể được sử dụng để huấn luyện mô hình. Trong video này, mình sẽ hướng dẫn các bạn các kỹ thuật encoding cơ bản nhất, mà bất cứ ai muốn tìm hiểu hay làm việc trong lĩnh vực này đều cần phải nắm được. Như mọi video trước, mình đã cố gắng trình bày đơn giản nhất có thể. Mình mong nhận được sự ủng hộ cũng như góp ý của các bạn. Video: https://youtu.be/2iO6vU2BCh4",#tutorial,,,,
"Hi mn, hiện tại e đang làm nhận diện biển số xe bằng model ssd mobilenetv2 nhưng gặp phải vấn đề về việc lấy ra được biển số xe theo thứ tự chính xác: từ trái sang phải rồi từ trên xuống dưới. Mong mọi người có thể góp ý giúp e ạ","Hi mn, hiện tại e đang làm nhận diện biển số xe bằng model ssd mobilenetv2 nhưng gặp phải vấn đề về việc lấy ra được biển số xe theo thứ tự chính xác: từ trái sang phải rồi từ trên xuống dưới. Mong mọi người có thể góp ý giúp e ạ",,,,,
"Nay mình giới thiệu với mọi người một cuốn sách tiếp cận AI (ML, DL) theo hướng hands-on, kiểu thực hành chứ không nhiều lý thuyết.
Sách đề cập và nêu hướng xử lý đa dạng các bài toán từ xử lý ảnh: phân loại ảnh, nhận dạng vật thể, các mô hình sinh,.. tới các bài toán về xử lý ngôn ngữ tự nhiên: tóm tắt văn bản, sinh chữ, nhận dạng chữ cái,...
Với mỗi chủ đề, sách nêu bài toán, hướng tiếp cập, các thư viện cần thiết và code đi kèm, vì sách xuất bản 2022 luôn nên các bạn không sợ code cũ không chạy hoặc thư viện bị out-of-date.","Nay mình giới thiệu với mọi người một cuốn sách tiếp cận AI (ML, DL) theo hướng hands-on, kiểu thực hành chứ không nhiều lý thuyết. Sách đề cập và nêu hướng xử lý đa dạng các bài toán từ xử lý ảnh: phân loại ảnh, nhận dạng vật thể, các mô hình sinh,.. tới các bài toán về xử lý ngôn ngữ tự nhiên: tóm tắt văn bản, sinh chữ, nhận dạng chữ cái,... Với mỗi chủ đề, sách nêu bài toán, hướng tiếp cập, các thư viện cần thiết và code đi kèm, vì sách xuất bản 2022 luôn nên các bạn không sợ code cũ không chạy hoặc thư viện bị out-of-date.",,,,,
"Hi anh em, nếu anh em theo dõi thị trường chứng khoán thì biết rồi, nửa năm nay thị trường diễn biến rất xấu, rất nhiều mã chia đôi, một số mã đặc biệt như DIG, L14 thì thậm chí còn chia 10. 
Điều này không chỉ ảnh hưởng tới danh mục của khách hàng, mà còn ảnh hưởng tới danh mục cho vay của các CTCK. Giải thích một cách dễ hiểu thì CTCK có nhận một số chứng khoán làm Tài sản đảm bảo để phát vay, họ sẽ định giá và có một Giá định giá theo họ là phù hợp. Thông thường thì cái giá định giá này sẽ bằng khoảng 1 nửa giá thị trường, tùy thời điểm nhưng có nghĩa là nếu cổ phiếu chia đôi mà chứng khoán không thể bán được, thì CTCK mất vốn.
Tuy nhiên, với thị trường này, thì có lẽ không có giá định giá nào là phù hợp nữa, sẽ có khá nhiều CTCK mất vốn. 

Mình hiện đang nghiên cứu chủ đề này, bài toán đặt ra là quản trị rủi ro cho danh mục cho vay ký quỹ của CTCK. Theo mình suy nghĩ thì rủi ro lớn nhất của CTCK chính là chứng khoán không bán được, nên ta phải xác định được:
Những chứng khoán nào thanh khoản ảo, khi thị trường xấu là không ai thèm mua.
Mức dự kiến lỗ tối đa cho từng mã chứng khoán
Xác suất mất vốn abc xyz
Có bạn nào đã từng làm có thể chia sẻ kinh nghiệm / gợi ý / cho keywords để tìm hiểu được không ạ. Nếu được tham khảo code và model thì tốt quá.

Xin cảm ơn anh em đã đọc mấy dòng lan man.","Hi anh em, nếu anh em theo dõi thị trường chứng khoán thì biết rồi, nửa năm nay thị trường diễn biến rất xấu, rất nhiều mã chia đôi, một số mã đặc biệt như DIG, L14 thì thậm chí còn chia 10. Điều này không chỉ ảnh hưởng tới danh mục của khách hàng, mà còn ảnh hưởng tới danh mục cho vay của các CTCK. Giải thích một cách dễ hiểu thì CTCK có nhận một số chứng khoán làm Tài sản đảm bảo để phát vay, họ sẽ định giá và có một Giá định giá theo họ là phù hợp. Thông thường thì cái giá định giá này sẽ bằng khoảng 1 nửa giá thị trường, tùy thời điểm nhưng có nghĩa là nếu cổ phiếu chia đôi mà chứng khoán không thể bán được, thì CTCK mất vốn. Tuy nhiên, với thị trường này, thì có lẽ không có giá định giá nào là phù hợp nữa, sẽ có khá nhiều CTCK mất vốn. Mình hiện đang nghiên cứu chủ đề này, bài toán đặt ra là quản trị rủi ro cho danh mục cho vay ký quỹ của CTCK. Theo mình suy nghĩ thì rủi ro lớn nhất của CTCK chính là chứng khoán không bán được, nên ta phải xác định được: Những chứng khoán nào thanh khoản ảo, khi thị trường xấu là không ai thèm mua. Mức dự kiến lỗ tối đa cho từng mã chứng khoán Xác suất mất vốn abc xyz Có bạn nào đã từng làm có thể chia sẻ kinh nghiệm / gợi ý / cho keywords để tìm hiểu được không ạ. Nếu được tham khảo code và model thì tốt quá. Xin cảm ơn anh em đã đọc mấy dòng lan man.",,,,,
"Chào mọi người, mọi người có thể cho em xin hướng đi cho bài toán tìm hệ số của hệ phương trình vi phân tuyến tính dưới đây được không ạ.
Cụ thể như sau: Cho dataset 1000 bộ (R, J) với time step = 0.001 Tìm a, b, c, d trong hệ bên dưới thỏa dataset và R(0) = -2, J(0) = 3. Biết dataset có vài dữ liệu nhiễu.","Chào mọi người, mọi người có thể cho em xin hướng đi cho bài toán tìm hệ số của hệ phương trình vi phân tuyến tính dưới đây được không ạ. Cụ thể như sau: Cho dataset 1000 bộ (R, J) với time step = 0.001 Tìm a, b, c, d trong hệ bên dưới thỏa dataset và R(0) = -2, J(0) = 3. Biết dataset có vài dữ liệu nhiễu.",,,,,
"Em chào anh chị ạ,
Em xin phép nhờ anh chị tư vấn một chút về vấn đề học thạc sĩ khoa học dữ liệu với ạ:
Em đang có dự định học tại thạc sĩ ngành khoa học dữ liệu tại Việt Nam, hiện tại em thấy có 2 trường là trường đại học Bách Khoa Hà Nội và trường đại học Quốc Gia Hà Nội. Anh chị có thể tư vấn giúp em lựa chọn nào sẽ là tốt hơn không ạ? Hoặc nếu anh chị có một trường nào khác có ngành thạc sĩ khoa học dữ liệu tốt có thể cho em một chút gợi ý cũng được ạ
Trong thời gian học đại học chuyên ngành của em là Tài Chính Ngân Hàng, hiện tại em đang làm modeling trong bank. Vậy với background này em sẽ gặp những khó khăn gì khi theo học thạc sĩ chuyên ngành khoa học dữ liệu này ạ. Và có cách nào để khắc phục không ạ.
Mong nhận được sự tư vấn của anh chị. Em cảm ơn anh chị ạ","Em chào anh chị ạ, Em xin phép nhờ anh chị tư vấn một chút về vấn đề học thạc sĩ khoa học dữ liệu với ạ: Em đang có dự định học tại thạc sĩ ngành khoa học dữ liệu tại Việt Nam, hiện tại em thấy có 2 trường là trường đại học Bách Khoa Hà Nội và trường đại học Quốc Gia Hà Nội. Anh chị có thể tư vấn giúp em lựa chọn nào sẽ là tốt hơn không ạ? Hoặc nếu anh chị có một trường nào khác có ngành thạc sĩ khoa học dữ liệu tốt có thể cho em một chút gợi ý cũng được ạ Trong thời gian học đại học chuyên ngành của em là Tài Chính Ngân Hàng, hiện tại em đang làm modeling trong bank. Vậy với background này em sẽ gặp những khó khăn gì khi theo học thạc sĩ chuyên ngành khoa học dữ liệu này ạ. Và có cách nào để khắc phục không ạ. Mong nhận được sự tư vấn của anh chị. Em cảm ơn anh chị ạ",,,,,
"E chào a/c, chúc a/c đầu tuần nghiên cứu hăng say, hiệu quả. Nhân tiện bắt đầu cho e xin hỏi trọng mạng ResNet50, 2 khối conv và identity e thấy có khác gì nhau đâu nhỉ, mỗi ở conv có thêm conv ở đường tắt, vậy ý nghĩa 2 khối này làm gì ạ, sao phải tách làm 2 và phân biệt tên thế ạ. E cảm ơn!","E chào a/c, chúc a/c đầu tuần nghiên cứu hăng say, hiệu quả. Nhân tiện bắt đầu cho e xin hỏi trọng mạng ResNet50, 2 khối conv và identity e thấy có khác gì nhau đâu nhỉ, mỗi ở conv có thêm conv ở đường tắt, vậy ý nghĩa 2 khối này làm gì ạ, sao phải tách làm 2 và phân biệt tên thế ạ. E cảm ơn!",,,,,
"Chào tất cả Anh/Chị/Em
Mình training 1 model với mục đích image segmentation cho 2 lớp ( 1 lớp background và 1 lớp bóng của cây cối). Sau khi train 200 epochs thì thu được độ chính xác IoU như hình dưới đây. Mình thấy dcx giao động rất nhiều, và ngày càng đi xuống. Vậy theo như kinh nghiệm của mọi người thì nguyên nhân do đâu và vui lòng gợi ý cách khắc phục giúp mình nhé.
Cảm ơn mọi người đã đọc và chia sẻ <3","Chào tất cả Anh/Chị/Em Mình training 1 model với mục đích image segmentation cho 2 lớp ( 1 lớp background và 1 lớp bóng của cây cối). Sau khi train 200 epochs thì thu được độ chính xác IoU như hình dưới đây. Mình thấy dcx giao động rất nhiều, và ngày càng đi xuống. Vậy theo như kinh nghiệm của mọi người thì nguyên nhân do đâu và vui lòng gợi ý cách khắc phục giúp mình nhé. Cảm ơn mọi người đã đọc và chia sẻ <3",,,,,
"Ngày nay, khi các mô hình về AI đạt được độ chính xác cao với các kết quả rất ấn tượng, thì vấn đề giải thích mô hình càng được chú trọng hơn. Trong workshop này, giáo sư Hima Lakkaraju, từ đại học Harvard chia sẻ về các kĩ thuật interpretable machine learning từ đó giúp giải thích, xác định lỗi của mô hình cũng như xây dựng được các mô hình tốt hơn.

Bài giảng: https://lnkd.in/gzfmJug9
Slide: https://lnkd.in/e_RsBVPx","Ngày nay, khi các mô hình về AI đạt được độ chính xác cao với các kết quả rất ấn tượng, thì vấn đề giải thích mô hình càng được chú trọng hơn. Trong workshop này, giáo sư Hima Lakkaraju, từ đại học Harvard chia sẻ về các kĩ thuật interpretable machine learning từ đó giúp giải thích, xác định lỗi của mô hình cũng như xây dựng được các mô hình tốt hơn. Bài giảng: https://lnkd.in/gzfmJug9 Slide: https://lnkd.in/e_RsBVPx",,,,,
"Cách đây tròn 6 năm, vào tháng 11/2016, Quick, Draw! - trò chơi trực tuyến dựa trên neural network đã được Google cho ra mắt. Đây cũng đồng thời là tên bộ dữ liệu khổng lồ với 50 triệu bản vẽ tay của người dùng trên khắp thế giới được Google chia sẻ công khai. Ngay từ thời điểm mới ra mắt, Quick, Draw! đã tạo nên 1 cơn sốt vô cùng lớn trên khắp các diễn đàn cũng như mạng xã hội về AI như reddit, twitter, linkedin. Nhân dịp kỷ niệm Quick, Draw! 6 năm tuổi, mình làm video demo này với AI model được huấn luyện với Quick, Draw!. Đồng thời mình cũng sẽ hướng dẫn các bạn cách làm việc với bộ dataset vô cùng thú vị này

Video: https://youtu.be/b8aJGt56b74
#AI #QuickDraw","Cách đây tròn 6 năm, vào tháng 11/2016, Quick, Draw! - trò chơi trực tuyến dựa trên neural network đã được Google cho ra mắt. Đây cũng đồng thời là tên bộ dữ liệu khổng lồ với 50 triệu bản vẽ tay của người dùng trên khắp thế giới được Google chia sẻ công khai. Ngay từ thời điểm mới ra mắt, Quick, Draw! đã tạo nên 1 cơn sốt vô cùng lớn trên khắp các diễn đàn cũng như mạng xã hội về AI như reddit, twitter, linkedin. Nhân dịp kỷ niệm Quick, Draw! 6 năm tuổi, mình làm video demo này với AI model được huấn luyện với Quick, Draw!. Đồng thời mình cũng sẽ hướng dẫn các bạn cách làm việc với bộ dataset vô cùng thú vị này Video: https://youtu.be/b8aJGt56b74",#AI	#QuickDraw,,,,
"Chào mọi người. Hôm nay mình muốn chia sẻ với mọi người mô hình học tự giám sát (self-supervised) cho dữ liệu audio dựa trên kiến trúc wav2vec2. Mô hình này mình đã huấn luyện sử dụng 13 nghìn giờ dữ liệu youtube với đa dạng các loại âm thanh như (clean, noisy, conversation, dialects,..). Mô hình này mình đã thử nghiệm và cho kết quả rất tốt cho bài toán nhận dạng tiếng nói. Hi vọng sẽ giúp ích cho mọi người.","Chào mọi người. Hôm nay mình muốn chia sẻ với mọi người mô hình học tự giám sát (self-supervised) cho dữ liệu audio dựa trên kiến trúc wav2vec2. Mô hình này mình đã huấn luyện sử dụng 13 nghìn giờ dữ liệu youtube với đa dạng các loại âm thanh như (clean, noisy, conversation, dialects,..). Mô hình này mình đã thử nghiệm và cho kết quả rất tốt cho bài toán nhận dạng tiếng nói. Hi vọng sẽ giúp ích cho mọi người.",,,,,
"Em chào anh chị ạ, em có 1 thắc mắc là không biết trong các dự án AI, Computer vision thì C/C++ có vai trò gì vậy ạ. Và có liên hệ gì giữa nhúng và Ai ạ.","Em chào anh chị ạ, em có 1 thắc mắc là không biết trong các dự án AI, Computer vision thì C/C++ có vai trò gì vậy ạ. Và có liên hệ gì giữa nhúng và Ai ạ.",,,,,
"Chào mọi người,
Liệu mình có thể chạy model inference với nhiều CPU không nhỉ, mình có 1 pretrained model và muốn dự đoán label của nhiều bức ảnh một cách song song. Nếu được thì làm thế có nhanh hơn so với việc dùng 1 CPU duy nhất không.
Mình đã thử search google nhưng ko tìm được câu trả lời rõ ràng. Bạn nào có kinh nghiệm implement rồi cho mình chút gợi ý.
Cảm ơn mọi người","Chào mọi người, Liệu mình có thể chạy model inference với nhiều CPU không nhỉ, mình có 1 pretrained model và muốn dự đoán label của nhiều bức ảnh một cách song song. Nếu được thì làm thế có nhanh hơn so với việc dùng 1 CPU duy nhất không. Mình đã thử search google nhưng ko tìm được câu trả lời rõ ràng. Bạn nào có kinh nghiệm implement rồi cho mình chút gợi ý. Cảm ơn mọi người",,,,,
"Cần lắm 1 chiến hữu lập team thi Zalo AI Challenge 2022. Năm nay giải thưởng tiếp tục siêu to khổng lồ, giải nhất tầm 90tr. Em thi để tiện hốt luôn bộ data của Zalo
Đề thi năm nay đây ạ:
E2E Question Answering - tìm câu trả lời chính xác nhất từ Wikipedia cho một câu hỏ
Liveness Detection - xác định khuôn mặt người trong video là thật hay giả mạo
Lyric Alignment - làm lời bài hát trùng khớp với nhạc.","Cần lắm 1 chiến hữu lập team thi Zalo AI Challenge 2022. Năm nay giải thưởng tiếp tục siêu to khổng lồ, giải nhất tầm 90tr. Em thi để tiện hốt luôn bộ data của Zalo Đề thi năm nay đây ạ: E2E Question Answering - tìm câu trả lời chính xác nhất từ Wikipedia cho một câu hỏ Liveness Detection - xác định khuôn mặt người trong video là thật hay giả mạo Lyric Alignment - làm lời bài hát trùng khớp với nhạc.",,,,,
"Hi mng, mình là tv mới. Mng cho mình hỏi với các projects ML sau khi được tạo ra thì sẽ push lên production ntn vậy ạ? Cụ thể như một project được viết trên Jupyter notebook thì sau khi xong thì mình áp dụng ntn vào thực tế ạ? Tại vì trước đây mình chỉ tạo model -> chạy kết quả -> làm nghiên cứu nên còn mơ hồ là nó sẽ áp dụng vào thực tế ntn ạ? tks all!","Hi mng, mình là tv mới. Mng cho mình hỏi với các projects ML sau khi được tạo ra thì sẽ push lên production ntn vậy ạ? Cụ thể như một project được viết trên Jupyter notebook thì sau khi xong thì mình áp dụng ntn vào thực tế ạ? Tại vì trước đây mình chỉ tạo model -> chạy kết quả -> làm nghiên cứu nên còn mơ hồ là nó sẽ áp dụng vào thực tế ntn ạ? tks all!",,,,,
Cho mình hỏi có ai biết thư viện hoặc code nào thực hiện word alignment tốt cho anh-việt không. Mình cảm ơn.,Cho mình hỏi có ai biết thư viện hoặc code nào thực hiện word alignment tốt cho anh-việt không. Mình cảm ơn.,,,,,
"Cách đây hai năm mình có viết một bài về T-Shaped Skills cho Data Scientist. Tuy nhiên trong quá trình làm việc, mình nhận thấy lĩnh vực này đã thay đổi rất nhiều so với hai năm trước. Sau khi trao đổi với nhiều công ty khác nhau ở Úc, mình càng một lần nữa thấy rõ đa phần mọi người đang đi theo một chiều hướng phát triển chung với tốc độ khá giống nhau và gặp những vấn đề tương tự nhau. Trong bài viết này mình sẽ refresh lại cái nhìn về lĩnh vực Data Science và chia sẻ những con đường khác nhau để các bạn chuẩn bị.
https://datasciencevn.com/chuyen-nghe/146-xu-huong-phat-trien-cua-nganh-data-science-va-ban-nen-hoc-gi-de-chuan-bi.html?fbclid=IwAR1x1snLjUVL4KCzQ1TYeIAFGu3Opmt1KsP_KrDzoEN-wVGbLKLRSKD6L0g","Cách đây hai năm mình có viết một bài về T-Shaped Skills cho Data Scientist. Tuy nhiên trong quá trình làm việc, mình nhận thấy lĩnh vực này đã thay đổi rất nhiều so với hai năm trước. Sau khi trao đổi với nhiều công ty khác nhau ở Úc, mình càng một lần nữa thấy rõ đa phần mọi người đang đi theo một chiều hướng phát triển chung với tốc độ khá giống nhau và gặp những vấn đề tương tự nhau. Trong bài viết này mình sẽ refresh lại cái nhìn về lĩnh vực Data Science và chia sẻ những con đường khác nhau để các bạn chuẩn bị. https://datasciencevn.com/chuyen-nghe/146-xu-huong-phat-trien-cua-nganh-data-science-va-ban-nen-hoc-gi-de-chuan-bi.html?fbclid=IwAR1x1snLjUVL4KCzQ1TYeIAFGu3Opmt1KsP_KrDzoEN-wVGbLKLRSKD6L0g",,,,,
"Mình đang có project OCR đọc bill hóa đơn, sau khi đọc xong thì tên sản phẩm bị sai hoặc bị thiếu. Các bạn có kinh nghiệm chỉ giúp mình key,giải pháp làm sao để ra đúng tên sản phẩm vs ạ. xin cảm ơn!","Mình đang có project OCR đọc bill hóa đơn, sau khi đọc xong thì tên sản phẩm bị sai hoặc bị thiếu. Các bạn có kinh nghiệm chỉ giúp mình key,giải pháp làm sao để ra đúng tên sản phẩm vs ạ. xin cảm ơn!",,,,,
"Các bác cho em hỏi: Em tính dùng AI tìm các hình tròn, hình vuông, đường kẻ trên phiếu trắc nghiệm ở Mobile. Em thấy bảo là AI tìm sẽ lâu và không phù hợp do chấm trắc nghiệm cần tgian <= 2 giây.
Các bác thấy phương án AI nào khả thi ko ạ?","Các bác cho em hỏi: Em tính dùng AI tìm các hình tròn, hình vuông, đường kẻ trên phiếu trắc nghiệm ở Mobile. Em thấy bảo là AI tìm sẽ lâu và không phù hợp do chấm trắc nghiệm cần tgian <= 2 giây. Các bác thấy phương án AI nào khả thi ko ạ?",,,,,
"Xin chào anh chị, em là sinh viên cdt sắp tốt nghiệp đại học Bách Khoa Hà Nội và là ngoại đạo,học AI đã được 1 năm, em đang tìm kiếm vị trí fresher AI tại Hà Nội mà thấy các job toàn yêu cầu ít nhất 1 năm kinh nghiệm Em mong nhận được lời khuyên của anh chị, em cam ơn rất nhiều ạ","Xin chào anh chị, em là sinh viên cdt sắp tốt nghiệp đại học Bách Khoa Hà Nội và là ngoại đạo,học AI đã được 1 năm, em đang tìm kiếm vị trí fresher AI tại Hà Nội mà thấy các job toàn yêu cầu ít nhất 1 năm kinh nghiệm Em mong nhận được lời khuyên của anh chị, em cam ơn rất nhiều ạ",,,,,
"Em hiện tại đang có bài tập về dự đoán giá sim theo dữ liệu cho trước. Mình nên phân loại các thuộc tính sim như thế nào ạ, mọi người có cách tiếp cận nào không ạ","Em hiện tại đang có bài tập về dự đoán giá sim theo dữ liệu cho trước. Mình nên phân loại các thuộc tính sim như thế nào ạ, mọi người có cách tiếp cận nào không ạ",,,,,
"Các ban ơi, mình đang nghiên cứu về resnet50, các bạn cho mình hỏi chi tiết về hiện tượng Vanishing, và ý nghĩa của khối phần dư.Thanks all","Các ban ơi, mình đang nghiên cứu về resnet50, các bạn cho mình hỏi chi tiết về hiện tượng Vanishing, và ý nghĩa của khối phần dư.Thanks all",,,,,
"Chào mọi người.
Em đang phân vân nên theo học master Khoa học dữ liệu ở trường Bách Khóa HN hay là Khoa Học Tự Nhiên HN, có anh/chị nào đã học 1 trong 2 trường này cho em xin ý kiến với ạ:
Tổng chi phí toàn khóa học là khoảng bao nhiêu.
Chất lượng dạy học như thế nào.
Cơ hội nghề nghiệp.
Nếu sau này xin học bổng nghiên cứu sinh ở các nước khác thì trường nào sẽ có lợi thế hơn ạ.
Cảm ơn mọi người nhiều.","Chào mọi người. Em đang phân vân nên theo học master Khoa học dữ liệu ở trường Bách Khóa HN hay là Khoa Học Tự Nhiên HN, có anh/chị nào đã học 1 trong 2 trường này cho em xin ý kiến với ạ: Tổng chi phí toàn khóa học là khoảng bao nhiêu. Chất lượng dạy học như thế nào. Cơ hội nghề nghiệp. Nếu sau này xin học bổng nghiên cứu sinh ở các nước khác thì trường nào sẽ có lợi thế hơn ạ. Cảm ơn mọi người nhiều.",,,,,
"Mọi người cho mình hỏi là sau khi tìm đạo hàm của hàm mất mát trong logistic regression thì nghiệm w của phương trình đạo hàm bằng 0 tính như thế nào?
Tai sao chúng ta ko dùng nghiệm w đó để làm w_init luôn thay tính gần đúng w bằng gradient decent?",Mọi người cho mình hỏi là sau khi tìm đạo hàm của hàm mất mát trong logistic regression thì nghiệm w của phương trình đạo hàm bằng 0 tính như thế nào? Tai sao chúng ta ko dùng nghiệm w đó để làm w_init luôn thay tính gần đúng w bằng gradient decent?,,,,,
"Kính chào các bác!
Nhân dịp đang nghiên cứu về Loan Repayment Prediction, em xin mạnh dạn chia sẻ cùng cả nhà.
Warning: Bài này chỉ dùng để học tập, khi áp dụng thực tế còn cần rất nhiều kỹ thuật xử lý nằm ngoài khả năng của em :)","Kính chào các bác! Nhân dịp đang nghiên cứu về Loan Repayment Prediction, em xin mạnh dạn chia sẻ cùng cả nhà. Warning: Bài này chỉ dùng để học tập, khi áp dụng thực tế còn cần rất nhiều kỹ thuật xử lý nằm ngoài khả năng của em :)",,,,,
"#rfm #segmentation #machinelearning #python #datascience
THE SERIES OF DATA SCIENCE WITH PYTHON - PART 3
Hi All,
Step by Step Customer Segmentation by RFM
Learn now 👇","THE SERIES OF DATA SCIENCE WITH PYTHON - PART 3 Hi All, Step by Step Customer Segmentation by RFM Learn now",#rfm	#segmentation	#machinelearning	#python	#datascience,,,,
"Tiệp cho mình share thông tin syllabus lớp Foundation of Machine Learning và Data Science (miễn phí học phí và bằng tiếng Việt) mà mình sẽ bắt đầu vào tháng 8 này.
Các bài giảng sẽ được record và upload trực tiếp trên page khoa học dữ liệu: https://www.facebook.com/khoahocvadulieu/
--- Lớp này sẽ chia thành 2 học phần: ""Introduction to Machine Learning and Data Science"" và ""Advanced Machine Learning and Data Science"".
Về học phần ""Introduction to Machine Learning and Data Science"", các bạn không cần biết nhiều về Toán vẫn học được (mình sẽ giảm thiểu Math nhiều nhất có thể). Chương trình dạy của lớp này sẽ gần với chương trình dạy về introduction to ML và DS tại các trường hàng đầu tại Mỹ.
Về học phần ""Advanced Machine Learning and Data Science"", đây dành cho các bạn muốn hiểu sâu về ML và Data Science (và có định hướng học cao học trong các ngành này). Các topics trong học phần này sẽ khá nặng về Math. Bạn học xong học phần này có thể bắt đầu làm nghiên cứu chất lượng cao bên ML và Deep Leanring.
Dưới đây mình sẽ highlight một số chủ đề mà mỗi học phần sẽ cover (danh sách các chủ đề sẽ liên tục được cập nhật tại page khoa học dữ liệu):
********** Học phần ""Introduction to Machine Learning and Data Science"":
--- Warm up: Review of basic Probability Notion
--- Introduction to Statistical Learning and Inference
--- Linear models
--- Classification Methods
--- Linear model selection and regularization
--- Nonlinear models
--- Tree-based methods
--- Support Vector Machine
--- Unsupervised Learning: Clustering, Dimension Reduction, etc.
--- Introduction to Deep Learning
--- Introduction to Reinforcement Learning
************** Học phần ""Advanced Machine Learning and Data Science"":
--- Deep Generative Model (Variational Auto Encoder, GANs, Diffusion Models, etc.)
--- Optimal Transport in Machine Learning and Data Science
--- Foundation of Variational Inference (for Bayesian methods)
--- Some Theoretical Perspective of Self-Supervised Learning
--- Transformer-based models: Old and New
--- Optimization in Statistical Models and Deep Learning
--- Deep Learning Theory
--- Foundation of Bayesian Hierarchical Mode","Tiệp cho mình share thông tin syllabus lớp Foundation of Machine Learning và Data Science (miễn phí học phí và bằng tiếng Việt) mà mình sẽ bắt đầu vào tháng 8 này. Các bài giảng sẽ được record và upload trực tiếp trên page khoa học dữ liệu: https://www.facebook.com/khoahocvadulieu/ --- Lớp này sẽ chia thành 2 học phần: ""Introduction to Machine Learning and Data Science"" và ""Advanced Machine Learning and Data Science"". Về học phần ""Introduction to Machine Learning and Data Science"", các bạn không cần biết nhiều về Toán vẫn học được (mình sẽ giảm thiểu Math nhiều nhất có thể). Chương trình dạy của lớp này sẽ gần với chương trình dạy về introduction to ML và DS tại các trường hàng đầu tại Mỹ. Về học phần ""Advanced Machine Learning and Data Science"", đây dành cho các bạn muốn hiểu sâu về ML và Data Science (và có định hướng học cao học trong các ngành này). Các topics trong học phần này sẽ khá nặng về Math. Bạn học xong học phần này có thể bắt đầu làm nghiên cứu chất lượng cao bên ML và Deep Leanring. Dưới đây mình sẽ highlight một số chủ đề mà mỗi học phần sẽ cover (danh sách các chủ đề sẽ liên tục được cập nhật tại page khoa học dữ liệu): ********** Học phần ""Introduction to Machine Learning and Data Science"": --- Warm up: Review of basic Probability Notion --- Introduction to Statistical Learning and Inference --- Linear models --- Classification Methods --- Linear model selection and regularization --- Nonlinear models --- Tree-based methods --- Support Vector Machine --- Unsupervised Learning: Clustering, Dimension Reduction, etc. --- Introduction to Deep Learning --- Introduction to Reinforcement Learning ************** Học phần ""Advanced Machine Learning and Data Science"": --- Deep Generative Model (Variational Auto Encoder, GANs, Diffusion Models, etc.) --- Optimal Transport in Machine Learning and Data Science --- Foundation of Variational Inference (for Bayesian methods) --- Some Theoretical Perspective of Self-Supervised Learning --- Transformer-based models: Old and New --- Optimization in Statistical Models and Deep Learning --- Deep Learning Theory --- Foundation of Bayesian Hierarchical Mode",,,,,
"Em hiện là sinh viên đang học tại trường Đại học Kinh tế TP.HCM.
Hiện em có một dự án Hackathon cần triển khai sản phẩm, đó là build một con máy phân loại rác tự động - giải pháp đã đạt giải Á quân cuộc thi UEH Biztech Hackathon 2022.
Hiện nhóm em đang cần làm một sản phẩm là một máy camera sử dụng công nghệ Machine Learning để phân loại rác. Em hiện đang cần tìm thêm đồng đội bên Tech, có hiểu biết về Machine Learning cùng nhóm em triển khai xây dựng sản phẩm này.
Nhóm em đã có 4 người, dự án được hoàn thành trong 6 tháng, hiện đã có khách hàng.
Dự án được hỗ trợ bởi Viện đổi mới sáng tạo UEH (UII).
Em rất mong muốn tìm gặp một teammate phù hợp đi cùng với nhóm, gặp gỡ, trao đổi cũng như học hỏi thêm nhiều kỹ năng Tech, khởi nghiệp, chiến lược marketing,... và đặc biệt là đồng hành lâu dài với nhóm.
Mọi người quan tâm liên hệ em gửi toàn bộ file kế hoạch về sản phẩm nhé ạ.
Cảm ơn mọi người rất nhiều.","Em hiện là sinh viên đang học tại trường Đại học Kinh tế TP.HCM. Hiện em có một dự án Hackathon cần triển khai sản phẩm, đó là build một con máy phân loại rác tự động - giải pháp đã đạt giải Á quân cuộc thi UEH Biztech Hackathon 2022. Hiện nhóm em đang cần làm một sản phẩm là một máy camera sử dụng công nghệ Machine Learning để phân loại rác. Em hiện đang cần tìm thêm đồng đội bên Tech, có hiểu biết về Machine Learning cùng nhóm em triển khai xây dựng sản phẩm này. Nhóm em đã có 4 người, dự án được hoàn thành trong 6 tháng, hiện đã có khách hàng. Dự án được hỗ trợ bởi Viện đổi mới sáng tạo UEH (UII). Em rất mong muốn tìm gặp một teammate phù hợp đi cùng với nhóm, gặp gỡ, trao đổi cũng như học hỏi thêm nhiều kỹ năng Tech, khởi nghiệp, chiến lược marketing,... và đặc biệt là đồng hành lâu dài với nhóm. Mọi người quan tâm liên hệ em gửi toàn bộ file kế hoạch về sản phẩm nhé ạ. Cảm ơn mọi người rất nhiều.",,,,,
"Em mới bắt đầu học python, nên bắt đầu từ đâu với có tài liệu em tham khảo với ạ","Em mới bắt đầu học python, nên bắt đầu từ đâu với có tài liệu em tham khảo với ạ",,,,,
"Hi ace, e dang làm final project của machine learning. topic là identify a bird dùng CNN. e định dùng matlab để làm. a chị nào có tài liệu liên quan tới topic nay ko? (Code, data...) cho e xin được ko?. e cảm ơn.","Hi ace, e dang làm final project của machine learning. topic là identify a bird dùng CNN. e định dùng matlab để làm. a chị nào có tài liệu liên quan tới topic nay ko? (Code, data...) cho e xin được ko?. e cảm ơn.",,,,,
"[The Kaggle book]

Mọi người học về Data Science, Machine Learning chắc không lạ gì Kaggle, một nền tảng tổ chức các cuộc thi về DS, cũng như có rất nhiều public dataset, notebook hay để học hỏi. Bằng việc tham gia các cuộc thi, các bạn sẽ được học thêm các kinh nghiệm trực quan hóa, xử lý dữ liệu, xây dựng, đánh giá mô hình. Người ta hay nói Kaggle chính là cầu nối giữa việc học lý thuyết và đi làm trong ngành khoa học dữ liệu.

Sách ""The Kaggle Book: Data analysis and machine learning for competitive data science"" được viết bởi hai Grandmaster - Kaggle. Sách tổng hợp các kĩ thuật được các Grandmaster đúc kết từ các cuộc thi mà họ tham gia. Sách phù hợp cho cả người học tham gia Kaggle lẫn những người đã có kinh nghiệm. Nếu bạn muốn cải thiện kĩ năng, học hỏi thêm kiến thức mới và đặc biệt dành thứ hạng cao hơn trong các cuộc thi trên Kaggle thì sách này là dành cho bạn.

Link sách: https://www.amazon.com/Data-Analysis-Machine-Learning-Kaggle-ebook/dp/B09F3STL34","[The Kaggle book] Mọi người học về Data Science, Machine Learning chắc không lạ gì Kaggle, một nền tảng tổ chức các cuộc thi về DS, cũng như có rất nhiều public dataset, notebook hay để học hỏi. Bằng việc tham gia các cuộc thi, các bạn sẽ được học thêm các kinh nghiệm trực quan hóa, xử lý dữ liệu, xây dựng, đánh giá mô hình. Người ta hay nói Kaggle chính là cầu nối giữa việc học lý thuyết và đi làm trong ngành khoa học dữ liệu. Sách ""The Kaggle Book: Data analysis and machine learning for competitive data science"" được viết bởi hai Grandmaster - Kaggle. Sách tổng hợp các kĩ thuật được các Grandmaster đúc kết từ các cuộc thi mà họ tham gia. Sách phù hợp cho cả người học tham gia Kaggle lẫn những người đã có kinh nghiệm. Nếu bạn muốn cải thiện kĩ năng, học hỏi thêm kiến thức mới và đặc biệt dành thứ hạng cao hơn trong các cuộc thi trên Kaggle thì sách này là dành cho bạn. Link sách: https://www.amazon.com/Data-Analysis-Machine-Learning-Kaggle-ebook/dp/B09F3STL34",,,,,
"dạ mọi người có ai bik về tài liệu hay trang web nào nói về cái thuật toán bi-directional matching được nhắc đến trong bài báo
Implicit Skills Extraction Using Document Embedding and Its Use in Job Recommendation , thì cho em xin link hoặc tài liệu với ạ , em đang cần tìm hiểu về thuật toán này, em cảm ơn","dạ mọi người có ai bik về tài liệu hay trang web nào nói về cái thuật toán bi-directional matching được nhắc đến trong bài báo Implicit Skills Extraction Using Document Embedding and Its Use in Job Recommendation , thì cho em xin link hoặc tài liệu với ạ , em đang cần tìm hiểu về thuật toán này, em cảm ơn",,,,,
"#hoidap
Chào mọi người ạ. Anh chị có thể giải thích đơn giản giúp em về phương sai không đổi trong trường hợp này không ạ ? em đọc mà không hiểu lắm. Cám ơn mọi người nhiều.",Chào mọi người ạ. Anh chị có thể giải thích đơn giản giúp em về phương sai không đổi trong trường hợp này không ạ ? em đọc mà không hiểu lắm. Cám ơn mọi người nhiều.,#hoidap,,,,
"Em chào mọi người
Em đang thử nghiệm bộ dataset car trong bài toán classification. Em dùng model resnet50 nhưng độ chính xác chỉ được tầm 60%
Mọi người cho em lời khuyên ạ",Em chào mọi người Em đang thử nghiệm bộ dataset car trong bài toán classification. Em dùng model resnet50 nhưng độ chính xác chỉ được tầm 60% Mọi người cho em lời khuyên ạ,,,,,
,nan,,,,,
"Tìm giải pháp cho bài toán Regression với imbalance data.
Chào cả nhà, em đang muốn tìm một giải pháp để giải quyết bài toán regression dự đoán sản lượng cá. Với output là 'ton' là sản lượng cá đánh đánh bắt được. Với giả thuyết đưa ra là nếu tàu đang quay đầu chuyển hướng thì lượng cá đánh bắt được là 0, còn nếu đi như bình thường thì lượng cá sẽ được record lại trong cột 'ton'. Với label ton = 0 đang chiếm đến 70% trên tổng số samples.Dưới đây là link đến report về dataset và feature detail.
Report: https://drive.google.com/file/d/172O870cB5btjVRaZyonaWNG98tiR_Rcj/view?usp=sharing
Data detail: https://github.com/NgToanRob/AI4Sea/blob/main/data_detail.md
Cảm ơn sự giúp đỡ của mọi người ạ! <3 ","Tìm giải pháp cho bài toán Regression với imbalance data. Chào cả nhà, em đang muốn tìm một giải pháp để giải quyết bài toán regression dự đoán sản lượng cá. Với output là 'ton' là sản lượng cá đánh đánh bắt được. Với giả thuyết đưa ra là nếu tàu đang quay đầu chuyển hướng thì lượng cá đánh bắt được là 0, còn nếu đi như bình thường thì lượng cá sẽ được record lại trong cột 'ton'. Với label ton = 0 đang chiếm đến 70% trên tổng số samples.Dưới đây là link đến report về dataset và feature detail. Report: https://drive.google.com/file/d/172O870cB5btjVRaZyonaWNG98tiR_Rcj/view?usp=sharing Data detail: https://github.com/NgToanRob/AI4Sea/blob/main/data_detail.md Cảm ơn sự giúp đỡ của mọi người ạ! <3",,,,,
"Mình muốn tìm khóa học AI online , vì mình mới chuyển sang mảng này và muốn có kiến thức nền tảng vững để xin việc
Mọi người review giúp mình với ạ
Mình cảm ơn nhiều","Mình muốn tìm khóa học AI online , vì mình mới chuyển sang mảng này và muốn có kiến thức nền tảng vững để xin việc Mọi người review giúp mình với ạ Mình cảm ơn nhiều",,,,,
"Tìm Model Speech-to-text cho tiếng Việt.text cho tiếng Việt.
Chào cả nhà, mình muốn tim 1 số model xử lý Speech-to-text cho tiếng Việt có thể nhận diện được những từ tiếng anh.
Hiện mình có dùng API của Azure và cũng đã xử lý khá tốt 1 số từ tiếng Anh trong business, tuy nhiên chỉ tầm 20% được cover.
(Trước mình có đọc được có team đã training model bằng data manual subtitle trên youtube. Mình thấy khá hay nhưng giờ không tìm thấy đâu)
Rất mong nhận được sự giúp đỡ của cả nhà ạ!","Tìm Model Speech-to-text cho tiếng Việt.text cho tiếng Việt. Chào cả nhà, mình muốn tim 1 số model xử lý Speech-to-text cho tiếng Việt có thể nhận diện được những từ tiếng anh. Hiện mình có dùng API của Azure và cũng đã xử lý khá tốt 1 số từ tiếng Anh trong business, tuy nhiên chỉ tầm 20% được cover. (Trước mình có đọc được có team đã training model bằng data manual subtitle trên youtube. Mình thấy khá hay nhưng giờ không tìm thấy đâu) Rất mong nhận được sự giúp đỡ của cả nhà ạ!",,,,,
"Em chào cả nhà. Dạ cho em hỏi hiện tại nhóm tụi em đang nghiên cứu về vấn đề nhận diện các động tác Yoga và hiện tại đang dự tính dùng bộ dataset Yoga-82. Nhưng sau khi em download data từ các link url ảnh về (vì các ảnh của bộ dataset này lưu dưới dạng file url) thì hiện có rất nhiều ảnh không còn lấy về được nữa. Không biết hiện tại còn cách nào lấy bộ dataset này trọn vẹn hay không ạ và còn những bộ dataset nào khác liên quan đến vấn đề này không ạ ?
Em xin cảm ơn ạ.",Em chào cả nhà. Dạ cho em hỏi hiện tại nhóm tụi em đang nghiên cứu về vấn đề nhận diện các động tác Yoga và hiện tại đang dự tính dùng bộ dataset Yoga-82. Nhưng sau khi em download data từ các link url ảnh về (vì các ảnh của bộ dataset này lưu dưới dạng file url) thì hiện có rất nhiều ảnh không còn lấy về được nữa. Không biết hiện tại còn cách nào lấy bộ dataset này trọn vẹn hay không ạ và còn những bộ dataset nào khác liên quan đến vấn đề này không ạ ? Em xin cảm ơn ạ.,,,,,
"Em chào mọi người ạ, mọi người cho em hỏi ai có tài liệu về triển khai mô hình seq2seq với Attention sử dụng BiLSTM và Depth LSTM ko ạ? Nếu có thì có thể cho em xin được ko ạ, hoặc ko thì có thể ib để em hỏi chút được ko ạ?
Em cảm ơn mọi người nhiều ạ","Em chào mọi người ạ, mọi người cho em hỏi ai có tài liệu về triển khai mô hình seq2seq với Attention sử dụng BiLSTM và Depth LSTM ko ạ? Nếu có thì có thể cho em xin được ko ạ, hoặc ko thì có thể ib để em hỏi chút được ko ạ? Em cảm ơn mọi người nhiều ạ",,,,,
"[𝐕𝐈𝐄𝐓𝐀𝐈 𝐱 𝐇𝐔𝐆𝐆𝐈𝐍𝐆 𝐅𝐀𝐂𝐄]
🔥VietAI kết hợp Hugging Face công bố mã nguồn JAX để huấn luyện Stable Diffusion, Textual Inversion, DreamBooth: Nhanh hơn 𝟕𝟎% so với PyTorch.
📌𝐅𝐢𝐧𝐞-𝐭𝐮𝐧𝐢𝐧𝐠 𝐒𝐭𝐚𝐛𝐥𝐞 𝐃𝐢𝐟𝐟𝐮𝐬𝐢𝐨𝐧: https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-flaxjax
📌𝐓𝐞𝐱𝐭𝐮𝐚𝐥 𝐈𝐧𝐯𝐞𝐫𝐬𝐢𝐨𝐧:
https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#training-with-flaxjax
📌𝐃𝐫𝐞𝐚𝐦𝐁𝐨𝐨𝐭𝐡: https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#running-with-flaxjax","[ ] VietAI kết hợp Hugging Face công bố mã nguồn JAX để huấn luyện Stable Diffusion, Textual Inversion, DreamBooth: Nhanh hơn % so với PyTorch. - : https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-flaxjax : https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#training-with-flaxjax : https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#running-with-flaxjax",,,,,
Anh chị cho em hỏi phần max_depth trong thuật toán cây quyết định với hàm đánh giá là entropy em để lớn hơn số thuộc tính thì khi đó độ chính xác nó vẫn tăng là thế nào ạ. A/c giải đáp e với ạ. E cám ơn.,Anh chị cho em hỏi phần max_depth trong thuật toán cây quyết định với hàm đánh giá là entropy em để lớn hơn số thuộc tính thì khi đó độ chính xác nó vẫn tăng là thế nào ạ. A/c giải đáp e với ạ. E cám ơn.,,,,,
"Chào mọi người, em là người mới nhập môn, thầy em có giao đề tài nghiên cứu về machine learning model pipeline. Em có vài thắc mắc. Pipeline có phải là quy trình tiền xử lý xong mình bỏ các model vào rồi chọn ra model tốt nhất không ạ.Với lại machine learning model pipeline gồm những model nào (có CNN hay mấy cái tương tự không ạ ?). Ai có tài liệu tham khảo, hay hướng dẫn giúp em với ạ. Em cảm ơn ạ!","Chào mọi người, em là người mới nhập môn, thầy em có giao đề tài nghiên cứu về machine learning model pipeline. Em có vài thắc mắc. Pipeline có phải là quy trình tiền xử lý xong mình bỏ các model vào rồi chọn ra model tốt nhất không ạ.Với lại machine learning model pipeline gồm những model nào (có CNN hay mấy cái tương tự không ạ ?). Ai có tài liệu tham khảo, hay hướng dẫn giúp em với ạ. Em cảm ơn ạ!",,,,,
"Hi mọi người,
Mọi người cho em hỏi là nếu mình train YOLO trên kaggle, config của YOLO là phải có label với images chung 1 thư mục.Với lượng data ở thư mục input nhỏ thì mình copy qua thư mục /kaggle/working và config train được. Nhưng trong trường hợp dữ liệu 30Gb ở thư mục input, thì làm sao để mình config và train được (nghĩa là tạo 1 thư mục sao cho có ảnh và label trong khi thư mục /kaggle/working chỉ có 19.5Gb).
Cảm ơn mọi người ạ.","Hi mọi người, Mọi người cho em hỏi là nếu mình train YOLO trên kaggle, config của YOLO là phải có label với images chung 1 thư mục.Với lượng data ở thư mục input nhỏ thì mình copy qua thư mục /kaggle/working và config train được. Nhưng trong trường hợp dữ liệu 30Gb ở thư mục input, thì làm sao để mình config và train được (nghĩa là tạo 1 thư mục sao cho có ảnh và label trong khi thư mục /kaggle/working chỉ có 19.5Gb). Cảm ơn mọi người ạ.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2022 vào comment của post này.
Xin lỗi vì đã sắp hết tháng 9 rồi.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2022 vào comment của post này. Xin lỗi vì đã sắp hết tháng 9 rồi.",,,,,
"Chào mọi người, hiện em muốn tìm 1 model huấn luyện để có thể tạo 1 câu hoàn chỉnh từ các key input. VD input: [A,B,5p] => Output: "" Đi từ A đến B mất 5p"". Mọi người giúp em với ạ","Chào mọi người, hiện em muốn tìm 1 model huấn luyện để có thể tạo 1 câu hoàn chỉnh từ các key input. VD input: [A,B,5p] => Output: "" Đi từ A đến B mất 5p"". Mọi người giúp em với ạ",,,,,
"Không biết có ai biết model translate VI -> EN nào tốt không ạ?
Hiện team mình có thử 1 số translation model nhưng kết quả không được như ý lắm... Mình cũng bẵng 1 thời gian không cập nhật thêm những dự án mới nên rất mong mọi người có thể giúp đỡ ạ
Mình có vừa thử 1 số model như ở dưới
https://huggingface.co/vinai/vinai-translate-vi2en
https://huggingface.co/NlpHUST/t5-en-vi-small",Không biết có ai biết model translate VI -> EN nào tốt không ạ? Hiện team mình có thử 1 số translation model nhưng kết quả không được như ý lắm... Mình cũng bẵng 1 thời gian không cập nhật thêm những dự án mới nên rất mong mọi người có thể giúp đỡ ạ Mình có vừa thử 1 số model như ở dưới https://huggingface.co/vinai/vinai-translate-vi2en https://huggingface.co/NlpHUST/t5-en-vi-small,,,,,
"Mọi người có biết platform nào ở VN mình cho (data) freelancers không? Giống như Got-it.ai hoặc Freelancer.com.
Mình hay có mấy việc nhỏ nhỏ cần khoảng 2 - 3 tiếng mà đều đăng lên freelancer.com hết cả. Mấy bạn Ấn Độ + Bangladesh nộp proposals nhiều, mà không thấy ai ở VN cả.","Mọi người có biết platform nào ở VN mình cho (data) freelancers không? Giống như Got-it.ai hoặc Freelancer.com. Mình hay có mấy việc nhỏ nhỏ cần khoảng 2 - 3 tiếng mà đều đăng lên freelancer.com hết cả. Mấy bạn Ấn Độ + Bangladesh nộp proposals nhiều, mà không thấy ai ở VN cả.",,,,,
"Em xin chào mọi người.
Hiện em có dự định nghiên cứu theo những mảng liên quan toán nhiều trong machine learning như machine reasoning, bayesian inference & optimal transport do em có hứng thú và tò mò về lý thuyết cũng như cách toán học vận hành trong ML/DL.
Về kiến thức em đã đọc qua các cuốn sách như ML Cơ bản của anh Tiệp, Dive into DL, Mathematics in ML, Linear Algebra by Gilbert Strang, All of Statistics: A Concise Course in Statistics (50%) và sách của Bishop (50%). Tuy nhiên về kinh nghiệm nghiên cứu thì em tập trung chủ yếu ở integrated CV & NLP cộng một ít về GNN, em vẫn chưa hình dung hết về cách nghiên cứu của các mảng thiên về toán nhiều.
Vấn đề hiện tại của em là tuy em đã cố gắng bổ sung các kiến thức nền tảng và khả năng nghiên cứu bên những mảng khác, nhưng em vẫn không thể đọc hiểu một paper thuần về mathematics và statistic trong ML, cũng như em vẫn chưa nắm khái niệm cụ thể về những cải tiến hay lý thuyết trong mảng này.
Em muốn hỏi kinh nghiệm mọi người đi trước là khi bắt đầu thì liệu có những paper tiêu biểu, hay những blog/link nào hướng dẫn chi tiết cho những bước khởi đầu này không ạ. Về hướng đi tiếp theo em vẫn tính theo những mảng đã có nghiên cứu, nhưng về những bước lâu hơn em vẫn muốn theo đuổi mảng nặng về toán và lý thuyết hơn, nên nếu được em mong mọi người chia sẻ em những kinh nghiệm và hướng hợp lý khi bắt đầu mảng này ạ :'>.
Em xin chân thành cảm ơn mọi người ạ.","Em xin chào mọi người. Hiện em có dự định nghiên cứu theo những mảng liên quan toán nhiều trong machine learning như machine reasoning, bayesian inference & optimal transport do em có hứng thú và tò mò về lý thuyết cũng như cách toán học vận hành trong ML/DL. Về kiến thức em đã đọc qua các cuốn sách như ML Cơ bản của anh Tiệp, Dive into DL, Mathematics in ML, Linear Algebra by Gilbert Strang, All of Statistics: A Concise Course in Statistics (50%) và sách của Bishop (50%). Tuy nhiên về kinh nghiệm nghiên cứu thì em tập trung chủ yếu ở integrated CV & NLP cộng một ít về GNN, em vẫn chưa hình dung hết về cách nghiên cứu của các mảng thiên về toán nhiều. Vấn đề hiện tại của em là tuy em đã cố gắng bổ sung các kiến thức nền tảng và khả năng nghiên cứu bên những mảng khác, nhưng em vẫn không thể đọc hiểu một paper thuần về mathematics và statistic trong ML, cũng như em vẫn chưa nắm khái niệm cụ thể về những cải tiến hay lý thuyết trong mảng này. Em muốn hỏi kinh nghiệm mọi người đi trước là khi bắt đầu thì liệu có những paper tiêu biểu, hay những blog/link nào hướng dẫn chi tiết cho những bước khởi đầu này không ạ. Về hướng đi tiếp theo em vẫn tính theo những mảng đã có nghiên cứu, nhưng về những bước lâu hơn em vẫn muốn theo đuổi mảng nặng về toán và lý thuyết hơn, nên nếu được em mong mọi người chia sẻ em những kinh nghiệm và hướng hợp lý khi bắt đầu mảng này ạ :'>. Em xin chân thành cảm ơn mọi người ạ.",,,,,
"Smartphone dùng kỹ thuật gì để nhận dạng khuôn mặt?
Siamese network hoạt động dựa trên kỹ thuật one shot learning không đòi hỏi huấn luyện, cho mạng hai bức ảnh khuôn mặt và mạng sẽ ra kết luận 2 ảnh này có giống nhau không.
Phương pháp này giúp nhận dạng tương đối chính xác, có thể cài đặt trên điện thoại Android hay iOS.
https://github.com/adityajn105/Face-Recognition-Siamese-Network
https://en.wikipedia.org/wiki/Siamese_neural_network","Smartphone dùng kỹ thuật gì để nhận dạng khuôn mặt? Siamese network hoạt động dựa trên kỹ thuật one shot learning không đòi hỏi huấn luyện, cho mạng hai bức ảnh khuôn mặt và mạng sẽ ra kết luận 2 ảnh này có giống nhau không. Phương pháp này giúp nhận dạng tương đối chính xác, có thể cài đặt trên điện thoại Android hay iOS. https://github.com/adityajn105/Face-Recognition-Siamese-Network https://en.wikipedia.org/wiki/Siamese_neural_network",,,,,
"End to End Machine Learning Projects
19 Videos
https://www.youtube.com/playlist?list=PL_1pt6K-CLoDcWw_c196kZn7aPeLdRDOU",End to End Machine Learning Projects 19 Videos https://www.youtube.com/playlist?list=PL_1pt6K-CLoDcWw_c196kZn7aPeLdRDOU,,,,,
"[AI Share - aman.ai]
aman.ai là một trang web tổng hợp rất nhiều thứ về AI, được chia làm 5 phần chính:
- Distilled AI: tổng hợp các khóa học của Stanford CS229, CS230, CS231n, CS224n, Recommendation Systems, Coursera Deep learning,...
- Research: tổng hợp các paper về NLP, CV,..
- Primers: tuyển tập các bài viết về các khái niệm, nguyên tắc, quá trình huấn luyện, đánh giá kết quả. Có cả chia theo các thư viện như Numpy, Pandas, Pytorch, Tensorflow,Math,..
- Coding: từ các thuật toán cơ bản như Sort/Search đến Cấu trúc dữ liệu nâng cao hơn như Stack, Queue, Linked List, Binary tree,.. và độ phức tạp của thuật toán.
- Reading: tổng hợp các list paper chia theo các chủ đề theo từng năm và những ghi chú, các sách, blog, video tham khảo theo các chủ đề.
Link: https://aman.ai/","[AI Share - aman.ai] aman.ai là một trang web tổng hợp rất nhiều thứ về AI, được chia làm 5 phần chính: - Distilled AI: tổng hợp các khóa học của Stanford CS229, CS230, CS231n, CS224n, Recommendation Systems, Coursera Deep learning,... - Research: tổng hợp các paper về NLP, CV,.. - Primers: tuyển tập các bài viết về các khái niệm, nguyên tắc, quá trình huấn luyện, đánh giá kết quả. Có cả chia theo các thư viện như Numpy, Pandas, Pytorch, Tensorflow,Math,.. - Coding: từ các thuật toán cơ bản như Sort/Search đến Cấu trúc dữ liệu nâng cao hơn như Stack, Queue, Linked List, Binary tree,.. và độ phức tạp của thuật toán. - Reading: tổng hợp các list paper chia theo các chủ đề theo từng năm và những ghi chú, các sách, blog, video tham khảo theo các chủ đề. Link: https://aman.ai/",,,,,
"[Câu hỏi về LSTM][Pytorch]
Mọi người cho mình hỏi là có cần thiết phải initialize hidden state trong hàm forward không ạ?
Mình cảm ơn mọi người ạ.",[Câu hỏi về LSTM][Pytorch] Mọi người cho mình hỏi là có cần thiết phải initialize hidden state trong hàm forward không ạ? Mình cảm ơn mọi người ạ.,,,,,
"Em chào mọi người ạ, hiện tại em đang làm tiểu luận chuyên ngành về sử dụng Polynomial Classifiers để dự đoán giá cổ phiếu nhưng đang gặp khó khăn về tìm kiếm thuật toán cũng như cách tính về model Polynomial Classifiers. Anh chị nào có tài liệu liên quan đến Polynomial Classifiers có thể cho em xin thao khảo được không ạ. Em cảm ơn mọi người nhiều ạ","Em chào mọi người ạ, hiện tại em đang làm tiểu luận chuyên ngành về sử dụng Polynomial Classifiers để dự đoán giá cổ phiếu nhưng đang gặp khó khăn về tìm kiếm thuật toán cũng như cách tính về model Polynomial Classifiers. Anh chị nào có tài liệu liên quan đến Polynomial Classifiers có thể cho em xin thao khảo được không ạ. Em cảm ơn mọi người nhiều ạ",,,,,
"Kính chào các bác. Em đang nghiên cứu về phần này nên mạnh dạn chia sẻ cho mọi người cách triển khai Object Detection on Video Streaming trên web.
Hi vọng giúp được cả nhà!",Kính chào các bác. Em đang nghiên cứu về phần này nên mạnh dạn chia sẻ cho mọi người cách triển khai Object Detection on Video Streaming trên web. Hi vọng giúp được cả nhà!,,,,,
"Hiện tại em đang thực hiện đồ án về self-driving car. Em có tham khảo git Behavioral Cloning thực hiện theo thì chạy được trên map mẫu, nhưng khi em thực hiện ở map custom khác thì không được tốt cho lắm. Anh chị nào có kinh nghiệm có thể gợi ý hoặc hỗ trợ em được không ạ.
Link github em tham khảo: https://github.com/ndrplz/self-driving-car
#self_driving_car #autonomous_vehicle","Hiện tại em đang thực hiện đồ án về self-driving car. Em có tham khảo git Behavioral Cloning thực hiện theo thì chạy được trên map mẫu, nhưng khi em thực hiện ở map custom khác thì không được tốt cho lắm. Anh chị nào có kinh nghiệm có thể gợi ý hoặc hỗ trợ em được không ạ. Link github em tham khảo: https://github.com/ndrplz/self-driving-car",#self_driving_car	#autonomous_vehicle,,,,
"Xin giới thiệu với mọi người một tài liệu khủng (2188 trang) do hai giáo sư Jean Gallier & Jocelyn Quaintance của University of Pennsylvania biên soạn. Tài liệu gần như cover hết những chủ đề toán học phục vụ cho Machine Learning/Deep Learning.
https://www.cis.upenn.edu/~jean/math-deep.pdf
Source: https://twitter.com/deliprao/status/1582531571394916352",Xin giới thiệu với mọi người một tài liệu khủng (2188 trang) do hai giáo sư Jean Gallier & Jocelyn Quaintance của University of Pennsylvania biên soạn. Tài liệu gần như cover hết những chủ đề toán học phục vụ cho Machine Learning/Deep Learning. https://www.cis.upenn.edu/~jean/math-deep.pdf Source: https://twitter.com/deliprao/status/1582531571394916352,,,,,
"Mình (mới) dùng RSelenium để download data từ URL này:
https://www.cbd.gov.au/get-assessed/how/find-rated-building
Mình muốn chọn một state (ví dụ ACT), click Search để hiển thị tất cả các kết quả của bang ACT này. Tuy nhiên khi findelement để chọn state thì đều bị lỗi không tìm được (NoSuchElement: An element could not be located on the page using the given search parameters).
Code dưới đây mình tìm theo Xpath:
stateselect <- remDr$findElement(using = 'xpath', value = '//*[@id=""state""]')
Hình đính kèm là inspect page và screenshots hiển thị kết quả mong muốn.
Nhờ mọi người xem giúp.
Mình cảm ơn.
C","Mình (mới) dùng RSelenium để download data từ URL này: https://www.cbd.gov.au/get-assessed/how/find-rated-building Mình muốn chọn một state (ví dụ ACT), click Search để hiển thị tất cả các kết quả của bang ACT này. Tuy nhiên khi findelement để chọn state thì đều bị lỗi không tìm được (NoSuchElement: An element could not be located on the page using the given search parameters). Code dưới đây mình tìm theo Xpath: stateselect <- remDr$findElement(using = 'xpath', value = '//*[@id=""state""]') Hình đính kèm là inspect page và screenshots hiển thị kết quả mong muốn. Nhờ mọi người xem giúp. Mình cảm ơn. C",,,,,
"Em chào mọi người , em có một câu hỏi về việc sử dụng các model text2text như BART, T5,... với các bài toán extract/abstract keywords/keyphrasekeyphrase được ngăn cách bởi dấu , hoặc ;

Khi model predict sử dụng model.generate chúng ta chỉ có thể kiểm soát đươc max_length của list keywords/keyphrase đó chứ không kiểm soát được max_length của từng keyword hoặc keyphrase đó

Mọi người có ý tưởng gì dể giải quyết vấn đề này không ạ, ở việc training hay đánh giá

Em cảm ơn mọi người !
#nlp #huggingface
 — đang cảm thấy bối rối.","Em chào mọi người , em có một câu hỏi về việc sử dụng các model text2text như BART, T5,... với các bài toán extract/abstract keywords/keyphrasekeyphrase được ngăn cách bởi dấu , hoặc ; Khi model predict sử dụng model.generate chúng ta chỉ có thể kiểm soát đươc max_length của list keywords/keyphrase đó chứ không kiểm soát được max_length của từng keyword hoặc keyphrase đó Mọi người có ý tưởng gì dể giải quyết vấn đề này không ạ, ở việc training hay đánh giá Em cảm ơn mọi người ! — đang cảm thấy bối rối.",#nlp	#huggingface,,,,
"Random Seed
A. VẤN ĐỀ
Khi bạn đọc 1 paper về chủ đề mình quan tâm và có public code trên github, hoặc đơn giản bạn muốn tải 1 project trên github về và thực hiện lại trên máy tính của mình, thì khả năng kết quả thu được không giống với paper gốc rất cao. Thậm chí khi bạn rebuild project với cấu hình máy tính, cấu trúc dữ liệu, kiến trúc mô hình, … giống nhau nhưng kết quả thu được sau mỗi lần chạy có thể khác nhau. Bạn đã lo lắng, nghi ngờ về khả năng của bản thân? Xin bạn hãy yên tâm, điều đó là hoàn toàn bình thường. Vậy nguyên nhân do đâu và cách khắc phục như thế nào? Chúng ta hãy cùng tìm hiểu về vấn đề này ngay sau đây.
B. CÁCH GIẢI QUYẾT
Có rất nhiều nguyên nhân và cách khắc phục tình trạng này, bạn có thể tham khảo trong các tài liệu về Reproducibility. Trong đó các vấn đề về random seed như việc khởi tạo các giá trị ngẫu nhiên, các biến đổi tăng cường dữ liệu ngẫu nhiên, thêm ngẫu nhiên các nhiễu, việc chọn lựa ngẫu nhiên các layers ẩn, dropout, … cũng một phần làm thay đổi kết quả sau mỗi lần thực hiện. Trong bài viết này chúng ta cùng tìm hiểu đôi chút về Random Seed, cũng như cách sử dụng Random Seed một cách hiệu quả.
Random Seed là gì?
Random Seed được sử dụng để đảm bảo khi thực hiện re-implement project qua nhiều lần với cùng input, code, model có thể cho ra cùng 1 output
2. Làm thế nào để sử dụng Random Seed 1 cách hiệu quả?
Sau đây là 3 phương pháp phổ biến được áp dụng:
2.1 Kiểm soát quá trình tạo số ngẫu nhiên (Random Number Generator - RNG) trong PyTorch cho các thiết bị CPU và CUDA
import torch
torch.manual_seed(seed_value)
2.2 Đặt cố định các giá trị ngẫu nhiên trong Python
import random
random.seed(seed_value)
2.3 Trình tạo giá trị ngẫu nhiên trong Numpy
import numpy as np
np.random.seed(seed_value)
Trong đó seed_value là 1 con số tùy ý.
C. VÍ DỤ MINH HỌA
import numpy as np
import torch
import os
import random
#############################
def seed_everything(seed):
os.environ['PYTHONHASHSEED'] = str(seed) # set environ
random.seed(seed) # set python seed
np.random.seed(seed) # seed the global NumPy RNG
torch.manual_seed(seed) # seed the RNG for all devices (both CPU and CUDA):
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)
#############################
seed = 152022 # set seed value
seed_everything(seed)
Áp dụng trong phân chia tập dữ liệu thành train/test sử dụng hàm train_test_split trong sklearn.model_selection. Khi đó tham số random_state = 42 set random seed cùng một giá trị mỗi khi bạn chạy code sau. Có nghĩa là bạn sẽ nhận cùng 1 phân bố dữ liệu (y_train, y_test) ở mỗi lần phân chia
from sklearn import datasets
from sklearn.model_selection import train_test_split
iris = datasets.load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
np.bincount(y_train) # array([31, 37, 37])
np.bincount(y_test) # array([19, 13, 13])
Nếu bỏ qua tham số random_state thì phân bố dữ liệu ở mỗi lần chạy sẽ khác nhau
X_train, X_tesst, y_train, y_test = train_test_split(X, y, test_size = 0.3 )
np.bincount(y_train) # array([37, 34, 34]) # array([33, 40, 32])
np.bincount(y_test) # array([13, 16, 16]) # array([17, 10, 18])
D. KẾT LUẬN
Khi kiểm soát được giá trị ngẫu nhiên thì mỗi lần thực hiện lại project với cùng input và kiến trúc mô hình, code, … thì sẽ thu được cùng output
E. THAM KHẢO
https://pytorch.org/docs/stable/notes/randomness.html
https://neptune.ai/blog/how-to-solve-reproducibility-in-ml?fbclid=IwAR0MgKGQ2orpoeV1H3yESOFDpwQd-YPrdu2HmjeCHOt4Fuj4LkguHEIqZw4
https://vitalflux.com/why-use-random-seed-in-machine-learning/
https://towardsdatascience.com/how-to-use-random-seeds-effectively-54a4cd855a79","Random Seed A. VẤN ĐỀ Khi bạn đọc 1 paper về chủ đề mình quan tâm và có public code trên github, hoặc đơn giản bạn muốn tải 1 project trên github về và thực hiện lại trên máy tính của mình, thì khả năng kết quả thu được không giống với paper gốc rất cao. Thậm chí khi bạn rebuild project với cấu hình máy tính, cấu trúc dữ liệu, kiến trúc mô hình, … giống nhau nhưng kết quả thu được sau mỗi lần chạy có thể khác nhau. Bạn đã lo lắng, nghi ngờ về khả năng của bản thân? Xin bạn hãy yên tâm, điều đó là hoàn toàn bình thường. Vậy nguyên nhân do đâu và cách khắc phục như thế nào? Chúng ta hãy cùng tìm hiểu về vấn đề này ngay sau đây. B. CÁCH GIẢI QUYẾT Có rất nhiều nguyên nhân và cách khắc phục tình trạng này, bạn có thể tham khảo trong các tài liệu về Reproducibility. Trong đó các vấn đề về random seed như việc khởi tạo các giá trị ngẫu nhiên, các biến đổi tăng cường dữ liệu ngẫu nhiên, thêm ngẫu nhiên các nhiễu, việc chọn lựa ngẫu nhiên các layers ẩn, dropout, … cũng một phần làm thay đổi kết quả sau mỗi lần thực hiện. Trong bài viết này chúng ta cùng tìm hiểu đôi chút về Random Seed, cũng như cách sử dụng Random Seed một cách hiệu quả. Random Seed là gì? Random Seed được sử dụng để đảm bảo khi thực hiện re-implement project qua nhiều lần với cùng input, code, model có thể cho ra cùng 1 output 2. Làm thế nào để sử dụng Random Seed 1 cách hiệu quả? Sau đây là 3 phương pháp phổ biến được áp dụng: 2.1 Kiểm soát quá trình tạo số ngẫu nhiên (Random Number Generator - RNG) trong PyTorch cho các thiết bị CPU và CUDA import torch torch.manual_seed(seed_value) 2.2 Đặt cố định các giá trị ngẫu nhiên trong Python import random random.seed(seed_value) 2.3 Trình tạo giá trị ngẫu nhiên trong Numpy import numpy as np np.random.seed(seed_value) Trong đó seed_value là 1 con số tùy ý. C. VÍ DỤ MINH HỌA import numpy as np import torch import os import random ############################# def seed_everything(seed): os.environ['PYTHONHASHSEED'] = str(seed) # set environ random.seed(seed) # set python seed np.random.seed(seed) # seed the global NumPy RNG torch.manual_seed(seed) # seed the RNG for all devices (both CPU and CUDA): torch.cuda.manual_seed_all(seed) torch.use_deterministic_algorithms(True) ############################# seed = 152022 # set seed value seed_everything(seed) Áp dụng trong phân chia tập dữ liệu thành train/test sử dụng hàm train_test_split trong sklearn.model_selection. Khi đó tham số random_state = 42 set random seed cùng một giá trị mỗi khi bạn chạy code sau. Có nghĩa là bạn sẽ nhận cùng 1 phân bố dữ liệu (y_train, y_test) ở mỗi lần phân chia from sklearn import datasets from sklearn.model_selection import train_test_split iris = datasets.load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) np.bincount(y_train) # array([31, 37, 37]) np.bincount(y_test) # array([19, 13, 13]) Nếu bỏ qua tham số random_state thì phân bố dữ liệu ở mỗi lần chạy sẽ khác nhau X_train, X_tesst, y_train, y_test = train_test_split(X, y, test_size = 0.3 ) np.bincount(y_train) # array([37, 34, 34]) # array([33, 40, 32]) np.bincount(y_test) # array([13, 16, 16]) # array([17, 10, 18]) D. KẾT LUẬN Khi kiểm soát được giá trị ngẫu nhiên thì mỗi lần thực hiện lại project với cùng input và kiến trúc mô hình, code, … thì sẽ thu được cùng output E. THAM KHẢO https://pytorch.org/docs/stable/notes/randomness.html https://neptune.ai/blog/how-to-solve-reproducibility-in-ml?fbclid=IwAR0MgKGQ2orpoeV1H3yESOFDpwQd-YPrdu2HmjeCHOt4Fuj4LkguHEIqZw4 https://vitalflux.com/why-use-random-seed-in-machine-learning/ https://towardsdatascience.com/how-to-use-random-seeds-effectively-54a4cd855a79",,,,,
"Có vẻ như Collab đã thay đổi dịch vụ sử dụng tài nguyên của mình.
Mọi người cho mình xin review gói Collab pro với, không biết có được P100 như ngày trước không ạ, mình xin cảm ơn !","Có vẻ như Collab đã thay đổi dịch vụ sử dụng tài nguyên của mình. Mọi người cho mình xin review gói Collab pro với, không biết có được P100 như ngày trước không ạ, mình xin cảm ơn !",,,,,
"Các tiền bối cho em xin bí quyết tăng độ chính xác của mạng Siamese Model . em dùng Xception + Triplet Loss ( Khoảng cách Euclid ).
Độ chính xác tầm 0.66 ~ 0.72 là hết cỡ",Các tiền bối cho em xin bí quyết tăng độ chính xác của mạng Siamese Model . em dùng Xception + Triplet Loss ( Khoảng cách Euclid ). Độ chính xác tầm 0.66 ~ 0.72 là hết cỡ,,,,,
"Chào mọi người,

Hiện tại em đang tìm cách xây dựng lại mô hình từ bài báo
Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion của Đại học Khoa Học Tự Nhiên TPHCM.
Trong bài báo có sử dụng bộ dữ liệu lifelog của CLEF. 
Nhưng em chưa có được dữ liệu.
Không biết anh chị nào có bộ dữ liệu này hoặc 1 bộ lifelog khác không ạ?","Chào mọi người, Hiện tại em đang tìm cách xây dựng lại mô hình từ bài báo Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion của Đại học Khoa Học Tự Nhiên TPHCM. Trong bài báo có sử dụng bộ dữ liệu lifelog của CLEF. Nhưng em chưa có được dữ liệu. Không biết anh chị nào có bộ dữ liệu này hoặc 1 bộ lifelog khác không ạ?",,,,,
# Huyen Chip,# Huyen Chip,,,,,
"#hoidap
Cho em hỏi trong 1 dataset có một số cột thì dùng standard scaler , 1 số cột dùng Min max sacler vậy có ảnh hưởng đến kết quả train không ?
Cám ơn mọi người ạ !","Cho em hỏi trong 1 dataset có một số cột thì dùng standard scaler , 1 số cột dùng Min max sacler vậy có ảnh hưởng đến kết quả train không ? Cám ơn mọi người ạ !",#hoidap,,,,
mình muốn tìm tài liệu trainning module của mediapipe về Hands. bạn nào biết ở đâu có cho mình tham khảo với ạ :'). mình cảm ơn rất nhiều,mình muốn tìm tài liệu trainning module của mediapipe về Hands. bạn nào biết ở đâu có cho mình tham khảo với ạ :'). mình cảm ơn rất nhiều,,,,,
"Mọi người ơi cho em hỏi là dữ liệu của em đã làm sạch, nhưng khi em dự đoán với mô hình CNN hồi quy và nó dự đoán sai khá là nhiều thì khả năng cao là em đã bị dính phải lỗi gì ạ? 
Em cảm ơn sự đóng góp của tất cả mọi người ạ !
 — đang cảm thấy tò mò.","Mọi người ơi cho em hỏi là dữ liệu của em đã làm sạch, nhưng khi em dự đoán với mô hình CNN hồi quy và nó dự đoán sai khá là nhiều thì khả năng cao là em đã bị dính phải lỗi gì ạ? Em cảm ơn sự đóng góp của tất cả mọi người ạ ! — đang cảm thấy tò mò.",,,,,
"Cho em hỏi câu này LUÔN đúng với mọi trường hợp không ạ? 
The less the Root Mean Squared Error (RMSE), The better the model is.

Mong mọi người giải thích giúp em
Cám ơn mọi người ạ ","Cho em hỏi câu này LUÔN đúng với mọi trường hợp không ạ? The less the Root Mean Squared Error (RMSE), The better the model is. Mong mọi người giải thích giúp em Cám ơn mọi người ạ",,,,,
"#hoidap
[Problem with LSTM]
Xin chào mọi người ạ.
HIện tại mình đang làm một project liên quan đến LSTM model nhưng mình phần accuracy của mình hơi có vấn đề.
Với mỗi random seed mình dùng thì mình sẽ nhận được accuracy khác nhau. Ví dụ random seed của mình là 0 thì accuracy là 80%, nhưng nếu nó là 42 thì accuracy là 1%, như kiểu model không hề learn ấy ạ.
Đã có ai gặp vấn đề này chưa ạ?
Mình xin cảm ơn ạ!
——————————————————-
UPDATE:
Cảm ơn mọi người đã giúp đỡ mình ạ ☺️. Mình đã tìm ra lỗi và đó là do sự ngoongok của mình khi quen tay set activation function là ReLU mà không phải là Sigmoid hay Tanh ạ 😞.","[Problem with LSTM] Xin chào mọi người ạ. HIện tại mình đang làm một project liên quan đến LSTM model nhưng mình phần accuracy của mình hơi có vấn đề. Với mỗi random seed mình dùng thì mình sẽ nhận được accuracy khác nhau. Ví dụ random seed của mình là 0 thì accuracy là 80%, nhưng nếu nó là 42 thì accuracy là 1%, như kiểu model không hề learn ấy ạ. Đã có ai gặp vấn đề này chưa ạ? Mình xin cảm ơn ạ! ——————————————————- UPDATE: Cảm ơn mọi người đã giúp đỡ mình ạ . Mình đã tìm ra lỗi và đó là do sự ngoongok của mình khi quen tay set activation function là ReLU mà không phải là Sigmoid hay Tanh ạ .",#hoidap,,,,
"Nguồn tổng hợp các tài liệu về toán học cho machine learning rất hữu ích bao gồm sách, paper và các video bài giảng... Các bạn lưu lại tham khảo","Nguồn tổng hợp các tài liệu về toán học cho machine learning rất hữu ích bao gồm sách, paper và các video bài giảng... Các bạn lưu lại tham khảo",,,,,
"Chào mọi người, 

Em đang có 1 bộ dữ liệu gồm:
Cột: 3 cột điểm,  tạm gọi là A,B,C
Hàng: mỗi hàng trong bảng biểu thị điểm của 1 lần test
Đối với mỗi lần test, điểm A,B, C càng cao càng tốt. Khi xem xét, em muốn sử dụng cả điểm A,B, C để xem test nào tốt nhất. Do đó, em có ý định tạo ra 1 cột giá trị (k) tổng hợp nào đó bằng cách đánh weights cho A,B, C. Vai trò của các điểm này không quá chênh lệch nhau nên weights sẽ không khác nhau là mấy. 

Vấn đề là, giá trị của A,B, C quá chênh lệch nhau. Ví dụ A thuộc đoạn [1;10] thì B thuộc đoạn [40;70] chả hạn nên lúc đánh weights em thấy nó dễ bị bias ạ. Mọi người có kinh nghiệm cho em hỏi trong TH này em nên tạo (k) như thế nào cho phù hợp ạ? Mọi người có cách hay tài liệu gì có thể giải quyết vấn đề này thì cho em xin. Em cảm ơn mọi người.","Chào mọi người, Em đang có 1 bộ dữ liệu gồm: Cột: 3 cột điểm, tạm gọi là A,B,C Hàng: mỗi hàng trong bảng biểu thị điểm của 1 lần test Đối với mỗi lần test, điểm A,B, C càng cao càng tốt. Khi xem xét, em muốn sử dụng cả điểm A,B, C để xem test nào tốt nhất. Do đó, em có ý định tạo ra 1 cột giá trị (k) tổng hợp nào đó bằng cách đánh weights cho A,B, C. Vai trò của các điểm này không quá chênh lệch nhau nên weights sẽ không khác nhau là mấy. Vấn đề là, giá trị của A,B, C quá chênh lệch nhau. Ví dụ A thuộc đoạn [1;10] thì B thuộc đoạn [40;70] chả hạn nên lúc đánh weights em thấy nó dễ bị bias ạ. Mọi người có kinh nghiệm cho em hỏi trong TH này em nên tạo (k) như thế nào cho phù hợp ạ? Mọi người có cách hay tài liệu gì có thể giải quyết vấn đề này thì cho em xin. Em cảm ơn mọi người.",,,,,
"Xin chào mọi người,
Vừa rồi thì OpenAI có giới thiệu mô hình Whisper - open source multilingual speech recognition, trong đó có cả tiếng Tiếng Việt. Tuy nhiên thì ở official code của OpenAI không có phần finetune mô hình, Nên mình có tham khảo một số nguồn và triển khai lại việc finetune trên tiếng Việt cho mô hình này, hi vọng có thể đóng góp cho cộng đồng,
Do hạn chế về mặt tài nguyên, nên mình có thử nghiệm nhanh trên GG colab với mô hình base với bộ data Vivos, thì kết quả chưa thực sự tốt (có thể nguyên nhân do chưa có phần chuẩn hóa riêng cho tiếng Việt). Sau khi finetune khoảng 5 epoch thì WER mô hình giảm từ 45.56% xuống 24.27%. Kết quả chưa thực sự tốt nhưng mình thấy khá triển vọng, nếu có nhiều dữ liệu hơn, được tuning, và hậu xử lý cẩn thận hơn thì có thể cải thiện độ chính xác của mô hình đáng kể, cá nhân mình thấy điểm mạnh của Whisper là mô hình xử lý khá tốt với các từ nước ngoài.
Mình cũng rất hoan nghênh đóng góp của mọi người để cải thiện kĩ năng code của bản thân, xa hơn có thể xây dựng một công cụ ASR mạnh, open-source cho Tiếng Việt.
Link Repo: https://github.com/ducanhdt/openai_whisper_finetuning
Link colab example:
https://colab.research.google.com/drive/1aXj6ssi_y3qow-h6M8M1Q5tFgHE0n3UP?usp=sharing
Thanks,","Xin chào mọi người, Vừa rồi thì OpenAI có giới thiệu mô hình Whisper - open source multilingual speech recognition, trong đó có cả tiếng Tiếng Việt. Tuy nhiên thì ở official code của OpenAI không có phần finetune mô hình, Nên mình có tham khảo một số nguồn và triển khai lại việc finetune trên tiếng Việt cho mô hình này, hi vọng có thể đóng góp cho cộng đồng, Do hạn chế về mặt tài nguyên, nên mình có thử nghiệm nhanh trên GG colab với mô hình base với bộ data Vivos, thì kết quả chưa thực sự tốt (có thể nguyên nhân do chưa có phần chuẩn hóa riêng cho tiếng Việt). Sau khi finetune khoảng 5 epoch thì WER mô hình giảm từ 45.56% xuống 24.27%. Kết quả chưa thực sự tốt nhưng mình thấy khá triển vọng, nếu có nhiều dữ liệu hơn, được tuning, và hậu xử lý cẩn thận hơn thì có thể cải thiện độ chính xác của mô hình đáng kể, cá nhân mình thấy điểm mạnh của Whisper là mô hình xử lý khá tốt với các từ nước ngoài. Mình cũng rất hoan nghênh đóng góp của mọi người để cải thiện kĩ năng code của bản thân, xa hơn có thể xây dựng một công cụ ASR mạnh, open-source cho Tiếng Việt. Link Repo: https://github.com/ducanhdt/openai_whisper_finetuning Link colab example: https://colab.research.google.com/drive/1aXj6ssi_y3qow-h6M8M1Q5tFgHE0n3UP?usp=sharing Thanks,",,,,,
"Xin chào mọi người ạ,
Em xin tự giới thiệu em là sinh viên mới ra trường, chuyên ngành về Trí tuệ nhân tạo. Trước đây em cũng đã có kinh nghiệm về làm research và cũng đã có một số bài báo ở các hội nghị, tuy nhiên chỉ ở mức nhỏ và ở trong nước thôi ạ. Em có dự định du học và học tiếp để lấy bằng cao hơn, nên rất cần tìm hiểu về các chương trình đào tạo nhân tài của các công ty mà định hướng về nghiên cứu AI đó ạ.
Xin nhờ mọi người cho em xin thông tin các nơi như thế, vì em muốn mình có bài báo ở các top-tier conferences để có thể dễ hơn trong việc xin học bổng PhD ạ.
Em cảm ơn mọi người 🤩","Xin chào mọi người ạ, Em xin tự giới thiệu em là sinh viên mới ra trường, chuyên ngành về Trí tuệ nhân tạo. Trước đây em cũng đã có kinh nghiệm về làm research và cũng đã có một số bài báo ở các hội nghị, tuy nhiên chỉ ở mức nhỏ và ở trong nước thôi ạ. Em có dự định du học và học tiếp để lấy bằng cao hơn, nên rất cần tìm hiểu về các chương trình đào tạo nhân tài của các công ty mà định hướng về nghiên cứu AI đó ạ. Xin nhờ mọi người cho em xin thông tin các nơi như thế, vì em muốn mình có bài báo ở các top-tier conferences để có thể dễ hơn trong việc xin học bổng PhD ạ. Em cảm ơn mọi người",,,,,
"Cho e hỏi ở dòng 15 và dòng 19 random_state để = 0 & 1 có mục đích gì thế ạ. Em cảm ơn !
#question",Cho e hỏi ở dòng 15 và dòng 19 random_state để = 0 & 1 có mục đích gì thế ạ. Em cảm ơn !,#question,,,,
"Cho em hỏi làm cách nào để lấy được phương trình y =mx +b của regplot hay lmplot không ạ?
Lí do là em có 2 cột dữ liệu nhưng 1 cột bị thiếu khoảng 30% . 2 cột này có tương quan khá cao nên em muốn lấy một cột để fill cột còn lại.
Hay có cách nào hay hơn mọi người hướng dẫn giúp em.
Cám ơn mọi người ạ😃",Cho em hỏi làm cách nào để lấy được phương trình y =mx +b của regplot hay lmplot không ạ? Lí do là em có 2 cột dữ liệu nhưng 1 cột bị thiếu khoảng 30% . 2 cột này có tương quan khá cao nên em muốn lấy một cột để fill cột còn lại. Hay có cách nào hay hơn mọi người hướng dẫn giúp em. Cám ơn mọi người ạ,,,,,
"Xin chào anh chị, em là sinh viên mới tốt nghiệp đại học Bách Khoa Hà Nội và là ngoại đạo,học AI đã được 1 năm, em đang tìm kiếm vị trí fresher AI tại Hà Nội mà thấy các job toàn yêu cầu ít nhất 1 năm kinh nghiệm, em đang rất hoang mang lo sợ tình trạng thất nghiệp. Em mong nhận được lời khuyên của anh chị, em cam ơn rất nhiều ạ","Xin chào anh chị, em là sinh viên mới tốt nghiệp đại học Bách Khoa Hà Nội và là ngoại đạo,học AI đã được 1 năm, em đang tìm kiếm vị trí fresher AI tại Hà Nội mà thấy các job toàn yêu cầu ít nhất 1 năm kinh nghiệm, em đang rất hoang mang lo sợ tình trạng thất nghiệp. Em mong nhận được lời khuyên của anh chị, em cam ơn rất nhiều ạ",,,,,
"xin chào mọi người, em là sinh viên năm 3 và đang tìm chủ đề để viết paper, em đã accept tạn số hội nghị nhỏ, tiếp theo em đang định tìm vài chủ đề để submit lên các hội nghị rank C hoặc B, mọi người cho em xin một số chủ đề với ạ. Cụ thể em làm về hướng computer vision hoặc tối ưu các giải thuật machine learning truyền thống. Em cảm ơn ạ","xin chào mọi người, em là sinh viên năm 3 và đang tìm chủ đề để viết paper, em đã accept tạn số hội nghị nhỏ, tiếp theo em đang định tìm vài chủ đề để submit lên các hội nghị rank C hoặc B, mọi người cho em xin một số chủ đề với ạ. Cụ thể em làm về hướng computer vision hoặc tối ưu các giải thuật machine learning truyền thống. Em cảm ơn ạ",,,,,
"Chào mọi người, em đang làm đề tài dùng Reinforcement Learning (RL) cho xe tự lái. Mục tiêu là cho xe chạy theo hành lang trong một toà nhà theo một lộ trình định sẵn và tránh được các vật cản. Trên xe có trang bị camera và Lidar. Xe được định vị dựa vào các điểm phát wifi của tòa nhà. Em đang vướng chỗ xác định các state, reward và action. Có ai đã làm rồi thì cho em xin ý kiến với ạ? Hoặc có tài liệu nào thì giới thiệu em với. Cảm ơn mọi người.","Chào mọi người, em đang làm đề tài dùng Reinforcement Learning (RL) cho xe tự lái. Mục tiêu là cho xe chạy theo hành lang trong một toà nhà theo một lộ trình định sẵn và tránh được các vật cản. Trên xe có trang bị camera và Lidar. Xe được định vị dựa vào các điểm phát wifi của tòa nhà. Em đang vướng chỗ xác định các state, reward và action. Có ai đã làm rồi thì cho em xin ý kiến với ạ? Hoặc có tài liệu nào thì giới thiệu em với. Cảm ơn mọi người.",,,,,
"xin chào mọi người, hiện tại em đang muốn theo học thạc sĩ về ngành machine learning hoặc data science ở tphcm, không biết mình có recommend về trường nào dạy ổn để em theo học không ạ ? và bằng thạc sĩ về machine learning hoặc data science chứ không phải kỹ sư phần mềm ạ, em cảm ơn mọi người nhiều","xin chào mọi người, hiện tại em đang muốn theo học thạc sĩ về ngành machine learning hoặc data science ở tphcm, không biết mình có recommend về trường nào dạy ổn để em theo học không ạ ? và bằng thạc sĩ về machine learning hoặc data science chứ không phải kỹ sư phần mềm ạ, em cảm ơn mọi người nhiều",,,,,
"Dạ mn cho em hỏi là có ai đã làm nlp về bên y tế chưa ạ, nếu có thì cho em xin thông tin và data ạ. Em xin cám ơn.","Dạ mn cho em hỏi là có ai đã làm nlp về bên y tế chưa ạ, nếu có thì cho em xin thông tin và data ạ. Em xin cám ơn.",,,,,
"Em chào mọi người, em là sinh viên vừa ra trường và chuyển sang mảng AI.
Em muốn tìm khóa học AI offline tại hà nội ạ
Mọi người recommend giúp em với ạ
Em cảm ơn ạ","Em chào mọi người, em là sinh viên vừa ra trường và chuyển sang mảng AI. Em muốn tìm khóa học AI offline tại hà nội ạ Mọi người recommend giúp em với ạ Em cảm ơn ạ",,,,,
"#hoidap
Cho em hỏi khi dùng standard scaller trong Pipeline thì sẽ áp dụng trên toàn dữ liệu đúng không ạ?
Nếu mình muốn scale 1 số cột nào đó thôi thì phải làm sao ?
Cám ơn mọi người đã quan tâm .",Cho em hỏi khi dùng standard scaller trong Pipeline thì sẽ áp dụng trên toàn dữ liệu đúng không ạ? Nếu mình muốn scale 1 số cột nào đó thôi thì phải làm sao ? Cám ơn mọi người đã quan tâm .,#hoidap,,,,
"VinAI Seminar - ""Enriching Communication between Humans and AI Agents""
Speaker: Khanh Nguyen, Postdoctoral Researcher at Princeton University
Time: 10:00 am - 11.00 am (GMT+7), Fri, Oct 7, 2022","VinAI Seminar - ""Enriching Communication between Humans and AI Agents"" Speaker: Khanh Nguyen, Postdoctoral Researcher at Princeton University Time: 10:00 am - 11.00 am (GMT+7), Fri, Oct 7, 2022",,,,,
"Cho em hỏi là em 23 tuổi, học trái ngành mà giờ mới đi intern AI thì có gọi là muộn không ạ? Với cả mọi người cho em xin chút thông tin về một số nơi intern về mảng NLP ở Hà Nội được ko ạ? Em định hướng phát triển theo NLP ạ","Cho em hỏi là em 23 tuổi, học trái ngành mà giờ mới đi intern AI thì có gọi là muộn không ạ? Với cả mọi người cho em xin chút thông tin về một số nơi intern về mảng NLP ở Hà Nội được ko ạ? Em định hướng phát triển theo NLP ạ",,,,,
"Chào các bạn. Mình cần tìm một hàm f(x1, x2) đáp ứng các yêu cầu sau:
(a) x1, x2 là hai số thực >= 0
(b) 0 <= x1 + x2 <= 1
(c) f(x1, x2) càng lớn khi x1 và x2 càng bé, đạt cực đại khi x1=0 và x2=0
(d) f( a, a ) > f(x1, x2) khi x1 != x2 và a = (x1 + x2)/2
Ví dụ:
f(0.1, 0.1) > f(0.15, 0.15)
f(0.1, 0.1) > f(0.095, 0.105) > f(0.09, 0.11)
Cảm ơn các bạn nhiều ạ.","Chào các bạn. Mình cần tìm một hàm f(x1, x2) đáp ứng các yêu cầu sau: (a) x1, x2 là hai số thực >= 0 (b) 0 <= x1 + x2 <= 1 (c) f(x1, x2) càng lớn khi x1 và x2 càng bé, đạt cực đại khi x1=0 và x2=0 (d) f( a, a ) > f(x1, x2) khi x1 != x2 và a = (x1 + x2)/2 Ví dụ: f(0.1, 0.1) > f(0.15, 0.15) f(0.1, 0.1) > f(0.095, 0.105) > f(0.09, 0.11) Cảm ơn các bạn nhiều ạ.",,"#Q&A, #math",,,
"Kính chào các bác, em có vấn đề này cần sự giúp đỡ của các cao thủ ạh.
Chả là em có 1 GPU để train model AI, hiện việc train cũng không thường xuyên lắm nên em muốn chia sẻ cho các bạn sinh viên (miễn phí) để các bạn train model cho đồ án, bài tập lớn.
Tuy nhiên nếu em share thủ công và thu hồi việc share thủ công thì khá mất time mà em cũng bận quá.
Nên mới hỏi các bác làm nhiều có cái opensource nào có thể cho phép các bạn vào đăng ký user, sử dụng GPU trong 1 thời gian giới hạn nào đó để train như kiểu Colab bây giờ.
Em kỳ vọng các bạn sẽ đăng ký, truy cập vào Jupyter Notebook và dùng trong thời gian đăng ký trước. Hết time đó thì quyền truy cập sẽ bị thu hồi để dành cho các bạn khác.
Mong được các bác giúp đỡ!","Kính chào các bác, em có vấn đề này cần sự giúp đỡ của các cao thủ ạh. Chả là em có 1 GPU để train model AI, hiện việc train cũng không thường xuyên lắm nên em muốn chia sẻ cho các bạn sinh viên (miễn phí) để các bạn train model cho đồ án, bài tập lớn. Tuy nhiên nếu em share thủ công và thu hồi việc share thủ công thì khá mất time mà em cũng bận quá. Nên mới hỏi các bác làm nhiều có cái opensource nào có thể cho phép các bạn vào đăng ký user, sử dụng GPU trong 1 thời gian giới hạn nào đó để train như kiểu Colab bây giờ. Em kỳ vọng các bạn sẽ đăng ký, truy cập vào Jupyter Notebook và dùng trong thời gian đăng ký trước. Hết time đó thì quyền truy cập sẽ bị thu hồi để dành cho các bạn khác. Mong được các bác giúp đỡ!",,,,,
"Kính chào các bác! Nhận dịp đang tìm hiểu em xin mạnh dạn chia sẻ chi tiết bằng kinh nghiệm cá nhân dành riêng cho các bạn sinh viên đang học AI, ML và đang vướng khi làm bài tập lớn, đồ án.
Chúc các bạn thành công!","Kính chào các bác! Nhận dịp đang tìm hiểu em xin mạnh dạn chia sẻ chi tiết bằng kinh nghiệm cá nhân dành riêng cho các bạn sinh viên đang học AI, ML và đang vướng khi làm bài tập lớn, đồ án. Chúc các bạn thành công!",,,,,
"Em chào anh chị ạ
Em có câu hỏi là: Để được đi Intern về Machine Learning Engineering thì cần những vùng kiến thức nào ạ, em cũng có đọc về các models trên trang machinelearningcoban, em cũng đang học về MLOps, hồi đi Intern anh chị làm project nào để đi phỏng vấn ạ, em cảm ơn anh chị nhiều ạ!","Em chào anh chị ạ Em có câu hỏi là: Để được đi Intern về Machine Learning Engineering thì cần những vùng kiến thức nào ạ, em cũng có đọc về các models trên trang machinelearningcoban, em cũng đang học về MLOps, hồi đi Intern anh chị làm project nào để đi phỏng vấn ạ, em cảm ơn anh chị nhiều ạ!",,,,,
"#hoidap
cho em hỏi sử dụng Principal Component Analysis cho hồi quy và phân loại đều được đúng không ạ?
Có khi nào sử dụng PCA xong rồi train mô hình cho ra kết quả xấu hơn không ạ ?",cho em hỏi sử dụng Principal Component Analysis cho hồi quy và phân loại đều được đúng không ạ? Có khi nào sử dụng PCA xong rồi train mô hình cho ra kết quả xấu hơn không ạ ?,#hoidap,,,,
"Mình thấy một repo hay về việc ứng dụng GPT-3 của OpenAI trong việc tìm kiếm trên website tại đây https://github.com/nat/natbot.
Demo theo tweet của tác giả tại đây https://twitter.com/natfriedman/status/1575631194032549888",Mình thấy một repo hay về việc ứng dụng GPT-3 của OpenAI trong việc tìm kiếm trên website tại đây https://github.com/nat/natbot. Demo theo tweet của tác giả tại đây https://twitter.com/natfriedman/status/1575631194032549888,,,,,
"VinAI Seminar - ""Variable Selection with Theoretical Guarantees on High-dimensional Data""
Speaker: Binh Nguyen, Postdoc at Telecom Paris, France
Time: 10.00 am - 11.00 am (GMT+7), Fri, Sep 30, 2022","VinAI Seminar - ""Variable Selection with Theoretical Guarantees on High-dimensional Data"" Speaker: Binh Nguyen, Postdoc at Telecom Paris, France Time: 10.00 am - 11.00 am (GMT+7), Fri, Sep 30, 2022",,,,,
"[FPS (frame per second) trên máy MACOS]
Chào mọi người, mình lần đầu dùng MACOS nên chưa có kinh nghiệm, nhờ mọi người chỉ giúp. Mình dùng opencv để đọc và xuất các frame ảnh của video thu được từ camera trên máy mac. Khi tính FPS thì đo được đâu đó tầm 30 fps à (hiện tại là chỉ đocj và ghi frame chứ không làm thêm tác vụ nào khác). Tốc độ này khá là chậm và mình chưa biết vấn đề nằm ở chỗ nào. Ở đây có ACE nào đã có kinh nghiệm tính toán FPS trên mac và tăng FPS rate thì chỉ giúp mình nha.
Cấu hình máy: Apple M1 pro, 16GB.
Xin cảm ơn mọi người.","[FPS (frame per second) trên máy MACOS] Chào mọi người, mình lần đầu dùng MACOS nên chưa có kinh nghiệm, nhờ mọi người chỉ giúp. Mình dùng opencv để đọc và xuất các frame ảnh của video thu được từ camera trên máy mac. Khi tính FPS thì đo được đâu đó tầm 30 fps à (hiện tại là chỉ đocj và ghi frame chứ không làm thêm tác vụ nào khác). Tốc độ này khá là chậm và mình chưa biết vấn đề nằm ở chỗ nào. Ở đây có ACE nào đã có kinh nghiệm tính toán FPS trên mac và tăng FPS rate thì chỉ giúp mình nha. Cấu hình máy: Apple M1 pro, 16GB. Xin cảm ơn mọi người.",,,,,
"Em chào anh/chị trong gr, Em là sinh viên đang thực hiện một bài tập lớn yêu cầu xây dựng mô hình dự đoán khả năng trả nợ của khách hàng cá nhân tại ngân hàng thương mại. Hiện em đang có bộ dữ liệu gồm 1143 dòng và 10 thuộc tính. Em muốn hỏi là nếu chỉ dùng thuật toán Random Forest để xây dựng mô hình này thì đã đủ chưa ạ (vì đây không phải môn chuyên ngành của em, nên em có hỏi những câu cơ bản :( ) Rất mong được anh/chị giúp đỡ. Em cảm ơn mọi người nhiều ạ","Em chào anh/chị trong gr, Em là sinh viên đang thực hiện một bài tập lớn yêu cầu xây dựng mô hình dự đoán khả năng trả nợ của khách hàng cá nhân tại ngân hàng thương mại. Hiện em đang có bộ dữ liệu gồm 1143 dòng và 10 thuộc tính. Em muốn hỏi là nếu chỉ dùng thuật toán Random Forest để xây dựng mô hình này thì đã đủ chưa ạ (vì đây không phải môn chuyên ngành của em, nên em có hỏi những câu cơ bản :( ) Rất mong được anh/chị giúp đỡ. Em cảm ơn mọi người nhiều ạ",,,,,
"Dạ mn cho em hỏi về feature engineering in NLP thì thông thường có những bước như nào ạ?, em tìm hiểu thì mình có thể thêm feature, CountVec hoặc TFIDF, còn nếu dùng pretrained model như BERT thì input vào model sẽ là có format riêng của model đó thì mình dùng feature engineer ở bước nào ạ? Em xin cám ơn","Dạ mn cho em hỏi về feature engineering in NLP thì thông thường có những bước như nào ạ?, em tìm hiểu thì mình có thể thêm feature, CountVec hoặc TFIDF, còn nếu dùng pretrained model như BERT thì input vào model sẽ là có format riêng của model đó thì mình dùng feature engineer ở bước nào ạ? Em xin cám ơn",,,,,
"Em chào mọi người ạ
Em có một câu hỏi ko liên quan đến ML lắm, mong mn giải đáp ạ. Em có 1 bảng gốc là bảng trên cùng, sau đó thực hiện groupby grouping set ra kết quả như bảng thứ 2. Bây giờ em muốn chuyển thành bảng thứ 3 ( thực hiện =trong Spark Dataset hoặc Spark SQL) thì mình có solution nào ko ạ.
Em cảm ơn mọi người","Em chào mọi người ạ Em có một câu hỏi ko liên quan đến ML lắm, mong mn giải đáp ạ. Em có 1 bảng gốc là bảng trên cùng, sau đó thực hiện groupby grouping set ra kết quả như bảng thứ 2. Bây giờ em muốn chuyển thành bảng thứ 3 ( thực hiện =trong Spark Dataset hoặc Spark SQL) thì mình có solution nào ko ạ. Em cảm ơn mọi người",,,,,
"Mình biết có nhiều bạn phải soạn bài giảng dựa trên Jupyter kết hợp với Markdown. Nay mình mới biết có dự án với tên Quarto (tại đây https://quarto.org/). Quarto hỗ trợ xuất file ra dạng *html, *docx, và cả *tex (XeLatex) rất tốt. Mình thấy có hướng dẫn khá thú vị này khi kết hợp dùng Quarto trên RStudio (series 4 bài giảng https://www.youtube.com/watch?v=31Q9ZTZOHIM). Và nó cũng hoạt động tốt với cả VSCode (https://quarto.org/docs/tools/vscode.html; https://arinbasu.medium.com/why-quarto-with-vscode-is-a...).
Quarto theo mình biết hiện nó đang hỗ trợ các ngôn ngữ lập trình hướng đối tượng như R, Python, Julia.
Hi vọng ngày cuối tuần có thêm thông tin giải trí cho mọi người!","Mình biết có nhiều bạn phải soạn bài giảng dựa trên Jupyter kết hợp với Markdown. Nay mình mới biết có dự án với tên Quarto (tại đây https://quarto.org/). Quarto hỗ trợ xuất file ra dạng *html, *docx, và cả *tex (XeLatex) rất tốt. Mình thấy có hướng dẫn khá thú vị này khi kết hợp dùng Quarto trên RStudio (series 4 bài giảng https://www.youtube.com/watch?v=31Q9ZTZOHIM). Và nó cũng hoạt động tốt với cả VSCode (https://quarto.org/docs/tools/vscode.html; https://arinbasu.medium.com/why-quarto-with-vscode-is-a...). Quarto theo mình biết hiện nó đang hỗ trợ các ngôn ngữ lập trình hướng đối tượng như R, Python, Julia. Hi vọng ngày cuối tuần có thêm thông tin giải trí cho mọi người!",,,,,
"Chào mọi người, 
Nhóm mình LQDBD may mắn đứng thứ hạng 2 private test trong bài toán AI4VN: Air Quality Forcasting trong khuôn khổ hội nghị AI Summit 2022. Cũng mong chia sẻ ý tưởng source code của nhóm trong cuộc thi lần này. Nếu có gì không rõ ràng và cần phải thảo luận thì mọi người có thể hỏi team mình trong issue của repo. Cám ơn mọi người.","Chào mọi người, Nhóm mình LQDBD may mắn đứng thứ hạng 2 private test trong bài toán AI4VN: Air Quality Forcasting trong khuôn khổ hội nghị AI Summit 2022. Cũng mong chia sẻ ý tưởng source code của nhóm trong cuộc thi lần này. Nếu có gì không rõ ràng và cần phải thảo luận thì mọi người có thể hỏi team mình trong issue của repo. Cám ơn mọi người.",,,,,
"Em chào các anh chị.

Em đang có thắc mắc về thuật toán Content Based Filtering khi tìm hiểu, mong được các anh chị giải đáp.

Theo em tìm hiểu thì Content Based Filtering sẽ dựa vào đặc điểm của một sản phẩm để đưa ra gợi ý cho người dùng khi họ ấn vào sản phẩm đó.

Em có thử chạy với thư viện Algolia: https://www.algolia.com/ trong mục Recommened -> Related Products. (Website đưa ra rằng hệ thống của họ chạy dựa trên Content Based Filtering)

Input nhập vào là một dataset các sản phẩm và Define key object attributes (ở đây em chọn dataset là giày và define key là category của sản phẩm)

Sau khi train thành công, khi em ấn vào một sản phẩm có category là ""Formal"" thì Algolia sẽ gợi ý ra các sản phẩm cũng có category là Formal.

Vậy, khác biệt của Content Based Filtering so với việc người dùng ấn vào một sản phẩm (tức là có id của sản phẩm ấy) rồi truy vấn đến database SELECT ra các sản phẩm có category tưởng tự là gì ạ?

Câu hỏi của em hơi dài, mong mọi người có thể giải đáp giúp em ạ. Em xin cảm ơn.","Em chào các anh chị. Em đang có thắc mắc về thuật toán Content Based Filtering khi tìm hiểu, mong được các anh chị giải đáp. Theo em tìm hiểu thì Content Based Filtering sẽ dựa vào đặc điểm của một sản phẩm để đưa ra gợi ý cho người dùng khi họ ấn vào sản phẩm đó. Em có thử chạy với thư viện Algolia: https://www.algolia.com/ trong mục Recommened -> Related Products. (Website đưa ra rằng hệ thống của họ chạy dựa trên Content Based Filtering) Input nhập vào là một dataset các sản phẩm và Define key object attributes (ở đây em chọn dataset là giày và define key là category của sản phẩm) Sau khi train thành công, khi em ấn vào một sản phẩm có category là ""Formal"" thì Algolia sẽ gợi ý ra các sản phẩm cũng có category là Formal. Vậy, khác biệt của Content Based Filtering so với việc người dùng ấn vào một sản phẩm (tức là có id của sản phẩm ấy) rồi truy vấn đến database SELECT ra các sản phẩm có category tưởng tự là gì ạ? Câu hỏi của em hơi dài, mong mọi người có thể giải đáp giúp em ạ. Em xin cảm ơn.",,,,,
"[Whisper - English speech recognition]
OpenAI mới đây cho ra mô hình Whisper có khả năng nhận diện giọng nói được huấn luyện từ 680,000 giờ từ các ngôn ngữ khác nhau. Riêng với ngôn ngữ tiếng anh, mô mình có độ chính xác đạt tới mức độ của con người. Mô hình có thể thực hiện tốt với các giọng địa phương và ngôn ngữ kĩ thuật. Cuối cùng, Whisper là mô hình dạng mã nguồn mở và miễn phí để sử dụng.
Chi tiết các bạn xem ở đây: https://openai.com/blog/whisper/","[Whisper - English speech recognition] OpenAI mới đây cho ra mô hình Whisper có khả năng nhận diện giọng nói được huấn luyện từ 680,000 giờ từ các ngôn ngữ khác nhau. Riêng với ngôn ngữ tiếng anh, mô mình có độ chính xác đạt tới mức độ của con người. Mô hình có thể thực hiện tốt với các giọng địa phương và ngôn ngữ kĩ thuật. Cuối cùng, Whisper là mô hình dạng mã nguồn mở và miễn phí để sử dụng. Chi tiết các bạn xem ở đây: https://openai.com/blog/whisper/",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 8/2022 vào trong comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 8/2022 vào trong comment của post này.",,,,,
"#NLP #beautifulsoup #KMeans #hierarchy #streamlit
======== RECOMMENDATION SYSTEM ========  
Hello Everyone,  
Have a great weekend!
I built End-to-end Recommendation System project with Python involving:     
1/ Web Scraping
Using Beautiful Soup to extract data from the web         
Clean the text and drop null entries              
2/ Clustering Analysis        
Using Unsupervised Learning techniques Kmeans, DBSCAN, Hierarchy & natural language processing (NLP)          
Tokenizing, stemming and are vectorized using NLTK's TF-IDF vectorizer         
 From the TF-IDF matrix, the similarity distances between the texts are computed by substracting the cosine of vectors from 1.
Finally, recommendations are queried using the matrix: once a game is selected, the top 5 closest games are returned.          
3/ Web App Development         
Using Streamlit framework to build local Web App         
To be... Deploy Web App

Please do not hesitate to contact us should you have any queries or require further information.
Thank All <3
Let's watch video on my Youtube channel ~~~
https://youtu.be/83HFInoJsTA",#ERROR!,#NLP	#beautifulsoup	#KMeans	#hierarchy	#streamlit,,,,
"Em chào mn, e có câu hỏi mong đc mn giải đáp ạ. Không biết trong nhóm mình đã có ai làm phần triển khai mô hình sau khi đã huấn luyện xong, cụ thể là phần triton inference server chưa ạ. E có làm theo hướng dẫn git chính thức của tác giả, tuy nhiên với các mô hình classification ví dụ Resnet thì e chạy và kết quả trả về đúng, còn đối với các mô hình detect ví dụ FasterRCNN hoặc Yolov5 thì khi e chạy client, response trả về k chứa kết quả bounding box, labels,... mong muốn. File config kết quả trả về thì e có đi tham khảo trên mạng và nghĩ vấn đề k phải do nó. Mô hình trước đó được chuyển qua ONNX và e cũng đã thử lại để chắc chắn mô hình sau khi chuyển đổi trả về đúng kết quả. K biết có phải do thư viện tritonclient đã xử lí kết quả này trước nên dẫn đến lỗi này k, vì e k thể truy cập đc vào thư viện này.
Ai có kinh nghiệm giúp e với, e xin cảm ơn ạ","Em chào mn, e có câu hỏi mong đc mn giải đáp ạ. Không biết trong nhóm mình đã có ai làm phần triển khai mô hình sau khi đã huấn luyện xong, cụ thể là phần triton inference server chưa ạ. E có làm theo hướng dẫn git chính thức của tác giả, tuy nhiên với các mô hình classification ví dụ Resnet thì e chạy và kết quả trả về đúng, còn đối với các mô hình detect ví dụ FasterRCNN hoặc Yolov5 thì khi e chạy client, response trả về k chứa kết quả bounding box, labels,... mong muốn. File config kết quả trả về thì e có đi tham khảo trên mạng và nghĩ vấn đề k phải do nó. Mô hình trước đó được chuyển qua ONNX và e cũng đã thử lại để chắc chắn mô hình sau khi chuyển đổi trả về đúng kết quả. K biết có phải do thư viện tritonclient đã xử lí kết quả này trước nên dẫn đến lỗi này k, vì e k thể truy cập đc vào thư viện này. Ai có kinh nghiệm giúp e với, e xin cảm ơn ạ",,,,,
"Chào mọi người ,em mới tập viết blog mong mọi người ủng hộ ạ.
https://viblo.asia/p/gioi-thieu-ve-diffussion-model-EvbLbOKWVnk?fbclid=IwAR2xDSfBkao6pkYfUhN1FGZ3C0JxaQV3MB9vZNba0k7qK01cdu2meEO8DHU","Chào mọi người ,em mới tập viết blog mong mọi người ủng hộ ạ. https://viblo.asia/p/gioi-thieu-ve-diffussion-model-EvbLbOKWVnk?fbclid=IwAR2xDSfBkao6pkYfUhN1FGZ3C0JxaQV3MB9vZNba0k7qK01cdu2meEO8DHU",,,,,
"Chào mọi người, sau khi train model với YOLO mình nhận ra 1 đặc điểm. Khi mình train 3 class lần lượt là 1,2,3 tương ứng với 3 hình vuông , tam giác, thoi thì model nhận diện rất tốt với ảnh test(hình 1). Tuy nhiên , nếu đổi chỗ 3 hình này cho nhau trong 1 ảnh thì model nhận diện sai nhãn ( hình 2 ).
Mọi người cho mình hỏi có cách nào khắc phục điều này không?
P/S:Mình nghĩ là nếu bổ sung thêm data cho các trường hợp sau có hợp lý không?","Chào mọi người, sau khi train model với YOLO mình nhận ra 1 đặc điểm. Khi mình train 3 class lần lượt là 1,2,3 tương ứng với 3 hình vuông , tam giác, thoi thì model nhận diện rất tốt với ảnh test(hình 1). Tuy nhiên , nếu đổi chỗ 3 hình này cho nhau trong 1 ảnh thì model nhận diện sai nhãn ( hình 2 ). Mọi người cho mình hỏi có cách nào khắc phục điều này không? P/S:Mình nghĩ là nếu bổ sung thêm data cho các trường hợp sau có hợp lý không?",,,,,
"Chào mn,
Theo mình tìm hiểu thì khi đánh giá mô hình, AUPR cho thấy mối tương quan giữa giá trị precision và recall theo các ngưỡng. Với bài của mình, AUPR = 0.758 cũng tạm chấp nhận, nhưng precision = 0.06 và recall = 0.95. False positive nhiều nên ảnh hưởng đến kết quả precision.
Cho mình hỏi, vì sao precision quá thấp mà AUPR tạm chấp nhận vậy ạ? Và cách cải thiện là gì?
Xin cám ơn ace.","Chào mn, Theo mình tìm hiểu thì khi đánh giá mô hình, AUPR cho thấy mối tương quan giữa giá trị precision và recall theo các ngưỡng. Với bài của mình, AUPR = 0.758 cũng tạm chấp nhận, nhưng precision = 0.06 và recall = 0.95. False positive nhiều nên ảnh hưởng đến kết quả precision. Cho mình hỏi, vì sao precision quá thấp mà AUPR tạm chấp nhận vậy ạ? Và cách cải thiện là gì? Xin cám ơn ace.",,,,,
Mình xin giới thiệu tới các bạn video của sự kiện DataQuest thứ 9 về ứng dụng phân tích tài chính trên package vnquant.,Mình xin giới thiệu tới các bạn video của sự kiện DataQuest thứ 9 về ứng dụng phân tích tài chính trên package vnquant.,,,,,
"Em chào mọi người ạ, em hiện tại đang viết tool bằng python để upload các file lên nền tảng youtube, và em đang muốn sử dụng machine learning để tự nhận diện các nút bấm trên giao diện web của youtube và khi đó tool của em có thể tự xử lý, hạn chế bấm nhầm nút. Vậy mọi người cho em hỏi thư viện nào thì phù hợp nhất với em để bắt đầu ạ? Em xin cảm ơn ạ","Em chào mọi người ạ, em hiện tại đang viết tool bằng python để upload các file lên nền tảng youtube, và em đang muốn sử dụng machine learning để tự nhận diện các nút bấm trên giao diện web của youtube và khi đó tool của em có thể tự xử lý, hạn chế bấm nhầm nút. Vậy mọi người cho em hỏi thư viện nào thì phù hợp nhất với em để bắt đầu ạ? Em xin cảm ơn ạ",,,,,
"Bên mình đang triển khai dự án cần outsource module phát hiện nội dung bậy bạ, chính trị, tin giả... Bạn nào đã có giải pháp inbox mình để hợp tác nhé.","Bên mình đang triển khai dự án cần outsource module phát hiện nội dung bậy bạ, chính trị, tin giả... Bạn nào đã có giải pháp inbox mình để hợp tác nhé.",,,,,
"Chào mọi người, mình đến từ team Laugh Tale, mình xin chia sẻ solution team mình đã áp dụng để giành được top 1 private test tại Quy Nhon AI Hackathon 2022 challenge smart menu.
Nếu có gì chưa rõ ràng và cần thảo luận thì các bạn có thể hỏi mình trong issue của repo. Thank all.
 — với Thiều Nguyễn.","Chào mọi người, mình đến từ team Laugh Tale, mình xin chia sẻ solution team mình đã áp dụng để giành được top 1 private test tại Quy Nhon AI Hackathon 2022 challenge smart menu. Nếu có gì chưa rõ ràng và cần thảo luận thì các bạn có thể hỏi mình trong issue của repo. Thank all. — với Thiều Nguyễn.",,,,,
"#hoidap
#python
Cho em hỏi OneHotEncoder và get_dummies khác nhau chỗ nao vậy ạ?",Cho em hỏi OneHotEncoder và get_dummies khác nhau chỗ nao vậy ạ?,#hoidap	#python,,,,
"[AI application] AI agent plays Color lines (Line 98)
Line 98 là 1 trong các tựa game mình vô cùng yêu thích khi còn bé. Hiện tại khi đang làm việc trong lĩnh vực AI, mà cụ thể hơn là Reinforcement Learning, mình đặc biệt có hứng thú với các ứng dụng của AI trong game. Trong project này, AI agent sẽ tự khám phá và học cách chơi Color lines.
Source code: https://github.com/uvipen/Color-lines-deep-Q-learning-pytorch
Demo: https://youtu.be/gd_EJJK_wQg","[AI application] AI agent plays Color lines (Line 98) Line 98 là 1 trong các tựa game mình vô cùng yêu thích khi còn bé. Hiện tại khi đang làm việc trong lĩnh vực AI, mà cụ thể hơn là Reinforcement Learning, mình đặc biệt có hứng thú với các ứng dụng của AI trong game. Trong project này, AI agent sẽ tự khám phá và học cách chơi Color lines. Source code: https://github.com/uvipen/Color-lines-deep-Q-learning-pytorch Demo: https://youtu.be/gd_EJJK_wQg",,,,,
"em xin chào mọi người ạ, em đang làm đồ án tốt nghiệp về speech2text cho tiếng việt và có biết tới bộ dataset VLNP. nhưng mà em không thể truy cập được qua web: https://vlsp.org.vn/resources. vì vậy mọi người ai đã downloaded bộ này có thể cho em xin không ạ?
em xin chân thành cảm ơn ạ!","em xin chào mọi người ạ, em đang làm đồ án tốt nghiệp về speech2text cho tiếng việt và có biết tới bộ dataset VLNP. nhưng mà em không thể truy cập được qua web: https://vlsp.org.vn/resources. vì vậy mọi người ai đã downloaded bộ này có thể cho em xin không ạ? em xin chân thành cảm ơn ạ!",,,,,
"Trong group có ai giúp bạn này được không? Mình chưa ở trong trường hợp của bạn ấy nên cũng không biết khuyên thế nào cho tốt. Mong các bạn giúp đỡ. Cảm ơn các bạn.
-------------------
Em chào anh ạ,
Em biết là anh đang rất bận cũng như nhiều việc nhưng nếu e không bị rơi vào hoàn cảnh này thì em cũng không dám làm phiền anh đâu ạ. Chỉ là e hiện đang quá rối và chông chênh và ngoài anh ra em không biết nên hỏi ai nữa ạ.
Em năm nay năm 4, học về kinh tế. Em năm ngoái có đi thực tập vị trí BA trong 1 sàn thương mại điện tử tại việt nam nhưng chủ yếu BA về mảng tài chính và làm bằng Google Spreadsheets. Tới khoảng gần 6 tháng gần đây em mới bắt đầu học python để đa nhiệm hóa khả năng của bản thân. Em tự học nhưng vừa đi làm vừa đi học nên em không tập trung 100% vào học code. Nghĩa là 6 tháng qua không phải hoàn toàn 100% e học python và cũng không có ai dạy cả nên em toàn tự nghĩ ra vấn đề, tự tìm hướng giải quyết. (Em tự đánh giá thấy em đang dừng lại ở Mining dữ liệu thôi ạ, còn các cái còn lại mới chỉ dừng lại ở basic).
Em định hướng sẽ theo Machine Learning vì tuy mới chỉ dừng lại ở Data Analysis nhưng em thấy nó rất cuốn hút và kiểu giống có gì đó ở nó tỏa ra rất hấp dẫn, giống như kiểu sự kì diệu của logic, toán học và dữ liệu ý anh ạ, em không chắc lắm nhưng hiện tại em cũng có tìm hiểu là em cần chắc về Data Science để phát triển sâu hơn sau này nếu muốn nâng cao làm ML.
Em có khá nhiều giới hạn như sắp ra trường, muốn theo ngành này nhưng không có nhiều kinh nghiệm và cũng không có biết được là con đường mình cần những gì để đạt được cái mong muốn đó.
Em thực sự là đang vô cùng bế tắc và khủng hoảng. Em không biết em nên bắt đầu từ đâu hay làm thế nào nữa anh ạ. Em rất muốn nhận được lời khuyên từ anh và có thể thì anh có thể cho e 1 vài mốc em cần làm và cần có những gì để đạt được điều đó ạ. Em thấy ở Vn có vẻ như các mảng ML hoặc DS hình như còn chưa nhiều lắm vì em đang rất thèm được sống trong cái hơi thở của DS,ML nhưng không tìm ra anh ạ. (đó cũng 1 trong số lý do khiến em khá căng thẳng và stress).
1 lần nữa em rất xin lỗi nếu làm phiền anh mà anh đang rất bận, nhưng em rất mong nhận được phản hồi từ anh.
Em cảm ơn anh đã dành thời gian cho em rất nhiều ạ.
Em rất mong sớm nhận được tin từ anh!","Trong group có ai giúp bạn này được không? Mình chưa ở trong trường hợp của bạn ấy nên cũng không biết khuyên thế nào cho tốt. Mong các bạn giúp đỡ. Cảm ơn các bạn. ------------------- Em chào anh ạ, Em biết là anh đang rất bận cũng như nhiều việc nhưng nếu e không bị rơi vào hoàn cảnh này thì em cũng không dám làm phiền anh đâu ạ. Chỉ là e hiện đang quá rối và chông chênh và ngoài anh ra em không biết nên hỏi ai nữa ạ. Em năm nay năm 4, học về kinh tế. Em năm ngoái có đi thực tập vị trí BA trong 1 sàn thương mại điện tử tại việt nam nhưng chủ yếu BA về mảng tài chính và làm bằng Google Spreadsheets. Tới khoảng gần 6 tháng gần đây em mới bắt đầu học python để đa nhiệm hóa khả năng của bản thân. Em tự học nhưng vừa đi làm vừa đi học nên em không tập trung 100% vào học code. Nghĩa là 6 tháng qua không phải hoàn toàn 100% e học python và cũng không có ai dạy cả nên em toàn tự nghĩ ra vấn đề, tự tìm hướng giải quyết. (Em tự đánh giá thấy em đang dừng lại ở Mining dữ liệu thôi ạ, còn các cái còn lại mới chỉ dừng lại ở basic). Em định hướng sẽ theo Machine Learning vì tuy mới chỉ dừng lại ở Data Analysis nhưng em thấy nó rất cuốn hút và kiểu giống có gì đó ở nó tỏa ra rất hấp dẫn, giống như kiểu sự kì diệu của logic, toán học và dữ liệu ý anh ạ, em không chắc lắm nhưng hiện tại em cũng có tìm hiểu là em cần chắc về Data Science để phát triển sâu hơn sau này nếu muốn nâng cao làm ML. Em có khá nhiều giới hạn như sắp ra trường, muốn theo ngành này nhưng không có nhiều kinh nghiệm và cũng không có biết được là con đường mình cần những gì để đạt được cái mong muốn đó. Em thực sự là đang vô cùng bế tắc và khủng hoảng. Em không biết em nên bắt đầu từ đâu hay làm thế nào nữa anh ạ. Em rất muốn nhận được lời khuyên từ anh và có thể thì anh có thể cho e 1 vài mốc em cần làm và cần có những gì để đạt được điều đó ạ. Em thấy ở Vn có vẻ như các mảng ML hoặc DS hình như còn chưa nhiều lắm vì em đang rất thèm được sống trong cái hơi thở của DS,ML nhưng không tìm ra anh ạ. (đó cũng 1 trong số lý do khiến em khá căng thẳng và stress). 1 lần nữa em rất xin lỗi nếu làm phiền anh mà anh đang rất bận, nhưng em rất mong nhận được phản hồi từ anh. Em cảm ơn anh đã dành thời gian cho em rất nhiều ạ. Em rất mong sớm nhận được tin từ anh!",,,,,
"Chào mọi người, mình (đã và đang) viết một blog nhỏ về Học tăng cường. Học tăng cường ở Việt Nam cũng đã có nhiều tài liệu chia sẻ nhiều kiến thức cơ bản cũng như những mô hình học tăng cường hiện đại, tuy nhiên, mình thấy rằng ít có tài liệu đề cập tới những vấn đề cơ bản nhất và dẫn dắt tới những ý tưởng hiện đại. Chính vì vậy, mình đã lựa chọn việc viết một blog để chia sẻ những ý tưởng căn bản nhất của Học tăng cường.
Mục đích của blog: Chia sẻ kiến thức mình hiểu biết về học tăng cường, hoàn toàn là mục đích cộng đồng.
Đối tượng blog này hướng tới: Những bạn mới bắt đầu với học tăng cường hoặc có ý định muốn tìm hiểu và mong muốn có 'shortcut' để có thể bắt đầu vào học lĩnh vực này sâu hơn.
Thành phần của blog: Blog tập trung trình bày những khái niệm cơ bản, ý tưởng thuật toán và cách cài đặt thuật toán, ngoài ra còn có một số phần cài đặt chương trình mẫu.
Blog này không có: Mình không trình bày các chứng minh chi tiết cũng như các kết quả Toán học quá sâu vì bản thân mình cũng không hiểu.
Một số lưu ý: Học tăng cường khác với các mô hình học máy thông thường. Các bạn không thể lên mạng tìm một tập dữ liệu rồi cài đặt các mô hình để chạy ra kết quả so sánh ngay. Để có thể cài đặt các thuật toán học tăng cường, bản thân bước đầu tiên đã phải tự mô hình MDP cho bài toán đó, chính vì vậy, mình không trình bày chi tiết về phần cài đặt, dù mình đã tự cài đặt một số các thuật toán này bằng tay rồi.
Công việc sắp tới: Blog này vẫn đang được mình viết và mình hy vọng được mọi người ủng hộ cũng như đóng góp ý kiến để có thể viết tiếp với đam mê của mình. Mình sẽ hoàn thiện thêm blog cũng như viết thêm về các bài toán trong không gian MDP vô hạn và có thể tìm hiểu về các thuật toán Học tăng cường hiện đại hơn.
Mình xin gửi link blog: https://anavuongdin.github.io/reinforcement-learning/
Cảm ơn mọi người vì đã đọc!","Chào mọi người, mình (đã và đang) viết một blog nhỏ về Học tăng cường. Học tăng cường ở Việt Nam cũng đã có nhiều tài liệu chia sẻ nhiều kiến thức cơ bản cũng như những mô hình học tăng cường hiện đại, tuy nhiên, mình thấy rằng ít có tài liệu đề cập tới những vấn đề cơ bản nhất và dẫn dắt tới những ý tưởng hiện đại. Chính vì vậy, mình đã lựa chọn việc viết một blog để chia sẻ những ý tưởng căn bản nhất của Học tăng cường. Mục đích của blog: Chia sẻ kiến thức mình hiểu biết về học tăng cường, hoàn toàn là mục đích cộng đồng. Đối tượng blog này hướng tới: Những bạn mới bắt đầu với học tăng cường hoặc có ý định muốn tìm hiểu và mong muốn có 'shortcut' để có thể bắt đầu vào học lĩnh vực này sâu hơn. Thành phần của blog: Blog tập trung trình bày những khái niệm cơ bản, ý tưởng thuật toán và cách cài đặt thuật toán, ngoài ra còn có một số phần cài đặt chương trình mẫu. Blog này không có: Mình không trình bày các chứng minh chi tiết cũng như các kết quả Toán học quá sâu vì bản thân mình cũng không hiểu. Một số lưu ý: Học tăng cường khác với các mô hình học máy thông thường. Các bạn không thể lên mạng tìm một tập dữ liệu rồi cài đặt các mô hình để chạy ra kết quả so sánh ngay. Để có thể cài đặt các thuật toán học tăng cường, bản thân bước đầu tiên đã phải tự mô hình MDP cho bài toán đó, chính vì vậy, mình không trình bày chi tiết về phần cài đặt, dù mình đã tự cài đặt một số các thuật toán này bằng tay rồi. Công việc sắp tới: Blog này vẫn đang được mình viết và mình hy vọng được mọi người ủng hộ cũng như đóng góp ý kiến để có thể viết tiếp với đam mê của mình. Mình sẽ hoàn thiện thêm blog cũng như viết thêm về các bài toán trong không gian MDP vô hạn và có thể tìm hiểu về các thuật toán Học tăng cường hiện đại hơn. Mình xin gửi link blog: https://anavuongdin.github.io/reinforcement-learning/ Cảm ơn mọi người vì đã đọc!",,,,,
"Chào mọi người ạ. Em có 1 câu hỏi như sau rất mong được giải đáp ạ.
Trong quá trình triển khai các model Yolo. Em gặp 1 vấn đề:
Khi chạy model bằng Dnn của openCV bằng Cpu thì tốc độ nhanh hơn Chạy model trên nền Darknet khoảng 10 lần(Có vẻ như là do opencv được phát triển bởi intel nên khi chạy trên cpu intel cho tốc độ nhanh).
Khi chạy bằng Gpu thì detect bằng Dnn-opencv và darknet cho tốc độ gần như nhau.
Em sử dụng con card RtX3060. Em có một câu hỏi là liệu có phải do card hình của em mạnh nên tốc độ xử lý gpu trên dnn và darknet như nhau không? và Nếu chạy bằng Gpu cấu hình nhỏ thì tốc độ xử lý của 2 mạng dnn -opencv và darknet thế nào? liệu dnn có nhanh hơn darknet không?
Em cảm ơn!",Chào mọi người ạ. Em có 1 câu hỏi như sau rất mong được giải đáp ạ. Trong quá trình triển khai các model Yolo. Em gặp 1 vấn đề: Khi chạy model bằng Dnn của openCV bằng Cpu thì tốc độ nhanh hơn Chạy model trên nền Darknet khoảng 10 lần(Có vẻ như là do opencv được phát triển bởi intel nên khi chạy trên cpu intel cho tốc độ nhanh). Khi chạy bằng Gpu thì detect bằng Dnn-opencv và darknet cho tốc độ gần như nhau. Em sử dụng con card RtX3060. Em có một câu hỏi là liệu có phải do card hình của em mạnh nên tốc độ xử lý gpu trên dnn và darknet như nhau không? và Nếu chạy bằng Gpu cấu hình nhỏ thì tốc độ xử lý của 2 mạng dnn -opencv và darknet thế nào? liệu dnn có nhanh hơn darknet không? Em cảm ơn!,,,,,
"Em xin chào tất cả các anh chị và các bạn trong nhóm ạ.
Hiện tại em đang tìm hiểu về topic 3D reconstruction và đang đọc paper BANMo: Building Animatable 3D Neural Models from Many Casual Videos (project page: https://banmo-www.github.io) thì gặp các thuật ngữ như canonical embedding, canonical 3D model, canonical space, neural implicit model, neural implicit function,... em chưa hiểu được nên nhờ mọi người giải thích giúp em được không ạ. 
Nhân thể mọi người có ai đã và đang làm hay tìm hiểu về topic 3D reconstruction hay cụ thể là deformable shape reconstruction from video(s) có thể cho em xin tham khảo tài liệu hay khoá học để tự học về topic này được không ạ. 
Em xin cảm ơn mọi người nhiều!","Em xin chào tất cả các anh chị và các bạn trong nhóm ạ. Hiện tại em đang tìm hiểu về topic 3D reconstruction và đang đọc paper BANMo: Building Animatable 3D Neural Models from Many Casual Videos (project page: https://banmo-www.github.io) thì gặp các thuật ngữ như canonical embedding, canonical 3D model, canonical space, neural implicit model, neural implicit function,... em chưa hiểu được nên nhờ mọi người giải thích giúp em được không ạ. Nhân thể mọi người có ai đã và đang làm hay tìm hiểu về topic 3D reconstruction hay cụ thể là deformable shape reconstruction from video(s) có thể cho em xin tham khảo tài liệu hay khoá học để tự học về topic này được không ạ. Em xin cảm ơn mọi người nhiều!",,,,,
"Hi các bác,
Em có một bài toán, bóc tách địa chỉ ở Việt Nam (kiểu dữ liệu text phi cấu trúc). Ví dụ:
gh cho e den 285 duong cmtt, p12, q10, hcm nha
gh cho e den 285 cmt8, p.12, q10, tp.hcm nha
gh cho em den 285 d.cmt8, p12, q.10, tp hcm nha
285 09028388993 cmt8 p12 q10
285 cmt8, p.12, q10 09028388993
Em muốn so sánh các chuỗi này sử dụng Machine learning, và đưa ra kết quả là match hoặc not match.
Em chưa biết giải pháp như thế nào, và các bác có tài liệu thì cho em xin tham khảo ạ. Em cảm ơn!","Hi các bác, Em có một bài toán, bóc tách địa chỉ ở Việt Nam (kiểu dữ liệu text phi cấu trúc). Ví dụ: gh cho e den 285 duong cmtt, p12, q10, hcm nha gh cho e den 285 cmt8, p.12, q10, tp.hcm nha gh cho em den 285 d.cmt8, p12, q.10, tp hcm nha 285 09028388993 cmt8 p12 q10 285 cmt8, p.12, q10 09028388993 Em muốn so sánh các chuỗi này sử dụng Machine learning, và đưa ra kết quả là match hoặc not match. Em chưa biết giải pháp như thế nào, và các bác có tài liệu thì cho em xin tham khảo ạ. Em cảm ơn!",,,,,
"Tiệp cho mình chia sẻ thêm thông tin về lớp Foundation of DS và ML (miễn phí và bằng tiếng Việt) cho các bạn nào quan tâm ở đây nhé.
--- Mình gửi slides cho bài giảng đầu tiên cho lớp ""Foundation of Data Science and Machine Learning"" (Học phần Cơ Bản) ở đây: https://nhatptnk8912.github.io/Warmup_New.pdf . Khi mình có thời gian thì mình sẽ thu âm lại bài giảng và gửi lên homepage và page khoa học dữ liệu (https://www.facebook.com/khoahocvadulieu). Mình sẽ dùng tiếng Việt để giảng mặc dù ngôn ngữ trong slides là tiếng Anh. Các bạn có thể đọc slides tham khảo trước.
Bài giảng đầu tiên chủ yếu review lại những chủ đề chúng mình sẽ học, những nguyên tắc khi làm DS và ML, và những khái niệm cơ bản về xác suất (sẽ hữu dụng cho các bạn khi học các bài giảng về sau).
Bài giảng phù hợp cho các bạn đang bắt đầu học về DS và ML. Bài giảng cũng dựa vào rất nhiều bài giảng mình dạy tại Mỹ cho các bạn muốn tìm hiểu về DS và ML. Vì vậy, các bạn không có background bên khối kỹ thuật như kinh tế, ngoại thương, xã hội học đều có thể học được.
Mình sẽ update thêm slides và bài giảng trên page trong thời gian sắp tới. Vì vậy, bạn nào quan tâm có thể theo dõi thêm.","Tiệp cho mình chia sẻ thêm thông tin về lớp Foundation of DS và ML (miễn phí và bằng tiếng Việt) cho các bạn nào quan tâm ở đây nhé. --- Mình gửi slides cho bài giảng đầu tiên cho lớp ""Foundation of Data Science and Machine Learning"" (Học phần Cơ Bản) ở đây: https://nhatptnk8912.github.io/Warmup_New.pdf . Khi mình có thời gian thì mình sẽ thu âm lại bài giảng và gửi lên homepage và page khoa học dữ liệu (https://www.facebook.com/khoahocvadulieu). Mình sẽ dùng tiếng Việt để giảng mặc dù ngôn ngữ trong slides là tiếng Anh. Các bạn có thể đọc slides tham khảo trước. Bài giảng đầu tiên chủ yếu review lại những chủ đề chúng mình sẽ học, những nguyên tắc khi làm DS và ML, và những khái niệm cơ bản về xác suất (sẽ hữu dụng cho các bạn khi học các bài giảng về sau). Bài giảng phù hợp cho các bạn đang bắt đầu học về DS và ML. Bài giảng cũng dựa vào rất nhiều bài giảng mình dạy tại Mỹ cho các bạn muốn tìm hiểu về DS và ML. Vì vậy, các bạn không có background bên khối kỹ thuật như kinh tế, ngoại thương, xã hội học đều có thể học được. Mình sẽ update thêm slides và bài giảng trên page trong thời gian sắp tới. Vì vậy, bạn nào quan tâm có thể theo dõi thêm.",,,,,
"Em chào tất cả các anh chị và các bạn trong nhóm.
Em muốn tìm hiểu về Face Recognition trước tiên là với ảnh, với video và cuối cùng là Face Recognition với video stream. Em có kiến thức cần thiết về Machine Learning, các thuật toán cơ bản trong Machine Learning. Có kiến thức về CNN và đã làm quen với tensorflow, keras. Em đang học khoá Deep Learnning Specialization trên coursera (đã học xong course 2 ), và khoá về triển khai Tensorflow developer. Em có đọc qua (chưa kỹ ) về cuốn sách Deep Learning cơ bản của anh Nguyễn Thanh Tuấn, cuốn sách Computer Vision with Jason Br.
Em đăng bài muốn nhờ các Anh (Chị) trong nhóm chia sẻ và tư vấn kinh nghiệm học tập (các kiến thức cần thiết, các mục quan trọng), hướng học tập phù hợp với em và các nguồn tài liệu học tập về mảng này.
Em cảm ơn rất nhiều ạ!","Em chào tất cả các anh chị và các bạn trong nhóm. Em muốn tìm hiểu về Face Recognition trước tiên là với ảnh, với video và cuối cùng là Face Recognition với video stream. Em có kiến thức cần thiết về Machine Learning, các thuật toán cơ bản trong Machine Learning. Có kiến thức về CNN và đã làm quen với tensorflow, keras. Em đang học khoá Deep Learnning Specialization trên coursera (đã học xong course 2 ), và khoá về triển khai Tensorflow developer. Em có đọc qua (chưa kỹ ) về cuốn sách Deep Learning cơ bản của anh Nguyễn Thanh Tuấn, cuốn sách Computer Vision with Jason Br. Em đăng bài muốn nhờ các Anh (Chị) trong nhóm chia sẻ và tư vấn kinh nghiệm học tập (các kiến thức cần thiết, các mục quan trọng), hướng học tập phù hợp với em và các nguồn tài liệu học tập về mảng này. Em cảm ơn rất nhiều ạ!",,,,,
"Em mới nhập môn FundaML mong anh chị thông cảm.
Em thắc mắc vì sao có thể giải 4.9 bằng cách tính đạo hàm bằng 0 ạ? Vì đạo hàm bằng 0 chỉ là chỉ ra tại đó hàm có cực trị thôi mà. Sao biết được là cực đại hay cực tiểu ạ? Em xin cảm ơn.",Em mới nhập môn FundaML mong anh chị thông cảm. Em thắc mắc vì sao có thể giải 4.9 bằng cách tính đạo hàm bằng 0 ạ? Vì đạo hàm bằng 0 chỉ là chỉ ra tại đó hàm có cực trị thôi mà. Sao biết được là cực đại hay cực tiểu ạ? Em xin cảm ơn.,,"#math, #Q&A",,,
"Em chào mọi người ạ!!! Em có dự định đăng kí khoá học data bên funix ạ. Ai đăng kí học bên funix rùi cho em xin ít review được không ạ ?? Mọi người recommend cho em một số chỗ học uy tín khác được không ạ
Em cảm ơn mọi người rất nhiều ạ 😄
P/s: Em là sinh viên năm 3 ạ !!!",Em chào mọi người ạ!!! Em có dự định đăng kí khoá học data bên funix ạ. Ai đăng kí học bên funix rùi cho em xin ít review được không ạ ?? Mọi người recommend cho em một số chỗ học uy tín khác được không ạ Em cảm ơn mọi người rất nhiều ạ P/s: Em là sinh viên năm 3 ạ !!!,,,,,
"Anh chị em học tiếng Anh chắc đều biết collocations, 
Hiện tại mình muốn tìm collocations patterns trong 1 đoạn văn, mà chưa biết  hướng làm như thế nào.
ví dụ She’s perfectly capable of running her business. 
thì perfectly capablevà running... business là 2 collocations.

Hiện tại mình mới nghĩ ra tách ra n-gram, rồi dùng metric nào đó để so sánh collocations trong từ điển để ra độ similarity. nhưng thế này độ phức tạp vẫn lớn quá.

ace có ý tưởng nào khác,  pretrain gì để xử lý bài này không ạ, em cám ơn.","Anh chị em học tiếng Anh chắc đều biết collocations, Hiện tại mình muốn tìm collocations patterns trong 1 đoạn văn, mà chưa biết hướng làm như thế nào. ví dụ She’s perfectly capable of running her business. thì perfectly capablevà running... business là 2 collocations. Hiện tại mình mới nghĩ ra tách ra n-gram, rồi dùng metric nào đó để so sánh collocations trong từ điển để ra độ similarity. nhưng thế này độ phức tạp vẫn lớn quá. ace có ý tưởng nào khác, pretrain gì để xử lý bài này không ạ, em cám ơn.",,,,,
"Em đang muốn build hệ thống quảng cáo trên mạng xã hội theo cá nhân hóa người dùng. Ai có thể sp thì inb em với, em cảm ơn ạ.","Em đang muốn build hệ thống quảng cáo trên mạng xã hội theo cá nhân hóa người dùng. Ai có thể sp thì inb em với, em cảm ơn ạ.",,,,,
"Em chào anh chị, anh chị cho em hỏi là có trang nào làm quiz machine learning ổn không ạ, tiếng việt càng tốt.
Em cảm ơn ạ","Em chào anh chị, anh chị cho em hỏi là có trang nào làm quiz machine learning ổn không ạ, tiếng việt càng tốt. Em cảm ơn ạ",,,,,
"Kính chào các bác, em không phải dân chuyên về DevOps hay Ops mà chỉ chuyên Dev. Tuy nhiên gần đây có nhiều bạn hỏi về Docker cho Deep Learning nên cũng mạnh dạn làm clip chia sẻ.
Clip là các kiến thức cơ bản nhất làm base cho các bạn tham khảo và nghiên cứu thêm. Hi vọng giúp được các bạn mới học!
Cảm ơn cả nhà!","Kính chào các bác, em không phải dân chuyên về DevOps hay Ops mà chỉ chuyên Dev. Tuy nhiên gần đây có nhiều bạn hỏi về Docker cho Deep Learning nên cũng mạnh dạn làm clip chia sẻ. Clip là các kiến thức cơ bản nhất làm base cho các bạn tham khảo và nghiên cứu thêm. Hi vọng giúp được các bạn mới học! Cảm ơn cả nhà!",,,,,
"Mình ma mới, các bạn nhiều kinh nghiệm xem giúp mình, trong 3 cái model mình train được, thì cái nào ổn nhất, và khi minh train mình để EarlyStopping(monitor='val_accuracy') thì trong quá trình val_accuracy tăng thì val_loss không giảm mà lại tăng như trong anh số 2, như vậy có được gọi là overfiting không, mức độ overfiting như vậy có lớn lắm không. Cảm ơn các bạn.","Mình ma mới, các bạn nhiều kinh nghiệm xem giúp mình, trong 3 cái model mình train được, thì cái nào ổn nhất, và khi minh train mình để EarlyStopping(monitor='val_accuracy') thì trong quá trình val_accuracy tăng thì val_loss không giảm mà lại tăng như trong anh số 2, như vậy có được gọi là overfiting không, mức độ overfiting như vậy có lớn lắm không. Cảm ơn các bạn.",,,,,
"Chào mọi người.
Lần trước mình có hỏi về vấn đề triển khai ứng dụng các mô hình AI/DL trên mobile và đã nhận được nhiều sự hỗ trợ. Team mình cũng đã hoàn thành được task này bằng việc dùng NCNN chuyển mô hình qua C++ rồi tích hợp vào mobile SDK. Mình rất cảm ơn mọi người đã hỗ trợ nhiệt tình!
Hôm nay, mình có một vấn đề khá tương tự mong mọi người tiếp tục giúp đỡ là triển khai mô hình AI/DL trên nền web client - tức là các yêu cầu được xử lý ngay tại phía client, không cần phải gửi request/ảnh về server. Vì mình không có nhiều kinh nghiệm về web (chỉ hiểu basic về html, css và javascript) nên mong mọi chỉ chi tiết một chút. Ngoài ra, mình làm Web SDK thay vì web app nên có mong muốn sử dụng ít thư việc, dependencies ít nhất có thể.
Rất mong nhận được sự hỗ trợ của mọi người.
Chân thành cảm ơn!","Chào mọi người. Lần trước mình có hỏi về vấn đề triển khai ứng dụng các mô hình AI/DL trên mobile và đã nhận được nhiều sự hỗ trợ. Team mình cũng đã hoàn thành được task này bằng việc dùng NCNN chuyển mô hình qua C++ rồi tích hợp vào mobile SDK. Mình rất cảm ơn mọi người đã hỗ trợ nhiệt tình! Hôm nay, mình có một vấn đề khá tương tự mong mọi người tiếp tục giúp đỡ là triển khai mô hình AI/DL trên nền web client - tức là các yêu cầu được xử lý ngay tại phía client, không cần phải gửi request/ảnh về server. Vì mình không có nhiều kinh nghiệm về web (chỉ hiểu basic về html, css và javascript) nên mong mọi chỉ chi tiết một chút. Ngoài ra, mình làm Web SDK thay vì web app nên có mong muốn sử dụng ít thư việc, dependencies ít nhất có thể. Rất mong nhận được sự hỗ trợ của mọi người. Chân thành cảm ơn!",,,,,
"Mình cần tìm ví dụ code về MLP mode trong keras để predict probabilities của từng class/label mà không dùng hàm predict_prob.
Nếu bạn nào có cho mình tham khảo với.
Rất cám ơn.",Mình cần tìm ví dụ code về MLP mode trong keras để predict probabilities của từng class/label mà không dùng hàm predict_prob. Nếu bạn nào có cho mình tham khảo với. Rất cám ơn.,,,,,
"I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Em chào mọi người , em đang học về ML cơ bản , thì học tới Binary Classification , TensorFlow lại báo lỗi như này ạ. 
Trên Colab thì nó chạy bthg nhma trên máy em lại bị lỗi như trên ạ

Mong mọi người giúp đỡ ạ","I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Em chào mọi người , em đang học về ML cơ bản , thì học tới Binary Classification , TensorFlow lại báo lỗi như này ạ. Trên Colab thì nó chạy bthg nhma trên máy em lại bị lỗi như trên ạ Mong mọi người giúp đỡ ạ",,,,,
"Mình chia sẻ slides về thuật toán gradient descent trong học máy mà mình đang dạy ở Mỹ tại đây: https://nhatptnk8912.github.io/Gradient_Descent...
Mặc dù thuật toán gradient descent đã quá quen thuộc với mọi người; tuy nhiên những gì chúng ta hiểu gì thuật toán này còn nhiều hạn chế, đặc biệt khi chúng ta nói về ngữ cảnh của các mô hình học máy. Trong bài giảng, mình cũng thêm vào nhiều kết quả nghiên cứu gần đây của thuật toán này và những vấn đề mở của nó. Trong bài giảng sau (mình cũng sẽ upload lên trang homepage cá nhân), mình sẽ đưa ra một số insight từ các công trình nghiên cứu gần đây của bản thân để làm sao giải quyết các vấn đề của gradient descent.
Do đây là lớp cho sinh viên tiến sĩ/ thạc sĩ nên lượng kiến thức trong bài giảng khá nhiều. Các bạn nào đã học qua các lớp cơ bản và muốn đi sâu vào học máy thì có thể đọc.","Mình chia sẻ slides về thuật toán gradient descent trong học máy mà mình đang dạy ở Mỹ tại đây: https://nhatptnk8912.github.io/Gradient_Descent... Mặc dù thuật toán gradient descent đã quá quen thuộc với mọi người; tuy nhiên những gì chúng ta hiểu gì thuật toán này còn nhiều hạn chế, đặc biệt khi chúng ta nói về ngữ cảnh của các mô hình học máy. Trong bài giảng, mình cũng thêm vào nhiều kết quả nghiên cứu gần đây của thuật toán này và những vấn đề mở của nó. Trong bài giảng sau (mình cũng sẽ upload lên trang homepage cá nhân), mình sẽ đưa ra một số insight từ các công trình nghiên cứu gần đây của bản thân để làm sao giải quyết các vấn đề của gradient descent. Do đây là lớp cho sinh viên tiến sĩ/ thạc sĩ nên lượng kiến thức trong bài giảng khá nhiều. Các bạn nào đã học qua các lớp cơ bản và muốn đi sâu vào học máy thì có thể đọc.",,,,,
"Kính chào các bác, nhân dịp đang nghiên cứu về YOLOv7 em mạnh dạn làm clip chia sẻ cách train YOLOv7, nhận diện YOLv7 nhanh bằng cách ""tận dụng"" GPU của Colab cùng các bạn mới học.
Xin cảm ơn cả nhà!","Kính chào các bác, nhân dịp đang nghiên cứu về YOLOv7 em mạnh dạn làm clip chia sẻ cách train YOLOv7, nhận diện YOLv7 nhanh bằng cách ""tận dụng"" GPU của Colab cùng các bạn mới học. Xin cảm ơn cả nhà!",,,,,
"Chào mọi người ạ
M.n trong nhóm ai đã từng làm mô hình Mask RCNN trên Net framework 4.7.x hoặc chuyển model mask qua ONNX để triển khai trên Net framework có thể cho em xin ít kinh nghiệm hoặc các hướng dẫn để triển khai được không ạ.
Em xin được gửi học phí nếu có ạ.
Thanks m.n.",Chào mọi người ạ M.n trong nhóm ai đã từng làm mô hình Mask RCNN trên Net framework 4.7.x hoặc chuyển model mask qua ONNX để triển khai trên Net framework có thể cho em xin ít kinh nghiệm hoặc các hướng dẫn để triển khai được không ạ. Em xin được gửi học phí nếu có ạ. Thanks m.n.,,,,,
"VinAI - AI Day 2022 - 26, 27 Aug 2022
💥Tutorial Zone at AI Day 2022💥
🚀An opportunity to learn from VinAI’s Best and Brightest Minds in the Tutorial Zone!🚀
Registration Link: please see below.","VinAI - AI Day 2022 - 26, 27 Aug 2022 Tutorial Zone at AI Day 2022 An opportunity to learn from VinAI’s Best and Brightest Minds in the Tutorial Zone! Registration Link: please see below.",,,,,
"Em chào mn!
Em đang train mô hình, em có đánh giá trên cả tập train và test theo AUC.
Em đang gặp một vấn đề là nhãn 1 thực tế của tập test là gần 300, nhưng khi predict mô hình chỉ dự đoán được 6 nhãn 1.
Em đã kiểm tra phân phối của các pred = 0 cũng không khác nhiều so với pred = 1.
Nhờ mn giải thích giúp em, em đang gặp lỗi gì được không ạ?
Em cảm ơn mn!","Em chào mn! Em đang train mô hình, em có đánh giá trên cả tập train và test theo AUC. Em đang gặp một vấn đề là nhãn 1 thực tế của tập test là gần 300, nhưng khi predict mô hình chỉ dự đoán được 6 nhãn 1. Em đã kiểm tra phân phối của các pred = 0 cũng không khác nhiều so với pred = 1. Nhờ mn giải thích giúp em, em đang gặp lỗi gì được không ạ? Em cảm ơn mn!",,,,,
"Em đang làm bài toán id card và đã detect ra được những thông tin cần thiết tuy nhiên chưa giải quyết được bài toán nhận dạng văn bản tiếng Việt.
Rất mong mọi người chia sẻ cho em ít tài liệu tham khảo với ạ....
Em đang làm với bộ thư viện tesseract thì kết quả chưa đc như ý.",Em đang làm bài toán id card và đã detect ra được những thông tin cần thiết tuy nhiên chưa giải quyết được bài toán nhận dạng văn bản tiếng Việt. Rất mong mọi người chia sẻ cho em ít tài liệu tham khảo với ạ.... Em đang làm với bộ thư viện tesseract thì kết quả chưa đc như ý.,,,,,
"xin chào mọi người và anh Tiệp, em đang convert model từ onnx -> tflite theo git này: https://github.com/sithu31296/PyTorch-ONNX-TFLite mà lại gặp lỗi dưới, em đã search và thử nhiều phương pháp khác nhau mà vẫn chưa fix đc. mn ai đã từng có kinh nghiệm giúp em với ạ!","xin chào mọi người và anh Tiệp, em đang convert model từ onnx -> tflite theo git này: https://github.com/sithu31296/PyTorch-ONNX-TFLite mà lại gặp lỗi dưới, em đã search và thử nhiều phương pháp khác nhau mà vẫn chưa fix đc. mn ai đã từng có kinh nghiệm giúp em với ạ!",,,,,
"Thưa anh chị, em hiện đang có vấn đề này. Hiện tại em đã có file đầu vào là dữ liệu các file speech to text về nội dung công việc. File này thì đã được gán nhán người giao việc, với content công việc. Vậy có cách nào để có thể trích xuất ra người được giao việc với nội dung công việc đó. Không biết có ai đã từng làm bài toán này chưa ạ hay có tài liệu nào có thể đưa giúp em tham khảo được không ạ . Bài toán dùng cho tiếng việt ạ.
Em cảm ơn mọi người rất nhiều","Thưa anh chị, em hiện đang có vấn đề này. Hiện tại em đã có file đầu vào là dữ liệu các file speech to text về nội dung công việc. File này thì đã được gán nhán người giao việc, với content công việc. Vậy có cách nào để có thể trích xuất ra người được giao việc với nội dung công việc đó. Không biết có ai đã từng làm bài toán này chưa ạ hay có tài liệu nào có thể đưa giúp em tham khảo được không ạ . Bài toán dùng cho tiếng việt ạ. Em cảm ơn mọi người rất nhiều",,,,,
"Xin chào. Mình đang tìm hiểu GNN để áp dụng cho 1 bài toán của mình. Cụ thể là mình có rất nhiều small knowledge graph (như hình đính kèm), và mình muốn sinh ra embedding vector cho từng graph đó. Vì các nodes/edges trong knowledge graph có nhãn (textual label) nên mình sẽ dùng 1 sentence embedding model để sinh embedding features cho các nodes/edges, sau đó dùng một mô hình GNN nào đó để sinh ra graph vector representation. Minh thử tìm trong PyG (PyTorch Geometric) nhưng chưa thấy ví dụ như mong muốn. Các bạn có ai biết thư viện nào hỗ trợ cách làm như vậy không? Nếu có thể, nhờ bạn tư vấn giúp xem mình nên làm như thế nào cho bài toán này. Thanks","Xin chào. Mình đang tìm hiểu GNN để áp dụng cho 1 bài toán của mình. Cụ thể là mình có rất nhiều small knowledge graph (như hình đính kèm), và mình muốn sinh ra embedding vector cho từng graph đó. Vì các nodes/edges trong knowledge graph có nhãn (textual label) nên mình sẽ dùng 1 sentence embedding model để sinh embedding features cho các nodes/edges, sau đó dùng một mô hình GNN nào đó để sinh ra graph vector representation. Minh thử tìm trong PyG (PyTorch Geometric) nhưng chưa thấy ví dụ như mong muốn. Các bạn có ai biết thư viện nào hỗ trợ cách làm như vậy không? Nếu có thể, nhờ bạn tư vấn giúp xem mình nên làm như thế nào cho bài toán này. Thanks",,,,,
"Em chào mọi người ạ,
Em đang đọc một bài báo về dùng ML để predict student performance [1]. Em đọc đến công thức tính thì có một cái annotation chưa hiểu lắm, em có đánh dấu trong hình ạ. Công thức thì em hiểu rồi nhưng chữ GPA cái mũ có nghĩa là gì ạ, tức là có mũ nó khác gì không có mũ ạ.
Em cám ơn mọi người ạ.
[1] Jie, Kyeong, Mihaela, A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs","Em chào mọi người ạ, Em đang đọc một bài báo về dùng ML để predict student performance [1]. Em đọc đến công thức tính thì có một cái annotation chưa hiểu lắm, em có đánh dấu trong hình ạ. Công thức thì em hiểu rồi nhưng chữ GPA cái mũ có nghĩa là gì ạ, tức là có mũ nó khác gì không có mũ ạ. Em cám ơn mọi người ạ. [1] Jie, Kyeong, Mihaela, A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs",,,,,
"Em là ma mới, e đang có bài tập cần train mô hình, em muốn dùng google colab nhưng data lại ở onedrive, cho em hỏi lấy dữ liệu từ onedrive sang colab kiểu gì ạ, hay phải up qua google drive, mà dung lượng gmail của em không đủ (T_T)","Em là ma mới, e đang có bài tập cần train mô hình, em muốn dùng google colab nhưng data lại ở onedrive, cho em hỏi lấy dữ liệu từ onedrive sang colab kiểu gì ạ, hay phải up qua google drive, mà dung lượng gmail của em không đủ (T_T)",,,,,
"[Khóa học miễn phí MLOps]
Nay mình chia sẻ với các bạn khóa học rất hay và chi tiết về MLOps. Khóa học này dạy mọi thứ liên quan đến MLOps từ thiết kế, mô hình hóa đến kiểm thử và triển khai mô hình Machine Learning.
Chi tiết khóa học xem ở đây: https://madewithml.com/#mlops","[Khóa học miễn phí MLOps] Nay mình chia sẻ với các bạn khóa học rất hay và chi tiết về MLOps. Khóa học này dạy mọi thứ liên quan đến MLOps từ thiết kế, mô hình hóa đến kiểm thử và triển khai mô hình Machine Learning. Chi tiết khóa học xem ở đây: https://madewithml.com/#mlops",,,,,
"[Góc chia sẻ - AI Webinar 2022 ]
Xin chào mọi người,
Xin phép chia sẻ với mọi người một webinar về AI sắp diễn ra mà mình cảm thấy khá hấp dẫn. Sự kiện có sự tham gia của các speaker là các Vice President, Director of AI, Chief Data, C-level trong ngành đến từ các tên tuổi lớn như Google, Microsoft, Mercedes-Benz, AWS, Volkswagen, IBM, SAP, Samsung SDS, Oxford, và các công ty tập đoàn lớn từ khắp 5 châu.
Mọi người ai có hứng thú đều có thể đăng ký vì nó hoàn toàn miễn phí, được giao lưu trực tiếp với diễn giả và được kết nối với cộng đồng AI từ khắp nơi trên thế giới.
Mặc dù có rất nhiều sự kiện trả phí trên mạng, nhưng Worldwide AI Webinar (https://wow-ai.com/event) không chỉ MIỄN PHÍ mà còn mang đến cho bạn cơ hội giành được tới 10.000 USD tiền mặt chỉ bằng cách hỏi những người thuyết trình thú vị. các câu hỏi. Tôi vừa nhận được thông tin này hôm nay và tôi muốn chia sẻ với bạn lựa chọn này.
Trong mỗi phiên, bạn được khuyến khích tương tác trực tiếp với các diễn giả chính bằng cách hỏi họ những câu hỏi thú vị. Vào cuối mỗi phiên, các diễn giả chính sẽ quyết định câu hỏi nào mà họ cho là thú vị nhất. Người thắng cuộc sẽ nhận được phần thưởng bằng tiền mặt ngay sau sự kiện.
Giải thưởng:
• 300 USD cho mỗi người chiến thắng trong mỗi buổi phát biểu quan trọng
• 500 USD cho mỗi người chiến thắng trong mỗi phiên hội thảo
Đây là liên kết để đăng ký:
https://meetyoo.live/register/1/worldwideAI2022","[Góc chia sẻ - AI Webinar 2022 ] Xin chào mọi người, Xin phép chia sẻ với mọi người một webinar về AI sắp diễn ra mà mình cảm thấy khá hấp dẫn. Sự kiện có sự tham gia của các speaker là các Vice President, Director of AI, Chief Data, C-level trong ngành đến từ các tên tuổi lớn như Google, Microsoft, Mercedes-Benz, AWS, Volkswagen, IBM, SAP, Samsung SDS, Oxford, và các công ty tập đoàn lớn từ khắp 5 châu. Mọi người ai có hứng thú đều có thể đăng ký vì nó hoàn toàn miễn phí, được giao lưu trực tiếp với diễn giả và được kết nối với cộng đồng AI từ khắp nơi trên thế giới. Mặc dù có rất nhiều sự kiện trả phí trên mạng, nhưng Worldwide AI Webinar (https://wow-ai.com/event) không chỉ MIỄN PHÍ mà còn mang đến cho bạn cơ hội giành được tới 10.000 USD tiền mặt chỉ bằng cách hỏi những người thuyết trình thú vị. các câu hỏi. Tôi vừa nhận được thông tin này hôm nay và tôi muốn chia sẻ với bạn lựa chọn này. Trong mỗi phiên, bạn được khuyến khích tương tác trực tiếp với các diễn giả chính bằng cách hỏi họ những câu hỏi thú vị. Vào cuối mỗi phiên, các diễn giả chính sẽ quyết định câu hỏi nào mà họ cho là thú vị nhất. Người thắng cuộc sẽ nhận được phần thưởng bằng tiền mặt ngay sau sự kiện. Giải thưởng: • 300 USD cho mỗi người chiến thắng trong mỗi buổi phát biểu quan trọng • 500 USD cho mỗi người chiến thắng trong mỗi phiên hội thảo Đây là liên kết để đăng ký: https://meetyoo.live/register/1/worldwideAI2022",,,,,
"Chào mọi người
Em đang thực hiện một PJ nhỏ ở THPT về ML , em có thắc mắc là mình có một căn nhà và N tọa độ GPS của N phòng thì mình phải thực hiện như thế nào để cho ML có thể di chuyển tới căn phòng mình mong muốn. Em có ý định là cài đặt sẵn đường đi cho từng căn phòng , nhma như vậy với một tòa nhà lớn hơn thì làm như vậy khá lâu. Em thử đổi qua Mapping nhưng lại k biết tìm hiểu từ đâu , mọi người có thể cho em tài liệu hay là các nguồn để em có thể tìm hiểu về Mapping được không ạ","Chào mọi người Em đang thực hiện một PJ nhỏ ở THPT về ML , em có thắc mắc là mình có một căn nhà và N tọa độ GPS của N phòng thì mình phải thực hiện như thế nào để cho ML có thể di chuyển tới căn phòng mình mong muốn. Em có ý định là cài đặt sẵn đường đi cho từng căn phòng , nhma như vậy với một tòa nhà lớn hơn thì làm như vậy khá lâu. Em thử đổi qua Mapping nhưng lại k biết tìm hiểu từ đâu , mọi người có thể cho em tài liệu hay là các nguồn để em có thể tìm hiểu về Mapping được không ạ",,,,,
"[Update – Worldwide AI Webinar 2022]
Chào mọi người,
Chắc mọi người đã xem qua các bài viết về Worldwide AI Webinar cũng như đã từng tham gia vào một số webinar về AI khác. Do hôm nay mình vừa nhận được thông tin mới từ phía host và thấy khá hay nên muốn chia sẻ lại với mọi người.
Theo như ban tổ chức, webinar lần này vừa là cơ hội miễn phí để mọi người học tập mà vừa có thể nhận được phần thưởng bằng cách đặt ra câu hỏi cho các speaker, tổng giá trị lên tới 10,000 USD. Cụ thể là ở mỗi live session, thính giả được khuyến khích tương tác trực tiếp với speaker và đưa ra các câu hỏi hay và khó về chủ đề AI/ML trong doanh nghiệp. Cuối mỗi session, speaker sẽ chọn ra câu hỏi họ cho là hấp dẫn nhất và giải thưởng 300 – 500 USD sẽ được trao cho người chiến thắng (tiền mặt được trao sau event).
Sự kiện free mà còn được nhận tiền 😁. Mọi người xem thêm thông tin tại đây: https://event.wow-ai.com/worldwideAI2022/","[Update – Worldwide AI Webinar 2022] Chào mọi người, Chắc mọi người đã xem qua các bài viết về Worldwide AI Webinar cũng như đã từng tham gia vào một số webinar về AI khác. Do hôm nay mình vừa nhận được thông tin mới từ phía host và thấy khá hay nên muốn chia sẻ lại với mọi người. Theo như ban tổ chức, webinar lần này vừa là cơ hội miễn phí để mọi người học tập mà vừa có thể nhận được phần thưởng bằng cách đặt ra câu hỏi cho các speaker, tổng giá trị lên tới 10,000 USD. Cụ thể là ở mỗi live session, thính giả được khuyến khích tương tác trực tiếp với speaker và đưa ra các câu hỏi hay và khó về chủ đề AI/ML trong doanh nghiệp. Cuối mỗi session, speaker sẽ chọn ra câu hỏi họ cho là hấp dẫn nhất và giải thưởng 300 – 500 USD sẽ được trao cho người chiến thắng (tiền mặt được trao sau event). Sự kiện free mà còn được nhận tiền . Mọi người xem thêm thông tin tại đây: https://event.wow-ai.com/worldwideAI2022/",,,,,
"Quá trình phát triển các sản phẩm AI luôn tồn tại những thách thức. Mời các bạn theo dõi sự kiện để lắng nghe chia sẻ từ chuyên gia hàng đầu về AI/Machine Learning/Data Scientist trong talkshow AI production challenge được tài trợ bởi công ty Neurond và tổ chức bởi DataScienceWorld.Kan.

Thời gian từ 10:45 AM-12:00 AM ngày 21-August-2022
- live stream youtube:
https://youtu.be/02FCIk8anVo
- link đặt câu hỏi:
https://forms.gle/igM4mncYFcjDhV1W6",Quá trình phát triển các sản phẩm AI luôn tồn tại những thách thức. Mời các bạn theo dõi sự kiện để lắng nghe chia sẻ từ chuyên gia hàng đầu về AI/Machine Learning/Data Scientist trong talkshow AI production challenge được tài trợ bởi công ty Neurond và tổ chức bởi DataScienceWorld.Kan. Thời gian từ 10:45 AM-12:00 AM ngày 21-August-2022 - live stream youtube: https://youtu.be/02FCIk8anVo - link đặt câu hỏi: https://forms.gle/igM4mncYFcjDhV1W6,,,,,
"Bài viết hay về phân tích yêu cầu và thiết kế MLOps platform của iFunny
---
Mọi người quan tâm tới ML Engineering/MLOps thì join group MLOps VN để cùng giao lưu học hỏi và chia sẻ kiến thức nhé ạ
https://www.facebook.com/groups/mlopsvn",Bài viết hay về phân tích yêu cầu và thiết kế MLOps platform của iFunny --- Mọi người quan tâm tới ML Engineering/MLOps thì join group MLOps VN để cùng giao lưu học hỏi và chia sẻ kiến thức nhé ạ https://www.facebook.com/groups/mlopsvn,,,,,
"Em chào các anh chị ạ, hiện tại e đang là sinh viên năm cuối định hướng theo AI. Em đang muốn sau khi ra trường e đi học lên lấy bằng Master luôn, thì theo mọi người khi mới ra trường mình đi học lên luôn hay đi làm 1, 2 năm rồi mới đi học ạ. Nếu có đi thì các anh chị có thể chia sẻ cho em kinh nghiệm lấy học bổng bên Đức, Pháp hoặc Mỹ được không ạ. Em cảm ơn mọi người rất nhiều.","Em chào các anh chị ạ, hiện tại e đang là sinh viên năm cuối định hướng theo AI. Em đang muốn sau khi ra trường e đi học lên lấy bằng Master luôn, thì theo mọi người khi mới ra trường mình đi học lên luôn hay đi làm 1, 2 năm rồi mới đi học ạ. Nếu có đi thì các anh chị có thể chia sẻ cho em kinh nghiệm lấy học bổng bên Đức, Pháp hoặc Mỹ được không ạ. Em cảm ơn mọi người rất nhiều.",,,,,
"Em chào các anh các chị, em có một bài toán là :
Em có một máy vision hình ảnh ngoại quan của sản phẩm thông qua camera. Dựa trên bề mặt của sản phẩm thông qua hình ảnh sẽ đánh giá xem sản phẩm có bị lỗi hay không nếu có lỗi thì bị lỗi gì, danh sách lỗi bao gồm khoảng 10 lỗi, mỗi lỗi sẽ có hình dạng khác nhau. Ví dụ như hình ảnh bên dưới phần em khoanh màu đỏ là đại diện cho lỗi A chẳng hạn. Trên một hình ảnh có thể có nhiều loại lỗi ở nhiều vị trí khác nhau trên ảnh hoặc chỉ 1 lỗi tùy vào sản phẩm. Em muốn áp dụng ML để đánh giá tự động dựa trên hình ảnh đầu vào từ camera thì em nên dùng mô hình có sẵn nào hay đi theo hướng nào mong các anh chị chỉ bảo ạ.","Em chào các anh các chị, em có một bài toán là : Em có một máy vision hình ảnh ngoại quan của sản phẩm thông qua camera. Dựa trên bề mặt của sản phẩm thông qua hình ảnh sẽ đánh giá xem sản phẩm có bị lỗi hay không nếu có lỗi thì bị lỗi gì, danh sách lỗi bao gồm khoảng 10 lỗi, mỗi lỗi sẽ có hình dạng khác nhau. Ví dụ như hình ảnh bên dưới phần em khoanh màu đỏ là đại diện cho lỗi A chẳng hạn. Trên một hình ảnh có thể có nhiều loại lỗi ở nhiều vị trí khác nhau trên ảnh hoặc chỉ 1 lỗi tùy vào sản phẩm. Em muốn áp dụng ML để đánh giá tự động dựa trên hình ảnh đầu vào từ camera thì em nên dùng mô hình có sẵn nào hay đi theo hướng nào mong các anh chị chỉ bảo ạ.",,,,,
"VinAI Seminar - Despoina Paschalidou - Stanford University - ""Learning Compositional Representations for Understanding and Generating Controllable 3D Environments""
11.00 am - 12.00 pm (Vietnam time, GMT+7), Fri, Aug 19, 2022","VinAI Seminar - Despoina Paschalidou - Stanford University - ""Learning Compositional Representations for Understanding and Generating Controllable 3D Environments"" 11.00 am - 12.00 pm (Vietnam time, GMT+7), Fri, Aug 19, 2022",,,,,
"Chào mọi người, hôm nay mình xin giới thiệu đến mọi người một pretrain sentence embeddings cho tiếng Việt mình vừa training xong tên là SimCSE_Vietnamese, mình sử dụng kiến trúc tương tự SimCSE với encoding input mình dùng là Phobert.
Mình đã áp dụng nó vào bài toán semantic search kết hợp với elasticsearch kết quả hiện tại rất ổn.
Mọi người có thể xem chi tiết đây : https://viblo.asia/p/nlp-cai-thien-elasticsearch-trong-bai-toan-semantic-search-su-dung-phuong-phap-sentence-embeddings-Qpmley4rlrd.
Mọi người có thể test SimCSE_Vietnamese trực tiếp tại link colab này : https://colab.research.google.com/drive/12__EXJoQYHe9nhi4aXLTf9idtXT8yr7H?usp=sharing
Chúc mọi người cuối tuần vui vẻ !","Chào mọi người, hôm nay mình xin giới thiệu đến mọi người một pretrain sentence embeddings cho tiếng Việt mình vừa training xong tên là SimCSE_Vietnamese, mình sử dụng kiến trúc tương tự SimCSE với encoding input mình dùng là Phobert. Mình đã áp dụng nó vào bài toán semantic search kết hợp với elasticsearch kết quả hiện tại rất ổn. Mọi người có thể xem chi tiết đây : https://viblo.asia/p/nlp-cai-thien-elasticsearch-trong-bai-toan-semantic-search-su-dung-phuong-phap-sentence-embeddings-Qpmley4rlrd. Mọi người có thể test SimCSE_Vietnamese trực tiếp tại link colab này : https://colab.research.google.com/drive/12__EXJoQYHe9nhi4aXLTf9idtXT8yr7H?usp=sharing Chúc mọi người cuối tuần vui vẻ !",,,,,
"Em chào mọi người ạ.
Em đang chuẩn bị vào năm nhất và có nhu cầu mua laptop mới để phục vụ việc trên model và học DS ạ.
Mọi người có thể recommend cho em con laptop tốt để học được không ạ.
Budget của em: 20m đổ xuống ạ
Em cảm ơn nhiều ạ",Em chào mọi người ạ. Em đang chuẩn bị vào năm nhất và có nhu cầu mua laptop mới để phục vụ việc trên model và học DS ạ. Mọi người có thể recommend cho em con laptop tốt để học được không ạ. Budget của em: 20m đổ xuống ạ Em cảm ơn nhiều ạ,,,,,
"xin chào mọi người, hiện em đang tìm hiểu bài toán nhận diện bảng thủ ngữ tiếng việt, nhưng em đang mắc ở bước có vài chữ cái gồm 2 cử chỉ tay thì không detect được, mong mọi người giúp em hướng giải quyết ạ","xin chào mọi người, hiện em đang tìm hiểu bài toán nhận diện bảng thủ ngữ tiếng việt, nhưng em đang mắc ở bước có vài chữ cái gồm 2 cử chỉ tay thì không detect được, mong mọi người giúp em hướng giải quyết ạ",,,,,
"Chào các bác, hơi k liên quan một chút nhưng mình muốn xin tư vấn ạ.
Mình hiện đang học ngành Ngôn ngữ, công nghệ và truyền thông. Ngành của mình theo hướng ứng dụng AI (artificial intelligence) và cần học về python, machine learning, NLP... Tuy nhiên thì mình chưa nắm đc python cơ bản lắm và những môn khác như Thuật toán và Cấu trúc dữ liệu cũng yếu vì trước đây mình dự định theo ngành ngôn ngữ nên đây là 1 hướng đi hơi mới với mình 😅
Vậy nên mình muốn học 1 khoá ngắn hạn và mình đã tìm hiểu khoá Python Developer for AI ở VTC Academy, k biết có bác nào đã học ở đây chưa ạ?
Ngoài ra có ai nhận dạy thêm trong tháng 9 những môn lquan trên k ạ? Vì mình cũng mong muốn tìm giờ học linh hoạt chút.
Cảm ơn cả nhà đã đọc bài ạ 😊😊😊","Chào các bác, hơi k liên quan một chút nhưng mình muốn xin tư vấn ạ. Mình hiện đang học ngành Ngôn ngữ, công nghệ và truyền thông. Ngành của mình theo hướng ứng dụng AI (artificial intelligence) và cần học về python, machine learning, NLP... Tuy nhiên thì mình chưa nắm đc python cơ bản lắm và những môn khác như Thuật toán và Cấu trúc dữ liệu cũng yếu vì trước đây mình dự định theo ngành ngôn ngữ nên đây là 1 hướng đi hơi mới với mình Vậy nên mình muốn học 1 khoá ngắn hạn và mình đã tìm hiểu khoá Python Developer for AI ở VTC Academy, k biết có bác nào đã học ở đây chưa ạ? Ngoài ra có ai nhận dạy thêm trong tháng 9 những môn lquan trên k ạ? Vì mình cũng mong muốn tìm giờ học linh hoạt chút. Cảm ơn cả nhà đã đọc bài ạ",,,,,
"Bài toán Detect Forged ID Card (Kiểm tra CCCD giả)
Em chào mọi người ạ, em hiện đang tìm hiểu về bài toán kiểm tra CCCD giả với các chi tiết như sau.
- Input: Ảnh chụp CCCD (có thể có yêu cầu người chụp để CCCD ở 1 góc cố định, tạo frame hướng dẫn chụp)
- Output: Classified CCCD có bị chỉnh sửa hình ảnh hay không
Bài toán của em chỉ giới hạn ở detect 2 dạng chỉnh sửa phổ biến:
- Chụp lại màn hình của điện thoại chứa ảnh CCCD bị chỉnh sửa -> Phân biệt ảnh thật và ảnh qua màn hình điện thoại
- In ảnh giả và đè lên CCCD thật và chụp ảnh CCCD đó
Em muốn xin ý kiến về hướng tiếp cận bài toán ạ. Em cảm ơn mọi người đã dành thời gian đọc ạ.","Bài toán Detect Forged ID Card (Kiểm tra CCCD giả) Em chào mọi người ạ, em hiện đang tìm hiểu về bài toán kiểm tra CCCD giả với các chi tiết như sau. - Input: Ảnh chụp CCCD (có thể có yêu cầu người chụp để CCCD ở 1 góc cố định, tạo frame hướng dẫn chụp) - Output: Classified CCCD có bị chỉnh sửa hình ảnh hay không Bài toán của em chỉ giới hạn ở detect 2 dạng chỉnh sửa phổ biến: - Chụp lại màn hình của điện thoại chứa ảnh CCCD bị chỉnh sửa -> Phân biệt ảnh thật và ảnh qua màn hình điện thoại - In ảnh giả và đè lên CCCD thật và chụp ảnh CCCD đó Em muốn xin ý kiến về hướng tiếp cận bài toán ạ. Em cảm ơn mọi người đã dành thời gian đọc ạ.",,,,,
"chào các bạn,
Mình đến từ team Wano, mình xin chia sẻ solution team mình đã áp dụng để giành được top 1 private test tại zalo AI challenge 2021 task hum to song.
Ở task này vì hạn chế về phần cứng nên hướng đi từ đầu của team mình là data centric, tập trung clean và augment data. Team mình sử dụng model resnet18 với một vài mở rộng và arcface loss để rút trích đặc trưng từ mel spectrogram. Trong quá trình training và inference tụi mình hoàn toàn chỉ sử dụng colab pro. Đối với tập private test vì không được thông báo từ trước data sẽ lớn như vậy (12.5GB) nên team đã không áp dụng các kỹ thuật như reranking để tăng độ chính xác mặc dù vậy team cũng đã may mắn giành được top 1 với thời gian xử lý khá nhanh ( khoảng 1h ).
Toàn bộ source code từ clean, augment data đến train và inference tụi mình đã để hết trong repo mọi người tham khảo ( nếu thấy hay cho mình xin 1 star 😍😍 )
Thiều Nguyễn, Thịnh Bá Lâm","chào các bạn, Mình đến từ team Wano, mình xin chia sẻ solution team mình đã áp dụng để giành được top 1 private test tại zalo AI challenge 2021 task hum to song. Ở task này vì hạn chế về phần cứng nên hướng đi từ đầu của team mình là data centric, tập trung clean và augment data. Team mình sử dụng model resnet18 với một vài mở rộng và arcface loss để rút trích đặc trưng từ mel spectrogram. Trong quá trình training và inference tụi mình hoàn toàn chỉ sử dụng colab pro. Đối với tập private test vì không được thông báo từ trước data sẽ lớn như vậy (12.5GB) nên team đã không áp dụng các kỹ thuật như reranking để tăng độ chính xác mặc dù vậy team cũng đã may mắn giành được top 1 với thời gian xử lý khá nhanh ( khoảng 1h ). Toàn bộ source code từ clean, augment data đến train và inference tụi mình đã để hết trong repo mọi người tham khảo ( nếu thấy hay cho mình xin 1 star ) Thiều Nguyễn, Thịnh Bá Lâm",,,,,
"Yann LeCun's Deep Learning 2021 course mới ra lò cách đây gần 1 tuần, ai quan tâm thì xem nhé. Có 1 blog đi cùng khá là chất lượng đó.","Yann LeCun's Deep Learning 2021 course mới ra lò cách đây gần 1 tuần, ai quan tâm thì xem nhé. Có 1 blog đi cùng khá là chất lượng đó.",,,,,
Buổi chia sẻ hữu ích từ một Senior ML Engineer 😍,Buổi chia sẻ hữu ích từ một Senior ML Engineer,,,,,
"[Góc chia sẻ - AI Webinar 2022 ]
Xin chào mọi người,
Xin phép chia sẻ với mọi người một webinar về AI sắp diễn ra mà mình cảm thấy khá hấp dẫn. Sự kiện có sự tham gia của các speaker là các Vice President, Director of AI, Chief Data, C-level trong ngành đến từ các tên tuổi lớn như Google, Microsoft, Mercedes-Benz, AWS, Volkswagen, IBM, SAP, Samsung SDS, Oxford, và các công ty tập đoàn lớn từ khắp 5 châu.
Link đăng ký mn có thể tham khảo tại đây:
https://meetyoo.live/register/1/worldwideAI2022","[Góc chia sẻ - AI Webinar 2022 ] Xin chào mọi người, Xin phép chia sẻ với mọi người một webinar về AI sắp diễn ra mà mình cảm thấy khá hấp dẫn. Sự kiện có sự tham gia của các speaker là các Vice President, Director of AI, Chief Data, C-level trong ngành đến từ các tên tuổi lớn như Google, Microsoft, Mercedes-Benz, AWS, Volkswagen, IBM, SAP, Samsung SDS, Oxford, và các công ty tập đoàn lớn từ khắp 5 châu. Link đăng ký mn có thể tham khảo tại đây: https://meetyoo.live/register/1/worldwideAI2022",,,,,
"Thay vì nói về ML/DL/AI, theo tôi mô hình thống kê có thể giải quyết được nhiều bài toán trong thực tiễn, đặc biệt trong y học vì tính Transparency, Reliability, và Explainability. Nay mình muốn giới thiệu 1 số ví dụ trong sách của chính GS Faraway nói về Mixed Effect Models viết bằng ngôn ngữ R tại đây https://github.com/julianfaraway/rexamples
Nội dung gồm:
1/ Single Random Effect - the pulp data
2/ Randomized Block Design - the penicillin data
3/ Split Plot Design - the irrigation data
4/ Nested Effects - the eggs data
5/ Crossed Effects - the abrasion data
6/ Multilevel Models - the jsp data
7/ Longitudinal Models - the psid data
8/ Repeated Measures - the vision data
9/ Multiple Response Models - the jsp data
Phần source code này thuộc 2 chương của cuốn sách có tên ""Extending the Linear Model with R"" của cùng tác giả.
Hi vọng nó góp vui cho các bạn ngày cuối tuần!","Thay vì nói về ML/DL/AI, theo tôi mô hình thống kê có thể giải quyết được nhiều bài toán trong thực tiễn, đặc biệt trong y học vì tính Transparency, Reliability, và Explainability. Nay mình muốn giới thiệu 1 số ví dụ trong sách của chính GS Faraway nói về Mixed Effect Models viết bằng ngôn ngữ R tại đây https://github.com/julianfaraway/rexamples Nội dung gồm: 1/ Single Random Effect - the pulp data 2/ Randomized Block Design - the penicillin data 3/ Split Plot Design - the irrigation data 4/ Nested Effects - the eggs data 5/ Crossed Effects - the abrasion data 6/ Multilevel Models - the jsp data 7/ Longitudinal Models - the psid data 8/ Repeated Measures - the vision data 9/ Multiple Response Models - the jsp data Phần source code này thuộc 2 chương của cuốn sách có tên ""Extending the Linear Model with R"" của cùng tác giả. Hi vọng nó góp vui cho các bạn ngày cuối tuần!",,,,,
"𝐯𝐢-𝐛𝐚𝐫𝐭𝐟𝐥𝐚𝐱-𝐥𝐚𝐫𝐠𝐞-𝐧𝐞𝐰𝐬: Mô hình BART trên miền tin tức tiếng Việt
🔥Sau khi công bố bartflax, VietAI đã nhận được những câu hỏi về kết quả khi sử dụng implementation này. Chúng tôi đã huấn luyện kiến trúc BART-large trên tập dữ liệu tin tức tiếng Việt gồm 20 triệu bài báo, và finetune mô hình thu được trên tập dữ liệu tóm tắt văn bản VNDS (Vietnews). Bảng 1 trình bày kết quả chúng tôi đạt được khi so sánh với một số mô hình BART tiếng Việt từng được công bố.
🔥Để reproduce kết quả trên, các có thể sử dụng ví dụ run_summarization_flax.py của thư viện transformers (link tại cuối bài viết), với tham số --dataset_name=duongna/vietnews và --model_name_or_path=VietAI/vi-bartflax-large-news.
👉Tham khảo bài viết trên VietAI để sử dụng mô hình này!","---: Mô hình BART trên miền tin tức tiếng Việt Sau khi công bố bartflax, VietAI đã nhận được những câu hỏi về kết quả khi sử dụng implementation này. Chúng tôi đã huấn luyện kiến trúc BART-large trên tập dữ liệu tin tức tiếng Việt gồm 20 triệu bài báo, và finetune mô hình thu được trên tập dữ liệu tóm tắt văn bản VNDS (Vietnews). Bảng 1 trình bày kết quả chúng tôi đạt được khi so sánh với một số mô hình BART tiếng Việt từng được công bố. Để reproduce kết quả trên, các có thể sử dụng ví dụ run_summarization_flax.py của thư viện transformers (link tại cuối bài viết), với tham số --dataset_name=duongna/vietnews và --model_name_or_path=VietAI/vi-bartflax-large-news. Tham khảo bài viết trên VietAI để sử dụng mô hình này!",,,,,
"Hi ACE,
Mình muốn hỏi một câu rất cơ bản, mình đang dùng thuật toán Random forest để phân loại bệnh. Hiện tại thì độ chính xác của nó khoảng 97%, nhưng mình còn một vài cá thể (3/100) mẫu bị phân loại sai. Làm thế nào để biết được lí do tại sao thuật toán lại ko thể phân loại đúng được các (3) cá thể trên.
Cảm ơn mọi người đã trả lời.","Hi ACE, Mình muốn hỏi một câu rất cơ bản, mình đang dùng thuật toán Random forest để phân loại bệnh. Hiện tại thì độ chính xác của nó khoảng 97%, nhưng mình còn một vài cá thể (3/100) mẫu bị phân loại sai. Làm thế nào để biết được lí do tại sao thuật toán lại ko thể phân loại đúng được các (3) cá thể trên. Cảm ơn mọi người đã trả lời.",,,,,
"VinAI Seminar - Dr. Thanh Nguyen - University of Oregon - ""Vulnerabilities in Data-Centered Decision Making""
11.00 am – 12.00 pm (Vietnam time, GMT+7), Monday, August 15, 2022","VinAI Seminar - Dr. Thanh Nguyen - University of Oregon - ""Vulnerabilities in Data-Centered Decision Making"" 11.00 am – 12.00 pm (Vietnam time, GMT+7), Monday, August 15, 2022",,,,,
"[Góc chia sẻ - Worldwide AI Webinar 2022 ]
Xin chào mọi người,
Xin phép chia sẻ với mọi người một webinar về AI sắp diễn ra mà em/mình cảm thấy khá hấp dẫn. Sự kiện có sự tham gia của các speaker là các Vice President, Director of AI, Chief Data, C-level trong ngành đến từ các tên tuổi lớn như Google, Microsoft, Mercedes-Benz, AWS, Volkswagen, IBM, SAP, Samsung SDS, Oxford, và các công ty tập đoàn lớn từ khắp 5 châu.
Mọi người ai có hứng thú đều có thể đăng ký vì nó hoàn toàn miễn phí, được giao lưu trực tiếp với diễn giả và được kết nối với cộng đồng AI từ khắp nơi trên thế giới.
Web: https://event.wow-ai.com/worldwideAI2022/","[Góc chia sẻ - Worldwide AI Webinar 2022 ] Xin chào mọi người, Xin phép chia sẻ với mọi người một webinar về AI sắp diễn ra mà em/mình cảm thấy khá hấp dẫn. Sự kiện có sự tham gia của các speaker là các Vice President, Director of AI, Chief Data, C-level trong ngành đến từ các tên tuổi lớn như Google, Microsoft, Mercedes-Benz, AWS, Volkswagen, IBM, SAP, Samsung SDS, Oxford, và các công ty tập đoàn lớn từ khắp 5 châu. Mọi người ai có hứng thú đều có thể đăng ký vì nó hoàn toàn miễn phí, được giao lưu trực tiếp với diễn giả và được kết nối với cộng đồng AI từ khắp nơi trên thế giới. Web: https://event.wow-ai.com/worldwideAI2022/",,,,,
"Giai đoạn từ sau khi OpenAI GPT-3 ra đời có thể xem như một thời kì trăm hoa đua nở của các mô hình AI về ngôn ngữ lớn với kích thước hàng trăm tỷ tham số. Những trở ngại về hạ tầng tính toán và dữ liệu lớn đã khiến cho những mô hình này được phát triển bởi những big tech tên tuổi lớn. BLOOM như một mô hình đầu tiên đánh dấu sự hợp tác từ các nhà nghiên cứu thuộc nhiều tổ chức khác nhau nhằm tạo ra một mô hình minh bạch, có thể kiếm toán được và mang tính đại diện. Dự án như một lời khẳng định rằng sự hợp tác có thể mang lại sức mạnh để chống lại sự độc quyền từ các big tech trong việc tạo ra Large Language Models.","Giai đoạn từ sau khi OpenAI GPT-3 ra đời có thể xem như một thời kì trăm hoa đua nở của các mô hình AI về ngôn ngữ lớn với kích thước hàng trăm tỷ tham số. Những trở ngại về hạ tầng tính toán và dữ liệu lớn đã khiến cho những mô hình này được phát triển bởi những big tech tên tuổi lớn. BLOOM như một mô hình đầu tiên đánh dấu sự hợp tác từ các nhà nghiên cứu thuộc nhiều tổ chức khác nhau nhằm tạo ra một mô hình minh bạch, có thể kiếm toán được và mang tính đại diện. Dự án như một lời khẳng định rằng sự hợp tác có thể mang lại sức mạnh để chống lại sự độc quyền từ các big tech trong việc tạo ra Large Language Models.",,,,,
"Em chào mọi người, em nhờ mọi người tư vấn giúp em câu hỏi.
Em là sinh viên năm 3, dự định theo hướng Machine Learning Engineer, tuy nhiên em có tìm hiểu thì thấy đa số không có một giáo trình đào tạo chuyên sâu cho mảng này.
Vậy em muốn theo Machine Learning Engineer thì em cần phải học những gì, lộ trình học như thế nào ạ? 
Hiện em biết về Python và các thuật toán ML cơ bản.
Em nhờ mọi người tư vấn giúp. Em là newbie ạ, xin cảm ơn mọi người rất nhiều.","Em chào mọi người, em nhờ mọi người tư vấn giúp em câu hỏi. Em là sinh viên năm 3, dự định theo hướng Machine Learning Engineer, tuy nhiên em có tìm hiểu thì thấy đa số không có một giáo trình đào tạo chuyên sâu cho mảng này. Vậy em muốn theo Machine Learning Engineer thì em cần phải học những gì, lộ trình học như thế nào ạ? Hiện em biết về Python và các thuật toán ML cơ bản. Em nhờ mọi người tư vấn giúp. Em là newbie ạ, xin cảm ơn mọi người rất nhiều.",,,,,
"[Formal Algorithms for Transformers]

Kể từ khi được giới thiệu bởi Google Brain vào năm 2017, mô hình Transformer đạt được rất nhiều thành công trên nhiều lĩnh vực như xử lý ngôn ngữ tự nhiên (NLP), xử lý ảnh (CV), Graph,... Rất nhiều paper liên quan tới Transformer với những cải tiến đáng kể đã được publish, tuy nhiên rất ít paper có pseudocode (giả mã) - giải thích các bước hoạt động của mô hình.

Gần đây, nhóm nghiên cứu tới từ DeepMind đã cho xuất bản paper Formal Algorithms for Transformers, biểu diễn hơn 15 biến thể của mô hình Transformer dưới dạng giả mã, ngắn gọn, xúc tích và đầy đủ các bước. Thêm vào đó, tác giả còn giải thích Transformer là gì, ý tưởng kiến trúc mô hình từ đâu, cách tokenize dữ liệu thế nào, làm sao để train mô hình và những lời khuyên khi sử dụng mô hình Transformer.

Paper: https://arxiv.org/pdf/2207.09238.pdf","[Formal Algorithms for Transformers] Kể từ khi được giới thiệu bởi Google Brain vào năm 2017, mô hình Transformer đạt được rất nhiều thành công trên nhiều lĩnh vực như xử lý ngôn ngữ tự nhiên (NLP), xử lý ảnh (CV), Graph,... Rất nhiều paper liên quan tới Transformer với những cải tiến đáng kể đã được publish, tuy nhiên rất ít paper có pseudocode (giả mã) - giải thích các bước hoạt động của mô hình. Gần đây, nhóm nghiên cứu tới từ DeepMind đã cho xuất bản paper Formal Algorithms for Transformers, biểu diễn hơn 15 biến thể của mô hình Transformer dưới dạng giả mã, ngắn gọn, xúc tích và đầy đủ các bước. Thêm vào đó, tác giả còn giải thích Transformer là gì, ý tưởng kiến trúc mô hình từ đâu, cách tokenize dữ liệu thế nào, làm sao để train mô hình và những lời khuyên khi sử dụng mô hình Transformer. Paper: https://arxiv.org/pdf/2207.09238.pdf",,,,,
"Xin chào mọi người!
Em/Mình đang vướng vấn đề triển khai mà lâu nay mình nghiên cứu mà chưa có giải pháp hữu hiệu. Bài toán triển khai đặt ra là:
Nhiều IP cameras (truy cập thông qua rtsp) gửi dữ liệu về cho server. Server (chứa các mô hình AI cho bài toán phát hiện đối tượng) sẽ xử lý, vẽ bouding boxes và lưu trữ khung ảnh. Server streaming kết quả cho client (trình duyệt web hoặc là desktop GUI (java, c#)).
Em/Mình đã thử nghiệm:
Server đọc bằng opencv, có kết quả numpy và đưa vào mô hình AI, vẽ bouding boxes lên frame, chuyển thành dạng byte và truyền về cho browser dưới dạng HTTP response.
Vấn đề:
+ Opencv đọc các luồng rtsps bị delay khá nhiều.
+ Việc convert thành bytes và gửi về client qua giao thức HTTP cũng mất nhiều thời gian.
Em/mình nghĩ có thể cách tiếp cận của mình đang quá đơn giản. Mong nhận được sự hỗ trợ từ mọi người ạ.","Xin chào mọi người! Em/Mình đang vướng vấn đề triển khai mà lâu nay mình nghiên cứu mà chưa có giải pháp hữu hiệu. Bài toán triển khai đặt ra là: Nhiều IP cameras (truy cập thông qua rtsp) gửi dữ liệu về cho server. Server (chứa các mô hình AI cho bài toán phát hiện đối tượng) sẽ xử lý, vẽ bouding boxes và lưu trữ khung ảnh. Server streaming kết quả cho client (trình duyệt web hoặc là desktop GUI (java, c#)). Em/Mình đã thử nghiệm: Server đọc bằng opencv, có kết quả numpy và đưa vào mô hình AI, vẽ bouding boxes lên frame, chuyển thành dạng byte và truyền về cho browser dưới dạng HTTP response. Vấn đề: + Opencv đọc các luồng rtsps bị delay khá nhiều. + Việc convert thành bytes và gửi về client qua giao thức HTTP cũng mất nhiều thời gian. Em/mình nghĩ có thể cách tiếp cận của mình đang quá đơn giản. Mong nhận được sự hỗ trợ từ mọi người ạ.",,,,,
"Chào mọi người! Mình đang làm đồ án nghiên cứu phương pháp phát hiện ảnh giả mạo dựa trên học máy. Anh chị nào có tài liệu hay model liên quan cho mình xin với ạ.
Cám ơn mọi người nhiều ạ!",Chào mọi người! Mình đang làm đồ án nghiên cứu phương pháp phát hiện ảnh giả mạo dựa trên học máy. Anh chị nào có tài liệu hay model liên quan cho mình xin với ạ. Cám ơn mọi người nhiều ạ!,,,,,
"MLOps là một khái niệm khá hot trong thời gian gần đây và ngày càng được nhiều công ty ứng dụng để rút ngắn thời gian triển khai và nâng cao hiệu quả cũng như độ tin cậy của các hệ thống ML. 
Vậy MLOps là gì và có ích như thế nào đối với dự án ML? Hy vọng bài talk nhỏ của mình có thể giúp mọi người nhận ra phần nào tầm quan trọng của việc ứng dụng MLOps.
Video: https://www.youtube.com/watch?v=tBY5DZ5v0jQ
Slide: https://docs.google.com/presentation/d/1WEESpXdcL8qQDG4aAbdrmSYC-1BDNJGf0G9T0SkXFi4",MLOps là một khái niệm khá hot trong thời gian gần đây và ngày càng được nhiều công ty ứng dụng để rút ngắn thời gian triển khai và nâng cao hiệu quả cũng như độ tin cậy của các hệ thống ML. Vậy MLOps là gì và có ích như thế nào đối với dự án ML? Hy vọng bài talk nhỏ của mình có thể giúp mọi người nhận ra phần nào tầm quan trọng của việc ứng dụng MLOps. Video: https://www.youtube.com/watch?v=tBY5DZ5v0jQ Slide: https://docs.google.com/presentation/d/1WEESpXdcL8qQDG4aAbdrmSYC-1BDNJGf0G9T0SkXFi4,,,,,
"Xin chào mọi người, mình đang tìm hiểu về transfer learning trong bài toán phân loại hình ảnh.
Hôm nay, mình có tham khảo một mã nguồn trên mạng (như ảnh). Tại đây, tác giả không frozen pre-train model & fine-tune mà sử dụng trực tiếp pre-train model EfficientNet để phân loại hình ảnh.
Kết quả khi evaluation với các tham số: acc, f1 score,... cũng khá tốt
Mọi người cho mình hỏi cách tiếp cận của tác gỉa được gọi là gì? Cảm ơn ace.","Xin chào mọi người, mình đang tìm hiểu về transfer learning trong bài toán phân loại hình ảnh. Hôm nay, mình có tham khảo một mã nguồn trên mạng (như ảnh). Tại đây, tác giả không frozen pre-train model & fine-tune mà sử dụng trực tiếp pre-train model EfficientNet để phân loại hình ảnh. Kết quả khi evaluation với các tham số: acc, f1 score,... cũng khá tốt Mọi người cho mình hỏi cách tiếp cận của tác gỉa được gọi là gì? Cảm ơn ace.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 6/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 6/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"[Homework challenge lesson 10 - Probability distribution]
Phân phối xác suất là kiến thức nguyên lý và nền tảng của rất nhiều các ứng dụng trong lĩnh vực thống kê và machine learning. Dựa vào phân phối xác suất chúng ta có thể mô phỏng được các đặc trưng về phân bố dữ liệu của nhiều loại biến ngẫu nhiên liên tục hoặc rời rạc. Từ đó có thể tính toán, ước lượng và dự báo xác suất, khoảng tin cậy, kiểm định giả thuyết…. và phát triển các thuật toán trong thống kê và Machine Learning.
Trên thực tế, các phân phối mà chúng ta được học như phân phối Gaussian, T-student, Fisher, Chi-square, Uniform, Binary, Bernoulli, Poisson,… hoàn toàn không được tìm ra một cách ngẫu nhiên mà chúng đều dựa trên việc quan sát và thống kê các mẫu kích thước lớn và theo một cách tình cờ nào đó, những qui luật này được phát hiện ra và đúng trên rất nhiều dữ liệu. Có nhiều nhà khoa học cho rằng toán học không phải là do con người nghĩ ra mà đó là qui luật của vũ trụ, con người chỉ tình cờ phát hiện ra chúng. Phải chăng điều này là sự thật? Sau cùng là nỗ lực lớn lao của các nhà Toán học nhằm mô hình hóa những qui luật tự nhiên này thành một phương trình mẫu mực mà chúng ta thường gọi là hàm mật độ xác suất (probability density function đối với biến liên tục) hoặc hàm khối xác suất (probability mass function đối với biến rời rạc).
Chẳng hạn nhà toán học Gauss đã tìm ra phân phối Gaussian khi ông quan sát đồ thị phân phối của các biến ngẫu nhiên liên tục và ông nhận ra chúng đều có hình dạng giống nhau là một quả chuông đối xứng hai bên. Với một nhà toán học thiên tài về tính toán như Gauss thì việc tìm ra một phương trình pdf dựa trên trung bình và phương sai là điều mà mình nghĩ là không khó. Đừng quên rằng ông đã biết cách tính nhẩm tổng các số từ 1 → 100 ngay từ lớp 2, công thức mà ngày nay học sinh tiểu học được học thuộc lòng. Sau này khi đã là một nhà toán học, nhờ khả năng tính toán, ông đã giúp tìm lại một tiểu hành tinh bị thất lạc dựa vào các số liệu ít ỏi trước đó được ghi chép trước khi nó bị lẫn vào ánh sáng của các ngôi sao khác. Người Đức vô cùng tự hào về việc phát minh ra phân phối chuẩn của Gauss và họ đã vinh danh ông trên đồng tiền kèm theo một hình quả chuông chuẩn. Công thức phân phối chuẩn đã trở thành một trong 10 phương trình đẹp nhất của nhất loại.
Sự tài tình của Gauss đó là ông đã tìm ra đúng các tham số ảnh để mô phỏng hình dạng quả chuông là trung bình và phương sai. Điều mà phải rất thông minh và tinh tế mới phát hiện ra được. Sau đó thì việc tìm kiếm phương trình mô phỏng chỉ như là công việc nhiều niêm vui và mang tính giải trí đối với ông, một nhà toán học lỗi lạc với rất nhiều kỉ lục.
Thành công của phân phối Gaussian giúp chúng ta nhận ra rằng việc xác định họ tham số đặc trưng (parameter family) của từng dạng phân phối là rất quan trọng. Từ đó tìm ra ước lượng hợp lý nhất của chúng trên một bộ dữ liệu cụ thể thông qua việc tìm kiếm giá trị tham số mà bộ dữ liệu là hợp lý nhất. Mức độ hợp lý được đo lường bằng một hàm hợp lý (Likelihood Function) mà hàm này thể hiện xác suất của dữ liệu. Và như vậy quá trình ước lượng thường dẫn tới bài toán tối đa hóa hàm hợp lý (Maximum Likelihood Function). Đồng thời ước lượng tìm được của bài toán tối ưu này được gọi là ước lượng hợp lý tối đa (Maximum Likelihood Estimation).
Ước lượng hợp lý tối đa có rất nhiều ứng dụng trong thống kê. Đây là phương pháp cho phép ta tìm ra được các tham số tối ưu của một phân phối xác suất cụ thể. Từ đó dựa vào phân phối của chúng, chúng ta có thể thực hiện được nhiều việc mà một trong số chúng quan trọng nhất là: Ước lượng khoảng tin cậy; xây dựng và kiểm định các giả thuyết với một mức độ tin cậy cao (thường là 95%); Tính toán xác suất của biến khi chúng rơi vào một miền giá trị cụ thể.
Thông kê là một ngành học lâu đời mà ban đầu được phát triển dựa trên nền tảng của phân phối xác suất và toán học. Chính vì vậy trước khi học về thống kê thì bạn cần nắm vững về xác suất trước. Kết hợp hai lĩnh vực này lại là môn học kinh điển Xác suất Thống kê, một môn học đầy thú vị, không quá khó, nhưng cũng tiêu tốn khá nhiều tiền học nâng điểm của sinh viên. Bạn có thể không yêu thích xác suất và thống kê lắm, và bạn nghĩ nó không quan trọng. Nhưng các bạn có biết rằng có rất nhiều các qui luật phân phối trong thống kê đang tri phối đến cuộc sống của chúng ta hàng ngày mà mình tin rằng nắm bắt được chúng sẽ rất rất hữu ích. Bởi những qui luật này rất quan trọng nên có nhiều lĩnh vực, ngành nghề khác nhau như quản trị rủi ro, tài chính ngân hàng, bảo hiểm, kinh tế học, xã hội học, marketing,… đang ứng dụng thống kê để tìm ra một phương cách quản trị và kiểm soát tốt hơn cho các sự kiện không chắc chắn.
Cuối cùng, để củng cố kiến thức về phân phối xác suất sẽ là một đề kiểm tra nho nhỏ dành cho các bạn.","[Homework challenge lesson 10 - Probability distribution] Phân phối xác suất là kiến thức nguyên lý và nền tảng của rất nhiều các ứng dụng trong lĩnh vực thống kê và machine learning. Dựa vào phân phối xác suất chúng ta có thể mô phỏng được các đặc trưng về phân bố dữ liệu của nhiều loại biến ngẫu nhiên liên tục hoặc rời rạc. Từ đó có thể tính toán, ước lượng và dự báo xác suất, khoảng tin cậy, kiểm định giả thuyết…. và phát triển các thuật toán trong thống kê và Machine Learning. Trên thực tế, các phân phối mà chúng ta được học như phân phối Gaussian, T-student, Fisher, Chi-square, Uniform, Binary, Bernoulli, Poisson,… hoàn toàn không được tìm ra một cách ngẫu nhiên mà chúng đều dựa trên việc quan sát và thống kê các mẫu kích thước lớn và theo một cách tình cờ nào đó, những qui luật này được phát hiện ra và đúng trên rất nhiều dữ liệu. Có nhiều nhà khoa học cho rằng toán học không phải là do con người nghĩ ra mà đó là qui luật của vũ trụ, con người chỉ tình cờ phát hiện ra chúng. Phải chăng điều này là sự thật? Sau cùng là nỗ lực lớn lao của các nhà Toán học nhằm mô hình hóa những qui luật tự nhiên này thành một phương trình mẫu mực mà chúng ta thường gọi là hàm mật độ xác suất (probability density function đối với biến liên tục) hoặc hàm khối xác suất (probability mass function đối với biến rời rạc). Chẳng hạn nhà toán học Gauss đã tìm ra phân phối Gaussian khi ông quan sát đồ thị phân phối của các biến ngẫu nhiên liên tục và ông nhận ra chúng đều có hình dạng giống nhau là một quả chuông đối xứng hai bên. Với một nhà toán học thiên tài về tính toán như Gauss thì việc tìm ra một phương trình pdf dựa trên trung bình và phương sai là điều mà mình nghĩ là không khó. Đừng quên rằng ông đã biết cách tính nhẩm tổng các số từ 1 → 100 ngay từ lớp 2, công thức mà ngày nay học sinh tiểu học được học thuộc lòng. Sau này khi đã là một nhà toán học, nhờ khả năng tính toán, ông đã giúp tìm lại một tiểu hành tinh bị thất lạc dựa vào các số liệu ít ỏi trước đó được ghi chép trước khi nó bị lẫn vào ánh sáng của các ngôi sao khác. Người Đức vô cùng tự hào về việc phát minh ra phân phối chuẩn của Gauss và họ đã vinh danh ông trên đồng tiền kèm theo một hình quả chuông chuẩn. Công thức phân phối chuẩn đã trở thành một trong 10 phương trình đẹp nhất của nhất loại. Sự tài tình của Gauss đó là ông đã tìm ra đúng các tham số ảnh để mô phỏng hình dạng quả chuông là trung bình và phương sai. Điều mà phải rất thông minh và tinh tế mới phát hiện ra được. Sau đó thì việc tìm kiếm phương trình mô phỏng chỉ như là công việc nhiều niêm vui và mang tính giải trí đối với ông, một nhà toán học lỗi lạc với rất nhiều kỉ lục. Thành công của phân phối Gaussian giúp chúng ta nhận ra rằng việc xác định họ tham số đặc trưng (parameter family) của từng dạng phân phối là rất quan trọng. Từ đó tìm ra ước lượng hợp lý nhất của chúng trên một bộ dữ liệu cụ thể thông qua việc tìm kiếm giá trị tham số mà bộ dữ liệu là hợp lý nhất. Mức độ hợp lý được đo lường bằng một hàm hợp lý (Likelihood Function) mà hàm này thể hiện xác suất của dữ liệu. Và như vậy quá trình ước lượng thường dẫn tới bài toán tối đa hóa hàm hợp lý (Maximum Likelihood Function). Đồng thời ước lượng tìm được của bài toán tối ưu này được gọi là ước lượng hợp lý tối đa (Maximum Likelihood Estimation). Ước lượng hợp lý tối đa có rất nhiều ứng dụng trong thống kê. Đây là phương pháp cho phép ta tìm ra được các tham số tối ưu của một phân phối xác suất cụ thể. Từ đó dựa vào phân phối của chúng, chúng ta có thể thực hiện được nhiều việc mà một trong số chúng quan trọng nhất là: Ước lượng khoảng tin cậy; xây dựng và kiểm định các giả thuyết với một mức độ tin cậy cao (thường là 95%); Tính toán xác suất của biến khi chúng rơi vào một miền giá trị cụ thể. Thông kê là một ngành học lâu đời mà ban đầu được phát triển dựa trên nền tảng của phân phối xác suất và toán học. Chính vì vậy trước khi học về thống kê thì bạn cần nắm vững về xác suất trước. Kết hợp hai lĩnh vực này lại là môn học kinh điển Xác suất Thống kê, một môn học đầy thú vị, không quá khó, nhưng cũng tiêu tốn khá nhiều tiền học nâng điểm của sinh viên. Bạn có thể không yêu thích xác suất và thống kê lắm, và bạn nghĩ nó không quan trọng. Nhưng các bạn có biết rằng có rất nhiều các qui luật phân phối trong thống kê đang tri phối đến cuộc sống của chúng ta hàng ngày mà mình tin rằng nắm bắt được chúng sẽ rất rất hữu ích. Bởi những qui luật này rất quan trọng nên có nhiều lĩnh vực, ngành nghề khác nhau như quản trị rủi ro, tài chính ngân hàng, bảo hiểm, kinh tế học, xã hội học, marketing,… đang ứng dụng thống kê để tìm ra một phương cách quản trị và kiểm soát tốt hơn cho các sự kiện không chắc chắn. Cuối cùng, để củng cố kiến thức về phân phối xác suất sẽ là một đề kiểm tra nho nhỏ dành cho các bạn.",,"#sharing, #math",,,
"[labmlai- Deep Learning Paper Implementations - 10,9k stars]
labmlai- Deep Learning Paper Implementations là tập hợp hướng dẫn triển khai các paper deep learning bằng Pytorch. Các triển khai này được viết với các hướng dẫn và code song song với nhau (side-by-side notes 📝), bên trái là hướng dẫn, giải thích và bên phải là code giúp người đọc có thể dễ hiểu nhất.
Các triển khai này bao gồm các topic quen thuộc như :
- transformers (original, xl, switch, feedback, vit, ...)
- optimizers (adam, adabelief, ...)
- gans(cyclegan, stylegan2, ...)
- reinforcement learning (ppo, dqn),
- capsnet, distillation
Mọi người xem chi tiết ở đây:
Web: https://nn.labml.ai/
Github: https://github.com/labmlai/annotated_deep_learning_paper_implementations","[labmlai- Deep Learning Paper Implementations - 10,9k stars] labmlai- Deep Learning Paper Implementations là tập hợp hướng dẫn triển khai các paper deep learning bằng Pytorch. Các triển khai này được viết với các hướng dẫn và code song song với nhau (side-by-side notes ), bên trái là hướng dẫn, giải thích và bên phải là code giúp người đọc có thể dễ hiểu nhất. Các triển khai này bao gồm các topic quen thuộc như : - transformers (original, xl, switch, feedback, vit, ...) - optimizers (adam, adabelief, ...) - gans(cyclegan, stylegan2, ...) - reinforcement learning (ppo, dqn), - capsnet, distillation Mọi người xem chi tiết ở đây: Web: https://nn.labml.ai/ Github: https://github.com/labmlai/annotated_deep_learning_paper_implementations",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 3/2022 vào trong comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 3/2022 vào trong comment của post này.",,,,,
"VietAI công bố mô hình ngôn ngữ dùng riêng cho tiếng Việt:
📌GPT-3 1.3 tỷ tham số (có thể trải nghiệm trực tiếp trên Hugging Face Hub giống như trong video): https://huggingface.co/VietAI/gpt-neo-1.3B-vietnamese-news.
📌GPT-3 6 tỷ tham số (cần download về một máy tính có đủ tài nguyên để thực hiện inference - tối thiểu 12GB RAM): https://huggingface.co/VietAI/gpt-j-6B-vietnamese-news.",VietAI công bố mô hình ngôn ngữ dùng riêng cho tiếng Việt: GPT-3 1.3 tỷ tham số (có thể trải nghiệm trực tiếp trên Hugging Face Hub giống như trong video): https://huggingface.co/VietAI/gpt-neo-1.3B-vietnamese-news. GPT-3 6 tỷ tham số (cần download về một máy tính có đủ tài nguyên để thực hiện inference - tối thiểu 12GB RAM): https://huggingface.co/VietAI/gpt-j-6B-vietnamese-news.,,,,,
"Em chào mọi người. Em có một bài tập nhỏ về linear programming và maximizing value by Excel. Cụ thể là về một doanh nghiệp đang muốn phân tích 2 loại máy để sản xuất và đạt tối đa doanh thu. Loại máy X và Y có các data khác nhau như: Amount/hour là một tiếng cơ sở đó có thể sản xuất được bao nhiêu máy. Profit/unit là mỗi máy bán ra thu dc bao nhiêu lợi nhuận. Capacity/week là mỗi tuần, nhà máy có thể sản xuất tối đa bao nhiêu máy. Và Hour/week là mỗi tuần nhà máy có thể vận hành trong bao nhiêu giờ, cơ bản là chỉ có 40 giờ mỗi tuần. Mong mọi người giúp em tìm hướng giải quyết vấn đề ạ (objective function and constraints) để thỏa mãn các câu hỏi ở ô bên phải. Em xin cảm ơn ạ!","Em chào mọi người. Em có một bài tập nhỏ về linear programming và maximizing value by Excel. Cụ thể là về một doanh nghiệp đang muốn phân tích 2 loại máy để sản xuất và đạt tối đa doanh thu. Loại máy X và Y có các data khác nhau như: Amount/hour là một tiếng cơ sở đó có thể sản xuất được bao nhiêu máy. Profit/unit là mỗi máy bán ra thu dc bao nhiêu lợi nhuận. Capacity/week là mỗi tuần, nhà máy có thể sản xuất tối đa bao nhiêu máy. Và Hour/week là mỗi tuần nhà máy có thể vận hành trong bao nhiêu giờ, cơ bản là chỉ có 40 giờ mỗi tuần. Mong mọi người giúp em tìm hướng giải quyết vấn đề ạ (objective function and constraints) để thỏa mãn các câu hỏi ở ô bên phải. Em xin cảm ơn ạ!",,,,,
"Chào mọi người,
Mình tên là Việt. Mình là AI engineer đang làm việc tại Berlin. Công ty của mình đang cần tuyển thêm 2-3 AI engineer. Nếu bạn nào có kinh nghiệm làm việc trong lĩnh vực trí tuệ nhân tạo/thị giác máy tính, thì liên lạc với mình nhé. Ngoài ra công ty mình cũng sponsor visa, nên nếu các bạn có bạn bè ở VN quan tâm cũng như phù hợp với vị trí, thì làm ơn giới thiệu dùm mình. Mình để link job description ở dưới. Cảm ơn mọi người nhiều","Chào mọi người, Mình tên là Việt. Mình là AI engineer đang làm việc tại Berlin. Công ty của mình đang cần tuyển thêm 2-3 AI engineer. Nếu bạn nào có kinh nghiệm làm việc trong lĩnh vực trí tuệ nhân tạo/thị giác máy tính, thì liên lạc với mình nhé. Ngoài ra công ty mình cũng sponsor visa, nên nếu các bạn có bạn bè ở VN quan tâm cũng như phù hợp với vị trí, thì làm ơn giới thiệu dùm mình. Mình để link job description ở dưới. Cảm ơn mọi người nhiều",,,,,
"Mình đang muốn ôn lại về Statistics. Các bạn Data Scientist có thể recommend mình 1-2 cuốn dạng textbook và có bản free pdf không?
Cảm ơn các bạn.
--- Update ---
Mình thấy cuốn này được giới thiệu ở dưới khá ok
https://www.kdnuggets.com/2020/04/statistics-concise-course-statistical-inference-free-ebook.html?fbclid=IwAR0xE3tjSZYHSC2Rw7lpXc0gST5rbPW3tGVPnUony1eAD6le7btf90NBPXg.",Mình đang muốn ôn lại về Statistics. Các bạn Data Scientist có thể recommend mình 1-2 cuốn dạng textbook và có bản free pdf không? Cảm ơn các bạn. --- Update --- Mình thấy cuốn này được giới thiệu ở dưới khá ok https://www.kdnuggets.com/2020/04/statistics-concise-course-statistical-inference-free-ebook.html?fbclid=IwAR0xE3tjSZYHSC2Rw7lpXc0gST5rbPW3tGVPnUony1eAD6le7btf90NBPXg.,,,,,
"Kính chào các bác. Em đang ngâm cứu món YOLOv6 nên mạnh dạn ra tiếp video Phần 2 về Deploy thành API, WEB cho các bạn mới học tham khảo.
Hi vọng giúp được các bạn ạ!","Kính chào các bác. Em đang ngâm cứu món YOLOv6 nên mạnh dạn ra tiếp video Phần 2 về Deploy thành API, WEB cho các bạn mới học tham khảo. Hi vọng giúp được các bạn ạ!",,,,,
"""Học machine learning""
chào mn,
e hiện đang là du học sinh đức ngành electrical engineering muốn học về machine learning cũng như data science. Do mới học nên có rất nhiều thắc mắc. Không biết a/c/b nào có nhu cầu học tiếng đức hoặc tiếng anh (ví dụ: để sang đức học hoặc làm việc) ko? E có bằng C1 English vs German nên có thể dạy a/c/b, đổi lại a/c/b có thể chỉ dạy e một chút về ML!
a/c/b nào quan tâm có thể comment or inbox trực tiếp e!","""Học machine learning"" chào mn, e hiện đang là du học sinh đức ngành electrical engineering muốn học về machine learning cũng như data science. Do mới học nên có rất nhiều thắc mắc. Không biết a/c/b nào có nhu cầu học tiếng đức hoặc tiếng anh (ví dụ: để sang đức học hoặc làm việc) ko? E có bằng C1 English vs German nên có thể dạy a/c/b, đổi lại a/c/b có thể chỉ dạy e một chút về ML! a/c/b nào quan tâm có thể comment or inbox trực tiếp e!",,,,,
"Em chào mn. Em đang muốn hiểu rõ sự khác biệt của dynamic programming(DP), monte carlo(MC) và temporal difference(TD). Theo như trên course của DeepMind thì ngoài sự khác biệt về cách update value function dựa trên estimation hoặc real value(boostrapping), thì còn khác nhau về cách sampling. Xét sampling experience thì cả MC và TD đều sample environment, còn DP thì không sample nhưng sẽ xét tất cả các possibilities sau đó và update lại value function. Nhưng DP cũng dùng max operator để lấy action value cao nhất. Như vậy khác gì MC và TD ạ. Hay cách sample của MC và TD là hoàn toàn ngẫu nhiên?","Em chào mn. Em đang muốn hiểu rõ sự khác biệt của dynamic programming(DP), monte carlo(MC) và temporal difference(TD). Theo như trên course của DeepMind thì ngoài sự khác biệt về cách update value function dựa trên estimation hoặc real value(boostrapping), thì còn khác nhau về cách sampling. Xét sampling experience thì cả MC và TD đều sample environment, còn DP thì không sample nhưng sẽ xét tất cả các possibilities sau đó và update lại value function. Nhưng DP cũng dùng max operator để lấy action value cao nhất. Như vậy khác gì MC và TD ạ. Hay cách sample của MC và TD là hoàn toàn ngẫu nhiên?",,,,,
"#hoidap
Xin chào mọi người,
Các bạn làm ơn cho mình hỏi, ở Việt Nam có các công ty nào làm về Data Annotation cho Computer Vision vậy ạ? Mình xin cám ơn mọi người nhiều.","Xin chào mọi người, Các bạn làm ơn cho mình hỏi, ở Việt Nam có các công ty nào làm về Data Annotation cho Computer Vision vậy ạ? Mình xin cám ơn mọi người nhiều.",#hoidap,,,,
"[Deep Learning free courses youtube]
Ngày nay các tài liệu về Deep Learning là vô cùng dồi dào và việc tiếp cận chúng cũng trở nên dễ dàng hơn nhờ các kênh youtube và chính sách cởi mở của các trường đại học, viện nghiên cứu. Thế nhưng không phải nguồn tài liệu nào về Deep Learning cũng chuẩn hóa và chất lượng. Đây là danh sách gần 500 khóa học đã được tuyển lựa về Deep Learning được tổ chức bởi các giáo sư tại những trường đại học hàng đầu thế giới về lĩnh vực AI kèm theo link youtube, năm phát hành và phân loại topics của mỗi khóa học. Những khóa học này hoàn toàn free và cover được rất nhiều chủ đề lớn trong Deep Learning mà bạn đọc có thể tìm thấy như:
• Deep Learning (Deep Neural Networks)
• Machine Learning Fundamentals
• Optimization for Machine Learning
• General Machine Learning
• Reinforcement Learning
• Bayesian Deep Learning
• Graph Neural Networks
• Probabilistic Graphical Models
• Natural Language Processing
• Automatic Speech Recognition
• Modern Computer Vision
• Boot Camps or Summer Schools
• Medical Imaging
• Bird's-eye view of Artificial Intelligence
Deep Learning thực sự rất rộng lớn để chúng ta có thể bao quát hết được, và để chinh phục chúng thì bạn có thể tuân theo lời khuyên của Prof. Geoffrey Hinton, University of Toronto:
""Read enough so you start developing intuitions and then trust your intuitions and go for it!""
Link website:
https://deep-learning-drizzle.github.io/index.html#contents","[Deep Learning free courses youtube] Ngày nay các tài liệu về Deep Learning là vô cùng dồi dào và việc tiếp cận chúng cũng trở nên dễ dàng hơn nhờ các kênh youtube và chính sách cởi mở của các trường đại học, viện nghiên cứu. Thế nhưng không phải nguồn tài liệu nào về Deep Learning cũng chuẩn hóa và chất lượng. Đây là danh sách gần 500 khóa học đã được tuyển lựa về Deep Learning được tổ chức bởi các giáo sư tại những trường đại học hàng đầu thế giới về lĩnh vực AI kèm theo link youtube, năm phát hành và phân loại topics của mỗi khóa học. Những khóa học này hoàn toàn free và cover được rất nhiều chủ đề lớn trong Deep Learning mà bạn đọc có thể tìm thấy như: • Deep Learning (Deep Neural Networks) • Machine Learning Fundamentals • Optimization for Machine Learning • General Machine Learning • Reinforcement Learning • Bayesian Deep Learning • Graph Neural Networks • Probabilistic Graphical Models • Natural Language Processing • Automatic Speech Recognition • Modern Computer Vision • Boot Camps or Summer Schools • Medical Imaging • Bird's-eye view of Artificial Intelligence Deep Learning thực sự rất rộng lớn để chúng ta có thể bao quát hết được, và để chinh phục chúng thì bạn có thể tuân theo lời khuyên của Prof. Geoffrey Hinton, University of Toronto: ""Read enough so you start developing intuitions and then trust your intuitions and go for it!"" Link website: https://deep-learning-drizzle.github.io/index.html#contents",,,,,
"[Ideas Worth Sharing]
Tàu ngầm của group đã lâu, nay mình chia sẻ những điều mình note lại từ 1 podcast của Lex Fridman - podcast chuyên phỏng vấn những chuyên gia hàng đầu thế giới, chắc ai học AI cũng ít nhiều biết đến podcast này, nghĩ lại nếu ko share giữ cho mình thì hơi phí =)).
Đoạn note mình chia sẻ là cuộc nói chuyện với Jeremy Howard, đồng sáng lập của course fast.ai nổi tiếng. Nói chung mình học được khá nhiều đặc biệt là cái nhìn thực tế và cách giải quyết vấn đề dùng AI của ổng.
Link podcast: https://www.youtube.com/watch?v=J6XcP4JOHmk&t=30s
--------------------------------------------------------------------------
02:49 - Programming coupled with Music may generate creative ideas
41:51 - Things that make high practical impact are transfer learning, active learning (get more of the human beings in the loop)
43:36 - ULMFiT algorithm introduced transfer learning to NLP and smashed the state-of-the-art in one of the most important dataset in the field he knows nothing about 
48:47 - Figure out how to accomplish faster with just 1 GPU that a normal person could afford
51:13 - Train Imagenet dataset with smaller image size (64), then later use fewer epoch to train with original size (224)
53:00 - Imagenette dataset, a small subset of Imagenet. The result training in this dataset in 10 minutes in a single GPU can be transferable to Imagenet nearly all the time. Link: https://github.com/fastai/imagenette
54:00 - Deep learning should be accessible for normal people and trained on a single GPU
55:48 - DeOldify, a technique to colorize white-and-black movies and can colorize the whole movie in a couple of hours using a single GPU
59:00 - Practicalities of training neural network quickly and accurately
1:01:01 - Super-Convergence: Fast Training of NN with Very Large Learning Rate paper
1:02:22 - No idea how optimizers really work (combination of epsilon, weight decay, learning rate,…)
1:04:58 - Looking at the data (particular from the lens of which part of the data the model says is important) is the key part before training the model => then finetune the model => learn to analyze the results of the model (looking at examples of misclassified images, classification matrix,…) => after that you can learn about important features, which groups are misclassified (like using the model to debug the data and to learn more about the data, becoming a domain expert more quickly)
1:07:49 - GCP is the best to start
1:19:18 - Strong coders and know nothing about statistics pickup fast.ai course the best
1:20:59 - The key difference between people who succeed and people who fail is tenacity (persistence)
1:21:40:
Advice for starting with deep learning:
 • Train a lot of models
 • Get a lot of experiments (like change the input a little and then look at how the output varies) to get an intuitive understanding of what is going on
 To become an expert:
 • We need experts at using deep learning for a specific domain area (passion area)
 • Interesting research is to try to solve an actual problem and solve it really well
 • Understanding sufficient tools for the deep learning side and becoming an expert at a particular domain (like if you're studying self-driving car then really experiencing driving the car to see what happen to you and improve upon it)

To create a successful startup:
 • Never give up and stick to it (key things)
 • Keep costs super low and try to save up some money beforehand
 • 1:28:14: (Do not understand)
 • Stay from venture capital (VC) money as long as possible preferably forever. Run a self-funded startup instead.

1:36:50: When reading papers, identify important concepts and understand them deeply and actually digest them and decided if it's worth incorporated into his library (implementing) or how he do things or it's not worth it
1:37:23: Committed to spending at least half of everyday learning or practicing something new => do everything faster 
1:38:30: How he designs every Chinese flashcards
1:40:22: Future of AI is to use existing tools to solve a lot of societally currently unsolved problems (like labor force displacement
--------------------------------------------------------------------------
Vì mình note khi đang xem video nên có thể nhiều phần khó hiểu với cả quá vắn tắt nên mọi người góp ý thêm nhé. Khuyến khích mọi người xem cả video 😊","[Ideas Worth Sharing] Tàu ngầm của group đã lâu, nay mình chia sẻ những điều mình note lại từ 1 podcast của Lex Fridman - podcast chuyên phỏng vấn những chuyên gia hàng đầu thế giới, chắc ai học AI cũng ít nhiều biết đến podcast này, nghĩ lại nếu ko share giữ cho mình thì hơi phí =)). Đoạn note mình chia sẻ là cuộc nói chuyện với Jeremy Howard, đồng sáng lập của course fast.ai nổi tiếng. Nói chung mình học được khá nhiều đặc biệt là cái nhìn thực tế và cách giải quyết vấn đề dùng AI của ổng. Link podcast: https://www.youtube.com/watch?v=J6XcP4JOHmk&t=30s -------------------------------------------------------------------------- 02:49 - Programming coupled with Music may generate creative ideas 41:51 - Things that make high practical impact are transfer learning, active learning (get more of the human beings in the loop) 43:36 - ULMFiT algorithm introduced transfer learning to NLP and smashed the state-of-the-art in one of the most important dataset in the field he knows nothing about 48:47 - Figure out how to accomplish faster with just 1 GPU that a normal person could afford 51:13 - Train Imagenet dataset with smaller image size (64), then later use fewer epoch to train with original size (224) 53:00 - Imagenette dataset, a small subset of Imagenet. The result training in this dataset in 10 minutes in a single GPU can be transferable to Imagenet nearly all the time. Link: https://github.com/fastai/imagenette 54:00 - Deep learning should be accessible for normal people and trained on a single GPU 55:48 - DeOldify, a technique to colorize white-and-black movies and can colorize the whole movie in a couple of hours using a single GPU 59:00 - Practicalities of training neural network quickly and accurately 1:01:01 - Super-Convergence: Fast Training of NN with Very Large Learning Rate paper 1:02:22 - No idea how optimizers really work (combination of epsilon, weight decay, learning rate,…) 1:04:58 - Looking at the data (particular from the lens of which part of the data the model says is important) is the key part before training the model => then finetune the model => learn to analyze the results of the model (looking at examples of misclassified images, classification matrix,…) => after that you can learn about important features, which groups are misclassified (like using the model to debug the data and to learn more about the data, becoming a domain expert more quickly) 1:07:49 - GCP is the best to start 1:19:18 - Strong coders and know nothing about statistics pickup fast.ai course the best 1:20:59 - The key difference between people who succeed and people who fail is tenacity (persistence) 1:21:40: Advice for starting with deep learning: • Train a lot of models • Get a lot of experiments (like change the input a little and then look at how the output varies) to get an intuitive understanding of what is going on To become an expert: • We need experts at using deep learning for a specific domain area (passion area) • Interesting research is to try to solve an actual problem and solve it really well • Understanding sufficient tools for the deep learning side and becoming an expert at a particular domain (like if you're studying self-driving car then really experiencing driving the car to see what happen to you and improve upon it) To create a successful startup: • Never give up and stick to it (key things) • Keep costs super low and try to save up some money beforehand • 1:28:14: (Do not understand) • Stay from venture capital (VC) money as long as possible preferably forever. Run a self-funded startup instead. 1:36:50: When reading papers, identify important concepts and understand them deeply and actually digest them and decided if it's worth incorporated into his library (implementing) or how he do things or it's not worth it 1:37:23: Committed to spending at least half of everyday learning or practicing something new => do everything faster 1:38:30: How he designs every Chinese flashcards 1:40:22: Future of AI is to use existing tools to solve a lot of societally currently unsolved problems (like labor force displacement -------------------------------------------------------------------------- Vì mình note khi đang xem video nên có thể nhiều phần khó hiểu với cả quá vắn tắt nên mọi người góp ý thêm nhé. Khuyến khích mọi người xem cả video",,,,,
"Em chào mn. Em có 1 bài toán tối ưu có ràng buộc như này. Em ko biết search từ khóa như nào để tìm ra algorithm phù hợp mong mn chỉ giúp em ạ. Em có 100 phường có lat long, 10 kho có lat long đang phục vụ ship hàng cho 100 phường kia( data có mỗi row là phường nào thuộc kho nào, lượng đơn của phường đó là bn).
Vấn đề là hiện tại có nhiều kho lại phục vụ phường rất xa (mà phường đó gần kho khác hơn) đại loại vị trí ko tối ưu. Hàm mục tiêu của em là tối ưu minimum hàm:
f()= xích ma ( số đơn của phường i * khoảng cách đến kho mới) ( chưa biết kho mới là kho nào nhé ạ)
trong đó khoảng cách thì tính theo lat long của phường và kho.
Em search từ khóa thì ra nó là Constraint optimization nhưng thấy nhiều thuật toán quá nên hơi rối, nếu đọc từng cái khá lâu. À RÀNG BUỘC của em là 10 kho kia sau khi rearrange lại các phường phục vụ thì tổng sản lượng của kho sau rearrange là <= 110% so với sản lượng đang phục vụ hiện tại do diện tích kho có hạn ( sản lượng của kho = tổng sản lượng của các phường nó phục vụ)","Em chào mn. Em có 1 bài toán tối ưu có ràng buộc như này. Em ko biết search từ khóa như nào để tìm ra algorithm phù hợp mong mn chỉ giúp em ạ. Em có 100 phường có lat long, 10 kho có lat long đang phục vụ ship hàng cho 100 phường kia( data có mỗi row là phường nào thuộc kho nào, lượng đơn của phường đó là bn). Vấn đề là hiện tại có nhiều kho lại phục vụ phường rất xa (mà phường đó gần kho khác hơn) đại loại vị trí ko tối ưu. Hàm mục tiêu của em là tối ưu minimum hàm: f()= xích ma ( số đơn của phường i * khoảng cách đến kho mới) ( chưa biết kho mới là kho nào nhé ạ) trong đó khoảng cách thì tính theo lat long của phường và kho. Em search từ khóa thì ra nó là Constraint optimization nhưng thấy nhiều thuật toán quá nên hơi rối, nếu đọc từng cái khá lâu. À RÀNG BUỘC của em là 10 kho kia sau khi rearrange lại các phường phục vụ thì tổng sản lượng của kho sau rearrange là <= 110% so với sản lượng đang phục vụ hiện tại do diện tích kho có hạn ( sản lượng của kho = tổng sản lượng của các phường nó phục vụ)",,"#Q&A,#math",,,
"Kính chào các bác. Nhân dịp đang tìm hiểu về YOLOv6 về cách train, cách test và deploy nên em mạnh dạn làm clip chia sẻ cho các bạn mới học.
Hi vọng giúp được các anh em!","Kính chào các bác. Nhân dịp đang tìm hiểu về YOLOv6 về cách train, cách test và deploy nên em mạnh dạn làm clip chia sẻ cho các bạn mới học. Hi vọng giúp được các anh em!",,,,,
"Xin chào các bạn,
Mình có một câu hỏi nhanh mong nhận được sự trợ giúp của các bạn:
Mình có một dict.key() gồm khoảng 500 sheet_ name trong một file excel, (mình copy vài cái như sau):
46 P09-4 (Position Terrestr…
47 P10_D09 (Position Terres…
48 P10_D10 (Position Terres…
49 P10_D11 (Position Terres…
giờ mình muốn dùng regular expression để cắt hết phần ""(Position Terrestr…"" rồi giữ lại phần đầu của tên cho tiện truy cập.
Mình đã dùng lệnh re.findall để lọc nhưng mình rất mong bạn nào có thể chỉ giúp mình cách dùng lệnh re.sub để cắt cho nhanh.
Xin cảm ơn mọi người và rất mong nhận được sự trợ giúp từ các bạn.","Xin chào các bạn, Mình có một câu hỏi nhanh mong nhận được sự trợ giúp của các bạn: Mình có một dict.key() gồm khoảng 500 sheet_ name trong một file excel, (mình copy vài cái như sau): 46 P09-4 (Position Terrestr… 47 P10_D09 (Position Terres… 48 P10_D10 (Position Terres… 49 P10_D11 (Position Terres… giờ mình muốn dùng regular expression để cắt hết phần ""(Position Terrestr…"" rồi giữ lại phần đầu của tên cho tiện truy cập. Mình đã dùng lệnh re.findall để lọc nhưng mình rất mong bạn nào có thể chỉ giúp mình cách dùng lệnh re.sub để cắt cho nhanh. Xin cảm ơn mọi người và rất mong nhận được sự trợ giúp từ các bạn.",,,,,
"Chào các bạn, mình có một vấn đề nhờ anh em hỗ trợ là các thuật toán nhận dạng bất thường (ngoại lai) của dữ liệu đơn biến như IQR (Interquartile Range), Median Absolute Deviation, phương pháp Generalized ESD (GESD), K-Nearest Neighbours, LOF (Local Outlier Factor). Histogram-based Outlier Score (HBOS), iForest. thường thì sẽ đưa dữ liệu vào đánh giá mô hình phù hợp. Vấn đề mình cần tìm hiểu là bộ dữ liệu cấu trúc như thế nào để mỗi thuật toán đạt được độ chinh xác cao nhất.
Bạn nào có paper hay tài liệu nào chỉ mình với.
Xin cảm ơn.","Chào các bạn, mình có một vấn đề nhờ anh em hỗ trợ là các thuật toán nhận dạng bất thường (ngoại lai) của dữ liệu đơn biến như IQR (Interquartile Range), Median Absolute Deviation, phương pháp Generalized ESD (GESD), K-Nearest Neighbours, LOF (Local Outlier Factor). Histogram-based Outlier Score (HBOS), iForest. thường thì sẽ đưa dữ liệu vào đánh giá mô hình phù hợp. Vấn đề mình cần tìm hiểu là bộ dữ liệu cấu trúc như thế nào để mỗi thuật toán đạt được độ chinh xác cao nhất. Bạn nào có paper hay tài liệu nào chỉ mình với. Xin cảm ơn.",,,,,
"For some technical reason, I don’t see any pending post from the OP. Helping him post this:
-------
Hi all, VinAI is pleased to publicly release the pre-trained text translation models ""vinai/vinai-translate-vi2en"" and ""vinai/vinai-translate-en2vi"" that are currently used in the translation component of the VinAI Translate system (https://vinai-translate.vinai.io). The pre-trained models are state-of-the-art text translation models for Vietnamese-to-English and English-to-Vietnamese, which can be used with the popular library ""transformers"".
Please find details about the pre-trained models at: https://github.com/VinAIResearch/VinAI_Translate
Experimental results of the pre-trained models can be found in the VinAI Translate system paper ""A Vietnamese-English Neural Machine Translation System"" (https://openreview.net/forum?id=CRg-RaxKnai), which will be presented at the Interspeech 2022 Show & Tell session.
Other Vietnamese NLP resources from VinAI:
- https://github.com/VinAIResearch/BARTpho (INTERSPEECH 2022): Pre-trained sequence-to-sequence models for Vietnamese.
- https://github.com/VinAIResearch/PhoMT (EMNLP 2021): A high-quality and large-scale benchmark dataset for Vietnamese-English machine translation.
- https://github.com/VinAIResearch/JointIDSF/tree/main/PhoATIS (INTERSPEECH 2021): An intent detection and slot filling dataset for Vietnamese.
- https://github.com/VinAIResearch/PhoNLP (NAACL 2021): A BERT-based multi-task learning toolkit for Vietnamese POS tagging, named entity recognition and dependency parsing.
- https://github.com/VinAIResearch/PhoNER_COVID19 (NAACL 2021): A dataset for Vietnamese named entity recognition.
- https://github.com/VinAIResearch/ViText2SQL (EMNLP 2020 Findings): A dataset for Vietnamese Text2SQL semantic parsing.
- https://github.com/VinAIResearch/PhoBERT (EMNLP 2020 Findings): Pre-trained language models for Vietnamese.
- https://github.com/datquocnguyen/PhoW2V (2020): Pre-trained Word2Vec syllable- and word-level embeddings for Vietnamese.
 — với Dat Quoc Nguyen.","For some technical reason, I don’t see any pending post from the OP. Helping him post this: ------- Hi all, VinAI is pleased to publicly release the pre-trained text translation models ""vinai/vinai-translate-vi2en"" and ""vinai/vinai-translate-en2vi"" that are currently used in the translation component of the VinAI Translate system (https://vinai-translate.vinai.io). The pre-trained models are state-of-the-art text translation models for Vietnamese-to-English and English-to-Vietnamese, which can be used with the popular library ""transformers"". Please find details about the pre-trained models at: https://github.com/VinAIResearch/VinAI_Translate Experimental results of the pre-trained models can be found in the VinAI Translate system paper ""A Vietnamese-English Neural Machine Translation System"" (https://openreview.net/forum?id=CRg-RaxKnai), which will be presented at the Interspeech 2022 Show & Tell session. Other Vietnamese NLP resources from VinAI: - https://github.com/VinAIResearch/BARTpho (INTERSPEECH 2022): Pre-trained sequence-to-sequence models for Vietnamese. - https://github.com/VinAIResearch/PhoMT (EMNLP 2021): A high-quality and large-scale benchmark dataset for Vietnamese-English machine translation. - https://github.com/VinAIResearch/JointIDSF/tree/main/PhoATIS (INTERSPEECH 2021): An intent detection and slot filling dataset for Vietnamese. - https://github.com/VinAIResearch/PhoNLP (NAACL 2021): A BERT-based multi-task learning toolkit for Vietnamese POS tagging, named entity recognition and dependency parsing. - https://github.com/VinAIResearch/PhoNER_COVID19 (NAACL 2021): A dataset for Vietnamese named entity recognition. - https://github.com/VinAIResearch/ViText2SQL (EMNLP 2020 Findings): A dataset for Vietnamese Text2SQL semantic parsing. - https://github.com/VinAIResearch/PhoBERT (EMNLP 2020 Findings): Pre-trained language models for Vietnamese. - https://github.com/datquocnguyen/PhoW2V (2020): Pre-trained Word2Vec syllable- and word-level embeddings for Vietnamese. — với Dat Quoc Nguyen.",,,,,
"Do có nhu cầu pre-train BART nhưng chưa tìm được open source implementation phù hợp nên mình đã thử viết. Các bạn có nhu cầu tương tự có thể tham khảo.
https://github.com/duongna21/bartflax
[More info] Implementation này đã được huggingface/transformers approved làm official example cho BART pre-training, sẽ merged into master branch trong vài ngày tới. PR discussion: https://github.com/huggingface/transformers/pull/18297.","Do có nhu cầu pre-train BART nhưng chưa tìm được open source implementation phù hợp nên mình đã thử viết. Các bạn có nhu cầu tương tự có thể tham khảo. https://github.com/duongna21/bartflax [More info] Implementation này đã được huggingface/transformers approved làm official example cho BART pre-training, sẽ merged into master branch trong vài ngày tới. PR discussion: https://github.com/huggingface/transformers/pull/18297.",,,,,
"Hi,
Mình đang làm senior program committee (meta-reviewer) cho AAAI-23 và có thể nominate reviewers. Bạn nào muốn có kinh nghiệm làm reviewer cho AAAI-23 thì có thể inbox mình để mình nominate (Yêu cầu: Đã có publications vì cần DBLP profile mới nominate được)
Best,
Thanh","Hi, Mình đang làm senior program committee (meta-reviewer) cho AAAI-23 và có thể nominate reviewers. Bạn nào muốn có kinh nghiệm làm reviewer cho AAAI-23 thì có thể inbox mình để mình nominate (Yêu cầu: Đã có publications vì cần DBLP profile mới nominate được) Best, Thanh",,,,,
"[CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation]
Gần đây, các ứng dụng của Transformer trong các bài toán Computer Vision đang ngày trở nên phổ biến. Không chỉ dừng lại ở Image Classification, Object Detection. Trong những năm gần đây, kiến trúc này còn được áp dụng rất thành công trong bài toán Image Segmentation nhờ khả năng capture được global context và clustering behavior của các tokens học được trên các chuỗi sequence được mã hóa từ hình ảnh đầu vào thông qua kiến trúc Transformer Encoder-Decoder mà thuật toán đã chứng minh được tính hiệu quả và giúp cải thiện accuracy so với các thuật toán sử dụng thuần CNN trước đó.
Trong tuần này chúng ta cùng nhau phân tích kiến trúc CMT-DeepLab đã kế thừa lại DETR nhằm giải quyết bài toán Panoptic Segmentation thành công như thế nào? Nghiên cứu được thực hiện bởi nhóm tác giả trẻ: Quihang Yu, Huiyu Wang và các cộng sự đến từ đại học John Hopskins, KAIST và Google Research","[CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation] Gần đây, các ứng dụng của Transformer trong các bài toán Computer Vision đang ngày trở nên phổ biến. Không chỉ dừng lại ở Image Classification, Object Detection. Trong những năm gần đây, kiến trúc này còn được áp dụng rất thành công trong bài toán Image Segmentation nhờ khả năng capture được global context và clustering behavior của các tokens học được trên các chuỗi sequence được mã hóa từ hình ảnh đầu vào thông qua kiến trúc Transformer Encoder-Decoder mà thuật toán đã chứng minh được tính hiệu quả và giúp cải thiện accuracy so với các thuật toán sử dụng thuần CNN trước đó. Trong tuần này chúng ta cùng nhau phân tích kiến trúc CMT-DeepLab đã kế thừa lại DETR nhằm giải quyết bài toán Panoptic Segmentation thành công như thế nào? Nghiên cứu được thực hiện bởi nhóm tác giả trẻ: Quihang Yu, Huiyu Wang và các cộng sự đến từ đại học John Hopskins, KAIST và Google Research",,,,,
"Chào mọi người
Mình có danh sách các môn học và đc chọn 5 units
Không biết thứ tự độ khó, vai trò quan trọng của units ntn xin mọi người hướng dẫn ?","Chào mọi người Mình có danh sách các môn học và đc chọn 5 units Không biết thứ tự độ khó, vai trò quan trọng của units ntn xin mọi người hướng dẫn ?",,,,,
Mình mới biết đến cuộc thi trên AIVIVN trong đó có Phân loại sắc thái bình luận nhưng đã kết thúc vào mấy tháng trước giờ mình đang muốn làm lại nhưng lại không còn data nữa không biết trước mọi người làm về bài thi này còn lưu lại data có thể share cho mình được không ạ 😃😃,Mình mới biết đến cuộc thi trên AIVIVN trong đó có Phân loại sắc thái bình luận nhưng đã kết thúc vào mấy tháng trước giờ mình đang muốn làm lại nhưng lại không còn data nữa không biết trước mọi người làm về bài thi này còn lưu lại data có thể share cho mình được không ạ,,,,,
"Hi mọi người.
Mình mới chuyển sang tìm hiểu lĩnh vực 3D printing. Mình đang muốn chuyển 1 file 3D obj có màu sang 3D obj xám bằng python. Mình đã tìm trên mạng nhưng chưa thấy được cách này. Bạn nào kinh nghiệm chia sẻ giúp mình tài liệu với.
Thanks mọi người.",Hi mọi người. Mình mới chuyển sang tìm hiểu lĩnh vực 3D printing. Mình đang muốn chuyển 1 file 3D obj có màu sang 3D obj xám bằng python. Mình đã tìm trên mạng nhưng chưa thấy được cách này. Bạn nào kinh nghiệm chia sẻ giúp mình tài liệu với. Thanks mọi người.,,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 7/2022 vào trong comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 7/2022 vào trong comment của post này.",,,,,
"Chào mn trong nhóm
Em/mình có một câu hỏi như sau:
Hiện tại em/mình đang sử dụng phần tracking của https://github.com/theAIGuysCode/yolov4-deepsort cho một phần của bài toán em/mình đang làm.
Mình có đọc paper thì ngta có nói rằng deepsort sinh ra để cải thiện vấn đề ID Shifting (switch) so với sort. Tuy nhiên, khi áp dụng, mình thấy hiện tượng trên vẫn thường xuyên xảy ra. Phải chăng mình config các params chưa đúng??
Mọi người có thể cho em/mình xin ý kiến được không ạ.
Với một vấn đề nữa là mình thấy deepsort này có vẻ khá nặng, mn có thể recomend cho mình cách khác nhẹ hơn được không ạ?
Cảm ơn mn ạ","Chào mn trong nhóm Em/mình có một câu hỏi như sau: Hiện tại em/mình đang sử dụng phần tracking của https://github.com/theAIGuysCode/yolov4-deepsort cho một phần của bài toán em/mình đang làm. Mình có đọc paper thì ngta có nói rằng deepsort sinh ra để cải thiện vấn đề ID Shifting (switch) so với sort. Tuy nhiên, khi áp dụng, mình thấy hiện tượng trên vẫn thường xuyên xảy ra. Phải chăng mình config các params chưa đúng?? Mọi người có thể cho em/mình xin ý kiến được không ạ. Với một vấn đề nữa là mình thấy deepsort này có vẻ khá nặng, mn có thể recomend cho mình cách khác nhẹ hơn được không ạ? Cảm ơn mn ạ",,,,,
"Em chào mọi người.
Hiện tại em đang tìm kiếm dữ liệu về giọng nói, để phục vụ cho việc fine tune model speech recognition tiếng Anh bias cho người Việt. Các nguồn hướng đến bao gồm video, podcast, webminar hay các bài representation của người Việt nhưng sử dụng ngôn ngữ tiếng Anh, về chủ đề gì cũng được ạ.
Mọi người có kinh nghiệm xem những nguồn trên nhiều có thể giúp em một vài keyword hoặc nguồn càng tốt ạ. Em cảm ơn mọi người trước.","Em chào mọi người. Hiện tại em đang tìm kiếm dữ liệu về giọng nói, để phục vụ cho việc fine tune model speech recognition tiếng Anh bias cho người Việt. Các nguồn hướng đến bao gồm video, podcast, webminar hay các bài representation của người Việt nhưng sử dụng ngôn ngữ tiếng Anh, về chủ đề gì cũng được ạ. Mọi người có kinh nghiệm xem những nguồn trên nhiều có thể giúp em một vài keyword hoặc nguồn càng tốt ạ. Em cảm ơn mọi người trước.",,,,,
"Hello cả nhà, mình mới tạo bảng so sánh tính năng của một số giải pháp model serving phổ biến (không phụ thuộc training framework), hy vọng có ích với mọi người.
---
Ai quan tâm tới ML engineering/MLOps thì có thể join cộng đồng này nha: https://www.facebook.com/groups/mlopsvn","Hello cả nhà, mình mới tạo bảng so sánh tính năng của một số giải pháp model serving phổ biến (không phụ thuộc training framework), hy vọng có ích với mọi người. --- Ai quan tâm tới ML engineering/MLOps thì có thể join cộng đồng này nha: https://www.facebook.com/groups/mlopsvn",,,,,
"Mình phép chia sẻ về Optimization Techniques cho các Deep Neural Network dựa trên các kinh nghiệm dự án của mình. Chúc các bạn cuối tuần vui vẻ.
Vì một cộng đồng AI vững mạnh hơn!",Mình phép chia sẻ về Optimization Techniques cho các Deep Neural Network dựa trên các kinh nghiệm dự án của mình. Chúc các bạn cuối tuần vui vẻ. Vì một cộng đồng AI vững mạnh hơn!,,,,,
Chào mọi người ạ. Anh chị (các bạn) có kinh nghiệm có thể cho em hỏi : có open source nào mà có thể nhận biết được phoneme(cách phát âm 1 từ) từ audio không ạ. Em search thì gần như không tìm thấy open source. Em (mình ) cảm ơn mọi người,Chào mọi người ạ. Anh chị (các bạn) có kinh nghiệm có thể cho em hỏi : có open source nào mà có thể nhận biết được phoneme(cách phát âm 1 từ) từ audio không ạ. Em search thì gần như không tìm thấy open source. Em (mình ) cảm ơn mọi người,,,,,
"[Xin gợi ý paper để reimplement cho dân beginner]
Em xin chào mọi người ạ, hiện tại em mới học xong 1 số course về Computer Vision và Python nên muốn ứng dụng kỹ năng thực hành.
Em muốn xin gợi ý các paper liên quan tới Computer Vision mà đã được reimplement nhiều để em thử code ạ. Em cảm ơn mn nhiều.","[Xin gợi ý paper để reimplement cho dân beginner] Em xin chào mọi người ạ, hiện tại em mới học xong 1 số course về Computer Vision và Python nên muốn ứng dụng kỹ năng thực hành. Em muốn xin gợi ý các paper liên quan tới Computer Vision mà đã được reimplement nhiều để em thử code ạ. Em cảm ơn mn nhiều.",,,,,
Xin hỏi các bạn một vài tài liệu cơ bản cho người mới bắt đầu với Data Science ạ. Cảm ơn mọi người.,Xin hỏi các bạn một vài tài liệu cơ bản cho người mới bắt đầu với Data Science ạ. Cảm ơn mọi người.,,,,,
"Chào mọi người. Em đang muốn tìm model để xác định vai trò của các từ (hoặc cụm từ) trong câu
Ví dụ như: My car has broken down. -> [My car](S) [has](A-Verb) [broken down](Verb)
Em cảm ơn ạ",Chào mọi người. Em đang muốn tìm model để xác định vai trò của các từ (hoặc cụm từ) trong câu Ví dụ như: My car has broken down. -> [My car](S) [has](A-Verb) [broken down](Verb) Em cảm ơn ạ,,,,,
"Xin chào anh em ML cơ bản.
Tranh thủ đang tìm hiểu về DBSCAN nên mình mạnh dạn làm clip chia sẻ cùng mọi người.
Hi vọng giúp được anh em có thêm công cụ chiến đấu và chơi với data.",Xin chào anh em ML cơ bản. Tranh thủ đang tìm hiểu về DBSCAN nên mình mạnh dạn làm clip chia sẻ cùng mọi người. Hi vọng giúp được anh em có thêm công cụ chiến đấu và chơi với data.,,,,,
Em đang làm một model để forecast dữ liệu theo time series dùng RNN ạ. Sau khi ra kết quả thì có câu hỏi yêu cầu tính mean square error và standard deviation (SD). Em thắc mắc là cái standard deviation này tính như thế nào? Em nghĩ nó liên quan đến đánh giá accuracy của model. Có phải tính SD dựa trên n dữ liệu predictions của model không? Hay là SD của error (y - y_predicted) ạ?,Em đang làm một model để forecast dữ liệu theo time series dùng RNN ạ. Sau khi ra kết quả thì có câu hỏi yêu cầu tính mean square error và standard deviation (SD). Em thắc mắc là cái standard deviation này tính như thế nào? Em nghĩ nó liên quan đến đánh giá accuracy của model. Có phải tính SD dựa trên n dữ liệu predictions của model không? Hay là SD của error (y - y_predicted) ạ?,,,,,
"Em chào các anh/chị trong group ạ, mong mọi người sẽ hỗ trợ em:
Em là sinh viên năm 3, dự định theo hướng AI Engineer, bên cạnh việc học các thuật toán cơ bản và các giáo trình chuyên sâu, em muốn tham khảo thêm các khóa học và chia sẻ từ chuyên gia trong ngành thì nên tìm ở đâu ạ?
Theo em biết thì hiện tại không có nhiều sự kiện hay khóa học về AI/ML, em ở hcm ạ. Nếu mng thấy khóa học hay hội thảo, tọa đàm nào hay hay thì chỉ em với nhé, xin cảm ơn mng rất nhiều","Em chào các anh/chị trong group ạ, mong mọi người sẽ hỗ trợ em: Em là sinh viên năm 3, dự định theo hướng AI Engineer, bên cạnh việc học các thuật toán cơ bản và các giáo trình chuyên sâu, em muốn tham khảo thêm các khóa học và chia sẻ từ chuyên gia trong ngành thì nên tìm ở đâu ạ? Theo em biết thì hiện tại không có nhiều sự kiện hay khóa học về AI/ML, em ở hcm ạ. Nếu mng thấy khóa học hay hội thảo, tọa đàm nào hay hay thì chỉ em với nhé, xin cảm ơn mng rất nhiều",,,,,
"Chào mọi người, em đang implement Q-Learning nhưng có chút thắc mắc về việc update Q-value ạ. Như trong cuốn ""Deep Reinforcement Learning Hands-On"" thì Q value được update như trong hình. Nhưng em có tham khảo một số nguồn khác như trên towardatascience thì lúc update Q value, ở future reward họ tính bằng cách lấy gamma * (max Q value(given state, action) - immediate reward). Em không biết cách update Q value nào là chuẩn nhất ạ. Cám ơn mọi người.","Chào mọi người, em đang implement Q-Learning nhưng có chút thắc mắc về việc update Q-value ạ. Như trong cuốn ""Deep Reinforcement Learning Hands-On"" thì Q value được update như trong hình. Nhưng em có tham khảo một số nguồn khác như trên towardatascience thì lúc update Q value, ở future reward họ tính bằng cách lấy gamma * (max Q value(given state, action) - immediate reward). Em không biết cách update Q value nào là chuẩn nhất ạ. Cám ơn mọi người.",,,,,
"[The best AI papers 2022 - Louis - Montreal Canada]
Một danh sách được tuyển chọn về những đột phá mới nhất trong AI theo ngày phát hành kèm theo lời giải thích bằng video rõ ràng, liên kết đến một bài viết chuyên sâu và code.
Trong khi thế giới vẫn đang phục hồi sau đại dịch thì lĩnh vực nghiên cứu không bị chậm lại tốc độ điên cuồng của nó, đặc biệt là trong lĩnh vực trí tuệ nhân tạo. Hơn nữa, nhiều khía cạnh quan trọng đã được nêu bật trong năm nay, như ethical, biases, governance, transparency và nhiều khía cạnh khác. Trí tuệ nhân tạo, sự hiểu biết của chúng ta về bộ não con người và mối liên kết của nó với AI đang không ngừng phát triển, cho thấy những ứng dụng đầy hứa hẹn cải thiện chất lượng cuộc sống của chúng ta trong tương lai gần. Tuy nhiên, chúng ta nên cẩn thận với công nghệ mà chúng ta chọn áp dụng.
""Science cannot tell us what we ought to do, only what we can do.""
- Jean-Paul Sartre, Being and Nothingness
Đây là công trình nghiên cứu các tài liệu nghiên cứu thú vị nhất cho năm 2022 đang được tiến hành. Tóm lại, Đây là danh sách các bước đột phá mới nhất trong AI và Khoa học dữ liệu được sắp xếp theo ngày phát hành kèm theo video giải thích rõ ràng, liên kết đến một bài báo chuyên sâu và code (nếu có). Chúc bạn đọc vui vẻ!
https://github.com/louisfb01/best_AI_papers_2022
---------------------------------------------------------------------------
A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code.
While the world is still recovering, research hasn't slowed its frenetic pace, especially in the field of artificial intelligence. More, many important aspects were highlighted this year, like the ethical aspects, important biases, governance, transparency and much more. Artificial intelligence and our understanding of the human brain and its link to AI are constantly evolving, showing promising applications improving our life's quality in the near future. Still, we ought to be careful with which technology we choose to apply.
""Science cannot tell us what we ought to do, only what we can do.""
- Jean-Paul Sartre, Being and Nothingness
Here is a work in progress of the most interesting research papers for 2022. In short, it is curated list of the latest breakthroughs in AI and Data Science by release date with a clear video explanation, link to a more in-depth article, and code (if applicable). Enjoy the read!
https://github.com/louisfb01/best_AI_papers_2022","[The best AI papers 2022 - Louis - Montreal Canada] Một danh sách được tuyển chọn về những đột phá mới nhất trong AI theo ngày phát hành kèm theo lời giải thích bằng video rõ ràng, liên kết đến một bài viết chuyên sâu và code. Trong khi thế giới vẫn đang phục hồi sau đại dịch thì lĩnh vực nghiên cứu không bị chậm lại tốc độ điên cuồng của nó, đặc biệt là trong lĩnh vực trí tuệ nhân tạo. Hơn nữa, nhiều khía cạnh quan trọng đã được nêu bật trong năm nay, như ethical, biases, governance, transparency và nhiều khía cạnh khác. Trí tuệ nhân tạo, sự hiểu biết của chúng ta về bộ não con người và mối liên kết của nó với AI đang không ngừng phát triển, cho thấy những ứng dụng đầy hứa hẹn cải thiện chất lượng cuộc sống của chúng ta trong tương lai gần. Tuy nhiên, chúng ta nên cẩn thận với công nghệ mà chúng ta chọn áp dụng. ""Science cannot tell us what we ought to do, only what we can do."" - Jean-Paul Sartre, Being and Nothingness Đây là công trình nghiên cứu các tài liệu nghiên cứu thú vị nhất cho năm 2022 đang được tiến hành. Tóm lại, Đây là danh sách các bước đột phá mới nhất trong AI và Khoa học dữ liệu được sắp xếp theo ngày phát hành kèm theo video giải thích rõ ràng, liên kết đến một bài báo chuyên sâu và code (nếu có). Chúc bạn đọc vui vẻ! https://github.com/louisfb01/best_AI_papers_2022 --------------------------------------------------------------------------- A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code. While the world is still recovering, research hasn't slowed its frenetic pace, especially in the field of artificial intelligence. More, many important aspects were highlighted this year, like the ethical aspects, important biases, governance, transparency and much more. Artificial intelligence and our understanding of the human brain and its link to AI are constantly evolving, showing promising applications improving our life's quality in the near future. Still, we ought to be careful with which technology we choose to apply. ""Science cannot tell us what we ought to do, only what we can do."" - Jean-Paul Sartre, Being and Nothingness Here is a work in progress of the most interesting research papers for 2022. In short, it is curated list of the latest breakthroughs in AI and Data Science by release date with a clear video explanation, link to a more in-depth article, and code (if applicable). Enjoy the read! https://github.com/louisfb01/best_AI_papers_2022",,,,,
"Chào mọi người. Khi bắt đầu một project deep learning, em không biết bắt đầu từ đâu nên đã đọc source code về các project đó. Tuy nhiên, code của họ bằng pytorch và có nhiều file và mỗi file rất nhiều. E đọc không liên kết được chúng nên k hiểu được nhiều. E muốn hỏi có cách nào để hiểu code tốt không ạ? Ngoài ra, làm thế nào để có thể tự viết code khác so với source code mk đã đọc ạ? E xin cảm ơn mn ạ.","Chào mọi người. Khi bắt đầu một project deep learning, em không biết bắt đầu từ đâu nên đã đọc source code về các project đó. Tuy nhiên, code của họ bằng pytorch và có nhiều file và mỗi file rất nhiều. E đọc không liên kết được chúng nên k hiểu được nhiều. E muốn hỏi có cách nào để hiểu code tốt không ạ? Ngoài ra, làm thế nào để có thể tự viết code khác so với source code mk đã đọc ạ? E xin cảm ơn mn ạ.",,,,,
"[AI Generate Documentation]

Có bao giờ bạn viết code xong nhưng lười viết document cho đoạn code ấy? Mintlify là tiện ích giúp mọi người sinh document cho code, nó có plugin để tích hợp trên VSCode và IntelliJ. Mintlify hỗ trợ rất nhiều ngôn ngữ, có thể kể đến như: Python, JavaScript, C and C++, PHP, Java, C#, Ruby, Rust, Dart, Go, etc. 

Website: https://www.mintlify.com/writer
Github: https://github.com/mintlify/writer","[AI Generate Documentation] Có bao giờ bạn viết code xong nhưng lười viết document cho đoạn code ấy? Mintlify là tiện ích giúp mọi người sinh document cho code, nó có plugin để tích hợp trên VSCode và IntelliJ. Mintlify hỗ trợ rất nhiều ngôn ngữ, có thể kể đến như: Python, JavaScript, C and C++, PHP, Java, C#, Ruby, Rust, Dart, Go, etc. Website: https://www.mintlify.com/writer Github: https://github.com/mintlify/writer",,,,,
"A great book with comprehensive guides to Machine Learning in Production. I have read all course materials online (link in comment) and am now reading the book. Hat off to the author!
 — với Huyen Nguyen.",A great book with comprehensive guides to Machine Learning in Production. I have read all course materials online (link in comment) and am now reading the book. Hat off to the author! — với Huyen Nguyen.,,,,,
"Chào mọi người e đang tìm hiểu về các thuật toán face detection.
E có tìm thì thấy một model Caffe và người ta có chú thích: res10_300x300_ssd_iter_140000.caffemodel
E không hiểu lắm mong mọi người giải đáp giùm e model và thuật toán này là như thế nào ạ. Chân thành cảm ơn mọi người",Chào mọi người e đang tìm hiểu về các thuật toán face detection. E có tìm thì thấy một model Caffe và người ta có chú thích: res10_300x300_ssd_iter_140000.caffemodel E không hiểu lắm mong mọi người giải đáp giùm e model và thuật toán này là như thế nào ạ. Chân thành cảm ơn mọi người,,,,,
Chia sẻ bài viết về các website cung cấp Open Data. Các website này sẽ giúp ích rất nhiều cho dự án của các bạn.,Chia sẻ bài viết về các website cung cấp Open Data. Các website này sẽ giúp ích rất nhiều cho dự án của các bạn.,,,,,
Hôm trước có bạn nào post bài toán OCR hóa đơn bán hàng mà mình tìm lại chưa được. Nếu bạn nào đã làm thuật toán này (không dùng tesseract) chạy ok rồi thì liên lạc với mình để trao đổi nhé. Mình cũng đang cần nó. Xin cảm ơn!,Hôm trước có bạn nào post bài toán OCR hóa đơn bán hàng mà mình tìm lại chưa được. Nếu bạn nào đã làm thuật toán này (không dùng tesseract) chạy ok rồi thì liên lạc với mình để trao đổi nhé. Mình cũng đang cần nó. Xin cảm ơn!,,,,,
"Nhờ admin duyệt tin. Xin cảm ơn.
Mình đang làm giải pháp y tế về dự báo dịch bệnh.
Hiện đang tham gia chương trình Đổi mới sáng tạo do Sở KH&CN Tp.HCM tổ chức. Giải pháp đang ở giai đoạn lấy ý kiến về cộng đồng.
Các bạn quan tâm like để ủng hộ giải pháp (dùng facebook để like)
Mình gửi link bình chọn dưới đây, cảm ơn các bạn đã like ủng hộ.
https://doimoisangtao.vn/giai-thuong-dmst/2022/3/17/n2001-hsmart-he-thong-canh-bao-som-dich-benh-su-dung-tri-tue-nhan-tao-du-lieu-lon
PS: Mình hy vọng nhận được nhiều góp ý chuyên gia y khoa, khoa học dữ liệu, CNTT sẵn sàng hợp tác để hoàn thiện giải pháp tối ưu hơn.","Nhờ admin duyệt tin. Xin cảm ơn. Mình đang làm giải pháp y tế về dự báo dịch bệnh. Hiện đang tham gia chương trình Đổi mới sáng tạo do Sở KH&CN Tp.HCM tổ chức. Giải pháp đang ở giai đoạn lấy ý kiến về cộng đồng. Các bạn quan tâm like để ủng hộ giải pháp (dùng facebook để like) Mình gửi link bình chọn dưới đây, cảm ơn các bạn đã like ủng hộ. https://doimoisangtao.vn/giai-thuong-dmst/2022/3/17/n2001-hsmart-he-thong-canh-bao-som-dich-benh-su-dung-tri-tue-nhan-tao-du-lieu-lon PS: Mình hy vọng nhận được nhiều góp ý chuyên gia y khoa, khoa học dữ liệu, CNTT sẵn sàng hợp tác để hoàn thiện giải pháp tối ưu hơn.",,,,,
"Chào mọi người, em có đang nghiên cứu về các thuật toán máy học. Về vấn đề hiểu thì e chỉ mới hiểu được cái phần cơ bản, mức nền. Khi đọc sâu vào phần toán thì e lại thấy hơi khó hiểu và phức tạp. Mọi người cho em hỏi em có cần phải học sâu về phần toán trong tất cả các thuật toán cơ bản cho hiểu trước hết rồi mới đi sâu vào hay khi nào cần thì mình mới đọc lại thôi ạ. Xin anh chị đi trước cho em xin ít ý kiến ạ. Em xin cảm ơn","Chào mọi người, em có đang nghiên cứu về các thuật toán máy học. Về vấn đề hiểu thì e chỉ mới hiểu được cái phần cơ bản, mức nền. Khi đọc sâu vào phần toán thì e lại thấy hơi khó hiểu và phức tạp. Mọi người cho em hỏi em có cần phải học sâu về phần toán trong tất cả các thuật toán cơ bản cho hiểu trước hết rồi mới đi sâu vào hay khi nào cần thì mình mới đọc lại thôi ạ. Xin anh chị đi trước cho em xin ít ý kiến ạ. Em xin cảm ơn",,,,,
"Xin chào mn ạ,
Mn có muốn tham gia workshop để tìm hiểu nhiều hơn về việc học Khoa học máy tính ở nhiều nước khác nhau trên thế giới ở nhiều bậc như cử nhân, thạc sĩ, đại học và tìm hiểu về quá trình start-up công ty công nghê hay xin việc làm trong ngành không ạ?
Mn quan tâm mình sẽ gửi thông tin ạ!","Xin chào mn ạ, Mn có muốn tham gia workshop để tìm hiểu nhiều hơn về việc học Khoa học máy tính ở nhiều nước khác nhau trên thế giới ở nhiều bậc như cử nhân, thạc sĩ, đại học và tìm hiểu về quá trình start-up công ty công nghê hay xin việc làm trong ngành không ạ? Mn quan tâm mình sẽ gửi thông tin ạ!",,,,,
"Em chào các anh chị, em đang làm đồ án 1, bọn em đang có một tập dữ liệu về doanh số bán hàng , sau khi tiến hành phân tích dữ liệu xong bọn em muốn hướng đến dự đoán dữ liệu ( dự đoán doanh số trong tương lai của một sản phẩm) . Vậy bọn em có thể có những hướng đi nào để giải quyết bài toán này ạ?
Bọn em còn chưa nhiều kiến thức và đang trong quá trình học hỏi, nếu câu hỏi quá amateur mong anh chị thông cảm và giúp đỡ ạ 🙏🏾☺️","Em chào các anh chị, em đang làm đồ án 1, bọn em đang có một tập dữ liệu về doanh số bán hàng , sau khi tiến hành phân tích dữ liệu xong bọn em muốn hướng đến dự đoán dữ liệu ( dự đoán doanh số trong tương lai của một sản phẩm) . Vậy bọn em có thể có những hướng đi nào để giải quyết bài toán này ạ? Bọn em còn chưa nhiều kiến thức và đang trong quá trình học hỏi, nếu câu hỏi quá amateur mong anh chị thông cảm và giúp đỡ ạ",,,,,
"Em chào mọi người!
Em mới tìm hiểu về Machine Learning em có test thử code về pose estimation mà khi chạy project bị lỗi này. Mọi người giúp em với ạ
Em xin cảm ơn ạ",Em chào mọi người! Em mới tìm hiểu về Machine Learning em có test thử code về pose estimation mà khi chạy project bị lỗi này. Mọi người giúp em với ạ Em xin cảm ơn ạ,,,,,
"Trong việc lựa chọn feature cho ML model mình nên loại bỏ các feature có độ tương quan cao đúng không các bác ? 
em tham khảo thấy có bài bỏ hết nết abs(corrlation) > 0.5, có bài lại bỏ hết abs(corrlation) > 0.5 nhưng giữ lại feature có correlation cao nhất. (như ảnh monthly_average giữ lại vì có tương quan cao nhất tới sales)
nên em không rõ có quy tắc nào để feature selection từ correlation không ? hay phần này mang nặng tính heuristic - kinh nghiệm của người làm nhỉ ?","Trong việc lựa chọn feature cho ML model mình nên loại bỏ các feature có độ tương quan cao đúng không các bác ? em tham khảo thấy có bài bỏ hết nết abs(corrlation) > 0.5, có bài lại bỏ hết abs(corrlation) > 0.5 nhưng giữ lại feature có correlation cao nhất. (như ảnh monthly_average giữ lại vì có tương quan cao nhất tới sales) nên em không rõ có quy tắc nào để feature selection từ correlation không ? hay phần này mang nặng tính heuristic - kinh nghiệm của người làm nhỉ ?",,,,,
"Chào các ace trên group.
E đang làm liên quan đến data fusion sử dụng Bayesian inference.
Sau khi tổng hợp từ các thông tin biết trước. Thì e thu được xác xuất xảy ra tại phần tử i là P(i) (có n phần tử)
Nhưng sau đó e lại thấy có tác giả dùng công thức như trên hình Q(i) = sqrt(P(i)*P(n+1-i). Điều này làm cho chúng đối xứng. Có bác nào từng dùng công thức này, hoặc thấy có giải thích ở lý do ở đâu thì chỉ cho e ạ.
E cám ơn!","Chào các ace trên group. E đang làm liên quan đến data fusion sử dụng Bayesian inference. Sau khi tổng hợp từ các thông tin biết trước. Thì e thu được xác xuất xảy ra tại phần tử i là P(i) (có n phần tử) Nhưng sau đó e lại thấy có tác giả dùng công thức như trên hình Q(i) = sqrt(P(i)*P(n+1-i). Điều này làm cho chúng đối xứng. Có bác nào từng dùng công thức này, hoặc thấy có giải thích ở lý do ở đâu thì chỉ cho e ạ. E cám ơn!",,"#math, #Q&A",,,
,nan,,,,,
YoloV7 real-time object detection through Visual Data Preparation (VDP) which is an open-source visual data ETL tool,YoloV7 real-time object detection through Visual Data Preparation (VDP) which is an open-source visual data ETL tool,,,,,
"Nhờ anh em giải toán (code python càng tốt ạ):
Có một ma trận số dương vuông (n x n ô). Tại mỗi hàng và mỗi cột lấy duy nhất một số, được n số. Tìm tọa độ n số sao cho tổng của chúng là lớn nhất.
Mình chỉ nghĩ ra được cách thủ công là tạo ra mọi tổ hợp tọa độ đáp ứng yêu cầu bài toán, tính tổng rồi chọn ra đáp án lớn nhất.
Anh em có cách nhanh hơn thì giúp mình với. Cảm ơn rất nhiều.","Nhờ anh em giải toán (code python càng tốt ạ): Có một ma trận số dương vuông (n x n ô). Tại mỗi hàng và mỗi cột lấy duy nhất một số, được n số. Tìm tọa độ n số sao cho tổng của chúng là lớn nhất. Mình chỉ nghĩ ra được cách thủ công là tạo ra mọi tổ hợp tọa độ đáp ứng yêu cầu bài toán, tính tổng rồi chọn ra đáp án lớn nhất. Anh em có cách nhanh hơn thì giúp mình với. Cảm ơn rất nhiều.",,"#Q&A, #math",,,
mọi người cho mình có api của dịch vụ nào giúp mình đưa vào keyword => cho mình output 1 câu dc ko?,mọi người cho mình có api của dịch vụ nào giúp mình đưa vào keyword => cho mình output 1 câu dc ko?,,,,,
"Chào các anh chị,
Hiện em đang cài đặt driver nvidia-515 cho ubutu 20.04 nhưng không cài được và gặp lỗi như trên hình ạ, không biết anh chị nào đã từng gặp lỗi như này chưa ạ, có thể chỉ em cách fix được không ạ, em cảm ơn.","Chào các anh chị, Hiện em đang cài đặt driver nvidia-515 cho ubutu 20.04 nhưng không cài được và gặp lỗi như trên hình ạ, không biết anh chị nào đã từng gặp lỗi như này chưa ạ, có thể chỉ em cách fix được không ạ, em cảm ơn.",,,,,
"Chào anh chị và các bạn!
Em có đang tìm hiểu paper phương pháp tách từ của VnCoreNLP (link paper: http://www.lrec-conf.org/proceedings/lrec2018/pdf/55.pdf) . Em có chút thắc mắc:
Trong paper tác giả có nói rằng sẽ học cây SCRDR rồi sau đó sử dụng cây đó để tách từ trong các câu mới. Theo em hiểu thì với mỗi từ sẽ có 1 cây SCRDR, không biết em đã hiểu đúng chưa ạ!
Em cảm ơn ac!","Chào anh chị và các bạn! Em có đang tìm hiểu paper phương pháp tách từ của VnCoreNLP (link paper: http://www.lrec-conf.org/proceedings/lrec2018/pdf/55.pdf) . Em có chút thắc mắc: Trong paper tác giả có nói rằng sẽ học cây SCRDR rồi sau đó sử dụng cây đó để tách từ trong các câu mới. Theo em hiểu thì với mỗi từ sẽ có 1 cây SCRDR, không biết em đã hiểu đúng chưa ạ! Em cảm ơn ac!",,,,,
"Hi mọi người,
Hiện tại, em đang tìm hiểu về seq2seq lstm cho machine translation (pytorch framework). Em đang còn chút ko rõ về việc preprocessing mong nên được mọi người giúp đỡ.
1) Khi tạo batch thì em thấy các batch (batch_size=8) có sequence length khác nhau (vd: torch.size((256,8)), torch.size((10009,8)) ). Trong trường hợp, mình có phải buộc tất cả các batch phải có cùng shape hay để tự nhiên như này được?
2) Mình có phải ràng buộc max length cho input hay length như thế nào cũng được?","Hi mọi người, Hiện tại, em đang tìm hiểu về seq2seq lstm cho machine translation (pytorch framework). Em đang còn chút ko rõ về việc preprocessing mong nên được mọi người giúp đỡ. 1) Khi tạo batch thì em thấy các batch (batch_size=8) có sequence length khác nhau (vd: torch.size((256,8)), torch.size((10009,8)) ). Trong trường hợp, mình có phải buộc tất cả các batch phải có cùng shape hay để tự nhiên như này được? 2) Mình có phải ràng buộc max length cho input hay length như thế nào cũng được?",,,,,
"Hi cả nhà, kubeflow pipeline tutorials mình đăng bên MLOps VN được khá nhiều người quan tâm nên hôm nay mình xin phép share lại ở đây, hy vọng sẽ giúp mọi người bớt phần nào khó khăn khi làm việc với tool này.
https://github.com/quan-dang/kubeflow-tutorials
----
Link group MLOps VN mình đề cập ở trên: https://www.facebook.com/groups/mlopsvn","Hi cả nhà, kubeflow pipeline tutorials mình đăng bên MLOps VN được khá nhiều người quan tâm nên hôm nay mình xin phép share lại ở đây, hy vọng sẽ giúp mọi người bớt phần nào khó khăn khi làm việc với tool này. https://github.com/quan-dang/kubeflow-tutorials ---- Link group MLOps VN mình đề cập ở trên: https://www.facebook.com/groups/mlopsvn",,,,,
"[Graph Transformer - a breakthrough of Transformer with graph data]
Transformer là lớp kiến trúc có thể học tập hiệu quả từ dữ liệu dạng sequential data chẳng hạn như văn bản, âm thanh, chuỗi thời gian. Tuy nhiên chúng cho thấy giới hạn trong việc học các dữ liệu dạng graph (là những dữ liệu kết hợp giữa nodes và edges như social network, logistic, entity relationship, consumption  behaviors). Một biến thể mới là Graph Transformer https://arxiv.org/abs/2012.09699 đã cho phép áp khái quát hóa Transformer trên một graph bất kì.
1. Điểm mới mẻ:
Vijay Prakash Dwivedi và Xavier Bresson tại Nanyang Technological University đã tạo ra Graph Transformer (GT), một Transfomer layer được thiết kế để xử lý dữ liệu graph. Xếp chồng các lớp GT sẽ cung cấp một giải pháp Transformer-based thay thế cho các Graph Neural Network điển hình, một dạng kiến trúc áp dụng trên dữ liệu dạng các node và edge kết nối lẫn nhau.  Một ví dụ về dữ liệu dạng này: khách hàng kết nối với sản phẩm họ đã mua; tài khoản facebook của bạn kết nối tới những người bạn tương tác;  Hoặc các nguyên tử kết nối với nhau trong phân tử.
2. Điểm mấu chốt:
Các nghiên cứu trước đây đã áp dụng Transformer cho graph data bằng cách gán một token cho mỗi node và tính toán attention giữa mỗi một cặp. Phương pháp này mã hóa được:
- local relationships: Chẳng hạn như các nodes nào là hàng xóm (đưa ra một hyperparameter giúp xác định vùng lân cận nằm trong một số mức độ tách biệt)
- global information: Chẳng hạn như khoảng cách của node với các nodes không hàng xóm.
Tuy nhiên, cách tiếp cận này cực kỳ tốn kém đối với các đồ thị lớn, vì tính toán cần thiết cho self-attention tăng lên số lần bằng bậc hai kích thước của đầu vào. Áp dụng attention chỉ tới các nodes hàng xóm giúp nắm bắt thông tin cục bộ quan trọng trong khi cắt giảm gánh nặng tính toán. Trong khi đó, một positional vector biểu thị khoảng cách tương đối của mỗi cặp nodes có thể nắm bắt global information theo cách hiệu quả về mặt tính toán.
3. Nguyên lý hoạt động
Các tác giả đã xây dựng ba mô hình, mỗi mô hình bao gồm các embedding layers, 10 GT layers (bao gồm self-attention và fully connected layers) theo sau là một mạng neural network thông thường. Mỗi mô hình được huấn luyện cho một nhiệm vụ khác nhau: phân loại hai lớp, phân loại sáu lớp và hồi quy ước tính độ hòa tan của các hợp chất hóa học có chứa kẽm.
- Cho một đồ thị, các embedding layers tạo ra một embedding vector và positional vector cho mỗi nút. Sử dụng constrastive learning để tạo ra các positional vector tương tự cho các node hàng xóm và các positinal vector khác nhau cho các nodes cách xa.  Cộng embedding vector và positional vector để tạo thành vector biểu diễn cho mỗi node.
- Lớp GT đã mài gọt từng biểu diễn node bằng cách áp dụng self-attention giữa node đó với các nodes lân cận. Sau đó, nó chuyển các biểu diễn nodes sang fully connected layer.
- Mô hình đã thực hiện liên tiếp những bước này qua 10 layers và cung cấp các biểu diễn cuối cùng cho một neural network thông thường nhằm thực hiện phân loại hoặc hồi quy.
4. Kết quả:
Mô hình của các tác giả lần lượt đạt được 73,17% và 84,81% accuracy đối với các nhiệm vụ phân loại nhị phân và sáu lớp. Một baseline khác là GAT (ICLR 2018): https://arxiv.org/pdf/1710.10903.pdfáp dụng attention qua các biểu diễn node hàng xóm đạt accuracy lần lượt là 70,58% và 78,27% accuracy. Trong nhiệm vụ hồi quy, mô hình của các tác giả đã đạt được sai số tuyệt đối trung bình (MAE) là 0,226 so với GAT là 0,384 (MAE càng thấp càng tốt). Tuy nhiên, nó hoạt động kém hơn một chút so với Gated Graph ConvNet: https://arxiv.org/abs/1711.07553trong cả ba tác vụ này
5. Suy nghĩ của tôi:
Kể từ khi ra đời vào năm 2017 trong paper “Attention is all you need”, các kiến trúc họ Transformer đã chứng minh được sức mạnh của chúng đối với ban đầu là dữ liệu văn bản, âm thanh và sau đó là hình ảnh, timeseries data. Không những thế, đối với graph data thì paper này đã chứng minh chúng là hoàn toàn hữu ích. Mặc dù Graph Transformer không phải là mạng neural network mạnh nhất nhưng đã tạo ra một strong baseline củng cố cho những nghiên cứu tiếp theo trong lĩnh vực này. Sẽ là thế nào nếu áp dụng Graph Transformer trên dữ liệu văn bản, âm thanh, hình ảnh?
------------------------------------------------------------------------
Khóa học vể Machine Learning in Production – MLOps, khai giảng 24/07, nhằm cung cấp kiến thức thực tiễn về cách xây dựng, duy trì, vận hành và quản trị mô hình Machine Learning và hướng tới trao quyền cho các doanh nghiệp vừa và nhỏ năng lực thực thi mô hình trên cloud service: https://forms.gle/o38H5RG479ojTQ6B6","[Graph Transformer - a breakthrough of Transformer with graph data] Transformer là lớp kiến trúc có thể học tập hiệu quả từ dữ liệu dạng sequential data chẳng hạn như văn bản, âm thanh, chuỗi thời gian. Tuy nhiên chúng cho thấy giới hạn trong việc học các dữ liệu dạng graph (là những dữ liệu kết hợp giữa nodes và edges như social network, logistic, entity relationship, consumption behaviors). Một biến thể mới là Graph Transformer https://arxiv.org/abs/2012.09699 đã cho phép áp khái quát hóa Transformer trên một graph bất kì. 1. Điểm mới mẻ: Vijay Prakash Dwivedi và Xavier Bresson tại Nanyang Technological University đã tạo ra Graph Transformer (GT), một Transfomer layer được thiết kế để xử lý dữ liệu graph. Xếp chồng các lớp GT sẽ cung cấp một giải pháp Transformer-based thay thế cho các Graph Neural Network điển hình, một dạng kiến trúc áp dụng trên dữ liệu dạng các node và edge kết nối lẫn nhau. Một ví dụ về dữ liệu dạng này: khách hàng kết nối với sản phẩm họ đã mua; tài khoản facebook của bạn kết nối tới những người bạn tương tác; Hoặc các nguyên tử kết nối với nhau trong phân tử. 2. Điểm mấu chốt: Các nghiên cứu trước đây đã áp dụng Transformer cho graph data bằng cách gán một token cho mỗi node và tính toán attention giữa mỗi một cặp. Phương pháp này mã hóa được: - local relationships: Chẳng hạn như các nodes nào là hàng xóm (đưa ra một hyperparameter giúp xác định vùng lân cận nằm trong một số mức độ tách biệt) - global information: Chẳng hạn như khoảng cách của node với các nodes không hàng xóm. Tuy nhiên, cách tiếp cận này cực kỳ tốn kém đối với các đồ thị lớn, vì tính toán cần thiết cho self-attention tăng lên số lần bằng bậc hai kích thước của đầu vào. Áp dụng attention chỉ tới các nodes hàng xóm giúp nắm bắt thông tin cục bộ quan trọng trong khi cắt giảm gánh nặng tính toán. Trong khi đó, một positional vector biểu thị khoảng cách tương đối của mỗi cặp nodes có thể nắm bắt global information theo cách hiệu quả về mặt tính toán. 3. Nguyên lý hoạt động Các tác giả đã xây dựng ba mô hình, mỗi mô hình bao gồm các embedding layers, 10 GT layers (bao gồm self-attention và fully connected layers) theo sau là một mạng neural network thông thường. Mỗi mô hình được huấn luyện cho một nhiệm vụ khác nhau: phân loại hai lớp, phân loại sáu lớp và hồi quy ước tính độ hòa tan của các hợp chất hóa học có chứa kẽm. - Cho một đồ thị, các embedding layers tạo ra một embedding vector và positional vector cho mỗi nút. Sử dụng constrastive learning để tạo ra các positional vector tương tự cho các node hàng xóm và các positinal vector khác nhau cho các nodes cách xa. Cộng embedding vector và positional vector để tạo thành vector biểu diễn cho mỗi node. - Lớp GT đã mài gọt từng biểu diễn node bằng cách áp dụng self-attention giữa node đó với các nodes lân cận. Sau đó, nó chuyển các biểu diễn nodes sang fully connected layer. - Mô hình đã thực hiện liên tiếp những bước này qua 10 layers và cung cấp các biểu diễn cuối cùng cho một neural network thông thường nhằm thực hiện phân loại hoặc hồi quy. 4. Kết quả: Mô hình của các tác giả lần lượt đạt được 73,17% và 84,81% accuracy đối với các nhiệm vụ phân loại nhị phân và sáu lớp. Một baseline khác là GAT (ICLR 2018): https://arxiv.org/pdf/1710.10903.pdfáp dụng attention qua các biểu diễn node hàng xóm đạt accuracy lần lượt là 70,58% và 78,27% accuracy. Trong nhiệm vụ hồi quy, mô hình của các tác giả đã đạt được sai số tuyệt đối trung bình (MAE) là 0,226 so với GAT là 0,384 (MAE càng thấp càng tốt). Tuy nhiên, nó hoạt động kém hơn một chút so với Gated Graph ConvNet: https://arxiv.org/abs/1711.07553trong cả ba tác vụ này 5. Suy nghĩ của tôi: Kể từ khi ra đời vào năm 2017 trong paper “Attention is all you need”, các kiến trúc họ Transformer đã chứng minh được sức mạnh của chúng đối với ban đầu là dữ liệu văn bản, âm thanh và sau đó là hình ảnh, timeseries data. Không những thế, đối với graph data thì paper này đã chứng minh chúng là hoàn toàn hữu ích. Mặc dù Graph Transformer không phải là mạng neural network mạnh nhất nhưng đã tạo ra một strong baseline củng cố cho những nghiên cứu tiếp theo trong lĩnh vực này. Sẽ là thế nào nếu áp dụng Graph Transformer trên dữ liệu văn bản, âm thanh, hình ảnh? ------------------------------------------------------------------------ Khóa học vể Machine Learning in Production – MLOps, khai giảng 24/07, nhằm cung cấp kiến thức thực tiễn về cách xây dựng, duy trì, vận hành và quản trị mô hình Machine Learning và hướng tới trao quyền cho các doanh nghiệp vừa và nhỏ năng lực thực thi mô hình trên cloud service: https://forms.gle/o38H5RG479ojTQ6B6",,,,,
Mình muốn chuyển từ chữ viết tay trên ảnh sang dữ liệu file text. Mọi người có tài liệu nào để tham khảo cho mình xin ạ.,Mình muốn chuyển từ chữ viết tay trên ảnh sang dữ liệu file text. Mọi người có tài liệu nào để tham khảo cho mình xin ạ.,,,,,
"Chào mn, e có một số câu hỏi về việc sử dụng package pytorch geometric/PyG ạ
1/ E xin cách mn cài pytorch_geometric trên colab ạ. E đã cài đúng như hướng dẫn trên doc của PyG, cài được nhưng khi import thì lại ra lỗi package not found
2/ Trong pytorch có class nào tương tự như cái pytorch_geometric.data.Data không ạ? E thấy thường code người ta sử dụng pytorch_geometric.data.Data có thể set đủ loại property cho datapoint, e cũng đang cần dùng tính năng này nhưng ko cần đến việc phải lưu dữ liệu đồ thị","Chào mn, e có một số câu hỏi về việc sử dụng package pytorch geometric/PyG ạ 1/ E xin cách mn cài pytorch_geometric trên colab ạ. E đã cài đúng như hướng dẫn trên doc của PyG, cài được nhưng khi import thì lại ra lỗi package not found 2/ Trong pytorch có class nào tương tự như cái pytorch_geometric.data.Data không ạ? E thấy thường code người ta sử dụng pytorch_geometric.data.Data có thể set đủ loại property cho datapoint, e cũng đang cần dùng tính năng này nhưng ko cần đến việc phải lưu dữ liệu đồ thị",,,,,
"[Pen and Paper Exercises in Machine Learning]

Bộ sưu tập hơn 200 trang, chủ yếu là các bài tập biến đổi bằng tay trong Machine Learning. Các bài tập thuộc các chủ đề sau: 
- Linear algebra
- Optimisation
- Directed graphical models
- Undirected graphical models
- Expressive power of graphical models
- Factor graphs and message passing
- Inference for hidden Markov models
- Model-based learning (including ICA and unnormalised models)
- Variational inference

Mọi người download ở đây:
https://arxiv.org/pdf/2206.13446.pdf","[Pen and Paper Exercises in Machine Learning] Bộ sưu tập hơn 200 trang, chủ yếu là các bài tập biến đổi bằng tay trong Machine Learning. Các bài tập thuộc các chủ đề sau: - Linear algebra - Optimisation - Directed graphical models - Undirected graphical models - Expressive power of graphical models - Factor graphs and message passing - Inference for hidden Markov models - Model-based learning (including ICA and unnormalised models) - Variational inference Mọi người download ở đây: https://arxiv.org/pdf/2206.13446.pdf",,,,,
"Hi anh, chị,
- Em đang làm luận văn Thạc Sĩ CNTT, em làm về machine learning, đề tài của em ""Loan Repayment Prediction Using Machine Learning Algorithms"",  
- Bộ dữ liệu được sử dụng trong bài báo này là từ Lending Club, một trang web kết nối người vay và nhà đầu tư qua Internet. Nó bao gồm 9.578 quan sát được tài trợ thông qua nền tảng LendingClub.com từ tháng 5 năm 2007 đến tháng 2 năm 2010
- Các bước em thực hiện bài toán: 
1. Tiền xử lý dữ liệu - Data preprocessing:
   a. Tìm hệ số tương quan giữa các thuộc tính.
   b. Xử lý các thuộc tính dạng văn bản
   c. Xử lý các giá trị bị thiếu(Data Imputation - Missing Data Replacement)
   d. Tách dữ liệu thành tập huấn luyện và tập kiểm tra
   e. Chuẩn hóa dữ liệu
2. Huấn luyện mô hình:
   a. Đánh giá mô hình độ chính xác bằng kiểm định chéo
   b. Đánh giá mô hình độ chính xác bằng ma trận nhầm lẫn
      + Presision
      + Recall
      + F1-Score
      + Đường cong ROC
=> Kết quả của em không được tốt lắm, Em mong anh, chi có kinh nghiệm hướng dẫn, góp ý giúp em hoặc có thể inbox, em xin đa tạ.
- Em upload link github gồm data, code nếu được anh, chị xem giúp ạ. em xin chân thành cảm ơn.","Hi anh, chị, - Em đang làm luận văn Thạc Sĩ CNTT, em làm về machine learning, đề tài của em ""Loan Repayment Prediction Using Machine Learning Algorithms"", - Bộ dữ liệu được sử dụng trong bài báo này là từ Lending Club, một trang web kết nối người vay và nhà đầu tư qua Internet. Nó bao gồm 9.578 quan sát được tài trợ thông qua nền tảng LendingClub.com từ tháng 5 năm 2007 đến tháng 2 năm 2010 - Các bước em thực hiện bài toán: 1. Tiền xử lý dữ liệu - Data preprocessing: a. Tìm hệ số tương quan giữa các thuộc tính. b. Xử lý các thuộc tính dạng văn bản c. Xử lý các giá trị bị thiếu(Data Imputation - Missing Data Replacement) d. Tách dữ liệu thành tập huấn luyện và tập kiểm tra e. Chuẩn hóa dữ liệu 2. Huấn luyện mô hình: a. Đánh giá mô hình độ chính xác bằng kiểm định chéo b. Đánh giá mô hình độ chính xác bằng ma trận nhầm lẫn + Presision + Recall + F1-Score + Đường cong ROC => Kết quả của em không được tốt lắm, Em mong anh, chi có kinh nghiệm hướng dẫn, góp ý giúp em hoặc có thể inbox, em xin đa tạ. - Em upload link github gồm data, code nếu được anh, chị xem giúp ạ. em xin chân thành cảm ơn.",,,,,
"Help Required!
I want to hide personal information in documents like passport number, national security number, name and home address in an image.
First: I have applied OCR to recognize all the words in document then i put If statement and some specific part like passport number.
The problem is that sometime OCR wrongly recognize the passport number and sometime other fields as well and bypass the if statement. Thus, it is fail to hide personal information in document.
Please recommend me any solution?","Help Required! I want to hide personal information in documents like passport number, national security number, name and home address in an image. First: I have applied OCR to recognize all the words in document then i put If statement and some specific part like passport number. The problem is that sometime OCR wrongly recognize the passport number and sometime other fields as well and bypass the if statement. Thus, it is fail to hide personal information in document. Please recommend me any solution?",,,,,
"Chào cả nhà, em/mình mong được hỏi thăm vài chỉ dẫn về cộng đồng ML/ Computer vision ở Việt Nam
Cty em/mình có 1 team dày dạn hơn 15+ năm kinh nghiệm ứng dụng AI vào xử lý & quản lý hơn 200+ triệu dữ liệu hình ảnh, và gần đây bắt đầu phân phối các dữ liệu này cho dự án AI tại Panasonic, Honda, SoftBank...
Bên mình nhận thấy dữ liệu tự nhiên trên thị trường cũng nhiều nhưng chưa được phân luồng hiệu quả, nên đã mạnh dạn nghiên cứu một kênh phân phối dữ liệu để giải quyết bài toán về việc tiếp cận dữ liệu hình ảnh số lượng lớn, chất lượng cao, chi phí hợp lý cho các dự án ML.
Hiện team em/mình rất mong được
1- Trao đổi insights từ các anh chị em data scientists, ML engineers có kinh nghiệm trong các bài toán computer vision để làm rõ các giả thiết về nhu cầu thực tiễn.
2- Giao lưu với cộng đồng AI/ML ở Việt Nam để tăng cường chia sẻ thông tin & cơ hội
Em/ mình rất mong được hỗ trợ
1- Giới thiệu một số anh chị em trong ngành có thể dành ra 20-30p chia sẻ với team em/mình về ý tưởng này
2- Giới thiệu một số kênh giao lưu hiệu quả. Hoặc nếu có anh chị em nào đã (hoặc sẽ) tham gia VinAI hoặc AI Summit thì mong được chia sẻ kinh nghiệm đăng ký workshop/expo ở đấy ạ :D
Cảm ơn mọi người rất nhiều 🫰🙏","Chào cả nhà, em/mình mong được hỏi thăm vài chỉ dẫn về cộng đồng ML/ Computer vision ở Việt Nam Cty em/mình có 1 team dày dạn hơn 15+ năm kinh nghiệm ứng dụng AI vào xử lý & quản lý hơn 200+ triệu dữ liệu hình ảnh, và gần đây bắt đầu phân phối các dữ liệu này cho dự án AI tại Panasonic, Honda, SoftBank... Bên mình nhận thấy dữ liệu tự nhiên trên thị trường cũng nhiều nhưng chưa được phân luồng hiệu quả, nên đã mạnh dạn nghiên cứu một kênh phân phối dữ liệu để giải quyết bài toán về việc tiếp cận dữ liệu hình ảnh số lượng lớn, chất lượng cao, chi phí hợp lý cho các dự án ML. Hiện team em/mình rất mong được 1- Trao đổi insights từ các anh chị em data scientists, ML engineers có kinh nghiệm trong các bài toán computer vision để làm rõ các giả thiết về nhu cầu thực tiễn. 2- Giao lưu với cộng đồng AI/ML ở Việt Nam để tăng cường chia sẻ thông tin & cơ hội Em/ mình rất mong được hỗ trợ 1- Giới thiệu một số anh chị em trong ngành có thể dành ra 20-30p chia sẻ với team em/mình về ý tưởng này 2- Giới thiệu một số kênh giao lưu hiệu quả. Hoặc nếu có anh chị em nào đã (hoặc sẽ) tham gia VinAI hoặc AI Summit thì mong được chia sẻ kinh nghiệm đăng ký workshop/expo ở đấy ạ :D Cảm ơn mọi người rất nhiều",,,,,
"Em chào mọi người ạ.
HIện tại e đang tập tành về Data Analyst. Em có một câu hỏi rất mong các a/c làm trong lĩnh vực này giải đáp ạ. Đó là khi lựa chọn đặc trưng để cho vào mô hình huấn luyện ( giả sử cho bài toán phân loại) . Em có
MỌi người thường tìm insight từ data như thế nào ngoài việc phân tích EDA ra ạ (giả sử đã qua bước tìm hiểu nghiệp vụ về thuộc tính đó rồi). E thấy phân tích EDA chỉ ra được insight đối với các bộ dữ liệu đẹp trên mạng, thực tế thì dữ liệu bên ngoài nó ko như thế ( tức chồng lấn lên nhau)
Dữ liệu thực tế e thấy rất hỗn độn. Trong thực tế các a/c có khi nào thu thập phải dữ liệu mà ko có tác dụng cho mô hình ko ạ
Em cảm ơn mn nhiều","Em chào mọi người ạ. HIện tại e đang tập tành về Data Analyst. Em có một câu hỏi rất mong các a/c làm trong lĩnh vực này giải đáp ạ. Đó là khi lựa chọn đặc trưng để cho vào mô hình huấn luyện ( giả sử cho bài toán phân loại) . Em có MỌi người thường tìm insight từ data như thế nào ngoài việc phân tích EDA ra ạ (giả sử đã qua bước tìm hiểu nghiệp vụ về thuộc tính đó rồi). E thấy phân tích EDA chỉ ra được insight đối với các bộ dữ liệu đẹp trên mạng, thực tế thì dữ liệu bên ngoài nó ko như thế ( tức chồng lấn lên nhau) Dữ liệu thực tế e thấy rất hỗn độn. Trong thực tế các a/c có khi nào thu thập phải dữ liệu mà ko có tác dụng cho mô hình ko ạ Em cảm ơn mn nhiều",,,,,
"Em chào các anh chị,
Hiện tại em là sinh viên năm 3 đại học Bách khoa, kì này nhóm em đang làm đồ án 1 nhiệm vụ là Viết website bán hàng bằng web framework django và kết hợp machine cho hệ thống recommendation ( gợi ý sản phẩm cho khách hàng). Em đang làm xong các phần cơ bản cho trang web, train và test dữ liệu trên kaggle nhưng khi đưa vào trang web đang gặp một số vấn đề:
+dữ liệu quá lớn không thể tự thêm các sản phẩm, hình ảnh vào web
+ Quá nhiều cách để thực hiện giao tiếp API như Flask, REST, GraphQL, hiện tại em không biết cái nào tốt nhất
+Em đã xem trên youtube về GraphQL cho django nhưng chỉ giới thiệu qua và khi đưa vào trong django lại không hoạt động được.
=>> Các anh chi có thể cho em lời khuyên và hướng dẫn được không ạ.
Em cảm ơn nhiều ạ.","Em chào các anh chị, Hiện tại em là sinh viên năm 3 đại học Bách khoa, kì này nhóm em đang làm đồ án 1 nhiệm vụ là Viết website bán hàng bằng web framework django và kết hợp machine cho hệ thống recommendation ( gợi ý sản phẩm cho khách hàng). Em đang làm xong các phần cơ bản cho trang web, train và test dữ liệu trên kaggle nhưng khi đưa vào trang web đang gặp một số vấn đề: +dữ liệu quá lớn không thể tự thêm các sản phẩm, hình ảnh vào web + Quá nhiều cách để thực hiện giao tiếp API như Flask, REST, GraphQL, hiện tại em không biết cái nào tốt nhất +Em đã xem trên youtube về GraphQL cho django nhưng chỉ giới thiệu qua và khi đưa vào trong django lại không hoạt động được. =>> Các anh chi có thể cho em lời khuyên và hướng dẫn được không ạ. Em cảm ơn nhiều ạ.",,,,,
"chào mọi người.
Có anh chị nào đã từng dùng api của google text to speech chưa ạ. Em đang gặp vấn đề không biết cách lấy phoneme (để làm mục đích khác ). Anh chị (các bạn) đã từng làm rồi có thể cho em keyword hoặc hướng dẫn em cách làm không ạ. Em cảm ơn .",chào mọi người. Có anh chị nào đã từng dùng api của google text to speech chưa ạ. Em đang gặp vấn đề không biết cách lấy phoneme (để làm mục đích khác ). Anh chị (các bạn) đã từng làm rồi có thể cho em keyword hoặc hướng dẫn em cách làm không ạ. Em cảm ơn .,,,,,
"Em đang học nên mạnh dạn chia sẻ cùng anh em một bài toán mới trong lĩnh vực Computer Vision là Intrusion Detection - phát hiện đột nhập.
Hi vọng giúp được anh em làm đồ án và mới học.
Cảm ơn các bác!",Em đang học nên mạnh dạn chia sẻ cùng anh em một bài toán mới trong lĩnh vực Computer Vision là Intrusion Detection - phát hiện đột nhập. Hi vọng giúp được anh em làm đồ án và mới học. Cảm ơn các bác!,,,,,
"Em chào mọi người.
Em đang làm đồ án đề tài ""Nghiên cứu thuật toán nhận diện đèn tín hiệu giao thông"". Em có tham khảo và có ý định phát triển thuật toán theo link github này https://github.com/SandeepAswathnarayana/traffic-light-classifier_faster-r-cnn
Em đã tự thu thập data riêng của em với hơn 300 ảnh, em định gán 3 nhãn đơn giản là ""Green"", ""Yellow"", ""Red"". Nhưng em đang không biết triển khai bộ data mới của em thay thế bộ data cũ của người ta như thế nào ạ.
Mọi người cho em xin ý kiến giúp đỡ với ạ!","Em chào mọi người. Em đang làm đồ án đề tài ""Nghiên cứu thuật toán nhận diện đèn tín hiệu giao thông"". Em có tham khảo và có ý định phát triển thuật toán theo link github này https://github.com/SandeepAswathnarayana/traffic-light-classifier_faster-r-cnn Em đã tự thu thập data riêng của em với hơn 300 ảnh, em định gán 3 nhãn đơn giản là ""Green"", ""Yellow"", ""Red"". Nhưng em đang không biết triển khai bộ data mới của em thay thế bộ data cũ của người ta như thế nào ạ. Mọi người cho em xin ý kiến giúp đỡ với ạ!",,,,,
"Dạ chào mọi người, em đang tập làm Chatbot, có theo dõi Series của anh Thắng. Hiện tại em đang bị vướng ngay bước đầu là cài đặt Rasa.
Python em dùng version: 8.0
Pip version 22.1.2
Em cài đặt Rasa theo 2 cách:
pip install rasa (Chạy cái này thì nó bảo là required tensorflow)
pip3 install rasa ( Ra log bên hình dưới )
Nhưng bị báo lỗi, em đã thử upgrade python lên bản 3.9 or 3.10 nhưng nó vẫn bị. Nhờ mọi người giúp đỡ
Cảm ơn mọi người
#Chatbot
#Rasa","Dạ chào mọi người, em đang tập làm Chatbot, có theo dõi Series của anh Thắng. Hiện tại em đang bị vướng ngay bước đầu là cài đặt Rasa. Python em dùng version: 8.0 Pip version 22.1.2 Em cài đặt Rasa theo 2 cách: pip install rasa (Chạy cái này thì nó bảo là required tensorflow) pip3 install rasa ( Ra log bên hình dưới ) Nhưng bị báo lỗi, em đã thử upgrade python lên bản 3.9 or 3.10 nhưng nó vẫn bị. Nhờ mọi người giúp đỡ Cảm ơn mọi người",#Chatbot	#Rasa,,,,
"Em chào mn ạ. Em có 1 topic về forecast time series ( có thêm 1 số biến giả categorical nữa) Em mới sử dụng 1 số techique kiểu MA, exponential, holt-winter. Em đang gặp khó khăn ở chỗ là chưa biết hệ thống hóa các technique/model để phân loại là với data như nào thì nên dùng model gì ạ, mà em đang cần khá gấp. Nhờ các anh chị guide giúp em chỗ này ạ. Em cảm ơn <3 <3 <3","Em chào mn ạ. Em có 1 topic về forecast time series ( có thêm 1 số biến giả categorical nữa) Em mới sử dụng 1 số techique kiểu MA, exponential, holt-winter. Em đang gặp khó khăn ở chỗ là chưa biết hệ thống hóa các technique/model để phân loại là với data như nào thì nên dùng model gì ạ, mà em đang cần khá gấp. Nhờ các anh chị guide giúp em chỗ này ạ. Em cảm ơn <3 <3 <3",,,,,
"Chào mọi người, hiện tại em đang nghiên cứu về chủ đề giải mã Captcha kí tự số, thông tin ảnh dựa trên mô hình máy học.
Em cũng đã nghiên cứu qua các mô hình Captcha hiện nay, tuy nhiên dữ liệu chưa phong phú, mọi người có giải pháp hay mô hình đã xây dựng sẵn thì cho em học hỏi thêm. Cảm ơn mọi người.","Chào mọi người, hiện tại em đang nghiên cứu về chủ đề giải mã Captcha kí tự số, thông tin ảnh dựa trên mô hình máy học. Em cũng đã nghiên cứu qua các mô hình Captcha hiện nay, tuy nhiên dữ liệu chưa phong phú, mọi người có giải pháp hay mô hình đã xây dựng sẵn thì cho em học hỏi thêm. Cảm ơn mọi người.",,,,,
"E đang làm bài tập nlp, đề bài cho 1 câu văn ( tiếng việt), fix các từ trong câu (thay bằng 1 từ trong data vocab có sẵn) sao cho câu thành 1 câu văn đúng ngữ pháp, ai có tài liệu cho e xin với ạ","E đang làm bài tập nlp, đề bài cho 1 câu văn ( tiếng việt), fix các từ trong câu (thay bằng 1 từ trong data vocab có sẵn) sao cho câu thành 1 câu văn đúng ngữ pháp, ai có tài liệu cho e xin với ạ",,,,,
playlist giới thiệu về một số cách xử lý dữ liệu khuyết cơ bản trong python,playlist giới thiệu về một số cách xử lý dữ liệu khuyết cơ bản trong python,,,,,
"Em chào các anh chị ạ. Em là người ngoại đạo đang tập tành học về ML nên có một số bài tập về phân loại 10 lớp với Softmax và phân loại hai lớp với logistic regression muốn xin được chỉ dạy và giải đáp từ các cao nhân về ML ạ.
Đặc biệt phần Class Softmax Classifier (Bài TODO 13-16)
Em xin đính kèm link Google Collab và một số hình ảnh đoạn code cần hoàn thành ạ.
Em xin trân trọng cảm ơn tất cả mọi người ạ.
https://colab.research.google.com/drive/1azkkce0FU4F3hN_evuj8vwWyfSkcBUhs?usp=sharing",Em chào các anh chị ạ. Em là người ngoại đạo đang tập tành học về ML nên có một số bài tập về phân loại 10 lớp với Softmax và phân loại hai lớp với logistic regression muốn xin được chỉ dạy và giải đáp từ các cao nhân về ML ạ. Đặc biệt phần Class Softmax Classifier (Bài TODO 13-16) Em xin đính kèm link Google Collab và một số hình ảnh đoạn code cần hoàn thành ạ. Em xin trân trọng cảm ơn tất cả mọi người ạ. https://colab.research.google.com/drive/1azkkce0FU4F3hN_evuj8vwWyfSkcBUhs?usp=sharing,,,,,
"💡 How to grow as an AI Engineer? 💡
by Nguyễn Văn Tâm, AI Engineer @ Instill AI
https://www.facebook.com/instilltech/posts/565849418605636
#AIEngineer #AI #career

After 7 years of working as a software engineer (firmware/middleware, mobile and backend application), I switched to AI engineer role 4 years ago. Here are my takeaways from a +10-year software industry journey:
❶ Polish your MLOps tooling skill
❷  Monitor production domain drift and iterate fast
❸ Collaborate across different roles","How to grow as an AI Engineer? by Nguyễn Văn Tâm, AI Engineer @ Instill AI https://www.facebook.com/instilltech/posts/565849418605636 After 7 years of working as a software engineer (firmware/middleware, mobile and backend application), I switched to AI engineer role 4 years ago. Here are my takeaways from a +10-year software industry journey: Polish your MLOps tooling skill Monitor production domain drift and iterate fast Collaborate across different roles",#AIEngineer	#AI	#career,,,,
"[Mô hình Minerva - Giải các bài toán phức tạp]
Gần đây, các mô hình ngôn ngữ cho ra các kết quả ấn tượng trong nhiều bài toán xử lý ngôn ngữ tự nhiên, như BERT, GPT-3, Gopher và PaLM. Điển hình có thể kể đến các mô hình sinh ra code, sinh ra thơ,...
Trong bài báo “Solving Quantitative Reasoning Problems With Language Models”, nhóm tác giả từ Google đề xuất mô hình Minerva với khả năng giải các câu hỏi toán học từng bước một.
Chi tiết mọi người xem ở đây: https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html","[Mô hình Minerva - Giải các bài toán phức tạp] Gần đây, các mô hình ngôn ngữ cho ra các kết quả ấn tượng trong nhiều bài toán xử lý ngôn ngữ tự nhiên, như BERT, GPT-3, Gopher và PaLM. Điển hình có thể kể đến các mô hình sinh ra code, sinh ra thơ,... Trong bài báo “Solving Quantitative Reasoning Problems With Language Models”, nhóm tác giả từ Google đề xuất mô hình Minerva với khả năng giải các câu hỏi toán học từng bước một. Chi tiết mọi người xem ở đây: https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html",,,,,
"Chào mọi người,
Tháng 7 này nếu anh chị nào ở Hà Nội quan tâm về Web Development, ML/AI, Google Developer Expert,…v.v… cùng nhiều chủ đề khác và gặp gỡ các speaker từ Google, em xin giới thiệu mọi người sự kiện này nè: https://www.facebook.com/events/1517940821993351
👉🏻 Link đăng ký: https://forms.gle/EtsCHAuXSwkK8rVL9Đã lâu rồi mới có sự kiện của GDG offline sau dịch, hi vọng mọi người sẽ thích thú và tham gia với những siêu quyền lợi:
👉🏻 Trở thành những người đầu tiên tham dự sự kiện Offline trở lại của GDGHanoi!
👉🏻 Gặp gỡ những chuyên gia Công nghệ cùng những bài chia sẻ chuyên sâu!
👉🏻 Trải nghiệm các gian hàng với nhiều hoạt động lý thú!
👉🏻 Gặp gỡ cộng đồng Công nghệ sau 2 năm xa cách!
👉🏻 Và hàng trăm phần quá hấp dẫn, quyền lợi khác tại ngày diễn ra sự kiện!
📌 Đăng ký ngay tại: https://forms.gle/EtsCHAuXSwkK8rVL9","Chào mọi người, Tháng 7 này nếu anh chị nào ở Hà Nội quan tâm về Web Development, ML/AI, Google Developer Expert,…v.v… cùng nhiều chủ đề khác và gặp gỡ các speaker từ Google, em xin giới thiệu mọi người sự kiện này nè: https://www.facebook.com/events/1517940821993351 Link đăng ký: https://forms.gle/EtsCHAuXSwkK8rVL9Đã lâu rồi mới có sự kiện của GDG offline sau dịch, hi vọng mọi người sẽ thích thú và tham gia với những siêu quyền lợi: Trở thành những người đầu tiên tham dự sự kiện Offline trở lại của GDGHanoi! Gặp gỡ những chuyên gia Công nghệ cùng những bài chia sẻ chuyên sâu! Trải nghiệm các gian hàng với nhiều hoạt động lý thú! Gặp gỡ cộng đồng Công nghệ sau 2 năm xa cách! Và hàng trăm phần quá hấp dẫn, quyền lợi khác tại ngày diễn ra sự kiện! Đăng ký ngay tại: https://forms.gle/EtsCHAuXSwkK8rVL9",,,,,
"Hi a/c/e!
Mình có vấn đề này mong nhận được sự giúp đỡ. Team mình đang đưa các mô hình DL xuống mobile dùng tflife của google. Hiện team đã convert được mô hình. Tuy nhiên, để sử dụng được mô hình thì cần viết thêm code postprocess. Trên python thì code này khá dễ chỉ cần dùng các thư viện numpy, opencv code một chút là được - ko khó đối với AI Engineer. Tuy nhiên, việc code postprocess đối với a/e coder Java (Android), Swift (iOS) thì khá là cực do ko có chuyên về thuật toán, về xử lý ảnh, về AI/DL. Không biết có giải pháp nào để đóng gói thành lib để bên app chỉ cần truyền input (ảnh) và nhận đầu ra là các class luôn ko?
Nhờ a/c/e tư vấn giúp. Many thanks ~
----
Ở trên mình mô tả bài toán chung. Bài toán cụ thể của mình là viết một chương trình AI chạy trên mobile nhận đầu vào là ảnh chụp khuôn mặt, đầu ra là kết quả phân loại mặt đang quay trái, quay phải, chính diện, ngước lên trên hay cúi xuống. Mình dùng mô hình AI để xác định 68 điểm landmark trên khuôn mặt. Từ 68 điểm landmark sẽ dùng các phép toán học để tính ra yaw, pitch và roll. Dựa vào yaw, pitch, roll để xác định hướng khuôn mặt.
Mình đã dùng tflife để convert mô hình xác định lamdmark sang định đạng trên mobile. Tuy nhiên, a/e dev mobile thì đang kêu khó khi viết code thuật toán hậu xử lý tiếp theo.
Mình cũng ko có rõ về mobile nên nhờ các a/c/e tư vấn giúp.","Hi a/c/e! Mình có vấn đề này mong nhận được sự giúp đỡ. Team mình đang đưa các mô hình DL xuống mobile dùng tflife của google. Hiện team đã convert được mô hình. Tuy nhiên, để sử dụng được mô hình thì cần viết thêm code postprocess. Trên python thì code này khá dễ chỉ cần dùng các thư viện numpy, opencv code một chút là được - ko khó đối với AI Engineer. Tuy nhiên, việc code postprocess đối với a/e coder Java (Android), Swift (iOS) thì khá là cực do ko có chuyên về thuật toán, về xử lý ảnh, về AI/DL. Không biết có giải pháp nào để đóng gói thành lib để bên app chỉ cần truyền input (ảnh) và nhận đầu ra là các class luôn ko? Nhờ a/c/e tư vấn giúp. Many thanks ~ ---- Ở trên mình mô tả bài toán chung. Bài toán cụ thể của mình là viết một chương trình AI chạy trên mobile nhận đầu vào là ảnh chụp khuôn mặt, đầu ra là kết quả phân loại mặt đang quay trái, quay phải, chính diện, ngước lên trên hay cúi xuống. Mình dùng mô hình AI để xác định 68 điểm landmark trên khuôn mặt. Từ 68 điểm landmark sẽ dùng các phép toán học để tính ra yaw, pitch và roll. Dựa vào yaw, pitch, roll để xác định hướng khuôn mặt. Mình đã dùng tflife để convert mô hình xác định lamdmark sang định đạng trên mobile. Tuy nhiên, a/e dev mobile thì đang kêu khó khi viết code thuật toán hậu xử lý tiếp theo. Mình cũng ko có rõ về mobile nên nhờ các a/c/e tư vấn giúp.",,,,,
"Em xin chào anh chị trong nhóm. Em có 1 xíu thắc mắc mong anh chị giúp đỡ ạ.
Trong dataset của em có 2 phương thức là GET và POST, thì chỉ có POST mới dùng dữ liệu cột length, content còn GET thì không có.
Vậy em nên xử lý các giá trị NaN hay là chia ra 1 cái làm cho POST 1 cái làm cho GET ạ, hay làm 1 cách khác a ? Em cảm ơn anh chị đã dành thời gian đọc bài viết","Em xin chào anh chị trong nhóm. Em có 1 xíu thắc mắc mong anh chị giúp đỡ ạ. Trong dataset của em có 2 phương thức là GET và POST, thì chỉ có POST mới dùng dữ liệu cột length, content còn GET thì không có. Vậy em nên xử lý các giá trị NaN hay là chia ra 1 cái làm cho POST 1 cái làm cho GET ạ, hay làm 1 cách khác a ? Em cảm ơn anh chị đã dành thời gian đọc bài viết",,,,,
"Tháng 9 này, mình sẽ dạy môn ""Statistical Optimization in Machine Learning and Data Science"" tại University of Texas, Austin. Lớp này là Advanced Level dành cho các bạn muốn làm nghiên cứu về ML, DS, và Deep Learning (DL).
Các bài giảng của lớp này mình sẽ đăng trên trang web cá nhân của mình tại đây: https://nhatptnk8912.github.io/index.html
+++++ Về syllabus của lớp thì gồm hai chủ đề lớn sau:
***** Foundation of Optimization Methods:
--- Gradient Descent for Constrained and Unconstrained problems
--- Adaptive Gradient Descent (Adagrad, RMSProp, etc.)
--- Accelerated Gradient Descent Methods
--- Stochastic Gradient Descent and Its Variants
--- Newton’s Method and Quasi-Newton’s Method
***** Applications of Optimization Methods for Machine Learning Models:
--- Computational Optimal Transport: Old and New
--- (Stochastic) Variational Inference, such as Mean-Field approximation, etc.
--- Optimization Landscape of Deep Neural Networks
--- Unsupervised Methods (e.g., K-means, Gaussian Mixture Models, Factor Analysis, PCA, etc.)
--- Minimax Optimization in Deep Generative Models
--- Federated Learning
++++++ Các bạn nào quan tâm thì theo dõi nhé.","Tháng 9 này, mình sẽ dạy môn ""Statistical Optimization in Machine Learning and Data Science"" tại University of Texas, Austin. Lớp này là Advanced Level dành cho các bạn muốn làm nghiên cứu về ML, DS, và Deep Learning (DL). Các bài giảng của lớp này mình sẽ đăng trên trang web cá nhân của mình tại đây: https://nhatptnk8912.github.io/index.html +++++ Về syllabus của lớp thì gồm hai chủ đề lớn sau: ***** Foundation of Optimization Methods: --- Gradient Descent for Constrained and Unconstrained problems --- Adaptive Gradient Descent (Adagrad, RMSProp, etc.) --- Accelerated Gradient Descent Methods --- Stochastic Gradient Descent and Its Variants --- Newton’s Method and Quasi-Newton’s Method ***** Applications of Optimization Methods for Machine Learning Models: --- Computational Optimal Transport: Old and New --- (Stochastic) Variational Inference, such as Mean-Field approximation, etc. --- Optimization Landscape of Deep Neural Networks --- Unsupervised Methods (e.g., K-means, Gaussian Mixture Models, Factor Analysis, PCA, etc.) --- Minimax Optimization in Deep Generative Models --- Federated Learning ++++++ Các bạn nào quan tâm thì theo dõi nhé.",,,,,
"Đăng lại bài từ group Viet Tech vì group đó kín.

Hôm trước trong buổi nói chuyện với Viet Tech mình có đề cập tới việc có thể start training model và đưa vào online serving trong vòng 1 ngày khi làm việc ở Google. Mình xin tóm tắt lại những hiểu biết của minh dựa trên công việc hàng ngày mình làm. Xin lỗi   mình viết nhanh nên còn nhiều cụm còn để tiếng Anh.
Lưu ý: Google có rất nhiều team với nhiều bài toán và infrastructure khác nhau. Những chia sẻ trong bài này dựa trên công việc hàng ngày của mình. Các Googlers ở team khác có thể sẽ thấy những sự khác biệt.
DATA
Có hai loại dữ liệu chính là thô và tinh. Dữ liệu thô được lưu gần như theo thời gian thực vào cơ sở dữ liệu lúc serving. Dữ liệu thô này được xử lý và tạo thành các feature mà mô hình ML có thể dùng được – tạm gọi là dữ liệu tinh, có độ trễ khoảng vài giờ so với thời gian thực. Các bộ dữ liệu này có TTL và liên tục được cập nhật theo stream của dữ liệu serving.
Dữ liệu tinh được sử dụng để huấn luyện các model. Thường thì vì dữ liệu lớn nên model chỉ cần train mỗi điểm dữ liệu một lần theo trình tự thời gian từ cũ đến mới. Mỗi model khi training có training delay nhất định, tuỳ thuộc vào độ trễ của dữ liệu tinh và label của bài toán. Có những bài toán mà label tới muộn sau cả tuần, e.g. khách hàng mua một sản phẩm sau khi nhìn thấy mẩu quảng cáo một tuần trước, thì training delay có thể là một tuần. Một khi model đã train tới dữ liệu ở thời điểm training delay thì ta gọi là model đã caught up. Khi đã caught up, model dừng ở đó và chờ khi nào có dữ liệu mới thì tự động train tiếp.
Ngay cả khi model đã đưa vào trong serving, việc training vẫn tiếp tục diễn ra như vậy. Model checkpoint được inject liên tục sau vài tiếng. Điều này nghĩa là model được thay đổi liên tục ngay cả khi đã deploy, chỉ có model architecture là được giữ nguyên.
HARDWARE AND RESOURCES
Trước khi caught up, model sử dụng rất nhiều computing resources (đang chuyển dần sang hoàn toàn dùng TPU) để có thể train hết lượng dữ liệu từ xa trong quá khứ cho tới khi caught up. Một khi đã caught up, model tự động chuyển sang chế độ sử dụng ít tài nguyên hơn và nhường tài nguyên cho các model khác.
Đôi khi vì lý do nào đó mà model bị dừng huấn luyện và “stale"". Stale rất nguy hiểm vì có dữ liệu trending có thể thay đổi hàng ngày nên team mình rất chú trọng vào việc monitor xem model có bị stale hay không. Với các mô hình trong production, khi bị stale, nó sẽ tự động lấy computing resources từ các experimental/offline model khác để đuổi kịp tới training delay.
Nhìn chung, hardware cho training và serving là khác nhau vì hai tasks này có nhiều điểm không tương đồng. Việc sử dụng hardware khác nhau này đôi khi dẫn đến những kết quả khác nhau giữa training và serving, chủ yếu do quantization. Vì vậy, có một bước quan trọng trước khi inject model checkpoint là validation. Tại bước validation, hardware tương tự như trong serving được sử dụng. Rất nhiều metrics có thể được kiểm duyệt ở bước này trước khi checkpoints mới được injected. Dữ liệu tại bước validation này thường là dữ liệu mới nhất trong dữ liệu tinh mà model chưa nhìn thấy.
OFFLINE EVALUATION
Bước validation mình nói ở trên được thực hiện tự động. Tất nhiên, không phải checkpoint nào đã pass validation đều có thể đưa lên serving được.
Trước khi có thể lên serving, các team thường phải kiểm tra kỹ các metrics liên quan và so sánh các metrics đó với baseline. Cái này có tool riêng rất powerful giúp ML engineers có thể so sánh trực tiếp nhiều model cùng nhau trên nhiều traffic slices khác nhau. Việc này rất quan trọng trước khi setup online experiment mới.
ONLINE EXPERIMENT
Một mô hình có offline metrics tốt chưa chắc đã mang lại hiệu quả trong online experiment vì có nhiều quan hệ phức tạp khi dự đoán của mô hình ảnh hưởng tới kết quả serving. Các online metrics cũng khác so với offline metrics khi các con số thực sự về revenue được quan tâm. Vì vậy, các online experiment được thực hiện từ lượng traffic từ nhỏ đến lớn và có thể bị dừng ngay nếu kết quả bất thường.
Việc đưa một mô hình từ offline sang online chỉ mất khoảng vài tiếng, chủ yếu là thời gian chờ việc phân bổ tài nguyên và chạy unit tests. Việc này team mình không phải làm gì nhiều mà có các team khác chuyên lo các vấn đề này.
MONITORING SYSTEM
Hệ thống monitoring có một nhiệm vụ rất quan trọng đối với online experiment. Với mỗi model, các team cần thiết lập các hệ thống theo dõi và báo động cho nhiều metrics khác nhau. Label, prediction, model staleness, skewed features là bốn trong số những thứ quan trọng nhất cần được monitor. Khi có dấu hiệu thay đổi trong những giá trị này thì oncall/SRE sẽ bị báo động và phải tìm hiểu nguyên nhân cũng như khắc phục trong thời gian ngắn nhất có thể.
Cuối cùng, hệ thống ML của Gg khá mature, mỗi một thành phần nhỏ đều có một đội ngũ đông đảo các engineers quản lý, theo dõi và nâng cấp. Quy trình xử lý sự cố cũng được ghi chép lại kỹ càng để hạn chế những tai nạn trong serving. Chính vì việc có những hệ thống cảnh báo nhiều lớp này mà rất nhiều các experiment có thể được thực hiện một cách nhanh chóng và ít gây rủi ro.
Tóm lại, những lý do chính khiến việc thực hiện các thí nghiệm ở Gg có thể diễn ra nhanh chóng:
Data pipeline giữa training và serving được theo dõi và đảm bảo ít có skew.
Computing resources lớn (TPU) và được quản lý một cách hiệu quả.
Có những công cụ giúp việc đánh giá kết quả offline một cách nhanh chóng.
Hệ thống hỗ trợ online experiment hiệu quả.
Hệ thống monitor các thí nghiệm cũng được thiết lập giúp các engineers xử lý kịp thời khi có incidents.","Đăng lại bài từ group Viet Tech vì group đó kín. Hôm trước trong buổi nói chuyện với Viet Tech mình có đề cập tới việc có thể start training model và đưa vào online serving trong vòng 1 ngày khi làm việc ở Google. Mình xin tóm tắt lại những hiểu biết của minh dựa trên công việc hàng ngày mình làm. Xin lỗi mình viết nhanh nên còn nhiều cụm còn để tiếng Anh. Lưu ý: Google có rất nhiều team với nhiều bài toán và infrastructure khác nhau. Những chia sẻ trong bài này dựa trên công việc hàng ngày của mình. Các Googlers ở team khác có thể sẽ thấy những sự khác biệt. DATA Có hai loại dữ liệu chính là thô và tinh. Dữ liệu thô được lưu gần như theo thời gian thực vào cơ sở dữ liệu lúc serving. Dữ liệu thô này được xử lý và tạo thành các feature mà mô hình ML có thể dùng được – tạm gọi là dữ liệu tinh, có độ trễ khoảng vài giờ so với thời gian thực. Các bộ dữ liệu này có TTL và liên tục được cập nhật theo stream của dữ liệu serving. Dữ liệu tinh được sử dụng để huấn luyện các model. Thường thì vì dữ liệu lớn nên model chỉ cần train mỗi điểm dữ liệu một lần theo trình tự thời gian từ cũ đến mới. Mỗi model khi training có training delay nhất định, tuỳ thuộc vào độ trễ của dữ liệu tinh và label của bài toán. Có những bài toán mà label tới muộn sau cả tuần, e.g. khách hàng mua một sản phẩm sau khi nhìn thấy mẩu quảng cáo một tuần trước, thì training delay có thể là một tuần. Một khi model đã train tới dữ liệu ở thời điểm training delay thì ta gọi là model đã caught up. Khi đã caught up, model dừng ở đó và chờ khi nào có dữ liệu mới thì tự động train tiếp. Ngay cả khi model đã đưa vào trong serving, việc training vẫn tiếp tục diễn ra như vậy. Model checkpoint được inject liên tục sau vài tiếng. Điều này nghĩa là model được thay đổi liên tục ngay cả khi đã deploy, chỉ có model architecture là được giữ nguyên. HARDWARE AND RESOURCES Trước khi caught up, model sử dụng rất nhiều computing resources (đang chuyển dần sang hoàn toàn dùng TPU) để có thể train hết lượng dữ liệu từ xa trong quá khứ cho tới khi caught up. Một khi đã caught up, model tự động chuyển sang chế độ sử dụng ít tài nguyên hơn và nhường tài nguyên cho các model khác. Đôi khi vì lý do nào đó mà model bị dừng huấn luyện và “stale"". Stale rất nguy hiểm vì có dữ liệu trending có thể thay đổi hàng ngày nên team mình rất chú trọng vào việc monitor xem model có bị stale hay không. Với các mô hình trong production, khi bị stale, nó sẽ tự động lấy computing resources từ các experimental/offline model khác để đuổi kịp tới training delay. Nhìn chung, hardware cho training và serving là khác nhau vì hai tasks này có nhiều điểm không tương đồng. Việc sử dụng hardware khác nhau này đôi khi dẫn đến những kết quả khác nhau giữa training và serving, chủ yếu do quantization. Vì vậy, có một bước quan trọng trước khi inject model checkpoint là validation. Tại bước validation, hardware tương tự như trong serving được sử dụng. Rất nhiều metrics có thể được kiểm duyệt ở bước này trước khi checkpoints mới được injected. Dữ liệu tại bước validation này thường là dữ liệu mới nhất trong dữ liệu tinh mà model chưa nhìn thấy. OFFLINE EVALUATION Bước validation mình nói ở trên được thực hiện tự động. Tất nhiên, không phải checkpoint nào đã pass validation đều có thể đưa lên serving được. Trước khi có thể lên serving, các team thường phải kiểm tra kỹ các metrics liên quan và so sánh các metrics đó với baseline. Cái này có tool riêng rất powerful giúp ML engineers có thể so sánh trực tiếp nhiều model cùng nhau trên nhiều traffic slices khác nhau. Việc này rất quan trọng trước khi setup online experiment mới. ONLINE EXPERIMENT Một mô hình có offline metrics tốt chưa chắc đã mang lại hiệu quả trong online experiment vì có nhiều quan hệ phức tạp khi dự đoán của mô hình ảnh hưởng tới kết quả serving. Các online metrics cũng khác so với offline metrics khi các con số thực sự về revenue được quan tâm. Vì vậy, các online experiment được thực hiện từ lượng traffic từ nhỏ đến lớn và có thể bị dừng ngay nếu kết quả bất thường. Việc đưa một mô hình từ offline sang online chỉ mất khoảng vài tiếng, chủ yếu là thời gian chờ việc phân bổ tài nguyên và chạy unit tests. Việc này team mình không phải làm gì nhiều mà có các team khác chuyên lo các vấn đề này. MONITORING SYSTEM Hệ thống monitoring có một nhiệm vụ rất quan trọng đối với online experiment. Với mỗi model, các team cần thiết lập các hệ thống theo dõi và báo động cho nhiều metrics khác nhau. Label, prediction, model staleness, skewed features là bốn trong số những thứ quan trọng nhất cần được monitor. Khi có dấu hiệu thay đổi trong những giá trị này thì oncall/SRE sẽ bị báo động và phải tìm hiểu nguyên nhân cũng như khắc phục trong thời gian ngắn nhất có thể. Cuối cùng, hệ thống ML của Gg khá mature, mỗi một thành phần nhỏ đều có một đội ngũ đông đảo các engineers quản lý, theo dõi và nâng cấp. Quy trình xử lý sự cố cũng được ghi chép lại kỹ càng để hạn chế những tai nạn trong serving. Chính vì việc có những hệ thống cảnh báo nhiều lớp này mà rất nhiều các experiment có thể được thực hiện một cách nhanh chóng và ít gây rủi ro. Tóm lại, những lý do chính khiến việc thực hiện các thí nghiệm ở Gg có thể diễn ra nhanh chóng: Data pipeline giữa training và serving được theo dõi và đảm bảo ít có skew. Computing resources lớn (TPU) và được quản lý một cách hiệu quả. Có những công cụ giúp việc đánh giá kết quả offline một cách nhanh chóng. Hệ thống hỗ trợ online experiment hiệu quả. Hệ thống monitor các thí nghiệm cũng được thiết lập giúp các engineers xử lý kịp thời khi có incidents.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 6/2022 vào trong comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 6/2022 vào trong comment của post này.",,,,,
"Làm cách nào để xây dựng một trang web cho mô hình dự đoán sử dụng thuật toán Machine Learning - XGBOOST?
Hi mọi người,
Video bên dưới mình chia sẻ cách cơ bản và đơn giản để xây dựng một trang Web Local cho việc dự đoán xem liệu nhân viên có rời bỏ công ty của mình hay không? Mình sẽ sử dụng mô hình XGBOOST, Flask API, JavaScripts, HTML, CSS để làm việc này.
Dựa trên data mẫu có sẵn mình sẽ thực hiện các bước sau:
1. Đọc dữ liệu - nhân viên có rời bỏ công ty hay không? 2. Chia dữ liệu thành tập train và test  3. Áp dụng mô hình XGBOOST (sử dụng các tham số mặc định, ở đây mình không hướng dẫn cách điều chỉnh/lựa chọn tham số tốt nhất cho mô hình) 4. Thử dự đoán với data mẫu 5. Lưu model bằng pickle/joblib 6. Viết API bằng Flask cho mô hình dự đoán 7. Test API bằng Postman 8. Xây dựng trang Web Local bằng JavaScripts, HTML, CSS
Done!
Thanks All! Enjoy your weekend
https://youtu.be/CNvPUH41eHU","Làm cách nào để xây dựng một trang web cho mô hình dự đoán sử dụng thuật toán Machine Learning - XGBOOST? Hi mọi người, Video bên dưới mình chia sẻ cách cơ bản và đơn giản để xây dựng một trang Web Local cho việc dự đoán xem liệu nhân viên có rời bỏ công ty của mình hay không? Mình sẽ sử dụng mô hình XGBOOST, Flask API, JavaScripts, HTML, CSS để làm việc này. Dựa trên data mẫu có sẵn mình sẽ thực hiện các bước sau: 1. Đọc dữ liệu - nhân viên có rời bỏ công ty hay không? 2. Chia dữ liệu thành tập train và test 3. Áp dụng mô hình XGBOOST (sử dụng các tham số mặc định, ở đây mình không hướng dẫn cách điều chỉnh/lựa chọn tham số tốt nhất cho mô hình) 4. Thử dự đoán với data mẫu 5. Lưu model bằng pickle/joblib 6. Viết API bằng Flask cho mô hình dự đoán 7. Test API bằng Postman 8. Xây dựng trang Web Local bằng JavaScripts, HTML, CSS Done! Thanks All! Enjoy your weekend https://youtu.be/CNvPUH41eHU",,,,,
"*Góc nhờ vả*
Chào tất cả mọi người, m đang theo đuổi đề tài xây dựng hệ thống việc làm dựa vào Content-based filtering và Natural Language Processing, m đang sử dụng US technology Jobs #dataset, thực tế là m muốn làm một hệ thống giới thiệu việc làm free cho người Việt, xin hỏi ace trong group ai có nguồn dataset cho hệ thống gợi ý việc làm không ạ, nếu được cho m xin được chia sẻ.
Cám ơn ace trước ạ.","*Góc nhờ vả* Chào tất cả mọi người, m đang theo đuổi đề tài xây dựng hệ thống việc làm dựa vào Content-based filtering và Natural Language Processing, m đang sử dụng US technology Jobs thực tế là m muốn làm một hệ thống giới thiệu việc làm free cho người Việt, xin hỏi ace trong group ai có nguồn dataset cho hệ thống gợi ý việc làm không ạ, nếu được cho m xin được chia sẻ. Cám ơn ace trước ạ.","#dataset,",,,,
"Dear cả nhà!
Hiện tại em  đang tìm mua dataset với labeling theo một số tiêu chí:
Dataset  về các loại xe ô tô gần đây với labelling vị trí box của xe dưới dang  *.xml hoặc *.json
Dataset về vị  trí biển số xe ô tô với labelling vị trí box của xe dưới dạng *.xml hoặc *.json
Em đăng bài đây nhờ các anh chị em nếu có ai có nguồn dataset vui lòng liên hệ với em để báo giá ạ. Em rất cảm ơn các anh chị.
Chúc cả nhà ngày tốt lành ạ!",Dear cả nhà! Hiện tại em đang tìm mua dataset với labeling theo một số tiêu chí: Dataset về các loại xe ô tô gần đây với labelling vị trí box của xe dưới dang *.xml hoặc *.json Dataset về vị trí biển số xe ô tô với labelling vị trí box của xe dưới dạng *.xml hoặc *.json Em đăng bài đây nhờ các anh chị em nếu có ai có nguồn dataset vui lòng liên hệ với em để báo giá ạ. Em rất cảm ơn các anh chị. Chúc cả nhà ngày tốt lành ạ!,,,,,
Anh/chị cho em hỏi là khóa học nổi tiếng về Machine Learning của thầy Andrew Ng trên coursera đã bị xóa rồi ạ ? Em tìm học thì không thấy nữa. Còn có cách nào khác để học khóa này không ạ ? Em cảm ơn!,Anh/chị cho em hỏi là khóa học nổi tiếng về Machine Learning của thầy Andrew Ng trên coursera đã bị xóa rồi ạ ? Em tìm học thì không thấy nữa. Còn có cách nào khác để học khóa này không ạ ? Em cảm ơn!,,,,,
"Mời các bạn thảo luận về việc dữ liệu cá nhân bị leak số lượng lớn ở VN. Theo các bạn, nguyên nhân của việc dữ liệu cá nhân bị rò rỉ là gì? Người dùng có cách nào để bảo vệ dữ liệu của mình hay không? Liệu rằng các công ty, cơ quan có cố gắng bảo về dữ liệu khách hàng hay không?","Mời các bạn thảo luận về việc dữ liệu cá nhân bị leak số lượng lớn ở VN. Theo các bạn, nguyên nhân của việc dữ liệu cá nhân bị rò rỉ là gì? Người dùng có cách nào để bảo vệ dữ liệu của mình hay không? Liệu rằng các công ty, cơ quan có cố gắng bảo về dữ liệu khách hàng hay không?",,,,,
"Kính chào các bác! Nhân dịp đang tìm hiểu về Airlfow cho ML, AI. Em mạnh dạn làm clip cho anh em về Airflow - crawl dữ liệu, train model hoàn toàn tự động.
Cảm ơn mọi người đã xem, hi vọng giúp được các bạn mới học!","Kính chào các bác! Nhân dịp đang tìm hiểu về Airlfow cho ML, AI. Em mạnh dạn làm clip cho anh em về Airflow - crawl dữ liệu, train model hoàn toàn tự động. Cảm ơn mọi người đã xem, hi vọng giúp được các bạn mới học!",,,,,
"Toán học là một phần không thể thiếu của một DataScientist/AI Researcher giỏi. Mời các bạn tham gia thử thách giải bài tập về đại số tuyến tính của DataScienceWorld.Kan.
Giải toán vì một cộng đồng AI vững mạnh hơn, Fighting!
 — đang cảm thấy mạnh mẽ.","Toán học là một phần không thể thiếu của một DataScientist/AI Researcher giỏi. Mời các bạn tham gia thử thách giải bài tập về đại số tuyến tính của DataScienceWorld.Kan. Giải toán vì một cộng đồng AI vững mạnh hơn, Fighting! — đang cảm thấy mạnh mẽ.",,,,,
"Chào cả nhà!
Hiện tại mình đang tìm dataset cho các loại xe và area biển số xe ô tô, trong group mình có biết source dataset nào có thể share cho mình được không? Source dataset có labelling bounding box thì càng tốt nhé ^^
Mình xin cảm ơn nhé!
P/s mình đã sài dataset của standford về car model do dataset đó khá lâu từ 2012 nên mình đang tìm thêm dataset mới cập nhật gần đây ạ.","Chào cả nhà! Hiện tại mình đang tìm dataset cho các loại xe và area biển số xe ô tô, trong group mình có biết source dataset nào có thể share cho mình được không? Source dataset có labelling bounding box thì càng tốt nhé ^^ Mình xin cảm ơn nhé! P/s mình đã sài dataset của standford về car model do dataset đó khá lâu từ 2012 nên mình đang tìm thêm dataset mới cập nhật gần đây ạ.",,,,,
"#Outmemorycuda
Chào mọi người
Cụ thể là mình đã có model rồi sau đó mình dùng model đi validate trên tập dữ liệu nhỏ hơn thì báo lỗi bị out memory cuda
Và sau nhiều lần debug mình tìm ra lỗi nếu thêm vào tính loss và call loss.backward() thì mình chạy trơn tru mượt mà. Vậy có phải loss.backward() thì sẽ free memory hay không ?",Chào mọi người Cụ thể là mình đã có model rồi sau đó mình dùng model đi validate trên tập dữ liệu nhỏ hơn thì báo lỗi bị out memory cuda Và sau nhiều lần debug mình tìm ra lỗi nếu thêm vào tính loss và call loss.backward() thì mình chạy trơn tru mượt mà. Vậy có phải loss.backward() thì sẽ free memory hay không ?,#Outmemorycuda,,,,
"## Update: Mình implement nhầm residual connection ở decoder
Link model mới: https://colab.research.google.com/drive/1Vf1BvVAC0s2LDVOWc0Rf9slT9cO6S9Ch#scrollTo=P8y7Xp5AQUFq

Chào mọi người, mình đang tập tành implement lại từ paper: https://arxiv.org/abs/2101.10804. Bài toán là image captioning train từ bộ data Flickr8k. train data là 1 ảnh -> 5 captions
Hiện tại mình có train 2 model, cả 2 model dùng pretrained vision transfomer làm encoder để trích xuất features từ ảnh
Model thứ nhất thì mình dùng 4 layers decoder từ transfomers thì kết quả là model không thể học được->loss không giảm nhiều. 
+ Link colab: https://colab.research.google.com/drive/1xbtIPWTX-f6m5cgjD_s5zS8nhyiKBx9Z?usp=sharing
Model thứ 2 mình giảm nó xuống còn 1 decoder layer thì mình cảm thấy model đang học tốt và  loss có giảm và inference ra được mặc dù kết quả không tốt.
+ Link colab: https://colab.research.google.com/drive/15ZRJ5E3a5H-yLkZl-7I853J3bxwnAsvj?usp=sharing
Theo lý thuyết thì dùng đến 4 decoder layers thì model sẽ học được features tốt hơn nhưng khi mình train thì 4 decoder layers thì model lại bị underfitting, mình vẫn chưa hiểu tại sai mong mọi người giải đáp.","## Update: Mình implement nhầm residual connection ở decoder Link model mới: https://colab.research.google.com/drive/1Vf1BvVAC0s2LDVOWc0Rf9slT9cO6S9Ch#scrollTo=P8y7Xp5AQUFq Chào mọi người, mình đang tập tành implement lại từ paper: https://arxiv.org/abs/2101.10804. Bài toán là image captioning train từ bộ data Flickr8k. train data là 1 ảnh -> 5 captions Hiện tại mình có train 2 model, cả 2 model dùng pretrained vision transfomer làm encoder để trích xuất features từ ảnh Model thứ nhất thì mình dùng 4 layers decoder từ transfomers thì kết quả là model không thể học được->loss không giảm nhiều. + Link colab: https://colab.research.google.com/drive/1xbtIPWTX-f6m5cgjD_s5zS8nhyiKBx9Z?usp=sharing Model thứ 2 mình giảm nó xuống còn 1 decoder layer thì mình cảm thấy model đang học tốt và loss có giảm và inference ra được mặc dù kết quả không tốt. + Link colab: https://colab.research.google.com/drive/15ZRJ5E3a5H-yLkZl-7I853J3bxwnAsvj?usp=sharing Theo lý thuyết thì dùng đến 4 decoder layers thì model sẽ học được features tốt hơn nhưng khi mình train thì 4 decoder layers thì model lại bị underfitting, mình vẫn chưa hiểu tại sai mong mọi người giải đáp.",,,,,
"""Responsible sources""? Đến cái cốc còn được đóng mác ""responsible"" thế này, thì các giải pháp AI còn phải cần ""đóng mác"" những gì? Nếu bạn đang làm nghiên cứu và quan tâm đến ReliableML thì cân nhắc submit vào special session team tổ chức ở ICONIP'22 nhé, chủ đề: Reliable, Robust, and Secure Machine Learning Algorithms. 

Accepted papers sẽ được index cùng với proceedings của conference nhé các bạn. Thông tin thêm mời các ACE đọc ở website [1].

****
Deadline of Special Session Papers: July 07, 2022
Organizers:
Monowar Bhuyan, Umeå University, Sweden
Xuan-Son Vu, Umeå University, Sweden
Harry Nguyen, University of Glasgow, Singapore
Erik Elmroth, Umeå University, Sweden
****
[1] Special session homepage: https://reliableml.cs.umu.se/","""Responsible sources""? Đến cái cốc còn được đóng mác ""responsible"" thế này, thì các giải pháp AI còn phải cần ""đóng mác"" những gì? Nếu bạn đang làm nghiên cứu và quan tâm đến ReliableML thì cân nhắc submit vào special session team tổ chức ở ICONIP'22 nhé, chủ đề: Reliable, Robust, and Secure Machine Learning Algorithms. Accepted papers sẽ được index cùng với proceedings của conference nhé các bạn. Thông tin thêm mời các ACE đọc ở website [1]. **** Deadline of Special Session Papers: July 07, 2022 Organizers: Monowar Bhuyan, Umeå University, Sweden Xuan-Son Vu, Umeå University, Sweden Harry Nguyen, University of Glasgow, Singapore Erik Elmroth, Umeå University, Sweden **** [1] Special session homepage: https://reliableml.cs.umu.se/",,,,,
"Need help for web development
Xin chào cả nhà
Mình làm nghiên cứu về dự báo mực nước trên sông sử dụng DL (LSTMs) và cần làm 1 website về dự báo dài hạn mực nước trên các sông lớn trên toàn thế giới. Về mặt số liệu và phương pháp đã tạm ổn nhưng bị hóc chỗ web development để cho kết quả lên do mình không có chuyên môn về mảng này nên viết nên đây hy vọng bạn nào có kinh nghiệm có thể giúp.
Website dự kiến có form giống như website sau: https://hydroweb.theia-land.fr/?lang=en& , với bản đồ background active/zoomable, trình bày và trích xuất số liệu (data đã chạy sẵn) tại cái điểm đo https://hydroweb.theia-land.fr/hydroweb/view/R_RHONE_DOUBS_KM0619?lang=en. Bạn nào có thể chỉ giúp minh làm cách nào để làm cái bản đồ như vậy không ạ? (website và số liệu trong bản đồ mình có)
Mình xin lỗi vì câu hỏi thiên về ứng dụng thực tế hơn là cơ bản, mong các bạn thông cảm.
Xin chân thành cảm ơn admin và sự giúp đỡ của các bạn
(mọi sự góp ý/giúp đỡ xin nhắn vào inbox giùm ạ).","Need help for web development Xin chào cả nhà Mình làm nghiên cứu về dự báo mực nước trên sông sử dụng DL (LSTMs) và cần làm 1 website về dự báo dài hạn mực nước trên các sông lớn trên toàn thế giới. Về mặt số liệu và phương pháp đã tạm ổn nhưng bị hóc chỗ web development để cho kết quả lên do mình không có chuyên môn về mảng này nên viết nên đây hy vọng bạn nào có kinh nghiệm có thể giúp. Website dự kiến có form giống như website sau: https://hydroweb.theia-land.fr/?lang=en& , với bản đồ background active/zoomable, trình bày và trích xuất số liệu (data đã chạy sẵn) tại cái điểm đo https://hydroweb.theia-land.fr/hydroweb/view/R_RHONE_DOUBS_KM0619?lang=en. Bạn nào có thể chỉ giúp minh làm cách nào để làm cái bản đồ như vậy không ạ? (website và số liệu trong bản đồ mình có) Mình xin lỗi vì câu hỏi thiên về ứng dụng thực tế hơn là cơ bản, mong các bạn thông cảm. Xin chân thành cảm ơn admin và sự giúp đỡ của các bạn (mọi sự góp ý/giúp đỡ xin nhắn vào inbox giùm ạ).",,,,,
"mọi người cho em hỏi với ạ, khi plot price của computer theo các variable khác em được graphs như này. thì sao mình có thể kết luận được đâu là promising predictors ạ. em cảm ơn ạ","mọi người cho em hỏi với ạ, khi plot price của computer theo các variable khác em được graphs như này. thì sao mình có thể kết luận được đâu là promising predictors ạ. em cảm ơn ạ",,,,,
"Kính chào cô chú và các anh chị.
Em xin nói thẳng em là dân ngoại đạo chứ không theo học khoa học dữ liệu hay IT nói chung. Em thỉnh thoảng có thú vui là đi câu cá. Trong khi câu thường có hàng tiếng đồng hồ ngồi chờ cá cắn và chỉ có 1-2 phút cần tập trung khi có tín hiệu ở phao. Điều này khiến cho việc vừa ngồi vừa thư giãn, nói chuyện với bạn bè bên cạnh là hơi khó vì dễ bỏ lỡ nhịp cá ăn.
Em muốn hỏi là có cách nào code được 1 cái app điện thoại. Tự động báo khi nó nhận diện được tín hiệu phao không ạ? Kiểu setup cái giá đỡ điện thoại, mở app cho nó zoom thẳng vào phao để nó nhận phao. Mỗi khi phao nổi lên hay thụt xuống nó sẽ có âm báo. Thậm chí cao cấp hơn có khả năng báo tương đối chính xác loại cá nào đang/vừa ăn mồi dựa vào tín hiệu phao nhưng cái này không quan trọng vì dân câu chuyên cũng chỉ đoán đúng tầm 70% thôi. Em nghĩ app này sẽ tác dụng vô cùng khi mà người câu cá không cần căng mắt mấy tiếng đồng hồ nhìn cái phao nữa. Họ sẽ thỏa mái ngồi thư giãn, nói chuyện với bạn bè và chỉ phải tập trung khi điện thoại báo có tín hiệu phao. Điều nữa là dân câu thường thuộc dạng thu nhập trung bình trở lên nên em nghĩ sẽ rất nhiều cơ hội để chèn quảng cáo vào app. Liệu code 1 cái app như vậy có khả thi không ạ?","Kính chào cô chú và các anh chị. Em xin nói thẳng em là dân ngoại đạo chứ không theo học khoa học dữ liệu hay IT nói chung. Em thỉnh thoảng có thú vui là đi câu cá. Trong khi câu thường có hàng tiếng đồng hồ ngồi chờ cá cắn và chỉ có 1-2 phút cần tập trung khi có tín hiệu ở phao. Điều này khiến cho việc vừa ngồi vừa thư giãn, nói chuyện với bạn bè bên cạnh là hơi khó vì dễ bỏ lỡ nhịp cá ăn. Em muốn hỏi là có cách nào code được 1 cái app điện thoại. Tự động báo khi nó nhận diện được tín hiệu phao không ạ? Kiểu setup cái giá đỡ điện thoại, mở app cho nó zoom thẳng vào phao để nó nhận phao. Mỗi khi phao nổi lên hay thụt xuống nó sẽ có âm báo. Thậm chí cao cấp hơn có khả năng báo tương đối chính xác loại cá nào đang/vừa ăn mồi dựa vào tín hiệu phao nhưng cái này không quan trọng vì dân câu chuyên cũng chỉ đoán đúng tầm 70% thôi. Em nghĩ app này sẽ tác dụng vô cùng khi mà người câu cá không cần căng mắt mấy tiếng đồng hồ nhìn cái phao nữa. Họ sẽ thỏa mái ngồi thư giãn, nói chuyện với bạn bè và chỉ phải tập trung khi điện thoại báo có tín hiệu phao. Điều nữa là dân câu thường thuộc dạng thu nhập trung bình trở lên nên em nghĩ sẽ rất nhiều cơ hội để chèn quảng cáo vào app. Liệu code 1 cái app như vậy có khả thi không ạ?",,,,,
"Chào các anh chị em trong group
Hiện tại mình đang tìm tài liệu cho việc phát triển tích hợp OCR vào swiftUI trên các thiết bị sử dụng apple silicon.
Các mô hình hiện nay như easyOCR, paddleOCR, .... & các framework đều đang được phát triển trên kiến trúc ARM
TesseractOCR thì ... à mà thôi, bỏ đi
Một số mô hình OCR hỗ trợ cho swift như SwiftOCR, MacOCR cũng đều là những lib đã cũ, chạy trên series apple đầu A, mình ko dám đảm bảo nó sẽ chạy được trên chip mới của apple
Không biết anh chị em nào có giải pháp cho vấn đề này không, em xin cảm ơn trước.
p/s: ở đây mình ko muốn sử dụng cloud service hay build server vật lý để giải quyết bài toán này.","Chào các anh chị em trong group Hiện tại mình đang tìm tài liệu cho việc phát triển tích hợp OCR vào swiftUI trên các thiết bị sử dụng apple silicon. Các mô hình hiện nay như easyOCR, paddleOCR, .... & các framework đều đang được phát triển trên kiến trúc ARM TesseractOCR thì ... à mà thôi, bỏ đi Một số mô hình OCR hỗ trợ cho swift như SwiftOCR, MacOCR cũng đều là những lib đã cũ, chạy trên series apple đầu A, mình ko dám đảm bảo nó sẽ chạy được trên chip mới của apple Không biết anh chị em nào có giải pháp cho vấn đề này không, em xin cảm ơn trước. p/s: ở đây mình ko muốn sử dụng cloud service hay build server vật lý để giải quyết bài toán này.",,,,,
"Xin chào mọi người. Em vừa mới theo học deep learning gần đây. Em có vấn đề muốn tham khảo các anh/chị đã có kinh nghiệm trong lĩnh vực này. Tình hình là em đang develop một hệ thống nhận diện chữ viết tay online, model em đang dùng là GRU. Hiện tại thì em đang dùng kiểu cho từng stroke vào rồi nhận diện stroke đó là chữ cái gì trong bảng alphabet. Input là các vector đại diện cho hướng tại các point, output là one-hot vector đại diện cho một chữ cái.
Đến đây chắc các anh/chị cũng nhận ra là có vấn đề rồi phải không ạ. Vâng, em đang có trường hợp là một số stroke viết tay nó đại diện cho nhiều chữ cái chứ không phải chỉ một. Ví dụ, người ta viết chữ ""moon"" chỉ với 1 nét, em không biết phải biểu diễn target output như thế nào để GRU có thể học được. Mong rằng có anh/chị nào đó tốt bụng giải đáp giúp em. Em xin cảm ơn trước.","Xin chào mọi người. Em vừa mới theo học deep learning gần đây. Em có vấn đề muốn tham khảo các anh/chị đã có kinh nghiệm trong lĩnh vực này. Tình hình là em đang develop một hệ thống nhận diện chữ viết tay online, model em đang dùng là GRU. Hiện tại thì em đang dùng kiểu cho từng stroke vào rồi nhận diện stroke đó là chữ cái gì trong bảng alphabet. Input là các vector đại diện cho hướng tại các point, output là one-hot vector đại diện cho một chữ cái. Đến đây chắc các anh/chị cũng nhận ra là có vấn đề rồi phải không ạ. Vâng, em đang có trường hợp là một số stroke viết tay nó đại diện cho nhiều chữ cái chứ không phải chỉ một. Ví dụ, người ta viết chữ ""moon"" chỉ với 1 nét, em không biết phải biểu diễn target output như thế nào để GRU có thể học được. Mong rằng có anh/chị nào đó tốt bụng giải đáp giúp em. Em xin cảm ơn trước.",,,,,
"Dạ, em chào mấy anh/chị ạ.
em đang làm về đề tài là nhận diện nấm linh chi đỏ trưởng thành bằng camera. Nhưng theo một số thông tin em đọc được thì điểm khác nhau giữa cây trưởng thành và cây chưa trưởng thành là độ đậm nhạt của màu đỏ trên cây ạ. Mọi người cho em hỏi có cách nào khả quan không ạ. Em cảm ơn mọi người ạ","Dạ, em chào mấy anh/chị ạ. em đang làm về đề tài là nhận diện nấm linh chi đỏ trưởng thành bằng camera. Nhưng theo một số thông tin em đọc được thì điểm khác nhau giữa cây trưởng thành và cây chưa trưởng thành là độ đậm nhạt của màu đỏ trên cây ạ. Mọi người cho em hỏi có cách nào khả quan không ạ. Em cảm ơn mọi người ạ",,,,,
"Hi các anh, chi,
Em đang làm luận văn tốt nghiệp Thạc Sĩ ngày CNTT, tháng 10/2022 này em bảo vệ luận văn tốt nghiệp, Em làm đề tài ""Loan Repayment Prediction Using Machine Learning Algorithms"", Em thực hiện các bước sau:
1. Tiền xử lý dữ liệu - Data preprocessing:    
a. Tìm hệ số tương quan giữa các thuộc tính.    
b. Xử lý các thuộc tính dạng văn bản   
c. Xử lý các giá trị bị thiếu(Data Imputation - Missing Data Replacement)    
d. Tách dữ liệu thành tập huấn luyện và tập kiểm tra    
e. Chuẩn hóa dữ liệu 
2. Huấn luyện mô hình:    
a. Đánh giá mô hình độ chính xác bằng kiểm định chéo    
b. Đánh giá mô hình độ chính xác bằng ma trận nhầm lẫn      
 + Presision      
 + Recall       
+ F1-Score       
+ Đường cong ROC
--> Tuy nhiên kết quả thì cũng không tốt lắm: đây là kết quả chạy thực nghiệm của em, Em xin ý kiến và góp ý của các anh, chị, em xin cảm ơn.
Em xin gửi link github database, code của em được không ạ. anh chị vào góp ý kiến dùm em, em xin cảm ơn.
Link: https://github.com/binhht7777/LuanVanTotNghiep","Hi các anh, chi, Em đang làm luận văn tốt nghiệp Thạc Sĩ ngày CNTT, tháng 10/2022 này em bảo vệ luận văn tốt nghiệp, Em làm đề tài ""Loan Repayment Prediction Using Machine Learning Algorithms"", Em thực hiện các bước sau: 1. Tiền xử lý dữ liệu - Data preprocessing: a. Tìm hệ số tương quan giữa các thuộc tính. b. Xử lý các thuộc tính dạng văn bản c. Xử lý các giá trị bị thiếu(Data Imputation - Missing Data Replacement) d. Tách dữ liệu thành tập huấn luyện và tập kiểm tra e. Chuẩn hóa dữ liệu 2. Huấn luyện mô hình: a. Đánh giá mô hình độ chính xác bằng kiểm định chéo b. Đánh giá mô hình độ chính xác bằng ma trận nhầm lẫn + Presision + Recall + F1-Score + Đường cong ROC --> Tuy nhiên kết quả thì cũng không tốt lắm: đây là kết quả chạy thực nghiệm của em, Em xin ý kiến và góp ý của các anh, chị, em xin cảm ơn. Em xin gửi link github database, code của em được không ạ. anh chị vào góp ý kiến dùm em, em xin cảm ơn. Link: https://github.com/binhht7777/LuanVanTotNghiep",,,,,
"Xin chào mọi người, Em có một vài thắc mắc về Speech Recognition muốn tham khảo ý kiến của mọi người.
Em đang làm project xử lý tiếng nói để cho robotic chuyển động: bộ từ vựng chỉ có ""lên, xuống, trái, phải""
Em đã thu dữ liệu bằng các đoạn file wav, trích xuất 39 MFCCs cho mỗi sample ~ 1 từ. Tuy nhiên em gặp 1 vấn đề trong lúc phân loại: em muốn normalize toàn bộ các sample bằng Z-score tuy nhiên các sample lại có shape ko giống nhau: (39, 5); (39, 10); (39, 8); (39, 25);... Khiến em chưa nghĩ ra phương pháp nào để normalize được các sample.
Em đã thử concatenate các sample theo chiều ngang để thành 1 tabulet to đùng shape (39, D) rồi normalize nhưng lại cho kết quả phân lớp vô cùng tệ hại. Em đã thử truncate tất cả về (39, 5) rồi flattened để normalize và áp dụng DTW thu được 0.84 accuracy (dataset của em là balance)
Em muốn hỏi là nếu ko truncate về sample size bé nhất thì còn có cách nào để normalize toàn bộ các sample không?
Và em cũng có 1 vấn đề nữa là nếu signal input không thuộc các class label trong train set thì làm cách nào để handle và bỏ qua nó? Liệu em có phải dựng 1 threehold về distance trong khi DTW để handle, hay em nên tính correlation giữa input và các sample trong trainset trước rồi mới sử dụng DTW?
Em xin cám ơn mọi sự giải đáp ạ.","Xin chào mọi người, Em có một vài thắc mắc về Speech Recognition muốn tham khảo ý kiến của mọi người. Em đang làm project xử lý tiếng nói để cho robotic chuyển động: bộ từ vựng chỉ có ""lên, xuống, trái, phải"" Em đã thu dữ liệu bằng các đoạn file wav, trích xuất 39 MFCCs cho mỗi sample ~ 1 từ. Tuy nhiên em gặp 1 vấn đề trong lúc phân loại: em muốn normalize toàn bộ các sample bằng Z-score tuy nhiên các sample lại có shape ko giống nhau: (39, 5); (39, 10); (39, 8); (39, 25);... Khiến em chưa nghĩ ra phương pháp nào để normalize được các sample. Em đã thử concatenate các sample theo chiều ngang để thành 1 tabulet to đùng shape (39, D) rồi normalize nhưng lại cho kết quả phân lớp vô cùng tệ hại. Em đã thử truncate tất cả về (39, 5) rồi flattened để normalize và áp dụng DTW thu được 0.84 accuracy (dataset của em là balance) Em muốn hỏi là nếu ko truncate về sample size bé nhất thì còn có cách nào để normalize toàn bộ các sample không? Và em cũng có 1 vấn đề nữa là nếu signal input không thuộc các class label trong train set thì làm cách nào để handle và bỏ qua nó? Liệu em có phải dựng 1 threehold về distance trong khi DTW để handle, hay em nên tính correlation giữa input và các sample trong trainset trước rồi mới sử dụng DTW? Em xin cám ơn mọi sự giải đáp ạ.",,,,,
"Chào mọi người, sau một thời gian dài triển khai các ứng dụng ML/DL làm mình quên đi một số lý thuyết cơ bản. Hôm nay rảnh rang ngồi đọc lại bài note ngày xưa khi tìm hiểu về kiến trúc transformer thấy khá hữu ích nên đăng lại trên viblo và chia sẻ cho mọi người.
Anh em làm ứng dụng nhiều thi thoảng cũng ôn luyện lại chút kẻo quên hết nhé =)))","Chào mọi người, sau một thời gian dài triển khai các ứng dụng ML/DL làm mình quên đi một số lý thuyết cơ bản. Hôm nay rảnh rang ngồi đọc lại bài note ngày xưa khi tìm hiểu về kiến trúc transformer thấy khá hữu ích nên đăng lại trên viblo và chia sẻ cho mọi người. Anh em làm ứng dụng nhiều thi thoảng cũng ôn luyện lại chút kẻo quên hết nhé =)))",,,,,
Mọi người ai có nguồn video cá trên băng chuyền kiểu này không cho em xin với . Em cảm ơn ạ.,Mọi người ai có nguồn video cá trên băng chuyền kiểu này không cho em xin với . Em cảm ơn ạ.,,,,,
"Máy móc có thể suy nghĩ như con người được không?
Nhà triết học Aristotle (384-322 TCN) đã phát minh ra thuyết âm tiết (một quá trình logic trong đó hai phát biểu chung dẫn đến một phát biểu cụ thể hơn) làm nền tảng cho lý luận và tư duy. Nghĩa là theo quan điểm hiện tại, chúng ta có thể công thức hóa tư duy của con người.
Từ đó, mạng nơ-ron được sinh ra để mô tả lại hệ thống não bộ của con người, nó giúp máy tính có thể có cách tư duy giống với con người.
Trong phần cuối của series về lý thuyết toán dành cho Machine Learning, chúng ta sẽ tìm hiểu về học sâu, và các bước tiến của việc đại số hóa suy nghĩ của con người.","Máy móc có thể suy nghĩ như con người được không? Nhà triết học Aristotle (384-322 TCN) đã phát minh ra thuyết âm tiết (một quá trình logic trong đó hai phát biểu chung dẫn đến một phát biểu cụ thể hơn) làm nền tảng cho lý luận và tư duy. Nghĩa là theo quan điểm hiện tại, chúng ta có thể công thức hóa tư duy của con người. Từ đó, mạng nơ-ron được sinh ra để mô tả lại hệ thống não bộ của con người, nó giúp máy tính có thể có cách tư duy giống với con người. Trong phần cuối của series về lý thuyết toán dành cho Machine Learning, chúng ta sẽ tìm hiểu về học sâu, và các bước tiến của việc đại số hóa suy nghĩ của con người.",,,,,
"Bên MLOps VN có share 1 link về cách tính AUC-ROC khi chưa có ground truth, mình xin share lại 
https://towardsdatascience.com/predict-your-models-performance-without-waiting-for-the-control-group-3f5c9363a7da","Bên MLOps VN có share 1 link về cách tính AUC-ROC khi chưa có ground truth, mình xin share lại https://towardsdatascience.com/predict-your-models-performance-without-waiting-for-the-control-group-3f5c9363a7da",,,,,
"[Vietnam stock bid/ask recommender system ]
☑️ The stock market is always uncertain, volatile, and risky but also brings opportunities to investors. Depending on two main approaches fundamental analysis and technical analysis, the investors can make evaluations, movement laws, and trends in order to release timely buy and sell.
☑️ Fundamental analysis usually relies on an analyst’s subjective thoughts. Therefore, its decisions are not consistent but majorly depended on each expert. Whereas, an AI model can draw a very consistent conclusion based on the rules learned from big data. Currently, technical analysis is mainly based on pattern recognition on the technical charts. The application of AI models in technical analysis is not comprehensive and effective whereas the AI model strength is gradually demonstrated through their drastic development today.
☑️ Therefore, the main target of this competition “Vietnam stock bid/ask recommender system” is to construct a recommender model, which recommends up and down sessions for investors on a portfolio of 30 encoded symbols in the Vietnam stock market. AI models take advantage of drawing rules from big data, having strong feature transformation via multi-layers deep learning architectures, and owning black box characteristics that cannot be explained by humans. That is why the application of the AI model in trading has proven its effectiveness in advanced markets like the USA, EU, Japan, etc.
☑️ In this contest, you are provided with a set of data, which is extracted from the vnquant package, including those important stock information: price, transaction volume; financial report, business report, and cashflow report. The price and volume data are organized according to time series with daily frequency whereas quarterly frequency for corporate reports.
☑️ Let’s use the potential Deep Learning architectures trained in provided tabular datasets to support the investor in forecasting the increasing/decreasing stocks session on the Vietnam stock market.
☑️ Contest information:
- Overview: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/overview
- Data: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/data
- Rule: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/rules
- Sample code: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/code","[Vietnam stock bid/ask recommender system ] The stock market is always uncertain, volatile, and risky but also brings opportunities to investors. Depending on two main approaches fundamental analysis and technical analysis, the investors can make evaluations, movement laws, and trends in order to release timely buy and sell. Fundamental analysis usually relies on an analyst’s subjective thoughts. Therefore, its decisions are not consistent but majorly depended on each expert. Whereas, an AI model can draw a very consistent conclusion based on the rules learned from big data. Currently, technical analysis is mainly based on pattern recognition on the technical charts. The application of AI models in technical analysis is not comprehensive and effective whereas the AI model strength is gradually demonstrated through their drastic development today. Therefore, the main target of this competition “Vietnam stock bid/ask recommender system” is to construct a recommender model, which recommends up and down sessions for investors on a portfolio of 30 encoded symbols in the Vietnam stock market. AI models take advantage of drawing rules from big data, having strong feature transformation via multi-layers deep learning architectures, and owning black box characteristics that cannot be explained by humans. That is why the application of the AI model in trading has proven its effectiveness in advanced markets like the USA, EU, Japan, etc. In this contest, you are provided with a set of data, which is extracted from the vnquant package, including those important stock information: price, transaction volume; financial report, business report, and cashflow report. The price and volume data are organized according to time series with daily frequency whereas quarterly frequency for corporate reports. Let’s use the potential Deep Learning architectures trained in provided tabular datasets to support the investor in forecasting the increasing/decreasing stocks session on the Vietnam stock market. Contest information: - Overview: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/overview - Data: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/data - Rule: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/rules - Sample code: https://www.kaggle.com/competitions/vietnam-stock-bidask-recommender-system/code",,,,,
"Đây là link tới bài lúc trước mình xoá vì tưởng là bán kem trộn.
Vì một lý do nào đó mà bạn tác giả không post bài được nữa nên mình post giúp.
https://hieuphung97.com/danh-gia-tham-my-anh-cung-tri-tue-nhan-tao.html?fbclid=IwAR3r2NGiKI7bVjARD8KaJVRyLc7avlZ2KwLMewc58HOJNS96wG6nog1rqMU",Đây là link tới bài lúc trước mình xoá vì tưởng là bán kem trộn. Vì một lý do nào đó mà bạn tác giả không post bài được nữa nên mình post giúp. https://hieuphung97.com/danh-gia-tham-my-anh-cung-tri-tue-nhan-tao.html?fbclid=IwAR3r2NGiKI7bVjARD8KaJVRyLc7avlZ2KwLMewc58HOJNS96wG6nog1rqMU,,,,,
"Hi cả nhà, em xin đóng góp một bài viết nhỏ về cách serve 1 model dùng FastAPI và Kubernetes (k8s), hy vọng có ích cho mọi người :D
https://quan-dang.github.io/2022/05/19/deploy-model-fastapi-k8s/
Nhân tiện em lại xin phép được PR cho group MLOps VN (https://www.facebook.com/groups/mlopsvn) dành cho mọi người quan tâm tới ML Engineering/MLOps nha :v","Hi cả nhà, em xin đóng góp một bài viết nhỏ về cách serve 1 model dùng FastAPI và Kubernetes (k8s), hy vọng có ích cho mọi người :D https://quan-dang.github.io/2022/05/19/deploy-model-fastapi-k8s/ Nhân tiện em lại xin phép được PR cho group MLOps VN (https://www.facebook.com/groups/mlopsvn) dành cho mọi người quan tâm tới ML Engineering/MLOps nha :v",,,,,
"Dạ cho em hỏi là nhóm mình có ai làm về Bioelectrical Impedance Analysis (BIA) chưa ạ và mình có thể kiếm data ở đâu ạ, em cám ơn ạ.","Dạ cho em hỏi là nhóm mình có ai làm về Bioelectrical Impedance Analysis (BIA) chưa ạ và mình có thể kiếm data ở đâu ạ, em cám ơn ạ.",,,,,
"Cuốn Thực hành học máy tiếp tục bị vi phạm bản quyền. Bản này được shared trên SCRIBD, hiện đã bị gỡ sau khi nhận được report từ team dịch.
Cảm ơn một bạn đọc (mình không nêu tên vì chưa xin ý kiến) đã thông báo với nhóm để có hành động kịp thời.","Cuốn Thực hành học máy tiếp tục bị vi phạm bản quyền. Bản này được shared trên SCRIBD, hiện đã bị gỡ sau khi nhận được report từ team dịch. Cảm ơn một bạn đọc (mình không nêu tên vì chưa xin ý kiến) đã thông báo với nhóm để có hành động kịp thời.",,,,,
"Vừa rồi có một post đăng ảnh một em gái xinh và tiêu đề có gì đó liên quan đến thẩm mỹ. Mình tay nhanh hơn mắt đã block bạn tác giả. Sau đó mới nhận ra là bài đó nói về dùng AI để đánh giá độ thẩm mỹ của ảnh thì đã quá muộn. Tại gần đây nhiều post spam quá mình quen tay.
Nếu bạn tác giả (đã bị block) bằng một cách nào đó nhìn thấy post này thì cho mình xin lỗi nhé. Bạn có thể connect qua DM để mình add lại vào group.",Vừa rồi có một post đăng ảnh một em gái xinh và tiêu đề có gì đó liên quan đến thẩm mỹ. Mình tay nhanh hơn mắt đã block bạn tác giả. Sau đó mới nhận ra là bài đó nói về dùng AI để đánh giá độ thẩm mỹ của ảnh thì đã quá muộn. Tại gần đây nhiều post spam quá mình quen tay. Nếu bạn tác giả (đã bị block) bằng một cách nào đó nhìn thấy post này thì cho mình xin lỗi nhé. Bạn có thể connect qua DM để mình add lại vào group.,,,,,
"#ETL #MLOPs
Hi All,
Mình có làm video về ETL/MLOPs sử dụng thư viện Perfect trong Python.
Thư viện này giúp việc ETL một cách tự động, chuyên nghiệp hơn. Cụ thể thì mình đã demo trong video.
Mọi người xem qua cho em góp ý cũng như có Tools nào hay hơn không ạ. Theo mình tìm hiểu thì ngoài Perfect thì còn có thư viện Airflow nữa.
Mong được admin duyệt bài ạ!
Thanks All!","Hi All, Mình có làm video về ETL/MLOPs sử dụng thư viện Perfect trong Python. Thư viện này giúp việc ETL một cách tự động, chuyên nghiệp hơn. Cụ thể thì mình đã demo trong video. Mọi người xem qua cho em góp ý cũng như có Tools nào hay hơn không ạ. Theo mình tìm hiểu thì ngoài Perfect thì còn có thư viện Airflow nữa. Mong được admin duyệt bài ạ! Thanks All!",#ETL	#MLOPs,,,,
Mời các bạn theo dõi phần tiếp theo của series cơ sở toán học cho Machine Learning.,Mời các bạn theo dõi phần tiếp theo của series cơ sở toán học cho Machine Learning.,,,,,
Mọi người cho em hỏi em đã cài opencv Cuda rồi nhưng khi chạy code yolov4 thì nó chuyển sang CPU thì có các nào khắc phục không ạ. Em chạy trên jetson xavier,Mọi người cho em hỏi em đã cài opencv Cuda rồi nhưng khi chạy code yolov4 thì nó chuyển sang CPU thì có các nào khắc phục không ạ. Em chạy trên jetson xavier,,,,,
"Xin chào mọi người,
Do nhiều bạn hỏi mình về liệu lớp có TA để hỗ trợ trả lời các câu hỏi về các bài giảng và thậm chí là bài tập (nếu có) cho 2 học phần ""Introduction to (Machine Learning) ML và (Data Science) DS"" và ""Advanced ML và DS"" (xem thêm thông tin các lớp này ở đây: https://www.facebook.com/groups/machinelearningcoban/posts/1472717289852340/), thì mình cũng muốn cập nhật danh sách TA hiện tại cho các lớp này.
********** Danh sách các bạn TA hiện tại (sẽ liên tục được cập nhật thêm):
--- Lê Quang Dũng (Huy Chương Vàng Toán Quốc Tế (IMO); Thủ Khoa Đại Học Khoa Học Tự Nhiên Hà Nội)
--- Nguyễn Tuấn Hải Đăng (Huy Chương Bạc Toán Quốc Tế (IMO); Thủ Khoa ngành công nghệ thông tin tại trường Đại Học ở Tokyo)
--- Nguyễn Minh Huy (Thủ Khoa toàn trường Đại Học Khoa Học Tự Nhiên Thành Phố Hồ Chí Minh; Điểm GPA có thể coi cao nhất trong lịch sử thành lập của trường)
--- Vũ Lê Thế Anh (Thủ Khoa ngành công nghệ thông tin trường Đại Học Khoa Học Tự Nhiên Thành Phố Hồ Chí Minh)","Xin chào mọi người, Do nhiều bạn hỏi mình về liệu lớp có TA để hỗ trợ trả lời các câu hỏi về các bài giảng và thậm chí là bài tập (nếu có) cho 2 học phần ""Introduction to (Machine Learning) ML và (Data Science) DS"" và ""Advanced ML và DS"" (xem thêm thông tin các lớp này ở đây: https://www.facebook.com/groups/machinelearningcoban/posts/1472717289852340/), thì mình cũng muốn cập nhật danh sách TA hiện tại cho các lớp này. ********** Danh sách các bạn TA hiện tại (sẽ liên tục được cập nhật thêm): --- Lê Quang Dũng (Huy Chương Vàng Toán Quốc Tế (IMO); Thủ Khoa Đại Học Khoa Học Tự Nhiên Hà Nội) --- Nguyễn Tuấn Hải Đăng (Huy Chương Bạc Toán Quốc Tế (IMO); Thủ Khoa ngành công nghệ thông tin tại trường Đại Học ở Tokyo) --- Nguyễn Minh Huy (Thủ Khoa toàn trường Đại Học Khoa Học Tự Nhiên Thành Phố Hồ Chí Minh; Điểm GPA có thể coi cao nhất trong lịch sử thành lập của trường) --- Vũ Lê Thế Anh (Thủ Khoa ngành công nghệ thông tin trường Đại Học Khoa Học Tự Nhiên Thành Phố Hồ Chí Minh)",,,,,
"Sau một thời gian dài nghiên cứu và học hỏi thì các bạn học viên của tớ đã cho ra đời một ứng dụng AI giúp đánh giá hàm lượng dinh dưỡng bữa ăn. Hi vọng sẽ giúp ích cho cộng đồng mình, đặc biệt là những ai đang quan tâm tới vấn đề chăm sóc sức khỏe và ăn uống. Sản phẩm đầu tay còn nhiều khó khăn, bỡ ngỡ. Mong nhận được ý kiến đánh giá từ các bạn để hoàn thiện hơn. Và đặc biệt chúng tớ cũng muốn nhận được sự đầu tư và hợp tác để phát triển sản phẩm này.","Sau một thời gian dài nghiên cứu và học hỏi thì các bạn học viên của tớ đã cho ra đời một ứng dụng AI giúp đánh giá hàm lượng dinh dưỡng bữa ăn. Hi vọng sẽ giúp ích cho cộng đồng mình, đặc biệt là những ai đang quan tâm tới vấn đề chăm sóc sức khỏe và ăn uống. Sản phẩm đầu tay còn nhiều khó khăn, bỡ ngỡ. Mong nhận được ý kiến đánh giá từ các bạn để hoàn thiện hơn. Và đặc biệt chúng tớ cũng muốn nhận được sự đầu tư và hợp tác để phát triển sản phẩm này.",,,,,
"Chào anh, chị,
Lại là em đây, em làm đề tài ""Loan Repayment Prediction Using Machine Learning Algorithms"", sau khi em tải tập dữ liệu khác trên Lending.club, tập dữ liệu có 21,968 record thì các mô hình của em chạy ra kết quả cũng khá tốt ạ, cách đây mấy ngày thì kết quả không tốt, chắc là do tập dữ liệu cũ không đủ tốt để huấn luyện, em cảm ơn anh, chị nhiều.
ROC:","Chào anh, chị, Lại là em đây, em làm đề tài ""Loan Repayment Prediction Using Machine Learning Algorithms"", sau khi em tải tập dữ liệu khác trên Lending.club, tập dữ liệu có 21,968 record thì các mô hình của em chạy ra kết quả cũng khá tốt ạ, cách đây mấy ngày thì kết quả không tốt, chắc là do tập dữ liệu cũ không đủ tốt để huấn luyện, em cảm ơn anh, chị nhiều. ROC:",,,,,
"Em chào mọi người ạ. Hiện tại e mới tập tành về phân tích dữ liệu, em có một câu hỏi mong mọi người giải đáp ạ. Đó là thường trước khi cho dữ liệu vào mô hình thì mình cần bước phân tích dữ liệu để lựa chọn đặc trưng đưa vào mô hình ( giả sử với bài toán phân loại thì cần xác định xem giữa các feature của các lớp thì nó khác nhau ntn). DO trước giờ em chỉ quen phân tích từng feature riêng rẽ nên e mong mn ai có kinh nghiệm giải đáp giúp e ạ
Trong thực thế ( giả sử mình đã hiểu business là cần phân tích thuộc tính nào), thì thực tế có khá nhiều thuộc tính cần phân tích ( giả sử 20 thuộc tính chẳng hạn), thì mình có pp nào để phân tích sự tương quan giữa các biến ko ạ. ( E có nghe đến pair plot nhưng chỉ vẽ đc tương quan 2 biết, PCA nhưng e ko rõ mình sẽ giảm về bao nhiêu chiều )
Dữ liệu ở dạng cùng một feature nhưng biến đổi theo tháng ( có rất nhiều sample như vậy), thì mình có pp nào phân tích hiệu quả ko ạ, vì vẽ time series lên thì có hàng ngàn sample thì ko quan sát đc )
3. Dữ liệu trong thực tế bị mất cân bằng rất nhiều thì mình có pp nào để phân tích khách quan hơn ko ạ ( E có thấy sampling từ phân phối của lớp nhiều sampe hơn để phân tích, nhưng e thấy data từ lớp nhiều sample hơn lại ko theo phân phối chuẩn thì mình có cách nào ngoài việc collect thêm data ko ạ)
3. Anh chị nào đang làm DA/Modelling có thể cho e xin ít kinh nghiệm/ lộ trình để đi đc xa trong ngành này đc ko ạ.
Em xin chân thành cảm ơn mọi người.","Em chào mọi người ạ. Hiện tại e mới tập tành về phân tích dữ liệu, em có một câu hỏi mong mọi người giải đáp ạ. Đó là thường trước khi cho dữ liệu vào mô hình thì mình cần bước phân tích dữ liệu để lựa chọn đặc trưng đưa vào mô hình ( giả sử với bài toán phân loại thì cần xác định xem giữa các feature của các lớp thì nó khác nhau ntn). DO trước giờ em chỉ quen phân tích từng feature riêng rẽ nên e mong mn ai có kinh nghiệm giải đáp giúp e ạ Trong thực thế ( giả sử mình đã hiểu business là cần phân tích thuộc tính nào), thì thực tế có khá nhiều thuộc tính cần phân tích ( giả sử 20 thuộc tính chẳng hạn), thì mình có pp nào để phân tích sự tương quan giữa các biến ko ạ. ( E có nghe đến pair plot nhưng chỉ vẽ đc tương quan 2 biết, PCA nhưng e ko rõ mình sẽ giảm về bao nhiêu chiều ) Dữ liệu ở dạng cùng một feature nhưng biến đổi theo tháng ( có rất nhiều sample như vậy), thì mình có pp nào phân tích hiệu quả ko ạ, vì vẽ time series lên thì có hàng ngàn sample thì ko quan sát đc ) 3. Dữ liệu trong thực tế bị mất cân bằng rất nhiều thì mình có pp nào để phân tích khách quan hơn ko ạ ( E có thấy sampling từ phân phối của lớp nhiều sampe hơn để phân tích, nhưng e thấy data từ lớp nhiều sample hơn lại ko theo phân phối chuẩn thì mình có cách nào ngoài việc collect thêm data ko ạ) 3. Anh chị nào đang làm DA/Modelling có thể cho e xin ít kinh nghiệm/ lộ trình để đi đc xa trong ngành này đc ko ạ. Em xin chân thành cảm ơn mọi người.",,,,,
"Chào mọi người, hiện tại em đang làm nhận dạng cá/tôm sử dụng YOLOv4 (AlexeyAB/darknet: YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet ) (github.com))
Em thử train 1 class cá trước và được kết quả sau khi train trên tập valid (ảnh 1) và tập test (ảnh 2) gần giống nhau
Tương tự với 1 class tôm (kết quả trên tập valid là ảnh 3 và test là ảnh 4)
Nhưng khi gộp dataset cá + tôm để train 2 class khì kết quả trên tập valid (ảnh 5) và tập test (ảnh 6) chênh lệch khá nhiều.
Mọi người có thể cho em xin cách xử lý trong trường hợp này được không ạ?","Chào mọi người, hiện tại em đang làm nhận dạng cá/tôm sử dụng YOLOv4 (AlexeyAB/darknet: YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet ) (github.com)) Em thử train 1 class cá trước và được kết quả sau khi train trên tập valid (ảnh 1) và tập test (ảnh 2) gần giống nhau Tương tự với 1 class tôm (kết quả trên tập valid là ảnh 3 và test là ảnh 4) Nhưng khi gộp dataset cá + tôm để train 2 class khì kết quả trên tập valid (ảnh 5) và tập test (ảnh 6) chênh lệch khá nhiều. Mọi người có thể cho em xin cách xử lý trong trường hợp này được không ạ?",,,,,
"#Phân_tích_thành_phần_chính
#pca
Chào mọi người. Em có chút thắc mắc về phân tích thành phần chính mong giải pháp từ các anh chị ạ.
Em có dữ liệu gồm 1000 ảnh về nhận dạng quần áo. Mỗi ảnh có kích thước 28*28 (gray scale). Khi kéo dãn ra sẽ là ma trận (1000*784) .
Em dùng phân tích thành phần chính giảm số chiều từ 784 xuống còn 2 chiều và trực quan hóa nó trong không gian 2d.
Ở 2 ảnh dưới chính là Trực quan hóa với 2 chiều sau khi em phân tích thành phần chính.
- Ảnh 1 : Lấy 2 chiều cuối cùng
- Ảnh 2 : Lấy 2 chiều đầu tiên
- Ảnh 3 : Code
Ở đây em có 2 câu hỏi ạ .
Q1 :  Như code em viết ở ảnh 3 , sau khi dùng thư viện pca của sklearn thì các chiều được xếp theo thứ tự chiều thể hiện ít dữ liệu xếp trước còn chiều nhiều dữ liệu thì xếp sau đúng không ạ ?
Q2 : Giả sử như Q1 đúng thì tại sao ở Ảnh 1 nơi có 2 chiều thể hiện dữ liệu nhiều nhất lại có phân bố một cách ngẫu nhiên xung quanh tâm (giao của 2 kì vọng 2 chiều đó). Còn ở Ảnh 2 có sự phân bố rõ ràng hơn  , không đến nỗi tách được tuyến tính hoặc gần tách được tuyến tính nhưng nhin nó lại dễ dàng phân ra các cụm hơn . Em không biết tại sao lại có hiện tượng này ạ ?","Chào mọi người. Em có chút thắc mắc về phân tích thành phần chính mong giải pháp từ các anh chị ạ. Em có dữ liệu gồm 1000 ảnh về nhận dạng quần áo. Mỗi ảnh có kích thước 28*28 (gray scale). Khi kéo dãn ra sẽ là ma trận (1000*784) . Em dùng phân tích thành phần chính giảm số chiều từ 784 xuống còn 2 chiều và trực quan hóa nó trong không gian 2d. Ở 2 ảnh dưới chính là Trực quan hóa với 2 chiều sau khi em phân tích thành phần chính. - Ảnh 1 : Lấy 2 chiều cuối cùng - Ảnh 2 : Lấy 2 chiều đầu tiên - Ảnh 3 : Code Ở đây em có 2 câu hỏi ạ . Q1 : Như code em viết ở ảnh 3 , sau khi dùng thư viện pca của sklearn thì các chiều được xếp theo thứ tự chiều thể hiện ít dữ liệu xếp trước còn chiều nhiều dữ liệu thì xếp sau đúng không ạ ? Q2 : Giả sử như Q1 đúng thì tại sao ở Ảnh 1 nơi có 2 chiều thể hiện dữ liệu nhiều nhất lại có phân bố một cách ngẫu nhiên xung quanh tâm (giao của 2 kì vọng 2 chiều đó). Còn ở Ảnh 2 có sự phân bố rõ ràng hơn , không đến nỗi tách được tuyến tính hoặc gần tách được tuyến tính nhưng nhin nó lại dễ dàng phân ra các cụm hơn . Em không biết tại sao lại có hiện tượng này ạ ?",#Phân_tích_thành_phần_chính	#pca,,,,
"[Chia sẻ]
Lâu rồi em/mình không viết blog, nay viết lại có hơi lủng củng mọi người thông cảm. Em/mình xin chia sẻ với góc nhìn data-centric, mong mọi người đóng góp ý kiến để em học hỏi nhiều hơn :)","[Chia sẻ] Lâu rồi em/mình không viết blog, nay viết lại có hơi lủng củng mọi người thông cảm. Em/mình xin chia sẻ với góc nhìn data-centric, mong mọi người đóng góp ý kiến để em học hỏi nhiều hơn :)",,,,,
"[Trí Tuệ Nhân Tạo Cho Mọi Người]
Chào mọi người,
Năm ngoái khi đang học PhD, tôi đã viết quyển sách ""Trí Tuệ Nhân Tạo Cho Mọi Người"" để giới thiệu trí tuệ nhân tạo tới số đông.
Hiện tại tôi đã tốt nghiệp PhD nên xin được chia sẻ với các bạn trên Forum Machine Learning Cơ Bản này.
Mời các bạn đọc sách (bản miễn phí) trên trang web cá nhân của tôi: https://www.maitieulong.com/2022/03/Sach-Tri-tue-nhan-tao-cho-moi-nguoi.html
-----------------------------------------
Lời mở đầu
Trí Tuệ Nhân Tạo Cho Mọi Người
Tác giả: Mai Tiểu Long
Trong những năm gần đây, trí tuệ nhân tạo đã trở thành một ngành thời thượng. Các ứng dụng liên quan cung cấp cho con người những tiện ích vượt trội mà chỉ vài năm trước tưởng chừng không thể nào tồn tại. Iphone cho phép mở khóa điện thoại bằng nhận dạng khuôn mặt. Youtube gợi ý phim theo sở thích. Chương trình chơi cờ vây vượt mặt con người. Tất cả tạo nên ánh hào quang xung quanh một công nghệ mới.
Mặc dù vậy, trí tuệ nhân tạo vẫn còn là một công nghệ khó hiểu đối với số đông. Đa phần các sách chuyên ngành tập trung vào việc truyền tải kiến thức cho kỹ sư phần mềm để đáp ứng nhu cầu nhân lực trước mắt. Ngược lại, báo chí và truyền thông đã ít nhiều đề cập tới nhưng vẫn chưa đủ sâu sắc và hoàn chỉnh. Nhiều độc giả cần tìm hiểu thêm về công nghệ mới thì không có nguồn để tham khảo. Đó là lý do cuốn sách này ra đời.
Đối tượng độc giả của cuốn sách này là tất cả bạn đọc quan tâm tới trí tuệ nhân tạo nhưng không nhất định phải trở thành nhà nghiên cứu chuyên nghiệp. Cuốn sách được viết theo phong cách đơn giản để có thể truyền tải hiệu quả những công nghệ liên quan. Ngoài ra, trong tình hình nước ta đang phát triển mạnh mảng kinh tế tri thức và trí tuệ nhân tạo là một mũi nhọn, tác giả hi vọng có thể góp phần giúp các nhà làm chính sách có thêm một kênh tiếp cận công nghệ mới thông qua quyển sách này.
Xin trân trọng cảm ơn!
Đại công quốc Luxembourg, tháng 4/2021,
Mai Tiểu Long
-----------------------------------------
Mục lục
Chương 0: Câu chuyện cờ vây
Chương 1: Bài kiểm tra Turing
Chương 2: Thuật toán lan truyền ngược
Chương 3: Hình ảnh và ngôn ngữ
Chương 4: Cuộc thi nhận dạng vật thể
Chương 5: Bài giảng cho mọi người
Chương 6: Người tạo giấc mơ
Chương 7: Tấn công và phòng thủ
Chương 8: Phân biệt 7 tỷ người
Chương 9: Một hướng đi khác
Chương 10: Thế nào là trí thông minh?
Lời kết","[Trí Tuệ Nhân Tạo Cho Mọi Người] Chào mọi người, Năm ngoái khi đang học PhD, tôi đã viết quyển sách ""Trí Tuệ Nhân Tạo Cho Mọi Người"" để giới thiệu trí tuệ nhân tạo tới số đông. Hiện tại tôi đã tốt nghiệp PhD nên xin được chia sẻ với các bạn trên Forum Machine Learning Cơ Bản này. Mời các bạn đọc sách (bản miễn phí) trên trang web cá nhân của tôi: https://www.maitieulong.com/2022/03/Sach-Tri-tue-nhan-tao-cho-moi-nguoi.html ----------------------------------------- Lời mở đầu Trí Tuệ Nhân Tạo Cho Mọi Người Tác giả: Mai Tiểu Long Trong những năm gần đây, trí tuệ nhân tạo đã trở thành một ngành thời thượng. Các ứng dụng liên quan cung cấp cho con người những tiện ích vượt trội mà chỉ vài năm trước tưởng chừng không thể nào tồn tại. Iphone cho phép mở khóa điện thoại bằng nhận dạng khuôn mặt. Youtube gợi ý phim theo sở thích. Chương trình chơi cờ vây vượt mặt con người. Tất cả tạo nên ánh hào quang xung quanh một công nghệ mới. Mặc dù vậy, trí tuệ nhân tạo vẫn còn là một công nghệ khó hiểu đối với số đông. Đa phần các sách chuyên ngành tập trung vào việc truyền tải kiến thức cho kỹ sư phần mềm để đáp ứng nhu cầu nhân lực trước mắt. Ngược lại, báo chí và truyền thông đã ít nhiều đề cập tới nhưng vẫn chưa đủ sâu sắc và hoàn chỉnh. Nhiều độc giả cần tìm hiểu thêm về công nghệ mới thì không có nguồn để tham khảo. Đó là lý do cuốn sách này ra đời. Đối tượng độc giả của cuốn sách này là tất cả bạn đọc quan tâm tới trí tuệ nhân tạo nhưng không nhất định phải trở thành nhà nghiên cứu chuyên nghiệp. Cuốn sách được viết theo phong cách đơn giản để có thể truyền tải hiệu quả những công nghệ liên quan. Ngoài ra, trong tình hình nước ta đang phát triển mạnh mảng kinh tế tri thức và trí tuệ nhân tạo là một mũi nhọn, tác giả hi vọng có thể góp phần giúp các nhà làm chính sách có thêm một kênh tiếp cận công nghệ mới thông qua quyển sách này. Xin trân trọng cảm ơn! Đại công quốc Luxembourg, tháng 4/2021, Mai Tiểu Long ----------------------------------------- Mục lục Chương 0: Câu chuyện cờ vây Chương 1: Bài kiểm tra Turing Chương 2: Thuật toán lan truyền ngược Chương 3: Hình ảnh và ngôn ngữ Chương 4: Cuộc thi nhận dạng vật thể Chương 5: Bài giảng cho mọi người Chương 6: Người tạo giấc mơ Chương 7: Tấn công và phòng thủ Chương 8: Phân biệt 7 tỷ người Chương 9: Một hướng đi khác Chương 10: Thế nào là trí thông minh? Lời kết",,,,,
"[Text to Image]
DALL-E2 vẫn chưa hết hot thì mới đây Google giới thiệu một mô hình mới là Imagen đánh bại DALL-E2, trở thành SOTA trong bài toán chuyển từ text sang ảnh ^^.
Về kiến trúc, Imagen đơn giản hơn nhiều so với DALL-E2.
Mọi người đọc thêm ở đây:
Page: https://imagen.research.google/
Github: https://github.com/lucidrains/imagen-pytorch
Paper: https://arxiv.org/pdf/2205.11487.pdf","[Text to Image] DALL-E2 vẫn chưa hết hot thì mới đây Google giới thiệu một mô hình mới là Imagen đánh bại DALL-E2, trở thành SOTA trong bài toán chuyển từ text sang ảnh ^^. Về kiến trúc, Imagen đơn giản hơn nhiều so với DALL-E2. Mọi người đọc thêm ở đây: Page: https://imagen.research.google/ Github: https://github.com/lucidrains/imagen-pytorch Paper: https://arxiv.org/pdf/2205.11487.pdf",,,,,
"Xin được chia sẻ với các bạn hai hướng tiếp cận trong trường hợp thiếu dữ liệu có nhãn là Active Learning và Semi supervised learning. Các bạn like share để mình có thêm động lực chia sẻ các phần tiếp theo nhé.
https://viblo.asia/p/lam-gi-khi-mo-hinh-hoc-may-thieu-du-lieu-co-nhan-phan-1-tong-quan-ve-active-learning-RQqKLRLbl7z
https://viblo.asia/p/lam-gi-khi-mo-hinh-hoc-may-thieu-du-lieu-co-nhan-phan-2-semi-supervised-learning-vyDZOREkKwj",Xin được chia sẻ với các bạn hai hướng tiếp cận trong trường hợp thiếu dữ liệu có nhãn là Active Learning và Semi supervised learning. Các bạn like share để mình có thêm động lực chia sẻ các phần tiếp theo nhé. https://viblo.asia/p/lam-gi-khi-mo-hinh-hoc-may-thieu-du-lieu-co-nhan-phan-1-tong-quan-ve-active-learning-RQqKLRLbl7z https://viblo.asia/p/lam-gi-khi-mo-hinh-hoc-may-thieu-du-lieu-co-nhan-phan-2-semi-supervised-learning-vyDZOREkKwj,,,,,
"Có bạn nào muốn qua Na Uy làm post doc không
Postdoctoral Research Fellow at the intersection of artificial intelligence and biology:
https://www.jobbnorge.no/en/available-jobs/job/227549/postdoctoral-research-fellow-at-the-intersection-of-artificial-intelligence-and-biology
Postdoctoral Research Fellow within explainable artificial intelligence for clinical prognostic tools:
https://www.simula.no/about/job/postdoctoral-fellow-within-explainable-artificial-intelligence-clinical-prognostic-tools
Qua mình dẫn đi chơi :))",Có bạn nào muốn qua Na Uy làm post doc không Postdoctoral Research Fellow at the intersection of artificial intelligence and biology: https://www.jobbnorge.no/en/available-jobs/job/227549/postdoctoral-research-fellow-at-the-intersection-of-artificial-intelligence-and-biology Postdoctoral Research Fellow within explainable artificial intelligence for clinical prognostic tools: https://www.simula.no/about/job/postdoctoral-fellow-within-explainable-artificial-intelligence-clinical-prognostic-tools Qua mình dẫn đi chơi :)),,,,,
"[Hopular - Đột phá của Deep Learning trong dữ liệu bảng]
Mặc dù Deep Learning xử lý rất tốt với dữ liệu dạng hình ảnh, ngôn ngữ tự nhiên, nhưng với dữ liệu dạng bảng thì các thuật toán boosting như XGBoost, CatBoost hay LightGBM thường có kết quả tốt hơn. Trong bài này tác giả đề suất mô hình deep learning “Hopular” cho hiệu suất tốt hơn các thuật toán Machine Learning kể trên với cả dữ liệu cỡ vừa (10,000 dữ liệu) và dữ liệu cỡ nhỏ (ít hơn 1000 dữ liệu). Cái hay của mạng Hopfield là mỗi layer có thể truy xuất trực tiếp tới các dữ liệu trong dataset thông qua mạng Hopfield.
Paper: https://arxiv.org/abs/2206.00664
Github: https://github.com/ml-jku/hopular
Project page: https://ml-jku.github.io/hopular/","[Hopular - Đột phá của Deep Learning trong dữ liệu bảng] Mặc dù Deep Learning xử lý rất tốt với dữ liệu dạng hình ảnh, ngôn ngữ tự nhiên, nhưng với dữ liệu dạng bảng thì các thuật toán boosting như XGBoost, CatBoost hay LightGBM thường có kết quả tốt hơn. Trong bài này tác giả đề suất mô hình deep learning “Hopular” cho hiệu suất tốt hơn các thuật toán Machine Learning kể trên với cả dữ liệu cỡ vừa (10,000 dữ liệu) và dữ liệu cỡ nhỏ (ít hơn 1000 dữ liệu). Cái hay của mạng Hopfield là mỗi layer có thể truy xuất trực tiếp tới các dữ liệu trong dataset thông qua mạng Hopfield. Paper: https://arxiv.org/abs/2206.00664 Github: https://github.com/ml-jku/hopular Project page: https://ml-jku.github.io/hopular/",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 4/2022 vào trong comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 4/2022 vào trong comment của post này.",,,,,
"Hỏi về Code R
Chào m.n. Em đang thực hiện phân loại whitepaper của các ICO theo ROI để đánh giá nó thành công hay không. (sử dụng mô hình Naive Bayes)
Hiện data em đang có tổng cộng 101 file text các whitepaper. Các whitepaper thành công sẽ có chỉ số ROI >100%, và không thành công là ROI dưới 30%, ngoài ra còn có điều kiện là whitepaper nào có lợi nhuận bé hơn 50% và có chỉ số ATH ROI bé hơn 100% thì cũng ko thành công nốt.
Em có được hỗ trợ 1 đoạn code để thực hiện (hình bên dưới). Nhưng có một phần ko hiểu nên muốn nhờ hỗ trợ. Đoạn code em đang ko hiểu là:
Data$type[36:101] <- ""unsuccessful""
Data$type[90:99] <- ""successful""
Mong nhận được sự hỗ trợ của m.n ạ! Em mới biết sơ lược về R nên nếu thông tin đưa ra có thiếu và cần bổ sung để dễ lý giải vấn đề mong m.n có thể cmt để em bổ sung, mong m.n hỗ trợ ạ!
 — với Hồng Yên.","Hỏi về Code R Chào m.n. Em đang thực hiện phân loại whitepaper của các ICO theo ROI để đánh giá nó thành công hay không. (sử dụng mô hình Naive Bayes) Hiện data em đang có tổng cộng 101 file text các whitepaper. Các whitepaper thành công sẽ có chỉ số ROI >100%, và không thành công là ROI dưới 30%, ngoài ra còn có điều kiện là whitepaper nào có lợi nhuận bé hơn 50% và có chỉ số ATH ROI bé hơn 100% thì cũng ko thành công nốt. Em có được hỗ trợ 1 đoạn code để thực hiện (hình bên dưới). Nhưng có một phần ko hiểu nên muốn nhờ hỗ trợ. Đoạn code em đang ko hiểu là: Data$type[36:101] <- ""unsuccessful"" Data$type[90:99] <- ""successful"" Mong nhận được sự hỗ trợ của m.n ạ! Em mới biết sơ lược về R nên nếu thông tin đưa ra có thiếu và cần bổ sung để dễ lý giải vấn đề mong m.n có thể cmt để em bổ sung, mong m.n hỗ trợ ạ! — với Hồng Yên.",,,,,
"Em chào mọi người ạ
Em hiện tại đang làm về project nhận diện hành động của người dựa trên khung xương. Em đã train được model để nhận diện rồi. Khi train thì mỗi hành động nó sẽ có số frame thực hiện khác nhau.
Bây giờ e muốn áp dụng model đó vào video ( video trên tập test ), nhưng bị gặp vấn đề như sau ạ
Vì là làm trên video có nhiều hành động nên e dùng cửa sổ trượt, cứ khi nào có hành động và stack đủ 32 frame thì e đưa vào nhận dạng. Vì vậy từ data train ban đầu, ví dụ hành động A thực hiện trong 60 frame chẳng hạn, thì e tách ra thành những đoạn 32 frame, stride = 4 frame để đưa vào train. Tuy nhiên, có vẻ như lựa chọn như vậy sẽ không đủ thông tin của hành động, nên khi gán nhãn sẽ có nhưng cửa sổ 32 frame có thông tin khung xương tương đồng nhưng nhãn lại khác nhau, nên khi train mô hình ko hội tụ được. 
Vì những hành động có thời gian thực hiện khác nhau nên e ko biết lựa chọn kích thước cửa sổ bao nhiêu để train cho phù hợp.  ( có hành động thực hiện trong 60 frame, có hành động lại thực hiện trong 30 frame , ... )
Em có tham khảo một số bài toán khác ( vd violence detection, real time action recognition open pose, hand gesture recognition ) thì khi áp dụng trên video real time thì họ đều tiếp cận dựa trên pp cửa sổ trượt như thế này ạ.
Vì vậy, e rất mong anh/chị nào đang làm về nhận dạng hành động nói chung hoặc nhận dạng dựa trên khung xương nói riêng có thể cho e góp ý để có thể áp dụng model đã train để nhận dạng đc hành động liên tục trên video ko ạ.
Em xin chân thành cảm ơn mọi người.","Em chào mọi người ạ Em hiện tại đang làm về project nhận diện hành động của người dựa trên khung xương. Em đã train được model để nhận diện rồi. Khi train thì mỗi hành động nó sẽ có số frame thực hiện khác nhau. Bây giờ e muốn áp dụng model đó vào video ( video trên tập test ), nhưng bị gặp vấn đề như sau ạ Vì là làm trên video có nhiều hành động nên e dùng cửa sổ trượt, cứ khi nào có hành động và stack đủ 32 frame thì e đưa vào nhận dạng. Vì vậy từ data train ban đầu, ví dụ hành động A thực hiện trong 60 frame chẳng hạn, thì e tách ra thành những đoạn 32 frame, stride = 4 frame để đưa vào train. Tuy nhiên, có vẻ như lựa chọn như vậy sẽ không đủ thông tin của hành động, nên khi gán nhãn sẽ có nhưng cửa sổ 32 frame có thông tin khung xương tương đồng nhưng nhãn lại khác nhau, nên khi train mô hình ko hội tụ được. Vì những hành động có thời gian thực hiện khác nhau nên e ko biết lựa chọn kích thước cửa sổ bao nhiêu để train cho phù hợp. ( có hành động thực hiện trong 60 frame, có hành động lại thực hiện trong 30 frame , ... ) Em có tham khảo một số bài toán khác ( vd violence detection, real time action recognition open pose, hand gesture recognition ) thì khi áp dụng trên video real time thì họ đều tiếp cận dựa trên pp cửa sổ trượt như thế này ạ. Vì vậy, e rất mong anh/chị nào đang làm về nhận dạng hành động nói chung hoặc nhận dạng dựa trên khung xương nói riêng có thể cho e góp ý để có thể áp dụng model đã train để nhận dạng đc hành động liên tục trên video ko ạ. Em xin chân thành cảm ơn mọi người.",,,,,
"Chào mọi người, hiện em đang tìm hiểu về các bài toán video understanding và đang thực hiện bài toán nhận biết vùng hành động, động tác, cụ thể là các động tác khởi động nhẹ như video dưới.
Hiện tại những bộ dữ liệu hiện có mà em tìm được (thumos14, actnet1.3, ...) thường gồm những hành động dài, camera không cố định và không có class nào phù hợp với bài của em (hoặc là em chưa tìm ra @@), còn những bộ data về khởi động em tìm được thường chỉ có features (vị trí các keypoints), chứ không có videos (UI-PRMD, KIMORE, ...). Em đang muốn một bộ dữ liệu bao gồm những động tác khởi động thể dục nhẹ (camera có thể cố định được thì tốt), với mỗi lần thực hiện xong một động tác thì được coi là 1 vùng hành động (segments).
Trong đây có bác nào tìm hiểu về mảng này có thể gợi ý hướng đi hoặc bộ dữ liệu phù hợp giúp em được không ạ.
Em xin cảm ơn!","Chào mọi người, hiện em đang tìm hiểu về các bài toán video understanding và đang thực hiện bài toán nhận biết vùng hành động, động tác, cụ thể là các động tác khởi động nhẹ như video dưới. Hiện tại những bộ dữ liệu hiện có mà em tìm được (thumos14, actnet1.3, ...) thường gồm những hành động dài, camera không cố định và không có class nào phù hợp với bài của em (hoặc là em chưa tìm ra @@), còn những bộ data về khởi động em tìm được thường chỉ có features (vị trí các keypoints), chứ không có videos (UI-PRMD, KIMORE, ...). Em đang muốn một bộ dữ liệu bao gồm những động tác khởi động thể dục nhẹ (camera có thể cố định được thì tốt), với mỗi lần thực hiện xong một động tác thì được coi là 1 vùng hành động (segments). Trong đây có bác nào tìm hiểu về mảng này có thể gợi ý hướng đi hoặc bộ dữ liệu phù hợp giúp em được không ạ. Em xin cảm ơn!",,,,,
" Hi anh, chị,
Anh, chị cho em hỏi là em có 4 mô hình sau: Random Forest, KNN, Logistic Regresstion, SVC, em muốn kết hợp 2 mô hình từng cặp với nhau: 
VD: kết hợp mô hình logistic Regression với Random Forest, em có dùng Stacking và Voting, như vậy thì có đúng không.?
- Em có đọc tài liệu thì biết được dùng ""ensemble methods classification"" tuy nhiên em còn chưa hiểu rõ lắm.
Bagging: Em chưa hiểu phương thức này
Stacking: kết hợp được nhiều mô hình - em đã làm được
Voting: kết hợp được nhiều mô hình - em đã làm được
Boosting: Em chưa hiểu phương thức này
- Em nhờ các anh, chị giải thích hoặc chỉ giúp em ""ensemble methods classification"", em xin cảm ơn.","Hi anh, chị, Anh, chị cho em hỏi là em có 4 mô hình sau: Random Forest, KNN, Logistic Regresstion, SVC, em muốn kết hợp 2 mô hình từng cặp với nhau: VD: kết hợp mô hình logistic Regression với Random Forest, em có dùng Stacking và Voting, như vậy thì có đúng không.? - Em có đọc tài liệu thì biết được dùng ""ensemble methods classification"" tuy nhiên em còn chưa hiểu rõ lắm. Bagging: Em chưa hiểu phương thức này Stacking: kết hợp được nhiều mô hình - em đã làm được Voting: kết hợp được nhiều mô hình - em đã làm được Boosting: Em chưa hiểu phương thức này - Em nhờ các anh, chị giải thích hoặc chỉ giúp em ""ensemble methods classification"", em xin cảm ơn.",,,,,
"Các anh chị cho em hỏi: Để đánh giá mô hình, em sẽ lặp 100 lần, mỗi lần đánh giá 5foldCV. Tính AUC trung bình thì em hiểu, nhưng đến khi vẽ biểu đồ thì em chưa biết nên thể hiện cái gì. Nếu chỉ lặp 1 lần thì em vẽ 1 biểu đồ gồm 5 đường tương ứng 5 fold. Bây giờ lặp 100 lần thì em nên thể hiện như thế nào ạ?
Cám ơn mn!","Các anh chị cho em hỏi: Để đánh giá mô hình, em sẽ lặp 100 lần, mỗi lần đánh giá 5foldCV. Tính AUC trung bình thì em hiểu, nhưng đến khi vẽ biểu đồ thì em chưa biết nên thể hiện cái gì. Nếu chỉ lặp 1 lần thì em vẽ 1 biểu đồ gồm 5 đường tương ứng 5 fold. Bây giờ lặp 100 lần thì em nên thể hiện như thế nào ạ? Cám ơn mn!",,,,,
"SHARE SOLUTION BKAI-NAVER Challenge 2022
Nhóm OVERFIT xin chia sẻ slide + báo cáo cho task Body Segmentation và Gesture Recognition. Solution đã đạt Top 1 tại cuộc thi. Về phần code hiện tại bọn mình có đang viết 1 paper liên quan nên khi nào có kết qủa của paper thì bên mình sẽ share full cả paper và source code sau ạ. Nhóm mình cũng rất hi vọng nhận được sự chia sẻ của các đội bạn ạ",SHARE SOLUTION BKAI-NAVER Challenge 2022 Nhóm OVERFIT xin chia sẻ slide + báo cáo cho task Body Segmentation và Gesture Recognition. Solution đã đạt Top 1 tại cuộc thi. Về phần code hiện tại bọn mình có đang viết 1 paper liên quan nên khi nào có kết qủa của paper thì bên mình sẽ share full cả paper và source code sau ạ. Nhóm mình cũng rất hi vọng nhận được sự chia sẻ của các đội bạn ạ,,,,,
"[Article Summarization - Final Project Deep Learning K1] English version in the last.
▶️ Sự bùng nổ internet và truyền thông đa phương tiện dẫn tới một lượng dữ liệu văn bản khổng lồ là tài nguyên rất quí giá nhưng chưa được khai phá hết. Có rất nhiều tác vụ NLP khác nhau được thiết kế để khai thác thông tin từ dữ liệu này như: Machine Learning Translation, Sentiment Analysis, Name Entity Recognition, Part of Speech, Question and Answering, Text Summarization, Text Generation,…. Từ những thuật toán này chúng ta có thể phát triển được những ứng dụng trong nhiều lĩnh vực khác nhau.
▶️ Trong đó Text Summarization là phương pháp quan trọng nhằm tổng hợp nội dung của một văn bản dài thành một văn bản nhỏ gọn nhưng vẫn bảo toàn được nội dung chính của của văn bản gốc. Trong nghiên cứu này hai bạn học viên Deep Learning K1 là Thức Đặng và Trọng Hiếu đã phát triển các mô hình tóm tắt văn bản tự động (ATS - Automatic Text Summarization) và cho ra đời một ứng dụng web hữu ích.
▶️ Về lớp các mô hình áp dụng, dữ liệu và kết quả đánh giá mô hình các bạn có thể theo dõi tại slide: https://docs.google.com/presentation/d/10hltPpkzHQ9otUaZunijWWbFddxPiEqa/edit?usp=sharing&ouid=102966915028274203909&rtpof=true&sd=true
▶️ Kết quả web demo được tiến hành đối với một số bài báo thuộc nhiều lĩnh vực khác nhau như Thể Thao, Kinh Tế, Chính Trị, Xã Hội tương đối chuẩn xác. Các bạn có thể theo dõi kết quả demo tại link: https://youtu.be/LBNWYvntjZ0 ----------------------------------------------------------------
▶️ The internet boom and multimedia facilitate a generation of the immense amount of document data that is a precious resource but was not yet thoroughly mined. a variety of NLP tasks was particularly created to serve mining this information from this resource such as Machine Learning Translation, Sentiment Analysis, Named Entity Recognition, Part of Speech, Question and Answering, Text Summarization, Text Generation,…. Based on those algorithms we can develop many applications in multidisciplinary.
▶️ Text summarization is an important method towards summarize the main content of a long-form document to become a short form whereas still reserving the main content of the original article. In this research, two young researchers Thuc Dang and Trong Hieu developed Automatic Text Summarization and launched a useful web application.
▶️ The algorithms, datasets, and evaluation reports were listed in the slide: https://docs.google.com/presentation/d/10hltPpkzHQ9otUaZunijWWbFddxPiEqa/edit?usp=sharing&ouid=102966915028274203909&rtpof=true&sd=true
▶️ The web demo makes predictions in the several articles of diverse sectors such as sport, economy, politics, and society are accurate and promising as the below link: https://youtu.be/LBNWYvntjZ0","[Article Summarization - Final Project Deep Learning K1] English version in the last. Sự bùng nổ internet và truyền thông đa phương tiện dẫn tới một lượng dữ liệu văn bản khổng lồ là tài nguyên rất quí giá nhưng chưa được khai phá hết. Có rất nhiều tác vụ NLP khác nhau được thiết kế để khai thác thông tin từ dữ liệu này như: Machine Learning Translation, Sentiment Analysis, Name Entity Recognition, Part of Speech, Question and Answering, Text Summarization, Text Generation,…. Từ những thuật toán này chúng ta có thể phát triển được những ứng dụng trong nhiều lĩnh vực khác nhau. Trong đó Text Summarization là phương pháp quan trọng nhằm tổng hợp nội dung của một văn bản dài thành một văn bản nhỏ gọn nhưng vẫn bảo toàn được nội dung chính của của văn bản gốc. Trong nghiên cứu này hai bạn học viên Deep Learning K1 là Thức Đặng và Trọng Hiếu đã phát triển các mô hình tóm tắt văn bản tự động (ATS - Automatic Text Summarization) và cho ra đời một ứng dụng web hữu ích. Về lớp các mô hình áp dụng, dữ liệu và kết quả đánh giá mô hình các bạn có thể theo dõi tại slide: https://docs.google.com/presentation/d/10hltPpkzHQ9otUaZunijWWbFddxPiEqa/edit?usp=sharing&ouid=102966915028274203909&rtpof=true&sd=true Kết quả web demo được tiến hành đối với một số bài báo thuộc nhiều lĩnh vực khác nhau như Thể Thao, Kinh Tế, Chính Trị, Xã Hội tương đối chuẩn xác. Các bạn có thể theo dõi kết quả demo tại link: https://youtu.be/LBNWYvntjZ0 ---------------------------------------------------------------- The internet boom and multimedia facilitate a generation of the immense amount of document data that is a precious resource but was not yet thoroughly mined. a variety of NLP tasks was particularly created to serve mining this information from this resource such as Machine Learning Translation, Sentiment Analysis, Named Entity Recognition, Part of Speech, Question and Answering, Text Summarization, Text Generation,…. Based on those algorithms we can develop many applications in multidisciplinary. Text summarization is an important method towards summarize the main content of a long-form document to become a short form whereas still reserving the main content of the original article. In this research, two young researchers Thuc Dang and Trong Hieu developed Automatic Text Summarization and launched a useful web application. The algorithms, datasets, and evaluation reports were listed in the slide: https://docs.google.com/presentation/d/10hltPpkzHQ9otUaZunijWWbFddxPiEqa/edit?usp=sharing&ouid=102966915028274203909&rtpof=true&sd=true The web demo makes predictions in the several articles of diverse sectors such as sport, economy, politics, and society are accurate and promising as the below link: https://youtu.be/LBNWYvntjZ0",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 5/2022 vào trong comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 5/2022 vào trong comment của post này.",,,,,
"Hi everyone,
LIVE NOW from Amazon Web Services Facebook page - the next episode of Learn with AWS Experts. In today's episodes, Donnie Prakoso, our Senior Developer Advocate, AWS will demo Amazon SageMaker Canvas and guide you how to build machine learning predictions without writing code. You can also ask the expert live questions. Don’t miss out.
https://www.facebook.com/amazonwebservices/videos/985065088939549/","Hi everyone, LIVE NOW from Amazon Web Services Facebook page - the next episode of Learn with AWS Experts. In today's episodes, Donnie Prakoso, our Senior Developer Advocate, AWS will demo Amazon SageMaker Canvas and guide you how to build machine learning predictions without writing code. You can also ask the expert live questions. Don’t miss out. https://www.facebook.com/amazonwebservices/videos/985065088939549/",,,,,
"xin chào mọi người,  em đang học đến bài này https://www.youtube.com/watch?v=ChoV5h7tw5A...
trong khóa DL của andrew Ng, em thắc mắc là tại sao trong bài giảng này càng những layer sâu đằng sau thì cái filter nó lại càng nhìn rõ ảnh hơn . Theo như e hiểu thì các feature map càng những layer sau model  học càng phức tạp thì feature map càng kiểu không thể nhận biết nó bằng mắt được,vậy thì áp filter vào thì nó cũng phải học được những thứ khó hình dung như ảnh chứ sao nó lại học đc những thứ nhìn rõ như trong video vậy ạ, mong mọi người giải thích giúp em, em cảm ơn","xin chào mọi người, em đang học đến bài này https://www.youtube.com/watch?v=ChoV5h7tw5A... trong khóa DL của andrew Ng, em thắc mắc là tại sao trong bài giảng này càng những layer sâu đằng sau thì cái filter nó lại càng nhìn rõ ảnh hơn . Theo như e hiểu thì các feature map càng những layer sau model học càng phức tạp thì feature map càng kiểu không thể nhận biết nó bằng mắt được,vậy thì áp filter vào thì nó cũng phải học được những thứ khó hình dung như ảnh chứ sao nó lại học đc những thứ nhìn rõ như trong video vậy ạ, mong mọi người giải thích giúp em, em cảm ơn",,,,,
"Xin hướng làm nhận diện hàng ô chữ số viết tay trong sheet.
Hi các bác, e đang tìm hiểu và làm 1 bài toán như sau.
Ý TƯỞNG
Có 1 sheet golf score và muốn lấy 1 hàng chữ số để điền vào ứng dụng khác. Thay vì bằng cơm thì bằng nhận diện OCR, mỗi ô là 1 số, có thể tưởng tượng như đang bê dãy số kết quả từ ảnh vào app khác có khung tương tự
CÁCH LÀM E ĐANG NGHĨ RA
Chụp hình (OK)
Cho user vẽ ô đỏ để giới hạn vùng nhận diện và xóa vùng khác (OK)
Nhận diện (chưa biết làm)
        - Nhận diện số viết tay  
        - Tách các số riêng dựa vào các ô mà chúng đang ở
    4. Đưa ra kết quả (chưa biết làm)

Ngoài ra còn có case viết tay trong 1 ô có ng viết như ảnh 2, e muốn xóa cái chữ nhỏ đi giữ lại số 8

KẾT QUẢ MONG MUỐN
Buget/ thời gian/ độ chính xác -> ưu tiên độ chính xác nhất
Ở mức độ chữ xấu vừa phải thì có nhận đúng dc khoảng 80% chữ số/ dãy
Nếu có dịch vụ cloud nào dạng mỳ ăn liền như của GCP, AWS càng tốt
Phân cách đúng dc theo ô

CÁC BÁC CÓ KINH NGHIỆM HOẶC Ý TƯỞNG HOẶC HƯỚNG LÀM XIN CHIA SẺ CHO EM Ạ.
CÁM ƠN CÁC BÁC ","Xin hướng làm nhận diện hàng ô chữ số viết tay trong sheet. Hi các bác, e đang tìm hiểu và làm 1 bài toán như sau. Ý TƯỞNG Có 1 sheet golf score và muốn lấy 1 hàng chữ số để điền vào ứng dụng khác. Thay vì bằng cơm thì bằng nhận diện OCR, mỗi ô là 1 số, có thể tưởng tượng như đang bê dãy số kết quả từ ảnh vào app khác có khung tương tự CÁCH LÀM E ĐANG NGHĨ RA Chụp hình (OK) Cho user vẽ ô đỏ để giới hạn vùng nhận diện và xóa vùng khác (OK) Nhận diện (chưa biết làm) - Nhận diện số viết tay - Tách các số riêng dựa vào các ô mà chúng đang ở 4. Đưa ra kết quả (chưa biết làm) Ngoài ra còn có case viết tay trong 1 ô có ng viết như ảnh 2, e muốn xóa cái chữ nhỏ đi giữ lại số 8 KẾT QUẢ MONG MUỐN Buget/ thời gian/ độ chính xác -> ưu tiên độ chính xác nhất Ở mức độ chữ xấu vừa phải thì có nhận đúng dc khoảng 80% chữ số/ dãy Nếu có dịch vụ cloud nào dạng mỳ ăn liền như của GCP, AWS càng tốt Phân cách đúng dc theo ô CÁC BÁC CÓ KINH NGHIỆM HOẶC Ý TƯỞNG HOẶC HƯỚNG LÀM XIN CHIA SẺ CHO EM Ạ. CÁM ƠN CÁC BÁC",,,,,
"Chào anh em. Hôm nay chúng ta sẽ cùng nhau nghiên cứu phương pháp và thử làm bài toán in-store heatmap cùng Yolov4 nhé.
Đây là một vấn đề rất hữu ích, hi vọng giúp được anh em!","Chào anh em. Hôm nay chúng ta sẽ cùng nhau nghiên cứu phương pháp và thử làm bài toán in-store heatmap cùng Yolov4 nhé. Đây là một vấn đề rất hữu ích, hi vọng giúp được anh em!",,,,,
"Xin chào mọi người,
Em đang có 1 bài tập lớn nhưng chưa biết phải bắt đầu thế nào ( đây là lần đầu em tiếp xúc với data dạng này )
Tổng Quan data em tìm hiểu được :
- Data là một chuỗi thời gian tính bằng giây có 3 trục x,y,z cho biết hành động (label) của 1 người trong 1 khoảng thời gian
- Tên file : là thời điểm 1 người bắt đầu theo format
[Accelerometer-[YYYY - MM - DD - Hour - Min - Sec]-[Label]-[Male/Female][ID]
VD : Accelerometer-2011-04-11-13-28-18-brush_teeth-f1
- Sensor hoạt động lúc 11/4/2011 vào lúc 13:28:18 với hành động đánh răng thực hiện bởi phụ nữ 1
- Data là file txt có 3 cột mỗi cột tương ứng x,y,z số dòng tương ứng với range thời gian thực hiện hành động đó như hình dưới là hơn 250 dòng tương đương hơn 250 giây
Mục tiêu : Giảm chiều các dữ liệu trên và cluster các điểm dữ liệu này và dự đoán Label
Hint từ thầy : Các điểm dữ liệu chính là mỗi file txt, sử dụng các thuật toán ( hàm thư viện ) giảm chiều dữ liệu như PCA, t-SNE, AutoEncoder,... Và dùng các bài toán cluster với K=14 tương đương với 14 label
Mọi người ai có tài liệu hoặc project gần giống với bài toán trên có thể cho em tham khảo được không ạ, công đoạn chuẩn bị data và split dataset cũng như ma trận data của bài toán sẽ như thế nào với ạ.
Em xin cảm ơn mọi người đã đọc và giúp đỡ ạ
Dataset của bài : https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer","Xin chào mọi người, Em đang có 1 bài tập lớn nhưng chưa biết phải bắt đầu thế nào ( đây là lần đầu em tiếp xúc với data dạng này ) Tổng Quan data em tìm hiểu được : - Data là một chuỗi thời gian tính bằng giây có 3 trục x,y,z cho biết hành động (label) của 1 người trong 1 khoảng thời gian - Tên file : là thời điểm 1 người bắt đầu theo format [Accelerometer-[YYYY - MM - DD - Hour - Min - Sec]-[Label]-[Male/Female][ID] VD : Accelerometer-2011-04-11-13-28-18-brush_teeth-f1 - Sensor hoạt động lúc 11/4/2011 vào lúc 13:28:18 với hành động đánh răng thực hiện bởi phụ nữ 1 - Data là file txt có 3 cột mỗi cột tương ứng x,y,z số dòng tương ứng với range thời gian thực hiện hành động đó như hình dưới là hơn 250 dòng tương đương hơn 250 giây Mục tiêu : Giảm chiều các dữ liệu trên và cluster các điểm dữ liệu này và dự đoán Label Hint từ thầy : Các điểm dữ liệu chính là mỗi file txt, sử dụng các thuật toán ( hàm thư viện ) giảm chiều dữ liệu như PCA, t-SNE, AutoEncoder,... Và dùng các bài toán cluster với K=14 tương đương với 14 label Mọi người ai có tài liệu hoặc project gần giống với bài toán trên có thể cho em tham khảo được không ạ, công đoạn chuẩn bị data và split dataset cũng như ma trận data của bài toán sẽ như thế nào với ạ. Em xin cảm ơn mọi người đã đọc và giúp đỡ ạ Dataset của bài : https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer",,,,,
"Chào mọi người, mình xin được chia sẻ code đạt top 2 cuộc thi BKAI-NAVER task gesture. Mời mọi người tham khảo ạ 😄
Link code: https://github.com/hungk64it1x/body-segmentation-and-gesture-recognition
Link cuộc thi: https://bitly.com.vn/yt3m0a","Chào mọi người, mình xin được chia sẻ code đạt top 2 cuộc thi BKAI-NAVER task gesture. Mời mọi người tham khảo ạ Link code: https://github.com/hungk64it1x/body-segmentation-and-gesture-recognition Link cuộc thi: https://bitly.com.vn/yt3m0a",,,,,
Chào mọi người. Mấy hôm nay mình đang tìm hiểu YOLOv5 và bây giờ mình có một số vấn đề không hiểu. Mọi người có thể dừng lại một chút giúp mình giải thích là mấy cái thông số kiểu 4096 ở hình dưới là làm sao ra được không?,Chào mọi người. Mấy hôm nay mình đang tìm hiểu YOLOv5 và bây giờ mình có một số vấn đề không hiểu. Mọi người có thể dừng lại một chút giúp mình giải thích là mấy cái thông số kiểu 4096 ở hình dưới là làm sao ra được không?,,,,,
"Dạ em chào m.n. em đang thực hiện text mining trên RStudio. Em thực hiện chuyển các file PDF sang dạng text và phân loại chúng.
Em đã làm sạch các dấm chấm câu và khoảng trắng trong câu. Nhưng đến phần loại bỏ sự thưa thớt của từ để tiến đến phân loại, em sử dụng đoạn code trong hình và bị báo lỗi này:
""Error in gsub(sprintf(""(*UCP)\\b(%s)\\b"", paste(sort(words, decreasing = TRUE), :
input string 135 is invalid UTF-8""
Em đã cài thư viện UTF-8 và tìm một số cách fix nhưng vẫn chưa khắc phục đc. Em chưa tìm hiểu đc nhiều về R nên mong đc các ac hỗ trợ ạ. Em cảm ơn ạ","Dạ em chào m.n. em đang thực hiện text mining trên RStudio. Em thực hiện chuyển các file PDF sang dạng text và phân loại chúng. Em đã làm sạch các dấm chấm câu và khoảng trắng trong câu. Nhưng đến phần loại bỏ sự thưa thớt của từ để tiến đến phân loại, em sử dụng đoạn code trong hình và bị báo lỗi này: ""Error in gsub(sprintf(""(*UCP)\\b(%s)\\b"", paste(sort(words, decreasing = TRUE), : input string 135 is invalid UTF-8"" Em đã cài thư viện UTF-8 và tìm một số cách fix nhưng vẫn chưa khắc phục đc. Em chưa tìm hiểu đc nhiều về R nên mong đc các ac hỗ trợ ạ. Em cảm ơn ạ",,,,,
Xin giới thiệu các bạn quan tâm đến việc phỏng vấn về Machine Learning ở các công ty FAANG. https://rebrand.ly/mldesignbook Bạn có thể đọc sample ở đây: https://rebrand.ly/MachineLearningDesignShort,Xin giới thiệu các bạn quan tâm đến việc phỏng vấn về Machine Learning ở các công ty FAANG. https://rebrand.ly/mldesignbook Bạn có thể đọc sample ở đây: https://rebrand.ly/MachineLearningDesignShort,,,,,
"em chào mn, em đang học về Perceptron thì thấy mắc đoạn này.
bình thường nếu data đơn giản ít chiều và dễ visualize thì sẽ biết là nhóm nào bên trên nhóm nào bên dưới, nhưng với data lớn và đa chiều hơn thì ko thể visual được. Giả sử ban đầu cứ gán nhãn x là +1 còn tam giác là -1 như kia, và giả sử bước đầu tiên máy chọn ra para ngẫu nhiên là (0 -1 -1) như kia thì kia tính tích vô hướng sẽ cho ra giá trị là trái dấu với label, tức là điểm x tọa độ (1,1) bị misclassified (nhưng thực ra là true classified)
Em muốn hỏi là làm sao máy nó phân biệt được để chọn para ban đầu phù hợp với cái label của data ạ? hay là nó thật sự sẽ hội tụ về para tốt nhất được dù para ban đầu có như nào ạ?
câu 2 là về phần chứng minh hội tụ, vì sao lại có thể đến 1 điểm ko còn điểm nào bị phân lỗi ạ? nếu 2 class data bị overlap nhau thì vẫn có thể có điểm bị phân lỗi chứ ạ?
Em cảm ơn!","em chào mn, em đang học về Perceptron thì thấy mắc đoạn này. bình thường nếu data đơn giản ít chiều và dễ visualize thì sẽ biết là nhóm nào bên trên nhóm nào bên dưới, nhưng với data lớn và đa chiều hơn thì ko thể visual được. Giả sử ban đầu cứ gán nhãn x là +1 còn tam giác là -1 như kia, và giả sử bước đầu tiên máy chọn ra para ngẫu nhiên là (0 -1 -1) như kia thì kia tính tích vô hướng sẽ cho ra giá trị là trái dấu với label, tức là điểm x tọa độ (1,1) bị misclassified (nhưng thực ra là true classified) Em muốn hỏi là làm sao máy nó phân biệt được để chọn para ban đầu phù hợp với cái label của data ạ? hay là nó thật sự sẽ hội tụ về para tốt nhất được dù para ban đầu có như nào ạ? câu 2 là về phần chứng minh hội tụ, vì sao lại có thể đến 1 điểm ko còn điểm nào bị phân lỗi ạ? nếu 2 class data bị overlap nhau thì vẫn có thể có điểm bị phân lỗi chứ ạ? Em cảm ơn!",,,,,
Ngày mai mình có buổi nói chuyện cùng nhiều chuyên gia về ML in production. Mời các bạn theo dõi.,Ngày mai mình có buổi nói chuyện cùng nhiều chuyên gia về ML in production. Mời các bạn theo dõi.,,,,,
"Chào mọi người, theo e được biết thì phải split train-test set rồi mới normalization hoặc standardization. Và test set phải được normalize/standardize theo tham số normalization/standardization của train set. Vậy giờ nếu e chọn normalization thì khi normalize test set, thay vì dùng
x_norm = (x - min_train)/(max_train - min_train)
(Tức, x_norm có thể < 0 hoặc > 1)
em dùng
if x > max_train: x = max_train
elif x < min_train: x = min_train
else pass
x_norm = (x - min_train)/(max_train - min_train)
(Tức, x_norm luôn thuộc đoạn [0,1])
thì có vấn đề gì ko ạ? Cảm ơn mọi người","Chào mọi người, theo e được biết thì phải split train-test set rồi mới normalization hoặc standardization. Và test set phải được normalize/standardize theo tham số normalization/standardization của train set. Vậy giờ nếu e chọn normalization thì khi normalize test set, thay vì dùng x_norm = (x - min_train)/(max_train - min_train) (Tức, x_norm có thể < 0 hoặc > 1) em dùng if x > max_train: x = max_train elif x < min_train: x = min_train else pass x_norm = (x - min_train)/(max_train - min_train) (Tức, x_norm luôn thuộc đoạn [0,1]) thì có vấn đề gì ko ạ? Cảm ơn mọi người",,,,,
"[Góc BERT]
Mình dùng PhoBERT của VinAI cho bài toán phân loại sắc thái bình luận của AIviVN. Kết quả hiện tại 0.90849 public leaderboard, so với cao nhất trước đó là 0.90087.
Một số kỹ thuật mình hay dùng để finetune Bert:
- Concat biểu diễn của token [CLS] của 4 layer cuối, (kinh nghiệm thì layer gần cuối thường là tốt nhất nếu chỉ muốn lấy 1)
- Train kiểu fastai, freeze phần pretrained trong epoch đầu để warmup rồi unfreeze train toàn bộ.
- Dùng optimizer mặc định AdamW và linear learning rate scheduler vẫn là tốt nhất.","[Góc BERT] Mình dùng PhoBERT của VinAI cho bài toán phân loại sắc thái bình luận của AIviVN. Kết quả hiện tại 0.90849 public leaderboard, so với cao nhất trước đó là 0.90087. Một số kỹ thuật mình hay dùng để finetune Bert: - Concat biểu diễn của token [CLS] của 4 layer cuối, (kinh nghiệm thì layer gần cuối thường là tốt nhất nếu chỉ muốn lấy 1) - Train kiểu fastai, freeze phần pretrained trong epoch đầu để warmup rồi unfreeze train toàn bộ. - Dùng optimizer mặc định AdamW và linear learning rate scheduler vẫn là tốt nhất.",,,,,
"UPDATE post: nhờ gợi ý của bạn Xuân Huy Nguyễn mình đã làm được. Xin cảm ơn.
Mình xin hỏi 1 vấn đề hơi out of group nhưng chưa biết hỏi ở đâu. Có bạn nào biết cách đổi tên file hàng loạt mà có hậu tố là số thứ tự cách nhau n đơn vị không, và số đầu tiên có thể bắt đầu từ một số bất kỳ.
Ví dụ mình có folder chứa 100 file tên là a_0, a_1, a_2, …, a_99. Giờ mình muốn đổi tên tất cả các file trong folder thành b_20, b_24, b_28, b_32… cho đến hết. Google mãi chưa ra cách. Xin nhờ giúp đỡ. Mình cảm ơn nhiều.","UPDATE post: nhờ gợi ý của bạn Xuân Huy Nguyễn mình đã làm được. Xin cảm ơn. Mình xin hỏi 1 vấn đề hơi out of group nhưng chưa biết hỏi ở đâu. Có bạn nào biết cách đổi tên file hàng loạt mà có hậu tố là số thứ tự cách nhau n đơn vị không, và số đầu tiên có thể bắt đầu từ một số bất kỳ. Ví dụ mình có folder chứa 100 file tên là a_0, a_1, a_2, …, a_99. Giờ mình muốn đổi tên tất cả các file trong folder thành b_20, b_24, b_28, b_32… cho đến hết. Google mãi chưa ra cách. Xin nhờ giúp đỡ. Mình cảm ơn nhiều.",,,,,
"Hi mọi người.
Em đang làm 1 project về image classification và mỗi label của e chỉ có 5 ảnh. E đã thử data augmentation và sử dụng simple model nhưng vẫn bị overfit ạ, còn nếu dùng object detection thì khá chính xác nhưng lại cần manually object annotation và project cần dễ scale nên hướng này khá khó triển khai ạ. Không biết với dataset ít như vậy thì có cách nào build đc model không bị overfitting và cũng ko bị underfiting không ạ. Em cảm ơn ạ.","Hi mọi người. Em đang làm 1 project về image classification và mỗi label của e chỉ có 5 ảnh. E đã thử data augmentation và sử dụng simple model nhưng vẫn bị overfit ạ, còn nếu dùng object detection thì khá chính xác nhưng lại cần manually object annotation và project cần dễ scale nên hướng này khá khó triển khai ạ. Không biết với dataset ít như vậy thì có cách nào build đc model không bị overfitting và cũng ko bị underfiting không ạ. Em cảm ơn ạ.",,,,,
"Hi anh, chị,
- Lại là em đây, Em làm đề tài ""Loan Repayment Prediction Using Machine Learning Algorithms"" sau khi em nghiên cứu những phương pháp anh chị góp ý kiến thì kết quả sau khi thực nghiệm của em như sau:
- Kết quả trước đó:




- Kết quả sau khi dùng Random Undersampling và Oversampling:

P/S: Với câu châm ngôn ""Học là chia sẽ""","Hi anh, chị, - Lại là em đây, Em làm đề tài ""Loan Repayment Prediction Using Machine Learning Algorithms"" sau khi em nghiên cứu những phương pháp anh chị góp ý kiến thì kết quả sau khi thực nghiệm của em như sau: - Kết quả trước đó: - Kết quả sau khi dùng Random Undersampling và Oversampling: P/S: Với câu châm ngôn ""Học là chia sẽ""",,,,,
"[Just for fun] New way to design captcha questions LoL
Credit: https://twitter.com/knmnyn/status/1298804839485304833?s=21",[Just for fun] New way to design captcha questions LoL Credit: https://twitter.com/knmnyn/status/1298804839485304833?s=21,,,,,
"Chào mọi người, em mới tìm hiểu ML, em đang có bài toán giải captcha ảnh này. Tính tổng mặt trên
Em muốn xác định các mặt(góc, cạnh) của xúc xắc từ đó tính tổng mặt trên của 2 xúc xắc trong mỗi hình liệu có khả thi không ạ?","Chào mọi người, em mới tìm hiểu ML, em đang có bài toán giải captcha ảnh này. Tính tổng mặt trên Em muốn xác định các mặt(góc, cạnh) của xúc xắc từ đó tính tổng mặt trên của 2 xúc xắc trong mỗi hình liệu có khả thi không ạ?",,,,,
"#textrecognition #labelize #labeltools
Chào mọi người,
Hiện tại e muốn làm 1 bài toán về nhận diện text trong ảnh (ảnh chỉ có 1 dòng)
Vấn đề của e bây giờ là muốn tạo label cho ảnh thực, tức là tạo text cho ảnh thực. E đã tạo synthetic data từ text ra ảnh, sau đó dùng ảnh synthetic để train, model chạy tốt, predict tốt trên ảnh test của synthetic nhưng lại không hề tốt trên ảnh thực. Vậy nên e muốn labelize thêm ảnh thực để train xem kết quả ra sao.
Hình dung là mình có tập ảnh có chứa text (mà chỉ có 1 dòng). Mục đích của mình là predict đoạn text trong ảnh ấy (bài toán OCR). Để train bài toán này thì cần có dataset là gồm có ảnh và text trong ảnh đó (ảnh và label). Hiện tại mình chỉ có ảnh mà chưa biết cách nào để có thể tạo file text (label của ảnh đó) 1 cách thuận lợi và nhanh chóng. Đó là cái mình muốn hỏi về 1 tool như thế.
1 tool giống như kiểu captcha dưới đây
Nhưng hiện tại e ko biết có công cụ/tools nào mà cho phép mình dễ dàng, thuận tiện trong việc tạo text label cho ảnh thực để làm công việc text recognition không ạ? E tìm trên mạng rồi mà vẫn chưa thấy.
E xin cảm ơn ạ.","Chào mọi người, Hiện tại e muốn làm 1 bài toán về nhận diện text trong ảnh (ảnh chỉ có 1 dòng) Vấn đề của e bây giờ là muốn tạo label cho ảnh thực, tức là tạo text cho ảnh thực. E đã tạo synthetic data từ text ra ảnh, sau đó dùng ảnh synthetic để train, model chạy tốt, predict tốt trên ảnh test của synthetic nhưng lại không hề tốt trên ảnh thực. Vậy nên e muốn labelize thêm ảnh thực để train xem kết quả ra sao. Hình dung là mình có tập ảnh có chứa text (mà chỉ có 1 dòng). Mục đích của mình là predict đoạn text trong ảnh ấy (bài toán OCR). Để train bài toán này thì cần có dataset là gồm có ảnh và text trong ảnh đó (ảnh và label). Hiện tại mình chỉ có ảnh mà chưa biết cách nào để có thể tạo file text (label của ảnh đó) 1 cách thuận lợi và nhanh chóng. Đó là cái mình muốn hỏi về 1 tool như thế. 1 tool giống như kiểu captcha dưới đây Nhưng hiện tại e ko biết có công cụ/tools nào mà cho phép mình dễ dàng, thuận tiện trong việc tạo text label cho ảnh thực để làm công việc text recognition không ạ? E tìm trên mạng rồi mà vẫn chưa thấy. E xin cảm ơn ạ.",#textrecognition	#labelize	#labeltools,,,,
"Mình chạy một cái captcha lấy source từ nguồn https://github.com/chxj1992/captcha_cracker , Captcha có định dạng 280x160 và có 6 kí tự , train 45k file và test 16,423 file.  trong file pack_data.py mình đã đổi
 classes = '12346789ABCDEFGHJMNPQRTUXYZ'
data = {'data': np.empty(shape=(0, 280, 160, 3), dtype=float), 'labels': np.empty(shape=(0, 1), dtype=int)}
Nhưng khi chạy file train.py thì bị lỗi này
    raise ValueError(str(e))
ValueError: Dimension 0 in both shapes must be equal, but are 165376 and 2688. Shapes are [165376,512] and [2688,512]. for 'Assign_8' (op: 'Assign') with input shapes: [165376,512], [2688,512].
bạn nào sửa giúp mình lỗi này cái , Thank ","Mình chạy một cái captcha lấy source từ nguồn https://github.com/chxj1992/captcha_cracker , Captcha có định dạng 280x160 và có 6 kí tự , train 45k file và test 16,423 file. trong file pack_data.py mình đã đổi classes = '12346789ABCDEFGHJMNPQRTUXYZ' data = {'data': np.empty(shape=(0, 280, 160, 3), dtype=float), 'labels': np.empty(shape=(0, 1), dtype=int)} Nhưng khi chạy file train.py thì bị lỗi này raise ValueError(str(e)) ValueError: Dimension 0 in both shapes must be equal, but are 165376 and 2688. Shapes are [165376,512] and [2688,512]. for 'Assign_8' (op: 'Assign') with input shapes: [165376,512], [2688,512]. bạn nào sửa giúp mình lỗi này cái , Thank",,,,,
"Chào mọi người ạ, hiện tại em đang làm một assignment về Machine Learning dataset, nhưng yêu cầu của thầy là không được dùng các dataset đã public.
Phiền ac nào có dataset(file .csv) và source code(.py) có thể share cho em được không ạ?
*Em cam đoan giữ tài liệu mật và không share lung tung ạ!
Dataset should contain a set of independent variables (3 or more) and a dependent variable.
*Dependent Variable: Variable which you are going to predict *Independent Variable: Variables that are used to predict the dependent variable. (Things that influence dependent variable) *Number of entries: No limit
Cảm ơn mn đã đọc bài,","Chào mọi người ạ, hiện tại em đang làm một assignment về Machine Learning dataset, nhưng yêu cầu của thầy là không được dùng các dataset đã public. Phiền ac nào có dataset(file .csv) và source code(.py) có thể share cho em được không ạ? *Em cam đoan giữ tài liệu mật và không share lung tung ạ! Dataset should contain a set of independent variables (3 or more) and a dependent variable. *Dependent Variable: Variable which you are going to predict *Independent Variable: Variables that are used to predict the dependent variable. (Things that influence dependent variable) *Number of entries: No limit Cảm ơn mn đã đọc bài,",,,,,
"Chào mọi người.
Em mới tìm hiểu về bài toán dịch máy và đang muốn xây dựng mô hình transformers. Mọi người cho em hỏi là với tokenizer thì mình có thể sử dụng lại tokenizer mà có cùng thuật toán như BPE của một model khác, hay là nên tự train lại tokenizer dựa trên dữ liệu dịch của chính mình thôi ạ.
Và với một bài toán dịch máy có phải là cần 2 tokenizer, một cái để encode và một cái để decode không ạ.
Mọi người có kinh nghiệm có thể giúp em thêm ít gợi ý với ạ. Cảm ơn mọi người đã đọc","Chào mọi người. Em mới tìm hiểu về bài toán dịch máy và đang muốn xây dựng mô hình transformers. Mọi người cho em hỏi là với tokenizer thì mình có thể sử dụng lại tokenizer mà có cùng thuật toán như BPE của một model khác, hay là nên tự train lại tokenizer dựa trên dữ liệu dịch của chính mình thôi ạ. Và với một bài toán dịch máy có phải là cần 2 tokenizer, một cái để encode và một cái để decode không ạ. Mọi người có kinh nghiệm có thể giúp em thêm ít gợi ý với ạ. Cảm ơn mọi người đã đọc",,,,,
"Em chào mọi người ạ
Em đang làm 1 project về ML ( một bài toán phân loại 3 lớp). Hiện tại bước đầu là phải xử lý data thô để tìm ra đặc trưng mà khác biệt giữa các lớp để tìm mô hình phù hợp ( gọi là phân tích khám phá dữ liệu EDA ạ).
Em xin mô tả dữ liệu một chút, đại khái nó gồm dữ liệu được ghi lại liên tục trong vòng 6 tháng và nhãn lớp của nó. Trong mỗi tháng đó bao gồm 10 trường thuộc tính khác nhau ( dữ liệu bao gồm cả numerical và categorical) => Quan sát 10 trường thuộc tính đó trong 6 tháng liên tiếp. Em đã thử phân tích đơn biến (univariate analysis) sử dụng một số biểu đồ như box plot, smooth density plot , tính mean, std, distribution,....(chỉ phân tích 1 trường thuộc tính qua các tháng) nhưng vẫn chưa phát hiện ra điểm khác biệt gì từ dữ liệu
Vậy e muốn hỏi các a/c là bây giờ mình phân tích đa biến multivariate thì mình phân tích có những kiểu pp nào ạ, và với dữ liệu như kiểu bài toán của e (quan sát trường thuộc tính trong 6 tháng liên tiếp) thì mình có cách EDA đa biến nào phù hợp để phân tích data ko ạ.
Rất mong a/c ai đã làm DE/DA/DS rồi có thể cho e lời khuyên ạ. Em cảm ơn các a/c nhiều","Em chào mọi người ạ Em đang làm 1 project về ML ( một bài toán phân loại 3 lớp). Hiện tại bước đầu là phải xử lý data thô để tìm ra đặc trưng mà khác biệt giữa các lớp để tìm mô hình phù hợp ( gọi là phân tích khám phá dữ liệu EDA ạ). Em xin mô tả dữ liệu một chút, đại khái nó gồm dữ liệu được ghi lại liên tục trong vòng 6 tháng và nhãn lớp của nó. Trong mỗi tháng đó bao gồm 10 trường thuộc tính khác nhau ( dữ liệu bao gồm cả numerical và categorical) => Quan sát 10 trường thuộc tính đó trong 6 tháng liên tiếp. Em đã thử phân tích đơn biến (univariate analysis) sử dụng một số biểu đồ như box plot, smooth density plot , tính mean, std, distribution,....(chỉ phân tích 1 trường thuộc tính qua các tháng) nhưng vẫn chưa phát hiện ra điểm khác biệt gì từ dữ liệu Vậy e muốn hỏi các a/c là bây giờ mình phân tích đa biến multivariate thì mình phân tích có những kiểu pp nào ạ, và với dữ liệu như kiểu bài toán của e (quan sát trường thuộc tính trong 6 tháng liên tiếp) thì mình có cách EDA đa biến nào phù hợp để phân tích data ko ạ. Rất mong a/c ai đã làm DE/DA/DS rồi có thể cho e lời khuyên ạ. Em cảm ơn các a/c nhiều",,,,,
"[Deep learning for Candlestick - Patterns recognition in Financial market]
Đây là một project khá nổi bật mà hai bạn sinh viên Trọng Duy và Đức Quang của Deep Learning K1 đã thực hiện về đề tài nhận dạng mẫu biểu đồ đối với thị trường chứng khoán.
1. Đặt vấn đề:
Thị trường chứng khoán luôn tiềm ẩn rủi ro cao đối với những nhà đầu tư. Có nhiều phương pháp khác nhau trong học máy và kinh tế lượng để phân tích và dự đoán cổ phiếu trong ngắn hạn và dài hạn. Trong đó kĩ thuật phân tích mẫu cổ phiếu là một trong những kĩ thuật đơn giản và hiệu quả được các nhà đầu tư thực hiện.
Thông thường quá trình nhận diện mẫu là một công việc đòi hỏi sự tinh tường và kinh nghiệm chuyên sâu từ chuyên gia. Do đó khó có thể large scale trên số lượng lớn vài trăm, hoặc vài ngàn cổ phiếu. Trong nghiên cứu này, hai bạn học viên của Deep Learning K1 là Trọng Duy và Đức Quang đã xây dựng một cách tiếp cận mới sử dụng bài toán computer vision trong nhận dạng mẫu cổ phiếu và đạt được độ chính xác cao so với các thuật toán hiện tại.
2. Phương pháp:
2.1. Tập dataset
Bộ dữ liệu được phục vụ cho trị trường chứng khoán Việt Nam nên các mẫu được lựa chọn là những pattern giao dịch của 30mã chứng khoán và 30 chỉ sốngoại hối của các các sàn chứng khoán Việt Nam. Nghiên cứu tập trung vào phát hiện pattern trong ngắn hạn nên các mẫu dữ liệu được lựa chọn là 5 phiên giao dịch của 1 tuần liên tiếp với kích thước bước nhảy là 1 ngày.  Nghiên cứu này tập trung vào phát hiện 4patterns chính bao gồm:
- HSA: Head and Shoulder. Pattern này mô tả sự đảo ngược của xu hướng tăng sang giảm với đỉnh Head ở giữa và hai đỉnh Shoulder thấp hơn ở hai bên.
https://imgur.com/8uV2uWm.png
- IHSA: Inverted Head and Shoulders nhằm mô tả sự đảo ngược từ giảm sang tăng với đỉnh Head ở giữa sẽ thấp hơn so với hai đỉnh Shoulder ở hai bên.
https://imgur.com/UVdHKP7.png
- DT: Double Topđược hình thành từ hai đỉnh liên tiếp. Phần giữa hai đỉnh tạo thành hình chữ U lộn ngược. Hai đỉnh có giá trị gần tương tự nhau với mức chênh lệch ít hơn 3%. Phần đáy  giữa hai đỉnh nhỏ hơn đỉnh từ 10 – 20%.
https://imgur.com/9ZruggJ.png
- DB: Double Bottom là đảo ngược của Double Top.
https://imgur.com/mHImCMd.png
2.2. Xử lý dữ liệu
Dữ liệu được chuyển từ candle chart sang segmentation chart để hạn chế nhiễu và sau đó thực hiện các bước xử lý như hình 1:
https://imgur.com/CDYA8ZB.png
Step 1: Chọn hai điểm trong một đoạn trong biểu đồ segmentation.
Step 2: Chọn một điểm khác sao cho có tổng khoảng cách lớn nhất để bắt đầu và kết thúc đoạn đã chọn.
Step 3: Lặp lại Step 1 và Step 2 cho đến khi có đủ điểm trong biểu đồ segmentation. Bằng cách này,các noise có thể được loại bỏ và sẽ thu được một đồ thị đường đơn giản hơn.
2.3. Sinh mẫu ngẫu nhiên
2.3.1. Biến đổi theo trục x:
- Đối với pattern HSA và IHSA.
Tác giả loại bỏ những mẫu đối với pattern HSA không hợp lệ (có đỉnh thứ 2 thấp hơn hai đỉnh còn lại). Sau đó tạo biến thể ngẫu nhiên trên những mẫu đạt tiêu chuẩn bằng cách khởi tạo ngẫu nhiên những biến động x_p1, x_p2, x_p3; xóa đi một đoạn bất kì tại vị trí đáy và thay thế một đoạn giá ngẫu nhiên. Qui trình thực hiện như hình 2:
https://imgur.com/e39JwNh.png
Phương pháp này được thực hiện tương tự đối với các pattern IHSA.
- Đối với pattern DT và DB:
Lựa chọn những mẫu mà có 2 đỉnh gần bằng nhau nhưng chưa phải là DT hoặc DB. Tiếp theo so sánh hai đỉnh, sau đó xóa đi đỉnh cao hơn và dùng phân phối chuẩn để tạo thành các mẫu DT và DB phù hợp như hình 3:
https://imgur.com/07dR2bg.png
2.3.2. Biến đổi theo trục y:
Theo trục y chúng ta chỉ cần thực hiện thay đổi giá. Đối với hai pattern HSA và IHSA phải đảm bảo đỉnh thứ 2 phải cao hơn 2 đỉnh còn lại. Đối với DT và DB cần đảm bảo 2 đỉnh có độ cao bằng nhau như hình 4:
https://imgur.com/dwXkIHw.png
3. Kết quả đạt được:
Thử nghiệm trên 5 lớp mô hình khác nhau của Object Detections: Scaled YOLOv4, YOLOX, YOLOR,  EfficientDet,  DETR cho thấy Scaled YOLOv4 nhận diện chính xác nhất đối với các pattern RHSA (90.6%) và DT (85/3%). YOLOX đạt độ chính xác lớn nhất trên pattern HSA (91.7%) và YOLOR đạt độ chính xác lớn nhất trên pattern DB đạt 87.4%. Hình 5:
https://imgur.com/hcAWqRm.png
4. Suy nghĩ của tôi:
Với sự phát triển của học sâu thì việc sử dụng những mô hình mạnh để giải quyết các bài toán nhận dạng pattern trong tài chính hoàn toàn có thể mang lại kết quả vượt trội. Hai học viên đã khéo léo áp dụng nhận diện pattern thông qua hình ảnh của chúng bằng những thuật toán mạnh trong học sâu. Với những ý tưởng trong nghiên cứu này sẽ có thể tạo ra những đột phá mới trong việc phát triển các mô hình trên thị trường tài chính trong tương lai.
5. Video thuyết trình:
https://youtu.be/NU1EILqqVIU","[Deep learning for Candlestick - Patterns recognition in Financial market] Đây là một project khá nổi bật mà hai bạn sinh viên Trọng Duy và Đức Quang của Deep Learning K1 đã thực hiện về đề tài nhận dạng mẫu biểu đồ đối với thị trường chứng khoán. 1. Đặt vấn đề: Thị trường chứng khoán luôn tiềm ẩn rủi ro cao đối với những nhà đầu tư. Có nhiều phương pháp khác nhau trong học máy và kinh tế lượng để phân tích và dự đoán cổ phiếu trong ngắn hạn và dài hạn. Trong đó kĩ thuật phân tích mẫu cổ phiếu là một trong những kĩ thuật đơn giản và hiệu quả được các nhà đầu tư thực hiện. Thông thường quá trình nhận diện mẫu là một công việc đòi hỏi sự tinh tường và kinh nghiệm chuyên sâu từ chuyên gia. Do đó khó có thể large scale trên số lượng lớn vài trăm, hoặc vài ngàn cổ phiếu. Trong nghiên cứu này, hai bạn học viên của Deep Learning K1 là Trọng Duy và Đức Quang đã xây dựng một cách tiếp cận mới sử dụng bài toán computer vision trong nhận dạng mẫu cổ phiếu và đạt được độ chính xác cao so với các thuật toán hiện tại. 2. Phương pháp: 2.1. Tập dataset Bộ dữ liệu được phục vụ cho trị trường chứng khoán Việt Nam nên các mẫu được lựa chọn là những pattern giao dịch của 30mã chứng khoán và 30 chỉ sốngoại hối của các các sàn chứng khoán Việt Nam. Nghiên cứu tập trung vào phát hiện pattern trong ngắn hạn nên các mẫu dữ liệu được lựa chọn là 5 phiên giao dịch của 1 tuần liên tiếp với kích thước bước nhảy là 1 ngày. Nghiên cứu này tập trung vào phát hiện 4patterns chính bao gồm: - HSA: Head and Shoulder. Pattern này mô tả sự đảo ngược của xu hướng tăng sang giảm với đỉnh Head ở giữa và hai đỉnh Shoulder thấp hơn ở hai bên. https://imgur.com/8uV2uWm.png - IHSA: Inverted Head and Shoulders nhằm mô tả sự đảo ngược từ giảm sang tăng với đỉnh Head ở giữa sẽ thấp hơn so với hai đỉnh Shoulder ở hai bên. https://imgur.com/UVdHKP7.png - DT: Double Topđược hình thành từ hai đỉnh liên tiếp. Phần giữa hai đỉnh tạo thành hình chữ U lộn ngược. Hai đỉnh có giá trị gần tương tự nhau với mức chênh lệch ít hơn 3%. Phần đáy giữa hai đỉnh nhỏ hơn đỉnh từ 10 – 20%. https://imgur.com/9ZruggJ.png - DB: Double Bottom là đảo ngược của Double Top. https://imgur.com/mHImCMd.png 2.2. Xử lý dữ liệu Dữ liệu được chuyển từ candle chart sang segmentation chart để hạn chế nhiễu và sau đó thực hiện các bước xử lý như hình 1: https://imgur.com/CDYA8ZB.png Step 1: Chọn hai điểm trong một đoạn trong biểu đồ segmentation. Step 2: Chọn một điểm khác sao cho có tổng khoảng cách lớn nhất để bắt đầu và kết thúc đoạn đã chọn. Step 3: Lặp lại Step 1 và Step 2 cho đến khi có đủ điểm trong biểu đồ segmentation. Bằng cách này,các noise có thể được loại bỏ và sẽ thu được một đồ thị đường đơn giản hơn. 2.3. Sinh mẫu ngẫu nhiên 2.3.1. Biến đổi theo trục x: - Đối với pattern HSA và IHSA. Tác giả loại bỏ những mẫu đối với pattern HSA không hợp lệ (có đỉnh thứ 2 thấp hơn hai đỉnh còn lại). Sau đó tạo biến thể ngẫu nhiên trên những mẫu đạt tiêu chuẩn bằng cách khởi tạo ngẫu nhiên những biến động x_p1, x_p2, x_p3; xóa đi một đoạn bất kì tại vị trí đáy và thay thế một đoạn giá ngẫu nhiên. Qui trình thực hiện như hình 2: https://imgur.com/e39JwNh.png Phương pháp này được thực hiện tương tự đối với các pattern IHSA. - Đối với pattern DT và DB: Lựa chọn những mẫu mà có 2 đỉnh gần bằng nhau nhưng chưa phải là DT hoặc DB. Tiếp theo so sánh hai đỉnh, sau đó xóa đi đỉnh cao hơn và dùng phân phối chuẩn để tạo thành các mẫu DT và DB phù hợp như hình 3: https://imgur.com/07dR2bg.png 2.3.2. Biến đổi theo trục y: Theo trục y chúng ta chỉ cần thực hiện thay đổi giá. Đối với hai pattern HSA và IHSA phải đảm bảo đỉnh thứ 2 phải cao hơn 2 đỉnh còn lại. Đối với DT và DB cần đảm bảo 2 đỉnh có độ cao bằng nhau như hình 4: https://imgur.com/dwXkIHw.png 3. Kết quả đạt được: Thử nghiệm trên 5 lớp mô hình khác nhau của Object Detections: Scaled YOLOv4, YOLOX, YOLOR, EfficientDet, DETR cho thấy Scaled YOLOv4 nhận diện chính xác nhất đối với các pattern RHSA (90.6%) và DT (85/3%). YOLOX đạt độ chính xác lớn nhất trên pattern HSA (91.7%) và YOLOR đạt độ chính xác lớn nhất trên pattern DB đạt 87.4%. Hình 5: https://imgur.com/hcAWqRm.png 4. Suy nghĩ của tôi: Với sự phát triển của học sâu thì việc sử dụng những mô hình mạnh để giải quyết các bài toán nhận dạng pattern trong tài chính hoàn toàn có thể mang lại kết quả vượt trội. Hai học viên đã khéo léo áp dụng nhận diện pattern thông qua hình ảnh của chúng bằng những thuật toán mạnh trong học sâu. Với những ý tưởng trong nghiên cứu này sẽ có thể tạo ra những đột phá mới trong việc phát triển các mô hình trên thị trường tài chính trong tương lai. 5. Video thuyết trình: https://youtu.be/NU1EILqqVIU",,,,,
"Xin chào mọi người.
Mọi người có thể nán lại một chút giúp em giải thích đoạn này được không.
Mấy nay em đang mò thuật toán yolov5 và sau khi em trai cái model xong thì nó lại hiện ra cái hình này. Cái hình này nói lên gì vậy ạ. Em không biết kết luận gì luôn.",Xin chào mọi người. Mọi người có thể nán lại một chút giúp em giải thích đoạn này được không. Mấy nay em đang mò thuật toán yolov5 và sau khi em trai cái model xong thì nó lại hiện ra cái hình này. Cái hình này nói lên gì vậy ạ. Em không biết kết luận gì luôn.,,,,,
"Chào mọi người.
Mình background kinh tế, hiện cũng đang làm về kinh tế.
Mình muốn học ĐH (hoặc master) về CS ở Hà Nội thì có thể học trường nào và chương trình nào ạ? Có đâu tuyển sinh trái ngành như trường hợp mình không?
Mình muốn kiếm thêm cái bằng. Như đồng nghiệp mình thì phần lớn học lên cao học ở NEU, FTU, BA nhưng mình ngán cái đúng sai không rõ ràng của kinh tế lắm rồi. Mình cảm giác như học kinh tế toàn nói phét vậy...","Chào mọi người. Mình background kinh tế, hiện cũng đang làm về kinh tế. Mình muốn học ĐH (hoặc master) về CS ở Hà Nội thì có thể học trường nào và chương trình nào ạ? Có đâu tuyển sinh trái ngành như trường hợp mình không? Mình muốn kiếm thêm cái bằng. Như đồng nghiệp mình thì phần lớn học lên cao học ở NEU, FTU, BA nhưng mình ngán cái đúng sai không rõ ràng của kinh tế lắm rồi. Mình cảm giác như học kinh tế toàn nói phét vậy...",,,,,
"Chào anh chị em, mình hỏi chút. Mình biết với dữ liệu panel data hay longitudinal data có mô hình FEM/REM dùng được. Ai có kinh nghiệm dùng với model khác kiểu như LSTM.... chia sẻ mình với. Xin cảm ơn.","Chào anh chị em, mình hỏi chút. Mình biết với dữ liệu panel data hay longitudinal data có mô hình FEM/REM dùng được. Ai có kinh nghiệm dùng với model khác kiểu như LSTM.... chia sẻ mình với. Xin cảm ơn.",,,,,
"Chào các bác. Em xin gửi đến anh em một bài tìm hiểu sơ về EfficientNet với mục tiêu Mì ăn liên, sử dụng được. Nhưng nói chung cách nghĩ của tác giá về vấn đề model scaling khá hay.
Hi vọng giúp được anh em mới học thôi ạ.","Chào các bác. Em xin gửi đến anh em một bài tìm hiểu sơ về EfficientNet với mục tiêu Mì ăn liên, sử dụng được. Nhưng nói chung cách nghĩ của tác giá về vấn đề model scaling khá hay. Hi vọng giúp được anh em mới học thôi ạ.",,,,,
"Chào mọi người. Về vấn đề deploy model on cloud. Mọi người thường triển khai trên nền tảng gì để tối ưu performance ạ? Vì mình biết các REST API frameworks như django, flask, fastapi on python không thể predict dynamic batch-size được.","Chào mọi người. Về vấn đề deploy model on cloud. Mọi người thường triển khai trên nền tảng gì để tối ưu performance ạ? Vì mình biết các REST API frameworks như django, flask, fastapi on python không thể predict dynamic batch-size được.",,,,,
"Có bạn nào có từng sử dụng OpenKE (https://github.com/thunlp/OpenKE) chưa ạ.
Mình gặp vấn đề là không thấy hướng dẫn chuyển entity sang id tương ứng (VD: Barack Obama -> id:157). Mình có lăn xem các issue nhưng vẫn chưa hình dung được cách để chuyển entity sang id như vậy. File entity2id thì để các giá trị khá lạ, mình không rõ làm cách nào để chuyển từ dạng text qua được.
Mong được mọi người giúp đỡ ạ, mình xin cảm ơn ạ.","Có bạn nào có từng sử dụng OpenKE (https://github.com/thunlp/OpenKE) chưa ạ. Mình gặp vấn đề là không thấy hướng dẫn chuyển entity sang id tương ứng (VD: Barack Obama -> id:157). Mình có lăn xem các issue nhưng vẫn chưa hình dung được cách để chuyển entity sang id như vậy. File entity2id thì để các giá trị khá lạ, mình không rõ làm cách nào để chuyển từ dạng text qua được. Mong được mọi người giúp đỡ ạ, mình xin cảm ơn ạ.",,,,,
"#question
#underthesea
Em chào các anh chị. Không biết các anh chị trong đây có ai biết cách ẩn thanh progress bar khi dùng thực hiện dependency parsing của thư viện underthesea có thể chỉ giúp em không ạ ? Vì document và github của thư viện không có nhắc đến nên em xin phép đặt câu hỏi ở đây. Em xin cảm ơn!",Em chào các anh chị. Không biết các anh chị trong đây có ai biết cách ẩn thanh progress bar khi dùng thực hiện dependency parsing của thư viện underthesea có thể chỉ giúp em không ạ ? Vì document và github của thư viện không có nhắc đến nên em xin phép đặt câu hỏi ở đây. Em xin cảm ơn!,#question	#underthesea,,,,
"Em,mình chào anh, chị và mọi người trong nhóm.
Em,mình đang muốn tìm một bộ dataset liên quan đến ""vihicles moving"" có thông số của đồng hồ đo tốc độ ạ.
Em,mình đã tìm rất nhiều trên mạng tuy nhiên vẫn chưa tìm được bộ dữ liệu nào có cả thông số của đồng hồ đo.
Vậy nếu mọi người có hoặc đã từng làm với bộ dữ liệu đó có thể chia sẻ cho em,mình được không ạ.
Em,mình cảm ơn mn ạ.","Em,mình chào anh, chị và mọi người trong nhóm. Em,mình đang muốn tìm một bộ dataset liên quan đến ""vihicles moving"" có thông số của đồng hồ đo tốc độ ạ. Em,mình đã tìm rất nhiều trên mạng tuy nhiên vẫn chưa tìm được bộ dữ liệu nào có cả thông số của đồng hồ đo. Vậy nếu mọi người có hoặc đã từng làm với bộ dữ liệu đó có thể chia sẻ cho em,mình được không ạ. Em,mình cảm ơn mn ạ.",,,,,
"Các anh chị ơi, có anh chị nào có tập dataset về Credit Risk Assessment và bài báo sử dụng dataset đó không ạ.
Em đang chập chững tìm hiểu về nó, mong các anh chị và cao nhân hỗ trợ ạ.","Các anh chị ơi, có anh chị nào có tập dataset về Credit Risk Assessment và bài báo sử dụng dataset đó không ạ. Em đang chập chững tìm hiểu về nó, mong các anh chị và cao nhân hỗ trợ ạ.",,,,,
"[Thứ tự của nhãn từ có ảnh hưởng gì đến huấn luyện lớp Embedding/ Huấn luyện mô hình không?]
#embedding_layer #reproducibility
Chào mọi nguòi, em muốn hỏi các bạn có kinh nghiệm làm việc về NLP là: thứ tự các nhãn của từ khác nhau có gây ra hai mô hình khác nhau hay không? 
Em đang tìm hiểu về một paper (https://doi.org/10.1093/bioinformatics/bty535) sử dụng GNN, trong đó mỗi nút được đánh nhãn và feed vào lớp Embedding để có vector đặc trưng.
Thuật toán mọi người xem ở hàm extract_fingerprint (https://github.com/masashitsubaki/CPI_prediction/blob/master/code/preprocess_data.py)
Một file để mọi người visualize kết quả trung gian của thuật toán: main2.py
và video: https://youtu.be/gybYJVqBW7o
Bản chất nó là một kiểu thuật toán tô màu Weisfeiler-Lehman
Một số tính chất của thuật toán đánh nhãn đó:
1/ Thuật toán đảm bảo các nút ""khác nhau"" thì được gán nhãn khác nhau, nhưng không có một ""từ điển universal"" nào cho nút giống như từ điển ngôn ngữ cả (và cả từ điển SMILES, e ko chắc?!), từ điển được xây dựng dần dần khi thuật toán lặp qua toàn bộ các đồ thị phân tử, và thứ tự của cùng một loại nút có thể  khác nhau nếu thứ tự lặp là khác nhau. 
2/ Có những nhãn nút sẽ không bao giờ được dùng đến, cũng có nghĩa là có những trọng số trong embedding layer không được cập nhật hoặc cập nhật mà không đem lại thông tin gì. Đó là vì việc đánh nhãn được thực hiện theo kiểu cập nhật, nhãn cũ được dùng để sinh nhãn mới và nút sẽ luôn cố để được cập nhật nhãn mới nhất có thể (mục đích nhằm đa dạng nhãn giữa các nút, để không chỉ phân biệt loại nguyên tố của các nút mà cả những sub-structure cảm sinh từ nút đó)
Từ 1/ và 2/, cộng với những kịch bản phức tạp khi train theo batch, dừng sớm, v.v. Không biết điều này có ảnh hưởng đến khả năng reproducible của thử nghiệm ko?","[Thứ tự của nhãn từ có ảnh hưởng gì đến huấn luyện lớp Embedding/ Huấn luyện mô hình không?] Chào mọi nguòi, em muốn hỏi các bạn có kinh nghiệm làm việc về NLP là: thứ tự các nhãn của từ khác nhau có gây ra hai mô hình khác nhau hay không? Em đang tìm hiểu về một paper (https://doi.org/10.1093/bioinformatics/bty535) sử dụng GNN, trong đó mỗi nút được đánh nhãn và feed vào lớp Embedding để có vector đặc trưng. Thuật toán mọi người xem ở hàm extract_fingerprint (https://github.com/masashitsubaki/CPI_prediction/blob/master/code/preprocess_data.py) Một file để mọi người visualize kết quả trung gian của thuật toán: main2.py và video: https://youtu.be/gybYJVqBW7o Bản chất nó là một kiểu thuật toán tô màu Weisfeiler-Lehman Một số tính chất của thuật toán đánh nhãn đó: 1/ Thuật toán đảm bảo các nút ""khác nhau"" thì được gán nhãn khác nhau, nhưng không có một ""từ điển universal"" nào cho nút giống như từ điển ngôn ngữ cả (và cả từ điển SMILES, e ko chắc?!), từ điển được xây dựng dần dần khi thuật toán lặp qua toàn bộ các đồ thị phân tử, và thứ tự của cùng một loại nút có thể khác nhau nếu thứ tự lặp là khác nhau. 2/ Có những nhãn nút sẽ không bao giờ được dùng đến, cũng có nghĩa là có những trọng số trong embedding layer không được cập nhật hoặc cập nhật mà không đem lại thông tin gì. Đó là vì việc đánh nhãn được thực hiện theo kiểu cập nhật, nhãn cũ được dùng để sinh nhãn mới và nút sẽ luôn cố để được cập nhật nhãn mới nhất có thể (mục đích nhằm đa dạng nhãn giữa các nút, để không chỉ phân biệt loại nguyên tố của các nút mà cả những sub-structure cảm sinh từ nút đó) Từ 1/ và 2/, cộng với những kịch bản phức tạp khi train theo batch, dừng sớm, v.v. Không biết điều này có ảnh hưởng đến khả năng reproducible của thử nghiệm ko?",#embedding_layer	#reproducibility,,,,
"Chào anh chị ạ, em năm nay học lớp 12, em muốn hỏi là nếu theo AI research thì nên học chuyên ngành toán tin hay học cntt ạ, lợi thế với lại khó khăn của 2 ngành như thế nào?
Vậy em học ở tphcm thì nên học ở trường nào ạ?","Chào anh chị ạ, em năm nay học lớp 12, em muốn hỏi là nếu theo AI research thì nên học chuyên ngành toán tin hay học cntt ạ, lợi thế với lại khó khăn của 2 ngành như thế nào? Vậy em học ở tphcm thì nên học ở trường nào ạ?",,,,,
"#question #freeze #deeplearning
Chào mọi người, mình có vài câu hỏi, đã nghiên cứu vòng vòng nhưng vẫn chưa clear, mong mọi người lướt ngang cho ý kiến ạ:
Thông thường mọi người freeze network thường có trick gì không? Đó giờ mình chỉ freeze phần encoder (thường backbone là resnet), phần decoder thì trainable. Nhưng gần đây mình phát hiện người ta chỉ freeze phần batchnorm thôi. Cũng không hiểu lắm ý định của họ là gì.
Giả sử mình có 1 mạng DNN, thông thường mình thấy phần early layers sẽ có ít parameters hơn các layer cuối. Mình tìm hiểu thấy họ bảo vì các layer đầu level thấp nên chưa có gì để học (mình không đồng tình lắm, học shape cũng được mà), có người thì bảo là làm như vậy tính toán sẽ nhanh hơn ?
Với trường hợp upsample feature, mình thường có 2 cách:
a) Interpolate (non-trainable)
b) Convolution (trainable)
Thông thường mình sử dụng Conv, tuy nhiên gần đây mình phát hiện có người xài interpolate. Mình không hiểu lý do gì mà họ lại dùng cái đó?
Mong mọi người giải đáp ạ!
Cảm ơn ạ.","Chào mọi người, mình có vài câu hỏi, đã nghiên cứu vòng vòng nhưng vẫn chưa clear, mong mọi người lướt ngang cho ý kiến ạ: Thông thường mọi người freeze network thường có trick gì không? Đó giờ mình chỉ freeze phần encoder (thường backbone là resnet), phần decoder thì trainable. Nhưng gần đây mình phát hiện người ta chỉ freeze phần batchnorm thôi. Cũng không hiểu lắm ý định của họ là gì. Giả sử mình có 1 mạng DNN, thông thường mình thấy phần early layers sẽ có ít parameters hơn các layer cuối. Mình tìm hiểu thấy họ bảo vì các layer đầu level thấp nên chưa có gì để học (mình không đồng tình lắm, học shape cũng được mà), có người thì bảo là làm như vậy tính toán sẽ nhanh hơn ? Với trường hợp upsample feature, mình thường có 2 cách: a) Interpolate (non-trainable) b) Convolution (trainable) Thông thường mình sử dụng Conv, tuy nhiên gần đây mình phát hiện có người xài interpolate. Mình không hiểu lý do gì mà họ lại dùng cái đó? Mong mọi người giải đáp ạ! Cảm ơn ạ.",#question	#freeze	#deeplearning,,,,
"Cho mình hỏi machine learning có thể phát hiện được thuật ngữ tiếng anh chuyên ngành trong một câu tiếng anh không. Nếu có thì làm thế nào?. Ví dụ trong cụm tiến anh ""A variable name must not have any keywords, for instance, float, int, etc."" thì ta có 4 từ chuyên ngành là variable, keyword, float và int. Cám ơn mọi người đã xem.","Cho mình hỏi machine learning có thể phát hiện được thuật ngữ tiếng anh chuyên ngành trong một câu tiếng anh không. Nếu có thì làm thế nào?. Ví dụ trong cụm tiến anh ""A variable name must not have any keywords, for instance, float, int, etc."" thì ta có 4 từ chuyên ngành là variable, keyword, float và int. Cám ơn mọi người đã xem.",,,,,
"Nhóm mình có ai làm bài toán semantic role labeling cho tiếng Việt chưa ạ, em tìm dataset Tiếng Việt mà khó quá :( Thấy có paper Vietnamese Propbank mà em lục tung google ko thấy chỗ tải đâu cả","Nhóm mình có ai làm bài toán semantic role labeling cho tiếng Việt chưa ạ, em tìm dataset Tiếng Việt mà khó quá :( Thấy có paper Vietnamese Propbank mà em lục tung google ko thấy chỗ tải đâu cả",,,,,
"Chuyển file audio thành text Tiếng Việt thì mình làm thế nào nhỉ, anh em có open source không cho mình thông tin vs nhé.","Chuyển file audio thành text Tiếng Việt thì mình làm thế nào nhỉ, anh em có open source không cho mình thông tin vs nhé.",,,,,
"""Training không đồ thị như các chị thiếu gương soi"" - chúng ta sẽ không thể biết được model đẹp hay xấu, ngon hay không? Hôm nay mình share cùng anh em cách visualize đồ thị các thông số training realtime theo nhu cầu của một số anh em có hỏi trên nhóm.
Hi vọng giúp được anh em!","""Training không đồ thị như các chị thiếu gương soi"" - chúng ta sẽ không thể biết được model đẹp hay xấu, ngon hay không? Hôm nay mình share cùng anh em cách visualize đồ thị các thông số training realtime theo nhu cầu của một số anh em có hỏi trên nhóm. Hi vọng giúp được anh em!",,,,,
Mình đang đọc tài liệu có gặp 2 thuật ngữ là bert và phobert. Mình ko biết là phobert có liên quan gì đến bert ko. Cám ơn mọi người đã xem.,Mình đang đọc tài liệu có gặp 2 thuật ngữ là bert và phobert. Mình ko biết là phobert có liên quan gì đến bert ko. Cám ơn mọi người đã xem.,,,,,
"Em xin chào các tiền bối! Mọi người cho em hỏi chút ạ! Mức độ sai số của một tập m sample trong dataset có tổng cộng n samples có thể đánh giá bằng variance như sau, thì tại sao phần mẫu là (m-(n-1) chứ không phải (n-(m-1)) ạ!
Em xin cảm ơn!","Em xin chào các tiền bối! Mọi người cho em hỏi chút ạ! Mức độ sai số của một tập m sample trong dataset có tổng cộng n samples có thể đánh giá bằng variance như sau, thì tại sao phần mẫu là (m-(n-1) chứ không phải (n-(m-1)) ạ! Em xin cảm ơn!",,,,,
"[Video DataQuest 5th - MLOps methodology]
Note: English version in the last.
Trong sự kiện DataQuest lần thứ 5 này, TowardDatascience cung cấp cho bạn kiến thức chung về dân chủ hóa AI để cho phép nhóm nội bộ có khả năng phát triển và triển khai các ứng dụng AI lên môi trường production. Hơn nữa, bạn có thể tổ chức dự án của mình theo quản lý vòng đời MLOps với sự hiểu biết sâu sắc về việc thích ứng với sự thay đổi mô hình theo dữ liệu động. Phương pháp MLOps là cách bạn phân biệt thành nhiều mức độ trưởng thành của MLOps tương ứng với mức độ tự động hóa và tăng tốc chu kì huấn luyện/triển khai và đánh giá mô hình.
I. Nội dung:
- Dân chủ hóa AI
- Vòng đời MLOps
- Phương pháp MLOps
II. Tài liệu:
Slide: https://drive.google.com/file/d/1PrGyG28c4Y2GrXuBUbO4ctYX4cTFArEd/view?usp=sharing
Video: https://youtu.be/qJrI3hNw7r0
-----------------------------------------------------------------------------------------------------
In this 5th DataQuest event, TowardDatascience provides you with a general knowledge of AI democratization that enables the in-house team the ability to develop and deploy AI applications in the production environment. Moreover, you can organize your project according to the MLOps lifecycle management with a deep understanding of adapting model change according to dynamic data. MLOps methodology is the way you differentiate into a variety of MLOps maturity levels relevant to the automation and acceleration of training, deployment, and evaluation.
I. Content:
- AI democratization
- MLOps lifecycle
- MLOps methodology
II. Resource:
Slide: https://drive.google.com/file/d/1PrGyG28c4Y2GrXuBUbO4ctYX4cTFArEd/view?usp=sharing
Video: https://youtu.be/qJrI3hNw7r0
-----‐‐------------------------‐-----‐--------------------------------
1 phút quảng cáo: Nhằm củng cố thêm kiến thức về đại số tuyến tính, giải tích, xác suất và thống kê là những trụ cột quan trong Machine Learning. hãy đăng kí khóa học Math for Machine Learning (start 22/05/2022) theo link bên dưới:
https://www.facebook.com/1127124110656257/posts/5031751303526832/","[Video DataQuest 5th - MLOps methodology] Note: English version in the last. Trong sự kiện DataQuest lần thứ 5 này, TowardDatascience cung cấp cho bạn kiến thức chung về dân chủ hóa AI để cho phép nhóm nội bộ có khả năng phát triển và triển khai các ứng dụng AI lên môi trường production. Hơn nữa, bạn có thể tổ chức dự án của mình theo quản lý vòng đời MLOps với sự hiểu biết sâu sắc về việc thích ứng với sự thay đổi mô hình theo dữ liệu động. Phương pháp MLOps là cách bạn phân biệt thành nhiều mức độ trưởng thành của MLOps tương ứng với mức độ tự động hóa và tăng tốc chu kì huấn luyện/triển khai và đánh giá mô hình. I. Nội dung: - Dân chủ hóa AI - Vòng đời MLOps - Phương pháp MLOps II. Tài liệu: Slide: https://drive.google.com/file/d/1PrGyG28c4Y2GrXuBUbO4ctYX4cTFArEd/view?usp=sharing Video: https://youtu.be/qJrI3hNw7r0 ----------------------------------------------------------------------------------------------------- In this 5th DataQuest event, TowardDatascience provides you with a general knowledge of AI democratization that enables the in-house team the ability to develop and deploy AI applications in the production environment. Moreover, you can organize your project according to the MLOps lifecycle management with a deep understanding of adapting model change according to dynamic data. MLOps methodology is the way you differentiate into a variety of MLOps maturity levels relevant to the automation and acceleration of training, deployment, and evaluation. I. Content: - AI democratization - MLOps lifecycle - MLOps methodology II. Resource: Slide: https://drive.google.com/file/d/1PrGyG28c4Y2GrXuBUbO4ctYX4cTFArEd/view?usp=sharing Video: https://youtu.be/qJrI3hNw7r0 -----‐‐------------------------‐-----‐-------------------------------- 1 phút quảng cáo: Nhằm củng cố thêm kiến thức về đại số tuyến tính, giải tích, xác suất và thống kê là những trụ cột quan trong Machine Learning. hãy đăng kí khóa học Math for Machine Learning (start 22/05/2022) theo link bên dưới: https://www.facebook.com/1127124110656257/posts/5031751303526832/",,,,,
"Chào mọi người,
Xin hỏi mọi người, model có thể nhận diện được thẻ CMND bị chỉnh sửa, bị photoshop hay thay đổi ảnh trong thẻ ạ?","Chào mọi người, Xin hỏi mọi người, model có thể nhận diện được thẻ CMND bị chỉnh sửa, bị photoshop hay thay đổi ảnh trong thẻ ạ?",,,,,
"em xin tài liệu hd nhận dạng tiền giả, hình như group mình có đăng 1 lần thì phải
xin cám ơn các anh/chị","em xin tài liệu hd nhận dạng tiền giả, hình như group mình có đăng 1 lần thì phải xin cám ơn các anh/chị",,,,,
"Xin chào mọi người. Mình hiện đang làm project về information retrieval từ file pdf. Mình có test qua một vài packages như: pdfminer, PyPDF2, pdfPlumber thì kết quả không được ok lắm. Sau đó mình có thử dùng OCR với Tessaract thì kết quả tốt hơn hẳn, tuy nhiên khá chậm + bị vướng footnote của các pages, mình dùng thêm OpenCV thì cũng xử lý dc sơ sơ footnote nhưng kết quả lên xuống tùy thuộc vào pdf file, hơn nữa chạy càng lâu. Mình mới tìm hiểu thêm thì có TextSnake model với framework MMOCR. Có ai từng có kinh nghiệm làm về mảng này hoặc đã từng dùng TextSnake, MMOCR có thể cho mình xin chút kinh nghiệm được không ạ.
Mình cũng có ngó qua Google Vision API thì thấy nó hơi nhiều policy.
Mình cảm ơn ạ","Xin chào mọi người. Mình hiện đang làm project về information retrieval từ file pdf. Mình có test qua một vài packages như: pdfminer, PyPDF2, pdfPlumber thì kết quả không được ok lắm. Sau đó mình có thử dùng OCR với Tessaract thì kết quả tốt hơn hẳn, tuy nhiên khá chậm + bị vướng footnote của các pages, mình dùng thêm OpenCV thì cũng xử lý dc sơ sơ footnote nhưng kết quả lên xuống tùy thuộc vào pdf file, hơn nữa chạy càng lâu. Mình mới tìm hiểu thêm thì có TextSnake model với framework MMOCR. Có ai từng có kinh nghiệm làm về mảng này hoặc đã từng dùng TextSnake, MMOCR có thể cho mình xin chút kinh nghiệm được không ạ. Mình cũng có ngó qua Google Vision API thì thấy nó hơi nhiều policy. Mình cảm ơn ạ",,,,,
"Chào các bác, em dân ngoài ngành(biết lập trình căn bản) muốn tìm hiểu về AI, em muốn làm một tool dạng như auto content vậy, em nhập vào khoảng 5 hay 10 từ gì đó, thì nó tự tạo ra một câu hoặc đoạn văn chứa đủ các từ này (các từ này là tiếng Anh), các bác chỉ giúp em hướng đi và vài keyword để nghiên cứu với.","Chào các bác, em dân ngoài ngành(biết lập trình căn bản) muốn tìm hiểu về AI, em muốn làm một tool dạng như auto content vậy, em nhập vào khoảng 5 hay 10 từ gì đó, thì nó tự tạo ra một câu hoặc đoạn văn chứa đủ các từ này (các từ này là tiếng Anh), các bác chỉ giúp em hướng đi và vài keyword để nghiên cứu với.",,,,,
"[Sách Introduction to Probability for Data Science]
Các sách viết về xác suất trước đây thì sẽ theo một trong hai hướng: sách cho lập trình viên hoặc sách cho dân toán. Trong khi sách xác suất cho lập trình viên thường tập trung vào cách dùng thư viện để tính toán, không chú trọng vào bản chất toán học; thì sách viết cho dân toán thường khô khan, nặng về biến đổi công thức, thành ra đọc rất dễ chán.
Nay mình giới thiệu mọi người cuốn ""Introduction to Probability for Data Science"" có kết hợp hài hòa giữa lý thuyết và thực hành, có cả code R và Python kèm theo sách. Ngoài ra sách còn giải thích các khái niệm một cách trực quan, lấy ví dụ liên quan tới khoa học dữ liệu. Phần cuối sách còn giới thiệu về ứng dụng xác suất trong các thuật toán về Machine Learning.
Thông tin về sách ở đây: https://probability4datascience.com/preface.html","[Sách Introduction to Probability for Data Science] Các sách viết về xác suất trước đây thì sẽ theo một trong hai hướng: sách cho lập trình viên hoặc sách cho dân toán. Trong khi sách xác suất cho lập trình viên thường tập trung vào cách dùng thư viện để tính toán, không chú trọng vào bản chất toán học; thì sách viết cho dân toán thường khô khan, nặng về biến đổi công thức, thành ra đọc rất dễ chán. Nay mình giới thiệu mọi người cuốn ""Introduction to Probability for Data Science"" có kết hợp hài hòa giữa lý thuyết và thực hành, có cả code R và Python kèm theo sách. Ngoài ra sách còn giải thích các khái niệm một cách trực quan, lấy ví dụ liên quan tới khoa học dữ liệu. Phần cuối sách còn giới thiệu về ứng dụng xác suất trong các thuật toán về Machine Learning. Thông tin về sách ở đây: https://probability4datascience.com/preface.html",,,,,
"THÔNG BÁO TỪ NHÓM DLBOOKVN

🇨🇦🇨🇦🇨🇦 Mila - Institut Québécois d'Intelligence Artificielle (Viện AI Quebec Canada) thông qua chuyến thăm làm việc với FPTSoftwareAILab (một trong số các nhà tài trợ kim cương của dự án này), đã được giới thiệu đến dự án Việt hóa cuốn DeepLearning của nhóm DLBookVN chúng tôi. Viện đã đăng tải thông tin trên fanpage chính thức của mình. Ngay sau đó, bác Yoshua Bengio cũng đã chia sẻ lại bài đăng này trên trang nhà. 🤩🥰🤪

Cảm ơn FPTSoftwareAILab và Viện AI Quebec Canada đã giúp chúng tôi thêm lan tỏa dự án này đến cộng đồng.  
#FPTSoftwareAILab #ACESoftware #SolarpowerVietnam #KaseEdutech #SucdenVietnam  
—-------------------------------------------------------- 
Thông tin liên hệ: 
🌐Website: https://dlbookvn.gitlab.io
📍Fanpage: https://www.facebook.com/deeplearningbookvn
📩Email: dlbookvn18@gmail.com","THÔNG BÁO TỪ NHÓM DLBOOKVN Mila - Institut Québécois d'Intelligence Artificielle (Viện AI Quebec Canada) thông qua chuyến thăm làm việc với FPTSoftwareAILab (một trong số các nhà tài trợ kim cương của dự án này), đã được giới thiệu đến dự án Việt hóa cuốn DeepLearning của nhóm DLBookVN chúng tôi. Viện đã đăng tải thông tin trên fanpage chính thức của mình. Ngay sau đó, bác Yoshua Bengio cũng đã chia sẻ lại bài đăng này trên trang nhà. Cảm ơn FPTSoftwareAILab và Viện AI Quebec Canada đã giúp chúng tôi thêm lan tỏa dự án này đến cộng đồng. —-------------------------------------------------------- Thông tin liên hệ: Website: https://dlbookvn.gitlab.io Fanpage: https://www.facebook.com/deeplearningbookvn Email: dlbookvn18@gmail.com",#FPTSoftwareAILab	#ACESoftware	#SolarpowerVietnam	#KaseEdutech	#SucdenVietnam,,,,
"Chào mọi người, em là dân ngoại đạo ML nên có 1 câu hỏi mong các ace tư vấn giúp ạ 
Hiện tại em đang có kế hoạch làm 1 mock app về nhận diện object, object nhận diện chỉ đơn giản như: tòa nhà, bức tường, sky 
Theo em research thì để detect các object này cần ML train  
vì mock app ko đem đi thương mại hay kinh doanh nên về phần chi phí khá eo hẹp, nên em muốn tham khảo các ace là có nơi nào hay chổ nào có sharing các model đã đc training để detect các object trên không, (độ chính xác <50% cũng ko vấn đề gì), hoặc trial không giưới hạn time chẳng hạn 
sr mọi người vì câu hỏi khá không liên quan đến ML cho lắm.
 ","Chào mọi người, em là dân ngoại đạo ML nên có 1 câu hỏi mong các ace tư vấn giúp ạ Hiện tại em đang có kế hoạch làm 1 mock app về nhận diện object, object nhận diện chỉ đơn giản như: tòa nhà, bức tường, sky Theo em research thì để detect các object này cần ML train vì mock app ko đem đi thương mại hay kinh doanh nên về phần chi phí khá eo hẹp, nên em muốn tham khảo các ace là có nơi nào hay chổ nào có sharing các model đã đc training để detect các object trên không, (độ chính xác <50% cũng ko vấn đề gì), hoặc trial không giưới hạn time chẳng hạn sr mọi người vì câu hỏi khá không liên quan đến ML cho lắm.",,,,,
"#transformer
Chào mọi người, không biết đã có ai từng áp dụng kiến trúc transformer/module transformer encoder cho dữ liệu có số channel là 1 chưa ạ? E đã Google một hồi nhưng chưa thấy một established transformer-based model nào cho dữ liệu có số channel là 1 như là CNN. Hiện tại thì e mới giải quyết bằng cách để một linear layer phía trước positional encoding (và bỏ embedding layer) (ko có activation) để tăng cỡ channel của đầu vào. Cảm ơn mn đã giúp đỡ","Chào mọi người, không biết đã có ai từng áp dụng kiến trúc transformer/module transformer encoder cho dữ liệu có số channel là 1 chưa ạ? E đã Google một hồi nhưng chưa thấy một established transformer-based model nào cho dữ liệu có số channel là 1 như là CNN. Hiện tại thì e mới giải quyết bằng cách để một linear layer phía trước positional encoding (và bỏ embedding layer) (ko có activation) để tăng cỡ channel của đầu vào. Cảm ơn mn đã giúp đỡ",#transformer,,,,
"VinAI Spring Workshop 2022 - Latest Research Results Presented by Our AI Residents + Ask me anything with our Scientists
https://fb.watch/cH2ByFRCWZ/",VinAI Spring Workshop 2022 - Latest Research Results Presented by Our AI Residents + Ask me anything with our Scientists https://fb.watch/cH2ByFRCWZ/,,,,,
"Kính chào các bác. Tranh thủ cuối tuần ở nhà em cày cho anh em clip Cross Sell Prediction trong series AI in Banking. Bài này không mới nhưng có nhiều điều em mới học nên mạnh dạn chia sẻ cùng anh em trong quá trình làm model 🙂
Hi vọng giúp được các anh em.",Kính chào các bác. Tranh thủ cuối tuần ở nhà em cày cho anh em clip Cross Sell Prediction trong series AI in Banking. Bài này không mới nhưng có nhiều điều em mới học nên mạnh dạn chia sẻ cùng anh em trong quá trình làm model Hi vọng giúp được các anh em.,,,,,
"Seldon Core là một framework hỗ trợ package, deploy, autoscale và monitor model một cách dễ dàng trên Kubernetes. Mình xin được chia sẻ một số kiến thức và kinh nghiệm khi làm việc với Seldon Core như sau:
https://quan-dang.github.io/2022/01/16/advanced-model-serving-using-seldon-core-and-kubernetes-p1/
https://quan-dang.github.io/2022/04/07/advanced-model-serving-using-seldon-core-and-kubernetes-p2/
Để học hỏi thêm và giao lưu chia sẻ kinh nghiệm về MLOps, mọi người join group này nha https://www.facebook.com/groups/mlopsvn/
Chúc mọi ngưởi ngày mới tốt lành! :D","Seldon Core là một framework hỗ trợ package, deploy, autoscale và monitor model một cách dễ dàng trên Kubernetes. Mình xin được chia sẻ một số kiến thức và kinh nghiệm khi làm việc với Seldon Core như sau: https://quan-dang.github.io/2022/01/16/advanced-model-serving-using-seldon-core-and-kubernetes-p1/ https://quan-dang.github.io/2022/04/07/advanced-model-serving-using-seldon-core-and-kubernetes-p2/ Để học hỏi thêm và giao lưu chia sẻ kinh nghiệm về MLOps, mọi người join group này nha https://www.facebook.com/groups/mlopsvn/ Chúc mọi ngưởi ngày mới tốt lành! :D",,,,,
"Chào mọi người, có một project đang cần deploy model pytorch lên server (EC2 aws), và có khoảng 3 request/s. Hiện tại đang deployed bằng fastapi, mỗi request thì inference với batch-size = 1, Mình muốn tối ưu inference time bằng cách tăng batch size lên. Không biết có dịch vụ nào có sẵn để tối ưu không ạ? 
Mình có biết tới Amazon SageMaker nhưng có vẻ nó deploy trên một phần cứng riêng. Nhưng mình đã có EC2 rồi và muốn tận dụng tài nguyên của EC2 để deploy được không ạ?","Chào mọi người, có một project đang cần deploy model pytorch lên server (EC2 aws), và có khoảng 3 request/s. Hiện tại đang deployed bằng fastapi, mỗi request thì inference với batch-size = 1, Mình muốn tối ưu inference time bằng cách tăng batch size lên. Không biết có dịch vụ nào có sẵn để tối ưu không ạ? Mình có biết tới Amazon SageMaker nhưng có vẻ nó deploy trên một phần cứng riêng. Nhưng mình đã có EC2 rồi và muốn tận dụng tài nguyên của EC2 để deploy được không ạ?",,,,,
"Chào mọi người, mình đang có 2 việc cần làm nhưng chưa biết bắt đầu như thế nào, nhờ mọi người hỗ trợ keyword để mình giải quyết được công việc
1. Đọc thong tin từ Passport
2. Xử lý dữ liệu data người dùng để phù hợp với các mục tiêu cụ thể.","Chào mọi người, mình đang có 2 việc cần làm nhưng chưa biết bắt đầu như thế nào, nhờ mọi người hỗ trợ keyword để mình giải quyết được công việc 1. Đọc thong tin từ Passport 2. Xử lý dữ liệu data người dùng để phù hợp với các mục tiêu cụ thể.",,,,,
"Mình thấy quyển này mới release trên reddit. Mình chưa đọc chi tiết, nhưng xem qua mục lục thì thấy khá thú vị. Nó là working-project nên những phần vẫn chưa có. Tuy nhiên thì có thể bookmark lại, khi nào ng ta release tiếp thì xem tiếp.","Mình thấy quyển này mới release trên reddit. Mình chưa đọc chi tiết, nhưng xem qua mục lục thì thấy khá thú vị. Nó là working-project nên những phần vẫn chưa có. Tuy nhiên thì có thể bookmark lại, khi nào ng ta release tiếp thì xem tiếp.",,,,,
Tài liệu làm quen với Computer Vision trong vòng 365 ngày cho người mới bắt đầu.,Tài liệu làm quen với Computer Vision trong vòng 365 ngày cho người mới bắt đầu.,,,,,
"Chào mọi người, hiện mình có gặp chút vấn đề muốn xin ý tưởng .
Chả là mình đang có 1 mạng CNN nhận đầu vào là 1 input ảnh 1 kênh (gray). giờ mình muốn finetuning nó nhưng lại muốn ảnh đầu vào phải là 3 kênh rgb . nếu như từ rgb mà convert về gray thì mình thấy sẽ bị mất thông tin ảnh ( ảnh low contrast). Thì không biết mn có ý tưởng cách nào có thể giúp mình không nhỉ. Mình cảm ơn","Chào mọi người, hiện mình có gặp chút vấn đề muốn xin ý tưởng . Chả là mình đang có 1 mạng CNN nhận đầu vào là 1 input ảnh 1 kênh (gray). giờ mình muốn finetuning nó nhưng lại muốn ảnh đầu vào phải là 3 kênh rgb . nếu như từ rgb mà convert về gray thì mình thấy sẽ bị mất thông tin ảnh ( ảnh low contrast). Thì không biết mn có ý tưởng cách nào có thể giúp mình không nhỉ. Mình cảm ơn",,,,,
"[AI - EditGAN]
Mạng GAN gần đây được áp dụng cho các ứng dụng chỉnh sửa hình ảnh. Tuy nhiên, hầu hết các phương pháp chỉnh sửa hình ảnh dựa trên mạng GAN thường yêu cầu bộ dữ liệu lớn kèm nhãn của nó. EditGAN là một phương pháp chỉnh sửa hình ảnh mới, cung cấp khả năng chỉnh sửa có độ chính xác rất cao mà chỉ yêu cầu rất ít dữ liệu có chú thích nhãn. Thêm vào đó, EditGAN có thể chạy trong thời gian thực, có thể thao tác chỉnh sửa dễ dàng và có thể chỉnh sửa nhiều lần một lúc và vẫn giữ được chất lượng hình ảnh.
Tham khảo thêm:
https://nv-tlabs.github.io/editGAN/","[AI - EditGAN] Mạng GAN gần đây được áp dụng cho các ứng dụng chỉnh sửa hình ảnh. Tuy nhiên, hầu hết các phương pháp chỉnh sửa hình ảnh dựa trên mạng GAN thường yêu cầu bộ dữ liệu lớn kèm nhãn của nó. EditGAN là một phương pháp chỉnh sửa hình ảnh mới, cung cấp khả năng chỉnh sửa có độ chính xác rất cao mà chỉ yêu cầu rất ít dữ liệu có chú thích nhãn. Thêm vào đó, EditGAN có thể chạy trong thời gian thực, có thể thao tác chỉnh sửa dễ dàng và có thể chỉnh sửa nhiều lần một lúc và vẫn giữ được chất lượng hình ảnh. Tham khảo thêm: https://nv-tlabs.github.io/editGAN/",,,,,
"[Hỏi về sự hội tụ của gradient norm trong mô hình neural networks] Mình muốn hỏi về tại sao khi train 1 số neural networks (ResNet, VGG,..) thì accuracy và classification loss (log-loss) của mô hình hội tụ theo epochs, tuy nhiên gradient norm của hàm loss với tham số thì không hội tụ ? Thậm chí gradient norm còn có chiều hướng đi lên thay vì giảm dần.
Mình đã thử thay đổi batch-size, learning rate ( nhỏ dần) nhưng gradient norm vân không hội tụ về giá trị gần 0.
Notes: Mình làm ML kha khá nhiều (8 năm), và đã publish as first authors kha khá ở ICML/ICLR,..va biết tốc độ hội tụ của hàm loss/ gradient norm của các hàm convex, smooth, Lipschitz. Tuy nhiên kiến thức về sự hội tụ của gradient norm với các hàm loss như trong neural networks thì mình không biết nhiều. Nên cần chuyên gia về optimization giải thích kĩ vấn đề này. Mình xin cảm ơn.","[Hỏi về sự hội tụ của gradient norm trong mô hình neural networks] Mình muốn hỏi về tại sao khi train 1 số neural networks (ResNet, VGG,..) thì accuracy và classification loss (log-loss) của mô hình hội tụ theo epochs, tuy nhiên gradient norm của hàm loss với tham số thì không hội tụ ? Thậm chí gradient norm còn có chiều hướng đi lên thay vì giảm dần. Mình đã thử thay đổi batch-size, learning rate ( nhỏ dần) nhưng gradient norm vân không hội tụ về giá trị gần 0. Notes: Mình làm ML kha khá nhiều (8 năm), và đã publish as first authors kha khá ở ICML/ICLR,..va biết tốc độ hội tụ của hàm loss/ gradient norm của các hàm convex, smooth, Lipschitz. Tuy nhiên kiến thức về sự hội tụ của gradient norm với các hàm loss như trong neural networks thì mình không biết nhiều. Nên cần chuyên gia về optimization giải thích kĩ vấn đề này. Mình xin cảm ơn.",,,,,
"Chào các bác, nhân dịp đang tìm hiểu sơ bộ về Multi Labels Classification em mạnh dạn làm clip chia sẻ cơ bản nhất cho các bạn mới học.
Mong giúp được mọi người!","Chào các bác, nhân dịp đang tìm hiểu sơ bộ về Multi Labels Classification em mạnh dạn làm clip chia sẻ cơ bản nhất cho các bạn mới học. Mong giúp được mọi người!",,,,,
"Cuối tuần rồi, xin góp vui với các bạn bài toán phân loại 8 loại động kinh (seizure) từ tín hiệu ElectroEncephaloGram (EEG) từ dataset TUH seizure v1.5.2 tại đây (https://isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg_seizure/v1.5.2/)
Phương pháp giải quyết của mình như sau:
B1. Trích xuất và chuyển dữ liệu dạng tín hiệu thành dạng spectrogram sử dụng thuật toán Fast Fourier Transform với các thiết lập như sau: window length = 0.25 giây, step length = 0.0625 giây, Frequency sampling = 250 Hz. 6 trong 8 loại động kinh sẽ được trích xuất hết dữ liệu, 2 trong 8 sẽ được trích xuất lần lượt là 100 và 500 spectrogram samples (lí do là 2 lớp GN và GN có rất nhiều dữ liệu với số bệnh nhân cũng như thời gian động kinh cho mỗi lần là dài).
B2. Nhận diện cạnh của spectrograms bằng thuật toán Sobel và chuyển nó sang dạng ảnh có kích thước 32x48x1
B3. Biến ảnh nhận diện cạnh này thành Graph-structured data
B4. Huấn luyện model GNN với 3 layers kiến trúc có tên là GraphConv (xem hình tiến trình đo lường quá trình huấn luyện)
B5. Dự đoán trên test set như hình confusion matrix
Chúc các bạn cuối tuần vui vẻ.
Ps. Nếu mỗi lần động kinh của bệnh nhân ta chỉ trích xuất ngẫu nhiên 1 spectrogram thì kết quả vẫn rất khả quan, tuy nhiên lớp MY không phân loại được vì chỉ có 3 lần động kinh loại này được ghi lại trên đúng 2 bệnh nhận (xem confusion matrix có số lượng ảnh test set nhỏ!!!!) và quá trình huấn luyện cũng lâu hội tụ hơn (xem training curve!)","Cuối tuần rồi, xin góp vui với các bạn bài toán phân loại 8 loại động kinh (seizure) từ tín hiệu ElectroEncephaloGram (EEG) từ dataset TUH seizure v1.5.2 tại đây (https://isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg_seizure/v1.5.2/) Phương pháp giải quyết của mình như sau: B1. Trích xuất và chuyển dữ liệu dạng tín hiệu thành dạng spectrogram sử dụng thuật toán Fast Fourier Transform với các thiết lập như sau: window length = 0.25 giây, step length = 0.0625 giây, Frequency sampling = 250 Hz. 6 trong 8 loại động kinh sẽ được trích xuất hết dữ liệu, 2 trong 8 sẽ được trích xuất lần lượt là 100 và 500 spectrogram samples (lí do là 2 lớp GN và GN có rất nhiều dữ liệu với số bệnh nhân cũng như thời gian động kinh cho mỗi lần là dài). B2. Nhận diện cạnh của spectrograms bằng thuật toán Sobel và chuyển nó sang dạng ảnh có kích thước 32x48x1 B3. Biến ảnh nhận diện cạnh này thành Graph-structured data B4. Huấn luyện model GNN với 3 layers kiến trúc có tên là GraphConv (xem hình tiến trình đo lường quá trình huấn luyện) B5. Dự đoán trên test set như hình confusion matrix Chúc các bạn cuối tuần vui vẻ. Ps. Nếu mỗi lần động kinh của bệnh nhân ta chỉ trích xuất ngẫu nhiên 1 spectrogram thì kết quả vẫn rất khả quan, tuy nhiên lớp MY không phân loại được vì chỉ có 3 lần động kinh loại này được ghi lại trên đúng 2 bệnh nhận (xem confusion matrix có số lượng ảnh test set nhỏ!!!!) và quá trình huấn luyện cũng lâu hội tụ hơn (xem training curve!)",,,,,
Tài liệu xử lý ngôn ngữ tự nhiên của đại học Oxford cho ai quan tâm,Tài liệu xử lý ngôn ngữ tự nhiên của đại học Oxford cho ai quan tâm,,,,,
"#objectdetection #yolor
Các paper thường nói detection head đặt tại layer ""nông"" hơn thường nhận diện object kích thước nhỏ tốt hơn và detection head đặt tại layer sâu hơn nhận diện object kích thước lớn tốt hơn. Mình đã kiểm nghiệm điều này với YOLOR, cụ thể với YOLOR-w6 được nhắc tới trong paper. Mình đã tách 4 detection head của YOLOR-w6 (tại conv layer thứ 28, 32, 36, 40) ra làm 4 model, mỗi model con có một detection head. Và kết quả nhận được đúng như trên lý thuyết:
log chi tiết tại: https://drive.google.com/file/d/1-319l7cs56rYbIt4y0FiS7Se20-yRTaN/view?usp=sharing
Như vậy, đối với bài toán nhận diện object kích thước nhỏ, ta có thể tối ưu chi phí tính toán bằng cách cắt giảm đi các nhánh không cần thiết mà vẫn giữ nguyên được chất lượng model. kết quả cho thấy model head1 có inference time giảm 20% so với model gốc Ngoài ra, dự vào số liệu thống kê cho thấy detection head 28 tuy ở vị trí nông hơn detection head 32 nhưng kết quả nhận diện object nhỏ tại detection head 32 tốt hơn, tại sao?😣
Mình cần làm gì để tối ưu thêm cho model?","Các paper thường nói detection head đặt tại layer ""nông"" hơn thường nhận diện object kích thước nhỏ tốt hơn và detection head đặt tại layer sâu hơn nhận diện object kích thước lớn tốt hơn. Mình đã kiểm nghiệm điều này với YOLOR, cụ thể với YOLOR-w6 được nhắc tới trong paper. Mình đã tách 4 detection head của YOLOR-w6 (tại conv layer thứ 28, 32, 36, 40) ra làm 4 model, mỗi model con có một detection head. Và kết quả nhận được đúng như trên lý thuyết: log chi tiết tại: https://drive.google.com/file/d/1-319l7cs56rYbIt4y0FiS7Se20-yRTaN/view?usp=sharing Như vậy, đối với bài toán nhận diện object kích thước nhỏ, ta có thể tối ưu chi phí tính toán bằng cách cắt giảm đi các nhánh không cần thiết mà vẫn giữ nguyên được chất lượng model. kết quả cho thấy model head1 có inference time giảm 20% so với model gốc Ngoài ra, dự vào số liệu thống kê cho thấy detection head 28 tuy ở vị trí nông hơn detection head 32 nhưng kết quả nhận diện object nhỏ tại detection head 32 tốt hơn, tại sao? Mình cần làm gì để tối ưu thêm cho model?",#objectdetection	#yolor,,,,
"[AI News – Explainable CNNs]
Chúng ta thường biết đến mô hình Deep Learning như một hộp đen. Để giải thích hộp đen đó, Explainable CNNs là một package giúp visualize cho bất kỳ mô hình nào dựa trên CNN, được viết bằng Pytorch. Package này sử dụng cách tiếp cận dựa trên dữ liệu ( data centric). Explainable CNNs tạo các giải thích theo từng lớp CNN dựa trên gradient và xây dựng các biểu diễn khác nhau bao gồm Saliency Map, Guided BackPropagation, Grad CAM and Guided Grad CAM.
Tham khảo thêm ở đây:
https://github.com/ashutosh1919/explainable-cnn
https://pypi.org/project/explainable-cnn/","[AI News – Explainable CNNs] Chúng ta thường biết đến mô hình Deep Learning như một hộp đen. Để giải thích hộp đen đó, Explainable CNNs là một package giúp visualize cho bất kỳ mô hình nào dựa trên CNN, được viết bằng Pytorch. Package này sử dụng cách tiếp cận dựa trên dữ liệu ( data centric). Explainable CNNs tạo các giải thích theo từng lớp CNN dựa trên gradient và xây dựng các biểu diễn khác nhau bao gồm Saliency Map, Guided BackPropagation, Grad CAM and Guided Grad CAM. Tham khảo thêm ở đây: https://github.com/ashutosh1919/explainable-cnn https://pypi.org/project/explainable-cnn/",,,,,
"E chào mọi người. E mới học ML và gặp 1 vấn đề như này mong mọi người giúp đỡ ạ: Khi tiếp cận với bài toán ML, e thường dùng GridSearchCV để giải quyết nhưng e k biết làm cách nào để hiểu được sao nó lại chọn ra được parameters tối ưu như vậy. Kiểu e cảm giác khá là random. Cho e hỏi làm sao để mình hiểu được việc GridSearchCV lựa chọn parameters tối ưu và thêm nữa là hiểu được bài toán này thì nên áp dụng model nào. Hay chỉ cần cho hết các model vào GridSearchCV là nó tự động giải quyết hết cho mình luôn ạ. E cảm ơn","E chào mọi người. E mới học ML và gặp 1 vấn đề như này mong mọi người giúp đỡ ạ: Khi tiếp cận với bài toán ML, e thường dùng GridSearchCV để giải quyết nhưng e k biết làm cách nào để hiểu được sao nó lại chọn ra được parameters tối ưu như vậy. Kiểu e cảm giác khá là random. Cho e hỏi làm sao để mình hiểu được việc GridSearchCV lựa chọn parameters tối ưu và thêm nữa là hiểu được bài toán này thì nên áp dụng model nào. Hay chỉ cần cho hết các model vào GridSearchCV là nó tự động giải quyết hết cho mình luôn ạ. E cảm ơn",,,,,
"Chào các bạn, mình đang convert ML model format của tensorflow sang tflite, trong đó có LSTM layer. Tuy nhiên, khi xem trên Netron, thì phần activation function của LSTM layer chỉ còn có mỗi hàm [tanh]. Trong khi đó, nếu convert sang format ONNX thì Netron cho thấy activation function của LSTM layer là [sigmoid, tanh, tanh]. Test thử vài data cho model trên tflite format thi kết quả ra inference cũng đúng. Nhưng mình vẫn lăn tăn cái format tflite liệu có mising bước nào trước khi convert không? Bạn nào có kinh nghiệm về tflite hoặc info hoặc document link cho mình xin với. Cảm ơn.","Chào các bạn, mình đang convert ML model format của tensorflow sang tflite, trong đó có LSTM layer. Tuy nhiên, khi xem trên Netron, thì phần activation function của LSTM layer chỉ còn có mỗi hàm [tanh]. Trong khi đó, nếu convert sang format ONNX thì Netron cho thấy activation function của LSTM layer là [sigmoid, tanh, tanh]. Test thử vài data cho model trên tflite format thi kết quả ra inference cũng đúng. Nhưng mình vẫn lăn tăn cái format tflite liệu có mising bước nào trước khi convert không? Bạn nào có kinh nghiệm về tflite hoặc info hoặc document link cho mình xin với. Cảm ơn.",,,,,
"WEBINAR ON INDUSTRIAL AI ENGINEERING - event date updated
(xin phép các Admins cho mình đính chính ngày giờ thành: sáng Thứ Bảy 23/4 VNT)

Mình xin phép chia sẻ nhanh với cộng đồng các anh chị em có backgrounds về Vật lý, Kỹ thuật, Công nghệ… về một online event bọn mình sắp làm về mảng AI trong ngành Công nghiệp bit.ly/industrial-ai-event-vn-fb, sẽ được tổ chức vào sáng Thứ Bảy 23/4 VNT.
Bọn mình sẽ chia sẻ về một số nội dung chính như sau về việc áp dụng AI trong các ngành công nghiệp nặng:
Các cơ hội và thử thách lớn trong Industrial AI
Các ứng dụng AI công nghiệp điển hình trong Hàng hải và Chuỗi cung ứng
Phương pháp áp dụng Domain Expertise vào AI công nghiệp (“Knowledge-First AI"")
Nghiệp vụ AI Engineering để xây dựng các hệ thống AI phức hợp
Các công việc hàng ngày của AI Engineers làm AI công nghiệp
Hẹn gặp mọi người ở Webinar! Mình xin cảm ơn các anh chị em nhiều!
P.S.: Sau webinar bọn mình cũng có 2 buổi in-person meet-ups ở Hà Nội và HCM, các anh chị em nào quan tâm cũng có thể đăng ký luôn ạ.","WEBINAR ON INDUSTRIAL AI ENGINEERING - event date updated (xin phép các Admins cho mình đính chính ngày giờ thành: sáng Thứ Bảy 23/4 VNT) Mình xin phép chia sẻ nhanh với cộng đồng các anh chị em có backgrounds về Vật lý, Kỹ thuật, Công nghệ… về một online event bọn mình sắp làm về mảng AI trong ngành Công nghiệp bit.ly/industrial-ai-event-vn-fb, sẽ được tổ chức vào sáng Thứ Bảy 23/4 VNT. Bọn mình sẽ chia sẻ về một số nội dung chính như sau về việc áp dụng AI trong các ngành công nghiệp nặng: Các cơ hội và thử thách lớn trong Industrial AI Các ứng dụng AI công nghiệp điển hình trong Hàng hải và Chuỗi cung ứng Phương pháp áp dụng Domain Expertise vào AI công nghiệp (“Knowledge-First AI"") Nghiệp vụ AI Engineering để xây dựng các hệ thống AI phức hợp Các công việc hàng ngày của AI Engineers làm AI công nghiệp Hẹn gặp mọi người ở Webinar! Mình xin cảm ơn các anh chị em nhiều! P.S.: Sau webinar bọn mình cũng có 2 buổi in-person meet-ups ở Hà Nội và HCM, các anh chị em nào quan tâm cũng có thể đăng ký luôn ạ.",,,,,
"Dạ hiện em đang kiếm dữ liệu ảnh để làm bộ dữ liệu cho nhận diện khuôn mặt, có anh/chị nào biết/đã làm qua cách crawl dữ liệu ảnh của một người nào từ facebook hay google cho người Việt Nam chưa ạ (Nếu có github cho em xin tham khảo với ạ).","Dạ hiện em đang kiếm dữ liệu ảnh để làm bộ dữ liệu cho nhận diện khuôn mặt, có anh/chị nào biết/đã làm qua cách crawl dữ liệu ảnh của một người nào từ facebook hay google cho người Việt Nam chưa ạ (Nếu có github cho em xin tham khảo với ạ).",,,,,
"#objectdetection #yolor #onnx
Tối ưu inference time YOLOR bằng Onnx.
Chào mọi người, sau khi có được model phù hợp cho bài toán, mình cố gắng export model sang dạng onnx (đây có lẽ là chặng đường cuối cùng khi giải bài toán). Khi xem qua code export của tác giả (https://github.com/WongKinYiu/yolor/blob/main/models/export.py), mình nhận thấy có bất thường: trong khi luồng xử lý yolor khi inference trên pytorch, input image có thể có kích thước bất kì miễn là không quá nhỏ. Còn khi export onnx, dựa vào code của tác giả có thể đoán tác giả muốn cố định input shape (vì tác giả không định nghĩa dynamic_axes cho hàm torch.onnx.export()). Như vậy khi inference bằng onnx, ta bắt buộc resize image về kích thước được chỉ định, nó sẽ không tối ưu nếu kích thước gốc của input image khác input shape của model. 
Mình đã update code export và convert model sang onnx có thể inference trên những kích thước ảnh bất kì, nhưng chỉ có thể convert được toàn bộ layer trừ detect head layer (khi inference thì forward model onnx, sau đó lấy output của nó forward tiếp qua detection head bằng pytorch). Có anh chị nào đã convert đầy đủ model yolor sang onnx chưa ạ?","Tối ưu inference time YOLOR bằng Onnx. Chào mọi người, sau khi có được model phù hợp cho bài toán, mình cố gắng export model sang dạng onnx (đây có lẽ là chặng đường cuối cùng khi giải bài toán). Khi xem qua code export của tác giả (https://github.com/WongKinYiu/yolor/blob/main/models/export.py), mình nhận thấy có bất thường: trong khi luồng xử lý yolor khi inference trên pytorch, input image có thể có kích thước bất kì miễn là không quá nhỏ. Còn khi export onnx, dựa vào code của tác giả có thể đoán tác giả muốn cố định input shape (vì tác giả không định nghĩa dynamic_axes cho hàm torch.onnx.export()). Như vậy khi inference bằng onnx, ta bắt buộc resize image về kích thước được chỉ định, nó sẽ không tối ưu nếu kích thước gốc của input image khác input shape của model. Mình đã update code export và convert model sang onnx có thể inference trên những kích thước ảnh bất kì, nhưng chỉ có thể convert được toàn bộ layer trừ detect head layer (khi inference thì forward model onnx, sau đó lấy output của nó forward tiếp qua detection head bằng pytorch). Có anh chị nào đã convert đầy đủ model yolor sang onnx chưa ạ?",#objectdetection	#yolor	#onnx,,,,
"e chào mn ạ, hiện tại e đang làm đồ án text-to-speech https://github.com/NVIDIA/tacotron2 . Hiện tại thì việc train, test đã xong rồi ạ. Em đang gặp khó khăn ở phần chuẩn hóa text đầu vào, các danh mục từ viết tắt, các đơn vị đo theo chuẩn hóa quốc tế ,.. Do thời gian còn lại ít nên mọi người đã ai giải quyết rồi có thế nói ý tưởng giúp em hoặc có api đùng cho việc chuẩn hóa này không ạ. Em xin cảm ơn ạ!","e chào mn ạ, hiện tại e đang làm đồ án text-to-speech https://github.com/NVIDIA/tacotron2 . Hiện tại thì việc train, test đã xong rồi ạ. Em đang gặp khó khăn ở phần chuẩn hóa text đầu vào, các danh mục từ viết tắt, các đơn vị đo theo chuẩn hóa quốc tế ,.. Do thời gian còn lại ít nên mọi người đã ai giải quyết rồi có thế nói ý tưởng giúp em hoặc có api đùng cho việc chuẩn hóa này không ạ. Em xin cảm ơn ạ!",,,,,
"e chào mn ạ, hiện tại e đang làm đồ án text-to-speech https://github.com/NVIDIA/tacotron2 . Hiện tại thì việc train, test đã xong rồi ạ. Em đang gặp khó khăn ở phần chuẩn hóa text đầu vào, các danh mục từ viết tắt, các đơn vị đo theo chuẩn hóa quốc tế ,.. Do thời gian còn lại ít nên mọi người đã ai giải quyết rồi có thế nói ý tưởng giúp em hoặc có api đùng cho việc chuẩn hóa này không ạ. Em xin cảm ơn ạ!","e chào mn ạ, hiện tại e đang làm đồ án text-to-speech https://github.com/NVIDIA/tacotron2 . Hiện tại thì việc train, test đã xong rồi ạ. Em đang gặp khó khăn ở phần chuẩn hóa text đầu vào, các danh mục từ viết tắt, các đơn vị đo theo chuẩn hóa quốc tế ,.. Do thời gian còn lại ít nên mọi người đã ai giải quyết rồi có thế nói ý tưởng giúp em hoặc có api đùng cho việc chuẩn hóa này không ạ. Em xin cảm ơn ạ!",,,,,
"cấu trúc model YOLOR-w6 gốc, chụp tại phần đầu","cấu trúc model YOLOR-w6 gốc, chụp tại phần đầu",,,,,
"e chào mn ạ, e có đang tìm hiểu về deep sort. Em có chút thắc mắc mong đc mn giải đáp. Deep Sort theo như e hiểu sẽ có 2 stage chính là detection và association, cả 2 stage này đều dùng deep learning (stage đầu sử dụng cho việc detect, stage 2 sử dụng cho việc trích chọn đặc trưng đối tượng). Vậy thì 2 stage này cần 2 model deep learning khác nhau đúng k ạ? do vậy mà khi chạy code, mk cũng sẽ train cả 2 model tương ứng với bài toán cụ thể? (ví dụ baì toán tracking ô tô sử dụng yolov5 e cần training yolov5 cho detect ô tô và một model nữa cho việc trích trọn đặc trưng ô tô)? Nếu e hiểu sai mn sửa giúp e với. E cảm ơn nhiều ạ!","e chào mn ạ, e có đang tìm hiểu về deep sort. Em có chút thắc mắc mong đc mn giải đáp. Deep Sort theo như e hiểu sẽ có 2 stage chính là detection và association, cả 2 stage này đều dùng deep learning (stage đầu sử dụng cho việc detect, stage 2 sử dụng cho việc trích chọn đặc trưng đối tượng). Vậy thì 2 stage này cần 2 model deep learning khác nhau đúng k ạ? do vậy mà khi chạy code, mk cũng sẽ train cả 2 model tương ứng với bài toán cụ thể? (ví dụ baì toán tracking ô tô sử dụng yolov5 e cần training yolov5 cho detect ô tô và một model nữa cho việc trích trọn đặc trưng ô tô)? Nếu e hiểu sai mn sửa giúp e với. E cảm ơn nhiều ạ!",,,,,
"Có ai đăng kí được tài khoản trên openai ko ạ? Hôm nay em vào đăng kí thì lại không hỗ trợ khu vực VN
Sẵn tiện em cần mua lại 1 tài khoản openai, ai có dư không dùng thì có thể share lại em với ạ. Em cảm ơn","Có ai đăng kí được tài khoản trên openai ko ạ? Hôm nay em vào đăng kí thì lại không hỗ trợ khu vực VN Sẵn tiện em cần mua lại 1 tài khoản openai, ai có dư không dùng thì có thể share lại em với ạ. Em cảm ơn",,,,,
"Mọi người cho em hỏi chút, em muốn tính shannon-entropy cho một batch ảnh, em tìm đc một số hàm tính toán entropy trong các thư viện như scipy, skimage,.. nhưng chỉ tính được trên từng ảnh đơn lẻ, có cách nào có thể tính được toàn bộ giá trị entropy của các ảnh trong một batch được không ạ. em cảm ơn mọi người","Mọi người cho em hỏi chút, em muốn tính shannon-entropy cho một batch ảnh, em tìm đc một số hàm tính toán entropy trong các thư viện như scipy, skimage,.. nhưng chỉ tính được trên từng ảnh đơn lẻ, có cách nào có thể tính được toàn bộ giá trị entropy của các ảnh trong một batch được không ạ. em cảm ơn mọi người",,,,,
"Chào các bạn,
Mình đang tìm hiểu về multi-point statistics network, và muốn áp dụng nó trong CNN. Tuy nhiên mình chưa biết cách tích hợp hard data vào trong mạng để yêu cầu mạng train model từ hard data. Mình có đọc thuật toán của cGan nhưng hình như chưa phải cái mình cần. Mong được các bạn chia sẻ kinh nghiệm, cám ơn nhiều","Chào các bạn, Mình đang tìm hiểu về multi-point statistics network, và muốn áp dụng nó trong CNN. Tuy nhiên mình chưa biết cách tích hợp hard data vào trong mạng để yêu cầu mạng train model từ hard data. Mình có đọc thuật toán của cGan nhưng hình như chưa phải cái mình cần. Mong được các bạn chia sẻ kinh nghiệm, cám ơn nhiều",,,,,
"Chào cả nhà, mình thấy Scikit-Learn có khá nhiều hàm trong pre-process, vậy trong trường hợp nào thì mình dùng cái nào?
minmaxscaler 
robustscaler 
standardscaler 
 normalizer","Chào cả nhà, mình thấy Scikit-Learn có khá nhiều hàm trong pre-process, vậy trong trường hợp nào thì mình dùng cái nào? minmaxscaler robustscaler standardscaler normalizer",,,,,
"Sau 7749 lần tra cứu google, mình đã cho ra lò 'a fused TFLite LSTM model'.
Như mọi người được biết thì hiện nay tensorflow lite là model được google tạo ra, nhầm hướng đến sử dụng AI trên các thiết bị mobile, IoT, . . .
Và tensorflow lite thì không thể train được model LSTM, hay convert trực tiếp từ model có LSTM.
Vì thế 'a fused TFLite LSTM model' là giải pháp hiện tại mà google tạo ra để có thể convert sang tflite model.
Dưới đây là project Human Activity Recognition sử dụng an accelerometer, a gyroscope, a magnetometer, and a linear acceleration sensor. Với thiết bị điện thoại được bỏ trong túi quần để được kết quả.
p/s: mình ngồi xổm để được kết quả như hình.
https://github.com/phuoctan4141/A-FUSED-TFLITE-LSTM-MODEL","Sau 7749 lần tra cứu google, mình đã cho ra lò 'a fused TFLite LSTM model'. Như mọi người được biết thì hiện nay tensorflow lite là model được google tạo ra, nhầm hướng đến sử dụng AI trên các thiết bị mobile, IoT, . . . Và tensorflow lite thì không thể train được model LSTM, hay convert trực tiếp từ model có LSTM. Vì thế 'a fused TFLite LSTM model' là giải pháp hiện tại mà google tạo ra để có thể convert sang tflite model. Dưới đây là project Human Activity Recognition sử dụng an accelerometer, a gyroscope, a magnetometer, and a linear acceleration sensor. Với thiết bị điện thoại được bỏ trong túi quần để được kết quả. p/s: mình ngồi xổm để được kết quả như hình. https://github.com/phuoctan4141/A-FUSED-TFLITE-LSTM-MODEL",,,,,
"Mình đang gặp lỗi resource exhausted trên colab pro khi áp dụng code mẫu seq2seq của tf. Mình đang sử dụng data là văn bản dài, có thể đến 2000 chữ. Mọi người có thể cho mình 1 số gợi ý để giải quyết không? Cảm ơn mọi người","Mình đang gặp lỗi resource exhausted trên colab pro khi áp dụng code mẫu seq2seq của tf. Mình đang sử dụng data là văn bản dài, có thể đến 2000 chữ. Mọi người có thể cho mình 1 số gợi ý để giải quyết không? Cảm ơn mọi người",,,,,
Chia sẻ thuật toán mới về học tăng cường cho mọi người,Chia sẻ thuật toán mới về học tăng cường cho mọi người,,,,,
"Chào mọi người, mình có một bài toán về phát hiện 2 class smoke, fire trong image. Mục tiêu là cần detect được nếu image có smoke hoặc fire, không quan trọng bounding box.
 Giả sử mình áp dụng YOLO (v1)  algorithm và muốn tùy chỉnh lại algorithm để tối ưu cho bài toán của mình như sau, mình mong các bạn góp ý để biết liệu rằng nó khả thi không: (các kí hiệu trong bài viết được định nghĩa giống trong paper)
Tùy chỉnh thứ nhất: Vì bài toán không yêu cầu thông tin về bounding box nên output loại bỏ bx, by, bh, hw ở mỗi grid cell.
mỗi bounding box gốc bao gồm thông tin {confidence, x, y, h, w}, nay sẽ thay đổi thành {confidence}.
cách tính confidence sẽ thay đổi từ confidence = Pr(Object) ∗ IOUtruth|pred thành confidence = Pr(Object).
output sẽ trở thành tensor S × S × (B ∗ 1 + C).
Loss function sẽ bằng L_cls.
Việc giảm bớt lượng params sẽ giúp inference nhanh hơn và giảm yêu cầu phần cứng.
Tùy chỉnh thứ hai: Vì kích thước của object smoke và fire nhỏ nên loại bỏ feature maps với grid cell nhỏ, giữ lại feature maps có grid cell lớn.
Để giảm chi phí tính toán.
link paper YOLOv1 https://arxiv.org/pdf/1506.02640.pdf","Chào mọi người, mình có một bài toán về phát hiện 2 class smoke, fire trong image. Mục tiêu là cần detect được nếu image có smoke hoặc fire, không quan trọng bounding box. Giả sử mình áp dụng YOLO (v1) algorithm và muốn tùy chỉnh lại algorithm để tối ưu cho bài toán của mình như sau, mình mong các bạn góp ý để biết liệu rằng nó khả thi không: (các kí hiệu trong bài viết được định nghĩa giống trong paper) Tùy chỉnh thứ nhất: Vì bài toán không yêu cầu thông tin về bounding box nên output loại bỏ bx, by, bh, hw ở mỗi grid cell. mỗi bounding box gốc bao gồm thông tin {confidence, x, y, h, w}, nay sẽ thay đổi thành {confidence}. cách tính confidence sẽ thay đổi từ confidence = Pr(Object) ∗ IOUtruth|pred thành confidence = Pr(Object). output sẽ trở thành tensor S × S × (B ∗ 1 + C). Loss function sẽ bằng L_cls. Việc giảm bớt lượng params sẽ giúp inference nhanh hơn và giảm yêu cầu phần cứng. Tùy chỉnh thứ hai: Vì kích thước của object smoke và fire nhỏ nên loại bỏ feature maps với grid cell nhỏ, giữ lại feature maps có grid cell lớn. Để giảm chi phí tính toán. link paper YOLOv1 https://arxiv.org/pdf/1506.02640.pdf",,,,,
"Chào mọi người,
mình là lính mới muốn nhờ các tiền bối giải đáp giúp vấn đề sau:
Mình cần làm binary classification (0,1), mình dùng SVM và vài features của data để train model . Mình thử train model với 1 features và 10 features thì thấy Accuracy và F1 score for test data là gần như nhau. Tuy nhiên nếu mình lấy model của 1 feature để predict label cho một unlabeled dataset khác thì mình thấy độ chính xác lại ko cao khi so với dùng nhiều features.
Khi mình plot data distribution của 1 feature(hình bên dưới) thì thấy: data range của phần training (hình trên), khác với data range của unlabeled dataset (hình dưới), liệu có phải đó là nguyên nhân khiến cho model của mình perform kém đi khi apply cho new dataset ko?
Cảm ơn các tiền bối!","Chào mọi người, mình là lính mới muốn nhờ các tiền bối giải đáp giúp vấn đề sau: Mình cần làm binary classification (0,1), mình dùng SVM và vài features của data để train model . Mình thử train model với 1 features và 10 features thì thấy Accuracy và F1 score for test data là gần như nhau. Tuy nhiên nếu mình lấy model của 1 feature để predict label cho một unlabeled dataset khác thì mình thấy độ chính xác lại ko cao khi so với dùng nhiều features. Khi mình plot data distribution của 1 feature(hình bên dưới) thì thấy: data range của phần training (hình trên), khác với data range của unlabeled dataset (hình dưới), liệu có phải đó là nguyên nhân khiến cho model của mình perform kém đi khi apply cho new dataset ko? Cảm ơn các tiền bối!",,,,,
"Giới thiệu m.n thuật toán RANSAC- Ứng dụng dùng để giải quyết bài toán trong các bài toán Location Determination Problem - LDP. Các bài toán LDP như xác định khoảng cách giữa các vật, xây dựng mô hình 3D từ ảnh 2D hay chụp ảnh panorama,...
Thank all.","Giới thiệu m.n thuật toán RANSAC- Ứng dụng dùng để giải quyết bài toán trong các bài toán Location Determination Problem - LDP. Các bài toán LDP như xác định khoảng cách giữa các vật, xây dựng mô hình 3D từ ảnh 2D hay chụp ảnh panorama,... Thank all.",,,,,
"#hoidap
Chào mọi người,
Chuyện là mình đang đọc paper về Swin Transformer và có chỗ hơi khó hiểu. Đó là khi tác giả so sánh các mô hình trong tác vụ image classification trên tập ImageNet, tác giả lại dùng cụm ""single crop"". Mình có tìm hiểu và thấy người ta chỉ nói đến ""top-1"", ""top-2"" ... chứ không có tài liệu nào nói về ""single crop"". Hoặc có thể là do mình search không đúng key words.
Nên mình muốn hỏi về thuật ngữ ""single crop"" trong ngữ cảnh dưới đây mang hàm ý gì:
Cảm ơn mọi người.","Chào mọi người, Chuyện là mình đang đọc paper về Swin Transformer và có chỗ hơi khó hiểu. Đó là khi tác giả so sánh các mô hình trong tác vụ image classification trên tập ImageNet, tác giả lại dùng cụm ""single crop"". Mình có tìm hiểu và thấy người ta chỉ nói đến ""top-1"", ""top-2"" ... chứ không có tài liệu nào nói về ""single crop"". Hoặc có thể là do mình search không đúng key words. Nên mình muốn hỏi về thuật ngữ ""single crop"" trong ngữ cảnh dưới đây mang hàm ý gì: Cảm ơn mọi người.",#hoidap,,,,
"Chào các bác. Em đang học về phần này nên mạnh dạn chia sẻ một chút về RNN, LSTM, Time Series Data và tìm hiểu cách chuẩn bị dữ liệu cho RNN model nhanh hơn 10 lần với Numpy nhé!
Hi vọng giúp được các bạn mới học ạ!","Chào các bác. Em đang học về phần này nên mạnh dạn chia sẻ một chút về RNN, LSTM, Time Series Data và tìm hiểu cách chuẩn bị dữ liệu cho RNN model nhanh hơn 10 lần với Numpy nhé! Hi vọng giúp được các bạn mới học ạ!",,,,,
"[Airflow tutorial for Data Scientist]
Airflow là 1 trong những tools phổ biến về orchestration data pipeline trong Python.
Mình có làm ra 1 tutorial hướng dẫn step-by-step về Airflow
Link: https://github.com/DatacollectorVN/Airflow-Tutorial
Trong repo mình ghi document kỹ về Airflow cho bạn nào không chỉ muốn ăn mì mà muốn hiểu rõ về Airflow.",[Airflow tutorial for Data Scientist] Airflow là 1 trong những tools phổ biến về orchestration data pipeline trong Python. Mình có làm ra 1 tutorial hướng dẫn step-by-step về Airflow Link: https://github.com/DatacollectorVN/Airflow-Tutorial Trong repo mình ghi document kỹ về Airflow cho bạn nào không chỉ muốn ăn mì mà muốn hiểu rõ về Airflow.,,,,,
"Chào mọi người, trước đó mình có một bài toán nhận diện khói lửa và từng muốn áp dụng YOLOv1, nhưng sau đó từ sự góp ý của mọi người mình chuyển qua hướng tiếp cận chủ đạo là classification, cụ thể áp dụng classification kết hợp với output của segmentation để việc phân loại trở nên chính xác hơn cho object có kích thước nhỏ như khói lửa. 
Chi tiết về cách tiếp cận này đã được mô tả chi tiết trong paper https://arxiv.org/pdf/1812.00291.pdf. Thay vì sử dụng model classification đơn thuần, paper này kết hợp thêm segmentation để 
cho model biết cần tập trung vào vị trí nào của ảnh để đưa ra dự đoán. Phương pháp này đã được chứng minh là hiệu quả hơn so với phương pháp classification truyền thống.
Giả sử chỉ áp dụng kiến trúc của paper này, mình có một tùy chỉnh nhỏ, liệu nó có khả thi và hiệu quả không? Đồng thời mình cũng mô tả suy nghĩ của mình về bài toán này. Liệu rằng suy nghĩ của mình có đúng không? Mong mọi người góp ý và chia sẻ thêm những sửa đổi tinh vi để giúp model đạt hiệu quả hơn:
Input của bài toán là video, như vậy thay vì build model  segmentation để có binary mask của fire/smoke, hệ thống có thể trích xuất binary mask của smoke và fire bằng chuyển động mà nó tạo ra (https://docs.opencv.org/4.x/d1/dc5/tutorial_background_subtraction.html). Sử dụng binary mask từ movement có thể sẽ cho ra đặc trưng không giống so với binary mask của model segmentation, cụ thể là bất kì object có chuyển động nào trong video đều sẽ tạo ra positive tại vị trí của object đó. Đây là một đặc trưng hữu ích cho bài toán vì model sẽ có nhiều context infomation (thông tin về bối cảnh) hơn. 
Mở rộng ra, đã có đủ cơ sở để xây dựng model cảnh báo có an toàn với khói lửa hay không (thay vì ở mức cơ bản là phát hiện có khói lửa hay không). Ví dụ: Người lớn đang nấu ăn là negative; trẻ nhỏ đang nấu ăn là positive; người lớn đang bất tỉnh khi đang nấu ăn là positive;... 
Phải nói thêm là với model classification đơn thuần cũng có thể build được model cảnh báo có an toàn với khói lửa hay không, tuy nhiên mình nghĩ hiệu suất sẽ không bằng vì có thêm binary mask giúp model biết nên tập trung vào phần nào trong ảnh để đưa ra dự đoán đồng thời loại bỏ nhiễu từ background.
Về cách xây dựng model, theo như số liệu và nội dụng phần 5. ""Results and discussion"", mình sẽ khởi tạo model theo kiến trúc được mô tả tại Figure 3c.
Ngoài ra mình có một số câu hỏi:
Với bài toán này nên chọn backbone nào để extract feature ?
Đối với một người làm về AI, để implement phương pháp này  sẽ mất thời gian bao lâu?
Trong paper có trình bày hai cách để kết hợp  output của segment vào nhánh chính (main branch) là element-wise addition và element-wise multiplication. Tuy nhiên mình nghĩ còn có một cách kết hợp khác là  ""concatenate channels"" - chiều dài và chiều rộng của layer không thay đổi, chiều sâu của layer tăng lên. Tại sao paper không sử dụng đến, nguyên nhân cụ thể là gì?","Chào mọi người, trước đó mình có một bài toán nhận diện khói lửa và từng muốn áp dụng YOLOv1, nhưng sau đó từ sự góp ý của mọi người mình chuyển qua hướng tiếp cận chủ đạo là classification, cụ thể áp dụng classification kết hợp với output của segmentation để việc phân loại trở nên chính xác hơn cho object có kích thước nhỏ như khói lửa. Chi tiết về cách tiếp cận này đã được mô tả chi tiết trong paper https://arxiv.org/pdf/1812.00291.pdf. Thay vì sử dụng model classification đơn thuần, paper này kết hợp thêm segmentation để cho model biết cần tập trung vào vị trí nào của ảnh để đưa ra dự đoán. Phương pháp này đã được chứng minh là hiệu quả hơn so với phương pháp classification truyền thống. Giả sử chỉ áp dụng kiến trúc của paper này, mình có một tùy chỉnh nhỏ, liệu nó có khả thi và hiệu quả không? Đồng thời mình cũng mô tả suy nghĩ của mình về bài toán này. Liệu rằng suy nghĩ của mình có đúng không? Mong mọi người góp ý và chia sẻ thêm những sửa đổi tinh vi để giúp model đạt hiệu quả hơn: Input của bài toán là video, như vậy thay vì build model segmentation để có binary mask của fire/smoke, hệ thống có thể trích xuất binary mask của smoke và fire bằng chuyển động mà nó tạo ra (https://docs.opencv.org/4.x/d1/dc5/tutorial_background_subtraction.html). Sử dụng binary mask từ movement có thể sẽ cho ra đặc trưng không giống so với binary mask của model segmentation, cụ thể là bất kì object có chuyển động nào trong video đều sẽ tạo ra positive tại vị trí của object đó. Đây là một đặc trưng hữu ích cho bài toán vì model sẽ có nhiều context infomation (thông tin về bối cảnh) hơn. Mở rộng ra, đã có đủ cơ sở để xây dựng model cảnh báo có an toàn với khói lửa hay không (thay vì ở mức cơ bản là phát hiện có khói lửa hay không). Ví dụ: Người lớn đang nấu ăn là negative; trẻ nhỏ đang nấu ăn là positive; người lớn đang bất tỉnh khi đang nấu ăn là positive;... Phải nói thêm là với model classification đơn thuần cũng có thể build được model cảnh báo có an toàn với khói lửa hay không, tuy nhiên mình nghĩ hiệu suất sẽ không bằng vì có thêm binary mask giúp model biết nên tập trung vào phần nào trong ảnh để đưa ra dự đoán đồng thời loại bỏ nhiễu từ background. Về cách xây dựng model, theo như số liệu và nội dụng phần 5. ""Results and discussion"", mình sẽ khởi tạo model theo kiến trúc được mô tả tại Figure 3c. Ngoài ra mình có một số câu hỏi: Với bài toán này nên chọn backbone nào để extract feature ? Đối với một người làm về AI, để implement phương pháp này sẽ mất thời gian bao lâu? Trong paper có trình bày hai cách để kết hợp output của segment vào nhánh chính (main branch) là element-wise addition và element-wise multiplication. Tuy nhiên mình nghĩ còn có một cách kết hợp khác là ""concatenate channels"" - chiều dài và chiều rộng của layer không thay đổi, chiều sâu của layer tăng lên. Tại sao paper không sử dụng đến, nguyên nhân cụ thể là gì?",,,,,
"Hi all,
Đang tìm hiểu về 3 kỹ thuật dự đoán là linear regression, sarimax và fbprophet, thấy có điểm khó hiểu.
1. linear regression đầu vào khá nhiều cột để ra dự đoán (array đem traing có nhiều phần tử)
2. sarimax và fbprophet : chỉ có 2 cột Thời gian và giá trị
VD dự báo GDP nền kinh tế, trong khi linear regression có thể dùng nhiều phần tử để traing như GDP/lạm phát/nợ nước ngoài/ vốn FDI/ . . . . thì sarimax /fbprophet chỉ có cột thời gian và GDP để dự đoán GDP của chính nó ?!
Vậy liệu sarimax /fbprophet có chính xác được hơn Linear regression
Ai có kinh nghiệm giải thịch hộ","Hi all, Đang tìm hiểu về 3 kỹ thuật dự đoán là linear regression, sarimax và fbprophet, thấy có điểm khó hiểu. 1. linear regression đầu vào khá nhiều cột để ra dự đoán (array đem traing có nhiều phần tử) 2. sarimax và fbprophet : chỉ có 2 cột Thời gian và giá trị VD dự báo GDP nền kinh tế, trong khi linear regression có thể dùng nhiều phần tử để traing như GDP/lạm phát/nợ nước ngoài/ vốn FDI/ . . . . thì sarimax /fbprophet chỉ có cột thời gian và GDP để dự đoán GDP của chính nó ?! Vậy liệu sarimax /fbprophet có chính xác được hơn Linear regression Ai có kinh nghiệm giải thịch hộ",,,,,
"#share #table_recognition #viblo 
Bài chia sẻ của mình về hướng tiếp cận và pipeline cho bài toán tái cấu trúc dữ liệu bảng biểu với Deep Learning - Table Recognition 
https://viblo.asia/p/Qbq5QBYLKD8

- Table Recognition là bài toán phát hiện, nhận dạng, trích rút thông tin và lưu giữ chính xác bố cục và hình thái của bảng biểu, từ đó có thể lưu trữ các thông tin dạng bảng biểu dưới các format có thể chỉnh sửa được như Docx, Excel,... Là bài toán có tính ứng dụng cao trong việc số hóa văn bản, chỉnh sửa, tìm kiếm và truy vấn các tài liệu. Trong thời gian gần đây mới nhận được sự quan tâm nhiều hơn từ cộng đồng nghiên cứu, nhiều SOTA model và solution được công bố hơn.
- Trong bài blog này, mình có review về 1 số các phương pháp hiện tại, giới thiệu về pipeline và hướng tiếp cận của mình, kết quả thực nghiệm và phương pháp đánh giá, kèm theo các nguồn tài liệu tham khảo để ở cuối bài viết. Hi vọng giúp ích với mọi người :D","Bài chia sẻ của mình về hướng tiếp cận và pipeline cho bài toán tái cấu trúc dữ liệu bảng biểu với Deep Learning - Table Recognition https://viblo.asia/p/Qbq5QBYLKD8 - Table Recognition là bài toán phát hiện, nhận dạng, trích rút thông tin và lưu giữ chính xác bố cục và hình thái của bảng biểu, từ đó có thể lưu trữ các thông tin dạng bảng biểu dưới các format có thể chỉnh sửa được như Docx, Excel,... Là bài toán có tính ứng dụng cao trong việc số hóa văn bản, chỉnh sửa, tìm kiếm và truy vấn các tài liệu. Trong thời gian gần đây mới nhận được sự quan tâm nhiều hơn từ cộng đồng nghiên cứu, nhiều SOTA model và solution được công bố hơn. - Trong bài blog này, mình có review về 1 số các phương pháp hiện tại, giới thiệu về pipeline và hướng tiếp cận của mình, kết quả thực nghiệm và phương pháp đánh giá, kèm theo các nguồn tài liệu tham khảo để ở cuối bài viết. Hi vọng giúp ích với mọi người :D",#share	#table_recognition	#viblo,,,,
Mọi người cho hỏi dùng Q learning dùng trong thị trường chứng khoán có code không mọi người tại tìm trên mạng Hoài không thấy,Mọi người cho hỏi dùng Q learning dùng trong thị trường chứng khoán có code không mọi người tại tìm trên mạng Hoài không thấy,,,,,
"Em chào mn ạ,em muốn mô phỏng graph network bằng c++ thì có gói công cụ hỗ trợ nào không ạ,mong được mn giải đáp,em cảm ơn ❤️","Em chào mn ạ,em muốn mô phỏng graph network bằng c++ thì có gói công cụ hỗ trợ nào không ạ,mong được mn giải đáp,em cảm ơn",,,,,
"Thế giới xác suất đắm chìm trong việc mô hình hóa tính chắc chắn của các sự kiện thông qua bản đồ niềm tin đặc tả bởi các con số, được xây dựng bởi các nhà toán học qua các phân bố xác suất.
Trong bài viết này chúng ta sẽ tìm hiểu sơ lược về Normalizing Flows mô hình hóa các phân bố phức tạp bằng công thức đổi biến qua các phép biến đổi khả nghịch.","Thế giới xác suất đắm chìm trong việc mô hình hóa tính chắc chắn của các sự kiện thông qua bản đồ niềm tin đặc tả bởi các con số, được xây dựng bởi các nhà toán học qua các phân bố xác suất. Trong bài viết này chúng ta sẽ tìm hiểu sơ lược về Normalizing Flows mô hình hóa các phân bố phức tạp bằng công thức đổi biến qua các phép biến đổi khả nghịch.",,,,,
"Dạ em chào anh chị, em có một đề tài về xây dựng 'a complex network of stock market', chủ yếu bằng matlab (có liên quan một chút đến Python và R cũng được ạ, em có thể học thêm), dựa vào đó để predict. Em gần như là mới tiếp xúc và chưa biết gì, nên em có vài câu hỏi:
- em cần những kiến thức nền gì để có thể build được complex network này? (online courses, documents, thesis, etc)
- việc lấy dữ liệu mẫu ở đâu ạ? (e.g: yahoo finance,..)
Em cảm ơn ạ!","Dạ em chào anh chị, em có một đề tài về xây dựng 'a complex network of stock market', chủ yếu bằng matlab (có liên quan một chút đến Python và R cũng được ạ, em có thể học thêm), dựa vào đó để predict. Em gần như là mới tiếp xúc và chưa biết gì, nên em có vài câu hỏi: - em cần những kiến thức nền gì để có thể build được complex network này? (online courses, documents, thesis, etc) - việc lấy dữ liệu mẫu ở đâu ạ? (e.g: yahoo finance,..) Em cảm ơn ạ!",,,,,
"[Build Deep Learning Workstation]
Chào mọi người!
Mình đang cần build phần cứng để phục vụ nhu cầu training trong team mình ( budget $25k~$30k).
Mọi người trong group có kinh nghiệm về xây dựng phần cứng về Deep Learning cho mình xin ít kinh nghiệm với ạ.
Mình xin chân thành cảm ơn!
Edited!
Với budget tầm $25k~$30k thì mình cân nhắc xây dựng phần cứng từ 4 x GPU ~ 8 x GPU, mục tiêu là nhiều người trong team có thể sử dụng cùng một lúc (từ 5~10 người). Mình đang xem xét dòng Nvidia A100 40 Gb vs RTX A6000 48Gb..(giả sử data chuẩn và đủ nhiều).","[Build Deep Learning Workstation] Chào mọi người! Mình đang cần build phần cứng để phục vụ nhu cầu training trong team mình ( budget $25k~$30k). Mọi người trong group có kinh nghiệm về xây dựng phần cứng về Deep Learning cho mình xin ít kinh nghiệm với ạ. Mình xin chân thành cảm ơn! Edited! Với budget tầm $25k~$30k thì mình cân nhắc xây dựng phần cứng từ 4 x GPU ~ 8 x GPU, mục tiêu là nhiều người trong team có thể sử dụng cùng một lúc (từ 5~10 người). Mình đang xem xét dòng Nvidia A100 40 Gb vs RTX A6000 48Gb..(giả sử data chuẩn và đủ nhiều).",,,,,
"[AI application in game] AI agent plays Sonic the Hedgehog
Bên cạnh các tựa game kinh điển như Super Mario Bros, Contra hay Tetris, thì Sonic là 1 trong các tựa game mình vô cùng yêu thích khi còn bé. Sau này khi làm việc trong lĩnh vực AI, mà cụ thể hơn là Reinforcement Learning, mình đặc biệt có hứng thú với các ứng dụng của AI trong game. Trong project này, AI agent sẽ tự khám phá và học cách chơi Sonic the Hedgehog. Dưới đây là 2 level đầu tiên. Let's enjoy it!
Source code: https://github.com/uvipen/Sonic-PPO-pytorch
Demo: https://youtu.be/hN3Gw-YunBc
#python #ReinforcementLearning","[AI application in game] AI agent plays Sonic the Hedgehog Bên cạnh các tựa game kinh điển như Super Mario Bros, Contra hay Tetris, thì Sonic là 1 trong các tựa game mình vô cùng yêu thích khi còn bé. Sau này khi làm việc trong lĩnh vực AI, mà cụ thể hơn là Reinforcement Learning, mình đặc biệt có hứng thú với các ứng dụng của AI trong game. Trong project này, AI agent sẽ tự khám phá và học cách chơi Sonic the Hedgehog. Dưới đây là 2 level đầu tiên. Let's enjoy it! Source code: https://github.com/uvipen/Sonic-PPO-pytorch Demo: https://youtu.be/hN3Gw-YunBc",#python	#ReinforcementLearning,,,,
"chào mọi người ạ, mình muốn hỏi là nếu bây giờ mình muốn tạo một random network đi N ngân hàng trên Python. Mỗi ngân hàng có thể kết nối với d ngân hàng khác. Vậy mình có thể tạo ra một adjacency matrix như thế nào? Cảm ơn mọi người ạ.","chào mọi người ạ, mình muốn hỏi là nếu bây giờ mình muốn tạo một random network đi N ngân hàng trên Python. Mỗi ngân hàng có thể kết nối với d ngân hàng khác. Vậy mình có thể tạo ra một adjacency matrix như thế nào? Cảm ơn mọi người ạ.",,,,,
"#feature_extraction
#handcrafted_feature
[Tại sao cần binning các numeric feature?]
Chào mọi người,
Paper sau đây là một ví dụ thực của việc binning numeric feature.
GraphDTA[2019] (https://academic.oup.com/bioinformatics/article-abstract/37/8/1140/5942970?redirectedFrom=fulltext) sử dụng GNN để encode embedding cho dữ liệu thuốc. Dữ liệu thuốc ban đầu là đồ thị phân tử với nút là các nguyên tử; đặc trưng ban đầu cho mỗi nút nguyên tử được xây dựng thành một vector 78-chiều. Trong đó, 44 bin đầu tiên làm one hot encoding cho loại nguyên tố, 11 bin tiếp theo làm one hot encoding cho bậc của nguyên tử với các bin là 0-10, 11 bin tiếp theo làm one hot encoding cho số nguyên tử Hydrogen liên kết với nút với các bin là 0-10, 11 tiếp theo làm one hot encoding cho số implicit Hyrdrogen liên kết với nút với các bin là 0-10, 1 bin cuối cùng làm binary encoding cho 'thơm hay không?'. Chi tiết các bạn có thể xem ở github của GraphDTA hoặc bảng 2 của paper này:https://pubs.rsc.org/en/content/articlelanding/2020/ra/d0ra02297g
Việc one hot encoding nhãn loại nguyên tố thì mình hiểu được, nhưng tại sao cần binning các numeric feature (bậc nguyên tử, số H, số imolicit H)? Tại sao lại muốn feature vector của nút là thưa? Mong được mọi người giúp đỡ, cảm ơn mọi người.
Các ví dụ khác cũng thực hiện binning numeric features: https://github.com/wengong-jin/iclr19-graph2graph/blob/master/fast_jtnn/mpn.py
https://colab.research.google.com/github/sunhwan/blog/blob/master/_notebooks/2021-02-20-Learning-Molecular-Representation-Using-Graph-Neural-Network-Molecular-Graph.ipynb","[Tại sao cần binning các numeric feature?] Chào mọi người, Paper sau đây là một ví dụ thực của việc binning numeric feature. GraphDTA[2019] (https://academic.oup.com/bioinformatics/article-abstract/37/8/1140/5942970?redirectedFrom=fulltext) sử dụng GNN để encode embedding cho dữ liệu thuốc. Dữ liệu thuốc ban đầu là đồ thị phân tử với nút là các nguyên tử; đặc trưng ban đầu cho mỗi nút nguyên tử được xây dựng thành một vector 78-chiều. Trong đó, 44 bin đầu tiên làm one hot encoding cho loại nguyên tố, 11 bin tiếp theo làm one hot encoding cho bậc của nguyên tử với các bin là 0-10, 11 bin tiếp theo làm one hot encoding cho số nguyên tử Hydrogen liên kết với nút với các bin là 0-10, 11 tiếp theo làm one hot encoding cho số implicit Hyrdrogen liên kết với nút với các bin là 0-10, 1 bin cuối cùng làm binary encoding cho 'thơm hay không?'. Chi tiết các bạn có thể xem ở github của GraphDTA hoặc bảng 2 của paper này:https://pubs.rsc.org/en/content/articlelanding/2020/ra/d0ra02297g Việc one hot encoding nhãn loại nguyên tố thì mình hiểu được, nhưng tại sao cần binning các numeric feature (bậc nguyên tử, số H, số imolicit H)? Tại sao lại muốn feature vector của nút là thưa? Mong được mọi người giúp đỡ, cảm ơn mọi người. Các ví dụ khác cũng thực hiện binning numeric features: https://github.com/wengong-jin/iclr19-graph2graph/blob/master/fast_jtnn/mpn.py https://colab.research.google.com/github/sunhwan/blog/blob/master/_notebooks/2021-02-20-Learning-Molecular-Representation-Using-Graph-Neural-Network-Molecular-Graph.ipynb",#feature_extraction	#handcrafted_feature,,,,
"Chào mọi người,
Em đang có một bài toán label misinformation của social media dựa trên tiếng Việt ạ. Em cũng đã nghiên cứu và tìm kiếm nhưng hiện vẫn chưa tìm bộ dataset tiếng Việt để giải quyết bài toán này ạ. Mọi người cho em hỏi là em có thể tìm bộ dataset của tiếng Việt ở đâu ạ và mọi người có thể suggest cho em không ạ. Em cảm ơn mọi người ạ!","Chào mọi người, Em đang có một bài toán label misinformation của social media dựa trên tiếng Việt ạ. Em cũng đã nghiên cứu và tìm kiếm nhưng hiện vẫn chưa tìm bộ dataset tiếng Việt để giải quyết bài toán này ạ. Mọi người cho em hỏi là em có thể tìm bộ dataset của tiếng Việt ở đâu ạ và mọi người có thể suggest cho em không ạ. Em cảm ơn mọi người ạ!",,,,,
"Mọi người cho hỏi là mình đang có đề tài về Q learning cho Financial Market Predictions ( dự đoán thị trường chứng khoán, tài chính) cho mình hỏi ý tưởng với code tham khảo với tại tìm trên mạng thấy toàn deep q learning","Mọi người cho hỏi là mình đang có đề tài về Q learning cho Financial Market Predictions ( dự đoán thị trường chứng khoán, tài chính) cho mình hỏi ý tưởng với code tham khảo với tại tìm trên mạng thấy toàn deep q learning",,,,,
"Mình xin giới thiệu 1 số NLP project từ cơ bản đến nâng cao có source code và dataset để ae tham khảo.
thanks all.",Mình xin giới thiệu 1 số NLP project từ cơ bản đến nâng cao có source code và dataset để ae tham khảo. thanks all.,,,,,
"Chào mọi người,
Nhóm nghiên cứu bên mình đang có 1 cuộc khảo sát về ""Độ khó của việc học trí tuệ nhân tạo trong giáo dục đại học ở thời điểm hiện tại"".
Qua khảo sát, giảng viên tại các trường Đại học có thể xác định được khó khăn của sinh viên trong việc tiếp cận với AI, từ đó cho ra những giáo trình phù hợp hơn với sinh viên của mình.
Khảo sát gồm 15 câu hỏi và sẽ chỉ mất khoảng 3-5 phút để hoàn thành. ✍️✍️✍️
Mong mọi người bỏ ra chút thời gian để giúp nhóm hoàn thành nghiên cứu lần này.
Nhóm xin chân thành cảm ơn !!! 🔥🔥🔥","Chào mọi người, Nhóm nghiên cứu bên mình đang có 1 cuộc khảo sát về ""Độ khó của việc học trí tuệ nhân tạo trong giáo dục đại học ở thời điểm hiện tại"". Qua khảo sát, giảng viên tại các trường Đại học có thể xác định được khó khăn của sinh viên trong việc tiếp cận với AI, từ đó cho ra những giáo trình phù hợp hơn với sinh viên của mình. Khảo sát gồm 15 câu hỏi và sẽ chỉ mất khoảng 3-5 phút để hoàn thành. Mong mọi người bỏ ra chút thời gian để giúp nhóm hoàn thành nghiên cứu lần này. Nhóm xin chân thành cảm ơn !!!",,,,,
"Group Viet Tech mới được thành lập giúp các bạn Việt trao đổi về cơ hội nghề nghiệp và các vấn đề liên quan trong ngành Tech. Nhóm Admin và active members gồm nhiều các bạn đã và đang làm việc trong nhiều công ty lớn.
https://www.facebook.com/groups/1177470863076165",Group Viet Tech mới được thành lập giúp các bạn Việt trao đổi về cơ hội nghề nghiệp và các vấn đề liên quan trong ngành Tech. Nhóm Admin và active members gồm nhiều các bạn đã và đang làm việc trong nhiều công ty lớn. https://www.facebook.com/groups/1177470863076165,,,,,
"Chào mọi người, em có đang làm 1 đề tài OCR cho chữ Hán-Nôm gồm 3 bước chính:
1. Xây dựng dữ liệu gồm bounding box cho các sentence cùng nhãn text của chúng trong từng page của 1 tác phẩm (Hình 1).
2. Model text detection để phát hiện bounding box của các sentence trong 1 page, từ đó có thể lấy ra hình ảnh riêng lẻ của từng sentence làm input cho text recognition.
3. Model text recognition để predict nhãn text tương ứng của từng hình ảnh các sentence ở trên.
Do text recognition ở bước 3 em sử dụng hướng tiếp cận là CRNN (CNN để trích xuất đặc trưng + RNN để nắm bắt thông tin ngữ cảnh) nên ở bước 1 em sẽ gán box theo câu (sentence level), do vậy sẽ tiết kiệm được rất nhiều thời gian so với gán box cho từng character. Như ở hình 1 là hình ảnh minh họa cho page đầu tiên của Truyện Kiều.
Nhưng em có đang gặp khó khăn khi thực hiện bước 1 đối với tác phẩm Đại Việt Sử Ký Toàn Thư. Do đây có vẻ là văn xuôi nên 1 câu có thể nằm ở nhiều cột và phân biệt bởi 1 dấu chấm (Hình 2) nên hiện tại em vẫn chưa nghĩ được cách gán bounding box và nhãn text của chúng như thế nào cùng cách chuyển giao hợp lý giữa bước 2 và 3 cho các trường hợp này.
Mong mọi người có thể gợi ý giúp em 1 số idea hoặc keyword nào đó để tham khảo. Cảm ơn mọi người.","Chào mọi người, em có đang làm 1 đề tài OCR cho chữ Hán-Nôm gồm 3 bước chính: 1. Xây dựng dữ liệu gồm bounding box cho các sentence cùng nhãn text của chúng trong từng page của 1 tác phẩm (Hình 1). 2. Model text detection để phát hiện bounding box của các sentence trong 1 page, từ đó có thể lấy ra hình ảnh riêng lẻ của từng sentence làm input cho text recognition. 3. Model text recognition để predict nhãn text tương ứng của từng hình ảnh các sentence ở trên. Do text recognition ở bước 3 em sử dụng hướng tiếp cận là CRNN (CNN để trích xuất đặc trưng + RNN để nắm bắt thông tin ngữ cảnh) nên ở bước 1 em sẽ gán box theo câu (sentence level), do vậy sẽ tiết kiệm được rất nhiều thời gian so với gán box cho từng character. Như ở hình 1 là hình ảnh minh họa cho page đầu tiên của Truyện Kiều. Nhưng em có đang gặp khó khăn khi thực hiện bước 1 đối với tác phẩm Đại Việt Sử Ký Toàn Thư. Do đây có vẻ là văn xuôi nên 1 câu có thể nằm ở nhiều cột và phân biệt bởi 1 dấu chấm (Hình 2) nên hiện tại em vẫn chưa nghĩ được cách gán bounding box và nhãn text của chúng như thế nào cùng cách chuyển giao hợp lý giữa bước 2 và 3 cho các trường hợp này. Mong mọi người có thể gợi ý giúp em 1 số idea hoặc keyword nào đó để tham khảo. Cảm ơn mọi người.",,,,,
"[Attention(•) nhưng nhận đầu vào là trọng số mô hình chứ không phải hidden state?]
Chào mọi người trong group ạ, không biết đã có ai gặp một paper nào sử dụng attention như trên tiêu đề không ạ? Thường chắc mọi người đã quen với việc sử dụng attention như một lớp để trích xuất đặc trưng bậc cao, và lớp này nhận đầu vào là hidden state. Nhưng bài báo sau đây
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04352-9
lại sử dụng attention để ép cho các trọng số thoả mãn điều kiện similarity nào đó. Dù chưa đọc kỹ code nhưng theo như paper trình bày thì thật ra lớp attention này chỉ là một bước ép scale lại các trọng số chứ trong hàm attention chẳng có trọng số nào để học cả.
Mà giả sử có thể thực hiện cho attention trọng số đựợc thì các trọng số sẽ thay đỏi như thế nào nhỉ? Bản thân optimizer đã cập nhật nó rồi mà? Rất mong được cùng mọi người bàn luận.","[Attention(•) nhưng nhận đầu vào là trọng số mô hình chứ không phải hidden state?] Chào mọi người trong group ạ, không biết đã có ai gặp một paper nào sử dụng attention như trên tiêu đề không ạ? Thường chắc mọi người đã quen với việc sử dụng attention như một lớp để trích xuất đặc trưng bậc cao, và lớp này nhận đầu vào là hidden state. Nhưng bài báo sau đây https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04352-9 lại sử dụng attention để ép cho các trọng số thoả mãn điều kiện similarity nào đó. Dù chưa đọc kỹ code nhưng theo như paper trình bày thì thật ra lớp attention này chỉ là một bước ép scale lại các trọng số chứ trong hàm attention chẳng có trọng số nào để học cả. Mà giả sử có thể thực hiện cho attention trọng số đựợc thì các trọng số sẽ thay đỏi như thế nào nhỉ? Bản thân optimizer đã cập nhật nó rồi mà? Rất mong được cùng mọi người bàn luận.",,,,,
"Trong 3 năm, chuỗi cà phê Starbucks thu thập dữ liệu hình ảnh khách hàng, nghiên cứu hành vi tiêu dùng trên toàn thế giới mỗi khi có người bước vào cửa hàng trong hệ thống.
3 năm sau, chỉ bằng động thái thay đổi cách bài trí của các quán cà phê, thương hiệu này đã tăng doanh thu mỗi cửa hàng lên 30%.
Camera AI phân biệt được quầy hàng này thì đối tượng nào mua hàng, độ tuổi là bao nhiêu và khác với quầy hàng khác ra sao. Từ đó, DN bố trí lại gian hàng, phân loại khách hàng trẻ từ 18-20 tuổi, tách với lứa tuổi tầm trung có mức thu nhập cao hơn cũng như lọc được khách VIP.
___________________
Đọc thêm: https://www.facebook.com/AI4Beginner/posts/176515428039297","Trong 3 năm, chuỗi cà phê Starbucks thu thập dữ liệu hình ảnh khách hàng, nghiên cứu hành vi tiêu dùng trên toàn thế giới mỗi khi có người bước vào cửa hàng trong hệ thống. 3 năm sau, chỉ bằng động thái thay đổi cách bài trí của các quán cà phê, thương hiệu này đã tăng doanh thu mỗi cửa hàng lên 30%. Camera AI phân biệt được quầy hàng này thì đối tượng nào mua hàng, độ tuổi là bao nhiêu và khác với quầy hàng khác ra sao. Từ đó, DN bố trí lại gian hàng, phân loại khách hàng trẻ từ 18-20 tuổi, tách với lứa tuổi tầm trung có mức thu nhập cao hơn cũng như lọc được khách VIP. ___________________ Đọc thêm: https://www.facebook.com/AI4Beginner/posts/176515428039297",,,,,
"Lang thang trên mạng, mình thấy cuốn sách ""Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning"" của GS Jean Gallier và Jocelyn Quaintance. Mn thấy hữu ích thì tải về tham khảo nhé.","Lang thang trên mạng, mình thấy cuốn sách ""Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning"" của GS Jean Gallier và Jocelyn Quaintance. Mn thấy hữu ích thì tải về tham khảo nhé.",,,,,
"Chào mọi người. Hiện mình đang có 1 project về human pose. Có những trường hợp khi người khuỵu gối xuống góc 90 độ, nhưng do cam chiếu hướng trực diện nên góc khớp gối vẫn là 180 độ. Hiện mình có giải pháp là đặt 2 cam khác góc chiếu. Nhưng để match cùng 1 người trên 2 cam khác nhau khá khó. Có 2 cách, 1 là reID deep learning, 2 là bản đồ hoá. Mọi người cho mình hỏi:
Có giải pháp nào tốt hơn cách sử dụng 2 cam không?
Có cách nào match 2 cam ngoài 2 cách trên không?
Model reID cho người hiệu quả nhất hiện nay?","Chào mọi người. Hiện mình đang có 1 project về human pose. Có những trường hợp khi người khuỵu gối xuống góc 90 độ, nhưng do cam chiếu hướng trực diện nên góc khớp gối vẫn là 180 độ. Hiện mình có giải pháp là đặt 2 cam khác góc chiếu. Nhưng để match cùng 1 người trên 2 cam khác nhau khá khó. Có 2 cách, 1 là reID deep learning, 2 là bản đồ hoá. Mọi người cho mình hỏi: Có giải pháp nào tốt hơn cách sử dụng 2 cam không? Có cách nào match 2 cam ngoài 2 cách trên không? Model reID cho người hiệu quả nhất hiện nay?",,,,,
"Chào mọi người. Hiện mình đang có 1 bài toán về tách biên ảnh. Nhưng thay vì sử dụng phép tích chập MAC thì bọn mình được yêu cầu sử dụng Xnor-popcount để thay thế.
Mình chưa có nhiều kiến thức về phần xnor -popcount nên mình không biết phép xnor-popcount sẽ có thể thay thế MAC để tách biên không và mình phải dùng kernel nào để xnor popcount ra được biên ảnh (như xử lý ảnh thông thường có thể sử dụng các kernel như Sobel, canny, ...). Không biết có ai biết về thuật toán xnor popcount cũng như ứng dụng nó vào tách biên ảnh có thể giúp đỡ cho bọn mình không ạ.
Mình xin cảm ơn mọi người.","Chào mọi người. Hiện mình đang có 1 bài toán về tách biên ảnh. Nhưng thay vì sử dụng phép tích chập MAC thì bọn mình được yêu cầu sử dụng Xnor-popcount để thay thế. Mình chưa có nhiều kiến thức về phần xnor -popcount nên mình không biết phép xnor-popcount sẽ có thể thay thế MAC để tách biên không và mình phải dùng kernel nào để xnor popcount ra được biên ảnh (như xử lý ảnh thông thường có thể sử dụng các kernel như Sobel, canny, ...). Không biết có ai biết về thuật toán xnor popcount cũng như ứng dụng nó vào tách biên ảnh có thể giúp đỡ cho bọn mình không ạ. Mình xin cảm ơn mọi người.",,,,,
"Nhóm nghiên cứu sử dụng công nghệ Trí tuệ Nhân tạo (AI) để dự đoán người dùng muốn di chuyển hay không.
Trong quá trình thử nghiệm, những người tham gia thực hiện các chuyển động khác nhau có thể bắt đầu theo cùng một cách - đứng lên, bắt chéo chân, nghiêng người về phía trước và thay đổi tư thế ngồi trên ghế.
Bộ khung xương ngoài sử dụng công nghệ máy học để dự đoán khi nào con người thực sự cố gắng đứng lên và thực hiện hỗ trợ chuyển động.
______________
Đọc thêm: https://www.facebook.com/AI4Beginner/posts/178495327841307","Nhóm nghiên cứu sử dụng công nghệ Trí tuệ Nhân tạo (AI) để dự đoán người dùng muốn di chuyển hay không. Trong quá trình thử nghiệm, những người tham gia thực hiện các chuyển động khác nhau có thể bắt đầu theo cùng một cách - đứng lên, bắt chéo chân, nghiêng người về phía trước và thay đổi tư thế ngồi trên ghế. Bộ khung xương ngoài sử dụng công nghệ máy học để dự đoán khi nào con người thực sự cố gắng đứng lên và thực hiện hỗ trợ chuyển động. ______________ Đọc thêm: https://www.facebook.com/AI4Beginner/posts/178495327841307",,,,,
"Thời gian vừa qua mình có cơ hội tìm hiểu về Variational Autoencoder nên mình muốn viết 1 bài vừa để tổng hợp kiến thức, chia sẻ những gì mình tìm hiểu được và hi vọng mọi người có thể sửa giúp những chỗ mình viết chưa rõ hoặc hiểu chưa đúng :D
https://forum.machinelearningcoban.com/t/variational-autoencoder-vae-co-ban/6494
Cảm ơn mọi người đã đọc bài ^^","Thời gian vừa qua mình có cơ hội tìm hiểu về Variational Autoencoder nên mình muốn viết 1 bài vừa để tổng hợp kiến thức, chia sẻ những gì mình tìm hiểu được và hi vọng mọi người có thể sửa giúp những chỗ mình viết chưa rõ hoặc hiểu chưa đúng :D https://forum.machinelearningcoban.com/t/variational-autoencoder-vae-co-ban/6494 Cảm ơn mọi người đã đọc bài ^^",,,,,
"Xin chào anh chị !
Em mới tìm hiểu về Machine Learning, theo lời khuyên của các tiền bối em bắt đầu với quyển Introduction to Machine Learning. Tuy nhiên khi đọc tới đoạn này em không hiểu cách giải thích về data points và cách sử dụng của BernoulliNB. Do đó, em hy vọng anh chị có thể giải đáp thắc mắc của em.
Em xin chân thành cảm ơn.","Xin chào anh chị ! Em mới tìm hiểu về Machine Learning, theo lời khuyên của các tiền bối em bắt đầu với quyển Introduction to Machine Learning. Tuy nhiên khi đọc tới đoạn này em không hiểu cách giải thích về data points và cách sử dụng của BernoulliNB. Do đó, em hy vọng anh chị có thể giải đáp thắc mắc của em. Em xin chân thành cảm ơn.",,,,,
Mọi người ơi cho em hỏi ai có kinh nghiệm anonymise data cho dự án ML cho em hỏi document nghiên cứu hay hướng nghiên cứu được không ạ. Em đang làm dự án phân tích độ thành công của học sinh nên cần anonymise dữ liệu ạ. Em xin cảm ơn,Mọi người ơi cho em hỏi ai có kinh nghiệm anonymise data cho dự án ML cho em hỏi document nghiên cứu hay hướng nghiên cứu được không ạ. Em đang làm dự án phân tích độ thành công của học sinh nên cần anonymise dữ liệu ạ. Em xin cảm ơn,,,,,
DETR là một thuật toán có rất nhiều điểm mới mẻ mà mình xin tổng hợp trong bài viết này. Dù đã cố gắng nhưng bài bài viết vẫn còn có thể có thiếu xót. Nhưng với mong muốn đóng góp và xây dựng cộng đồng vững mạnh hơn mình mong nhận được các ý kiến phản hồi từ bạn đọc.,DETR là một thuật toán có rất nhiều điểm mới mẻ mà mình xin tổng hợp trong bài viết này. Dù đã cố gắng nhưng bài bài viết vẫn còn có thể có thiếu xót. Nhưng với mong muốn đóng góp và xây dựng cộng đồng vững mạnh hơn mình mong nhận được các ý kiến phản hồi từ bạn đọc.,,,,,
"Chào các bạn,
Mình đang tìm hiểu về kiến trúc Unet sử dụng Pytorch cho bài toán multi-classes instance segmentation. Mình đang học theo hướng dẫn trong [github này](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/image_segmentation/semantic_segmentation_unet), tuy nhiên bạn này chỉ làm cho binary case. Mình đã chạy đc với binary case cho dữ liệu của riêng mình. Và đang cố gắng chuyển code sang multi-classes (out_channel=5) khi sử dụng crossentropyloss() function trong file train.py cùng 1 số thay đổi khác nhưng ko thành công. Bạn nào có kinh nghiệm tư vấn giùm mình cách đổi từ binary sang multiclasses model với. Mình xin cám ơn.","Chào các bạn, Mình đang tìm hiểu về kiến trúc Unet sử dụng Pytorch cho bài toán multi-classes instance segmentation. Mình đang học theo hướng dẫn trong [github này](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/image_segmentation/semantic_segmentation_unet), tuy nhiên bạn này chỉ làm cho binary case. Mình đã chạy đc với binary case cho dữ liệu của riêng mình. Và đang cố gắng chuyển code sang multi-classes (out_channel=5) khi sử dụng crossentropyloss() function trong file train.py cùng 1 số thay đổi khác nhưng ko thành công. Bạn nào có kinh nghiệm tư vấn giùm mình cách đổi từ binary sang multiclasses model với. Mình xin cám ơn.",,,,,
"1. Lộ trình để giỏi trong mảng computer vision / object detection là gì ạ?
2. Hiện nay team AI thường nghiên cứu chủ đề gì để có model object detection tốt ạ?
3. Có phải để xin được việc, người đó cần cả kiến thức khoa học về Object detection và cả kiến thức về framework (ví dụ tensorflow) hay có thể giỏi một trong hai?
4. Khi làm việc trong team, các thành viên trong team có những vai trò/vị trí gì? (để em hiểu những vị trí chuyên môn để tập trung học vào một trong những vị trí đó để có thể xin việc dễ dàng hơn)
5. đối với người làm trong ngành AI được 2 năm, lượng kiến thức họ đã tích lũy bao nhiêu là bình thường ( ví dụ người 2 năm KN chỉ biết về một mảng object detection có được xem là bình thường không?)
6. Việc đọc hiểu paper đối với một người 2 năm KN phải ở mức độ nào? ( mức độ có thể tùy chỉnh layer tốt hơn paper gốc, có bắt buộc phải hiểu ý nghĩa của cost function không?)
7. Cần tiêu chí gì để giúp người mới có thể trụ vững với cty?
8. Nếu một người bằng cấp Cao đẳng, điểm GPA thấp hoặc các trường hợp tương tự. Cần phải có chứng chỉ gì hay có những giải pháp gì giúp người đó có thể bổ sung kiến thức để có thể được nhận vào làm việc?","1. Lộ trình để giỏi trong mảng computer vision / object detection là gì ạ? 2. Hiện nay team AI thường nghiên cứu chủ đề gì để có model object detection tốt ạ? 3. Có phải để xin được việc, người đó cần cả kiến thức khoa học về Object detection và cả kiến thức về framework (ví dụ tensorflow) hay có thể giỏi một trong hai? 4. Khi làm việc trong team, các thành viên trong team có những vai trò/vị trí gì? (để em hiểu những vị trí chuyên môn để tập trung học vào một trong những vị trí đó để có thể xin việc dễ dàng hơn) 5. đối với người làm trong ngành AI được 2 năm, lượng kiến thức họ đã tích lũy bao nhiêu là bình thường ( ví dụ người 2 năm KN chỉ biết về một mảng object detection có được xem là bình thường không?) 6. Việc đọc hiểu paper đối với một người 2 năm KN phải ở mức độ nào? ( mức độ có thể tùy chỉnh layer tốt hơn paper gốc, có bắt buộc phải hiểu ý nghĩa của cost function không?) 7. Cần tiêu chí gì để giúp người mới có thể trụ vững với cty? 8. Nếu một người bằng cấp Cao đẳng, điểm GPA thấp hoặc các trường hợp tương tự. Cần phải có chứng chỉ gì hay có những giải pháp gì giúp người đó có thể bổ sung kiến thức để có thể được nhận vào làm việc?",,,,,
"Chào các tiền bối.
Mình main web backend(.net) giờ chuyển qua bộ phận R&D và đc nhận task đầu tiên là phân biệt mặt người(kiểu chấm công bằng face recognition) và phân biệt các loại rau củ quả(cả size thì càng tốt). 
Hiện h chưa biết chút gì về Python và AI,ML nên có vài câu hỏi rất mong được sự hồi đáp hoặc từ khóa.
Muốn làm nhận face recognition thì chạy trên trình duyệt(web-non mobile app) của thiết bị tablet có đc ko?
Muốn làm về nhận diện và phân tích xu hướng bằng dữ liệu đầu vào(Data Mining??) thì không học sâu về toán, chỉ học platform như Tensorflow và Pytorch có ổn không?
Pytorch và Tensorflow có ưu và nhược điểm ntn trong use-case thực tế.
Nếu không muốn đi sâu thì chỉ sử dụng các framework có sẵn như DeepFace có ổn không? Có cái TeachableMachine dùng thấy đủ xài rồi cơ mà không biết làm tn để thêm dữ liệu vào trong Trained Model. ;(
Cám ơn mn..","Chào các tiền bối. Mình main web backend(.net) giờ chuyển qua bộ phận R&D và đc nhận task đầu tiên là phân biệt mặt người(kiểu chấm công bằng face recognition) và phân biệt các loại rau củ quả(cả size thì càng tốt). Hiện h chưa biết chút gì về Python và AI,ML nên có vài câu hỏi rất mong được sự hồi đáp hoặc từ khóa. Muốn làm nhận face recognition thì chạy trên trình duyệt(web-non mobile app) của thiết bị tablet có đc ko? Muốn làm về nhận diện và phân tích xu hướng bằng dữ liệu đầu vào(Data Mining??) thì không học sâu về toán, chỉ học platform như Tensorflow và Pytorch có ổn không? Pytorch và Tensorflow có ưu và nhược điểm ntn trong use-case thực tế. Nếu không muốn đi sâu thì chỉ sử dụng các framework có sẵn như DeepFace có ổn không? Có cái TeachableMachine dùng thấy đủ xài rồi cơ mà không biết làm tn để thêm dữ liệu vào trong Trained Model. ;( Cám ơn mn..",,,,,
"Kính chào các bác ML cơ bản.
Nhiều anh em mới học đã mua GPU về train nhưng vẫn thấy train model chậm và không tận dụng được hết khả năng của GPU. Em đang nghiên cứu nên mạnh dạn chia sẻ cùng cả nhà!",Kính chào các bác ML cơ bản. Nhiều anh em mới học đã mua GPU về train nhưng vẫn thấy train model chậm và không tận dụng được hết khả năng của GPU. Em đang nghiên cứu nên mạnh dạn chia sẻ cùng cả nhà!,,,,,
"Các nhà nghiên cứu về giao tiếp động vật sử dụng một nhánh của AI gọi là học có giám sát (Supervised Machine Learning) - nền tảng gần đây đã chứng minh hiệu quả trong việc xử lý ngôn ngữ của con người. Nhánh AI này có tính ứng dụng cao trong việc phân tích các tiếng kêu của động vật.

Khác với AI thông thường chỉ cần ""học"" dựa trên một lượng dữ liệu nhất định được gắn nhãn phân loại theo từng lĩnh vực, AI học có giám sát có thể tự phân tích các dữ liệu sẵn có, đồng thời thu nhận thêm nội dung ở các nơi khác một cách thường xuyên và chủ động.
________________
Đọc thêm: https://www.facebook.com/107102171647290/posts/178060687884771/","Các nhà nghiên cứu về giao tiếp động vật sử dụng một nhánh của AI gọi là học có giám sát (Supervised Machine Learning) - nền tảng gần đây đã chứng minh hiệu quả trong việc xử lý ngôn ngữ của con người. Nhánh AI này có tính ứng dụng cao trong việc phân tích các tiếng kêu của động vật. Khác với AI thông thường chỉ cần ""học"" dựa trên một lượng dữ liệu nhất định được gắn nhãn phân loại theo từng lĩnh vực, AI học có giám sát có thể tự phân tích các dữ liệu sẵn có, đồng thời thu nhận thêm nội dung ở các nơi khác một cách thường xuyên và chủ động. ________________ Đọc thêm: https://www.facebook.com/107102171647290/posts/178060687884771/",,,,,
"[DualStyleGAN - style transfer]
Tại hội thảo CVPR 2022, mô hình DualStyleGAN được đề xuất để có thể sinh ra những ảnh hoạt hình, biếm họa, anime từ ảnh gốc ban đầu của bạn. Ảnh sinh ra vẫn có những nét từ ảnh ban đầu của bạn, nhưng phong cách sẽ được chuyển đổi.
Github: https://github.com/williamyang1991/DualStyleGAN
Project: https://www.mmlab-ntu.com/project/dualstylegan/
Demo: https://huggingface.co/spaces/hysts/DualStyleGAN","[DualStyleGAN - style transfer] Tại hội thảo CVPR 2022, mô hình DualStyleGAN được đề xuất để có thể sinh ra những ảnh hoạt hình, biếm họa, anime từ ảnh gốc ban đầu của bạn. Ảnh sinh ra vẫn có những nét từ ảnh ban đầu của bạn, nhưng phong cách sẽ được chuyển đổi. Github: https://github.com/williamyang1991/DualStyleGAN Project: https://www.mmlab-ntu.com/project/dualstylegan/ Demo: https://huggingface.co/spaces/hysts/DualStyleGAN",,,,,
"Gần đây mình mới phát hiện ra 3 thư viện có tên skimpy (tại đây https://github.com/aeturrell/skimpy), pandas-profiling (https://github.com/ydataai/pandas-profiling) và lux (https://github.com/lux-org/lux) hỗ trợ rất tốt việc khai phá dữ liệu dạng bảng (Explore Data Analysis).
Việc cài skimpy và pandas-profiling khá dễ dàng nhưng việc cài lux có phần phức tạp hơn, như cần cài nodejs, luxwidget, và nvm. Mình mới chỉ cài đặt thành công trên Linux, đang cố gắng cài trên Mac M1 xem sao.
Còn đây là jupyterlab mình test thử skimpy, pandas-profiling và lux, hi vọng hữu ích với mọi người.
Nếu các bạn thích, hãy hào phóng cho mình xin 1 sao (star) vào repository của mình nhé. Cảm ơn các bạn!
https://github.com/linhduongtuan/data_explore_analysis_visualization","Gần đây mình mới phát hiện ra 3 thư viện có tên skimpy (tại đây https://github.com/aeturrell/skimpy), pandas-profiling (https://github.com/ydataai/pandas-profiling) và lux (https://github.com/lux-org/lux) hỗ trợ rất tốt việc khai phá dữ liệu dạng bảng (Explore Data Analysis). Việc cài skimpy và pandas-profiling khá dễ dàng nhưng việc cài lux có phần phức tạp hơn, như cần cài nodejs, luxwidget, và nvm. Mình mới chỉ cài đặt thành công trên Linux, đang cố gắng cài trên Mac M1 xem sao. Còn đây là jupyterlab mình test thử skimpy, pandas-profiling và lux, hi vọng hữu ích với mọi người. Nếu các bạn thích, hãy hào phóng cho mình xin 1 sao (star) vào repository của mình nhé. Cảm ơn các bạn! https://github.com/linhduongtuan/data_explore_analysis_visualization",,,,,
"Chào mọi người! Em đang nghiên cứu việc xây dựng app android để nhận dạng đối tượng, nhưng yêu cầu là lấy ảnh/video từ camera gắn ngoài (endoscope camera) qua cổng USB OTG của điện thoại Android. Em đang gặp khó khăn làm sao lấy tín hiệu từ usb camera để view lên và áp dụng mô hình lên hình ảnh đó. Đối với ảnh/video từ camera mặc định trên điện thoại thì có git này đã hoạt động ok rồi: https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection
Mọi người có kinh nghiệm hay tài liệu gì có thể hướng dẫn thêm cho em với ạ! Thanks!","Chào mọi người! Em đang nghiên cứu việc xây dựng app android để nhận dạng đối tượng, nhưng yêu cầu là lấy ảnh/video từ camera gắn ngoài (endoscope camera) qua cổng USB OTG của điện thoại Android. Em đang gặp khó khăn làm sao lấy tín hiệu từ usb camera để view lên và áp dụng mô hình lên hình ảnh đó. Đối với ảnh/video từ camera mặc định trên điện thoại thì có git này đã hoạt động ok rồi: https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection Mọi người có kinh nghiệm hay tài liệu gì có thể hướng dẫn thêm cho em với ạ! Thanks!",,,,,
"Mình xin chia sẻ với mọi người ý tưởng, code, slide một vài project của các bạn lớp Deep Learning.
1. Image Super-Resolution: tăng độ phân giải của ảnh sử dụng bộ dữ liệu DIV2K với các phương pháp: - SRCNN - ResNet Super Resolution - Autoencoder Super Resolution
2. Car Detection and Tracking: phát hiện và theo dõi phương tiện trong video giao thông từ camera giám sát sử dụng SSD MobileNet V2 và Deep Sort
3. Trigger Word Detection
4. SMS Spam Classification
Slide và code mọi người xem ở đây: https://drive.google.com/drive/u/0/folders/1ohkFxElbhM_LMwCY_xJwSRWWRwLu75yB?fbclid=IwAR3HFUH5T6KEBmIGLdTMLWz9b68hZrObUn1M_wFn-L51qQYV8PX1wblpmeE","Mình xin chia sẻ với mọi người ý tưởng, code, slide một vài project của các bạn lớp Deep Learning. 1. Image Super-Resolution: tăng độ phân giải của ảnh sử dụng bộ dữ liệu DIV2K với các phương pháp: - SRCNN - ResNet Super Resolution - Autoencoder Super Resolution 2. Car Detection and Tracking: phát hiện và theo dõi phương tiện trong video giao thông từ camera giám sát sử dụng SSD MobileNet V2 và Deep Sort 3. Trigger Word Detection 4. SMS Spam Classification Slide và code mọi người xem ở đây: https://drive.google.com/drive/u/0/folders/1ohkFxElbhM_LMwCY_xJwSRWWRwLu75yB?fbclid=IwAR3HFUH5T6KEBmIGLdTMLWz9b68hZrObUn1M_wFn-L51qQYV8PX1wblpmeE",,,,,
"Hello m.n,
Việc Download 1 tập datasets từ Kaggle hay COCO không còn xa lạ gì với ae học A.I. Nhưng việc tinh lọc, điều chỉnh lại dataset là điều vô cùng mệt mỏi vì khối lượng data cũng như sự thủ công trong các khâu điều chỉnh. Các công việc như tìm lỗi annotation, Label lại objects, thật sự khiến ae chán nản.
Tiếp tục chủ đề Data-Centric Approach, mình xin giới thiệu 1 công cụ FiftyOne- được xem là 1 trong những tools tốt nhất để xây dựng datasets chất lượng.
Thank all.","Hello m.n, Việc Download 1 tập datasets từ Kaggle hay COCO không còn xa lạ gì với ae học A.I. Nhưng việc tinh lọc, điều chỉnh lại dataset là điều vô cùng mệt mỏi vì khối lượng data cũng như sự thủ công trong các khâu điều chỉnh. Các công việc như tìm lỗi annotation, Label lại objects, thật sự khiến ae chán nản. Tiếp tục chủ đề Data-Centric Approach, mình xin giới thiệu 1 công cụ FiftyOne- được xem là 1 trong những tools tốt nhất để xây dựng datasets chất lượng. Thank all.",,,,,
chào các anh chị. Em đang tính làm một bài toán dự đoán lương dựa trên kĩ năng và em có dataset như hình bên dưới. nhưng hiện tại cái đặc trưng top_skills nó dạng danh sách và chiều dài nó không bằng nhau như vậy làm sao để đưa vô hồi quy ạ. em cảm ơn,chào các anh chị. Em đang tính làm một bài toán dự đoán lương dựa trên kĩ năng và em có dataset như hình bên dưới. nhưng hiện tại cái đặc trưng top_skills nó dạng danh sách và chiều dài nó không bằng nhau như vậy làm sao để đưa vô hồi quy ạ. em cảm ơn,,,,,
"Xin chào mọi người ạ. Em là thành viên mới, hiện tại em đang tìm hiểu về trí tuệ nhân tạo và đang cố gắng viết một AI có khả năng học và giải toán bằng cách trình bày lời giải. Chẳng hạn như sau khi cho máy ""đọc"" một số bài giải phương trình bậc nhất dưới kiểu xâu, nó có thể đưa ra lời giải cho một đề bài phương trình bậc nhất khác. Em có hai câu hỏi là một AI như vậy thuộc vào loại nào của ML (vì em chưa tìm được sự giống nhau của nó với 4 loại ML được nói trong các tài liệu), và có tài liệu nào liên quan đến vấn đề này hay không. Em cảm ơn mọi người nhiều ạ!","Xin chào mọi người ạ. Em là thành viên mới, hiện tại em đang tìm hiểu về trí tuệ nhân tạo và đang cố gắng viết một AI có khả năng học và giải toán bằng cách trình bày lời giải. Chẳng hạn như sau khi cho máy ""đọc"" một số bài giải phương trình bậc nhất dưới kiểu xâu, nó có thể đưa ra lời giải cho một đề bài phương trình bậc nhất khác. Em có hai câu hỏi là một AI như vậy thuộc vào loại nào của ML (vì em chưa tìm được sự giống nhau của nó với 4 loại ML được nói trong các tài liệu), và có tài liệu nào liên quan đến vấn đề này hay không. Em cảm ơn mọi người nhiều ạ!",,,,,
"Trong các stt trước của mình về GNN trong bài toán phân loại ảnh, có bạn hỏi là liệu GNN có thể phân loại được ảnh màu với các data thu thập tự nhiên (thay vì các Medical Modalities) hay không tại đây (https://www.facebook.com/groups/machinelearningcoban/posts/1355120224945381/?comment_id=1355778824879521&reply_comment_id=1355792078211529). Vì vậy mình chọn 1 dataset khá là challenge cho các mạng CNNs/Transformers trong bài toán phân loại ảnh là 5-class flowers dataset trên Kaggle tại đây https://www.kaggle.com/alxmamaev/flowers-recognition. Trong dataset này có tới 379 Notebooks liên quan tới nó, một trong số chúng là Notebook này (https://www.kaggle.com/georgiisirotenko/pytorch-flowers-translearing-ensemble-test-99-67) với số lượt votes đạt 91, đứng thứ 5 trên tổng số 379 notebooks. Tác giả của notebooks này đã làm thí nghiệm với nhiều models CNNs và dùng kĩ thuật Ensemble (với 5 models là 'DenseNet_0', 'DenseNet_1', 'GooglNet', 'ResNet101', 'VGG16 with BN') để đạt accuracy ~0.9967. Việc này đòi hỏi rất nhiều tài nguyên tính toán, và trên thực tế rất khó để đưa vào ứng dụng vì thời gian inference/prediction sẽ kéo dài, thậm trí Out-of-Memory nếu dùng GPU có dung lượng VRAM nhỏ!
Một vấn đề quan trọng nhất là GNN models có thể giải quyết bài toán phân loại này rất nhanh, tốn tổng thời gian < 01 giờ mà thôi! Hơn nữa, thời gian tiêu tốn chủ yếu liên quan tới quá trình tiền xử lí ảnh và tạo Graph-structured data, thời gian training model chỉ chiếm ~ 10 phút cho 100 epochs mà thôi (thực tế chỉ cần train models với 10 epochs là đã đạt được hội tụ rồi!)
Chúc các bạn 1 tuần mới làm việc năng suất và hiệu quả!
Với các bạn học tập và làm việc ở phương Tây, chúc mùa nghỉ lễ giáng sinh và đón năm mới an lành!","Trong các stt trước của mình về GNN trong bài toán phân loại ảnh, có bạn hỏi là liệu GNN có thể phân loại được ảnh màu với các data thu thập tự nhiên (thay vì các Medical Modalities) hay không tại đây (https://www.facebook.com/groups/machinelearningcoban/posts/1355120224945381/?comment_id=1355778824879521&reply_comment_id=1355792078211529). Vì vậy mình chọn 1 dataset khá là challenge cho các mạng CNNs/Transformers trong bài toán phân loại ảnh là 5-class flowers dataset trên Kaggle tại đây https://www.kaggle.com/alxmamaev/flowers-recognition. Trong dataset này có tới 379 Notebooks liên quan tới nó, một trong số chúng là Notebook này (https://www.kaggle.com/georgiisirotenko/pytorch-flowers-translearing-ensemble-test-99-67) với số lượt votes đạt 91, đứng thứ 5 trên tổng số 379 notebooks. Tác giả của notebooks này đã làm thí nghiệm với nhiều models CNNs và dùng kĩ thuật Ensemble (với 5 models là 'DenseNet_0', 'DenseNet_1', 'GooglNet', 'ResNet101', 'VGG16 with BN') để đạt accuracy ~0.9967. Việc này đòi hỏi rất nhiều tài nguyên tính toán, và trên thực tế rất khó để đưa vào ứng dụng vì thời gian inference/prediction sẽ kéo dài, thậm trí Out-of-Memory nếu dùng GPU có dung lượng VRAM nhỏ! Một vấn đề quan trọng nhất là GNN models có thể giải quyết bài toán phân loại này rất nhanh, tốn tổng thời gian < 01 giờ mà thôi! Hơn nữa, thời gian tiêu tốn chủ yếu liên quan tới quá trình tiền xử lí ảnh và tạo Graph-structured data, thời gian training model chỉ chiếm ~ 10 phút cho 100 epochs mà thôi (thực tế chỉ cần train models với 10 epochs là đã đạt được hội tụ rồi!) Chúc các bạn 1 tuần mới làm việc năng suất và hiệu quả! Với các bạn học tập và làm việc ở phương Tây, chúc mùa nghỉ lễ giáng sinh và đón năm mới an lành!",,,,,
"Em chào các anh chị.
Hiện tại em muốn thu về 1 file dữ liệu để đánh giá sự thành công của 100 loại tiền ảo từ whitepaper của nó dựa trên chỉ số ROI. Tức là sử dụng text mining để chuyen whitepaper dạng pdf sang txt (điều kiện là filter factor ROI)
Em có sử dụng PyPDF2 nhưng chỉ có thể lấy text từng file 1, chưa biết phải lọc theo điều kiện như thế nào.
Em không biết có anh chị nào đã từng làm hoặc có biết về công việc tương tự có thể cho em hỏi thăm một số vấn đề không ạ?
Em xin cảm ơn ạ!","Em chào các anh chị. Hiện tại em muốn thu về 1 file dữ liệu để đánh giá sự thành công của 100 loại tiền ảo từ whitepaper của nó dựa trên chỉ số ROI. Tức là sử dụng text mining để chuyen whitepaper dạng pdf sang txt (điều kiện là filter factor ROI) Em có sử dụng PyPDF2 nhưng chỉ có thể lấy text từng file 1, chưa biết phải lọc theo điều kiện như thế nào. Em không biết có anh chị nào đã từng làm hoặc có biết về công việc tương tự có thể cho em hỏi thăm một số vấn đề không ạ? Em xin cảm ơn ạ!",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 1/2022 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới năm mới mạnh khỏe và bình an.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 1/2022 vào trong comment của post này. Chúc các bạn ngày mới tháng mới năm mới mạnh khỏe và bình an.",,,,,
"Xin chào mọi người ạ.
Hiện tại mình có được cuốn Pattern Recognition and Machine Learning của Bishop. Mình có gặp nhiều vấn đề khi hiểu nội dung của quyển sách, có rất nhiều mục và cũng như các lý thuyết về toán cao cấp phức tạp, cũng như rất khó để hiểu được. VD là phần Hessian Matrix của Neural Networks, mình đọc chỉ hiểu được khoảng dưới 50% nội dung.
Mình muốn hỏi mọi người từng đọc cuốn sách này là khi gặp những phần nội dung khó như vậy, mình nên tìm tòi đọc hiểu như thế nào hay đọc thêm gì để có thể hiểu được. Mình cũng có đọc qua cuốn Introduction to Linear Algebra và cũng nắm được các kiến thức nền tảng bên Statistic (Mình đã đọc 14 chương đầu của All of Statistics - A Concise Course in Statistical Inference, cũng hiểu kha khá nội dung) , nhưng thật sự việc đọc hiểu được toàn bộ nội dung cuốn sách với mình quá khó khăn. Mình có nghe nhiều người nói cuốn này là kinh thư và cơ bản nên mình muốn cố gắng hiểu hết nội dung cuốn sách.
Mình xin cảm ơn mọi người ạ.","Xin chào mọi người ạ. Hiện tại mình có được cuốn Pattern Recognition and Machine Learning của Bishop. Mình có gặp nhiều vấn đề khi hiểu nội dung của quyển sách, có rất nhiều mục và cũng như các lý thuyết về toán cao cấp phức tạp, cũng như rất khó để hiểu được. VD là phần Hessian Matrix của Neural Networks, mình đọc chỉ hiểu được khoảng dưới 50% nội dung. Mình muốn hỏi mọi người từng đọc cuốn sách này là khi gặp những phần nội dung khó như vậy, mình nên tìm tòi đọc hiểu như thế nào hay đọc thêm gì để có thể hiểu được. Mình cũng có đọc qua cuốn Introduction to Linear Algebra và cũng nắm được các kiến thức nền tảng bên Statistic (Mình đã đọc 14 chương đầu của All of Statistics - A Concise Course in Statistical Inference, cũng hiểu kha khá nội dung) , nhưng thật sự việc đọc hiểu được toàn bộ nội dung cuốn sách với mình quá khó khăn. Mình có nghe nhiều người nói cuốn này là kinh thư và cơ bản nên mình muốn cố gắng hiểu hết nội dung cuốn sách. Mình xin cảm ơn mọi người ạ.",,,,,
"Em chào mọi người ạ,
Cho em hỏi làm thế nào để có thể có kết quả training ko thay đổi khi train đi train lại nhiều lần ạ.
Em đã thử dùng seed như này rồi : http://codepad.org/P7mmvuqt nhưng kết quả train vẫn bị thay đổi mỗi lần train lại ạ.
Em cũng đã thử seed lại mỗi sau khi khởi tạo model và dataloader nhưng cũng ko dc ạ.
Mong mn chỉ giúp ạ, e cảm ơn","Em chào mọi người ạ, Cho em hỏi làm thế nào để có thể có kết quả training ko thay đổi khi train đi train lại nhiều lần ạ. Em đã thử dùng seed như này rồi : http://codepad.org/P7mmvuqt nhưng kết quả train vẫn bị thay đổi mỗi lần train lại ạ. Em cũng đã thử seed lại mỗi sau khi khởi tạo model và dataloader nhưng cũng ko dc ạ. Mong mn chỉ giúp ạ, e cảm ơn",,,,,
"[Help][Import turicreate trong Jupyter Notebook] Mình đã cài turicreate trong Ubuntu 1804 LS, nhưng import trong Jupyter không được. Bạn nào giúp đỡ mình với. Cảm ơn nhiều!!","[Help][Import turicreate trong Jupyter Notebook] Mình đã cài turicreate trong Ubuntu 1804 LS, nhưng import trong Jupyter không được. Bạn nào giúp đỡ mình với. Cảm ơn nhiều!!",,,,,
"Chào mọi người, mình hiện đã xài model YOLOv5m với Deepsort để phát hiện và phân loại id xe, tuy nhiên mình chưa biết cách tùy chỉnh để có thể đếm được xe như hình dưới. Do mình clone thư mục Deepsort từ: https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch thay đổi các thông số chạy lệnh command nên giờ tùy chỉnh phần code mình mò nhưng vẫn không ra. Có bác nào rành về cái này cho mình xin hỏi tìm hiểu với ạ.
Mình cảm ơn.","Chào mọi người, mình hiện đã xài model YOLOv5m với Deepsort để phát hiện và phân loại id xe, tuy nhiên mình chưa biết cách tùy chỉnh để có thể đếm được xe như hình dưới. Do mình clone thư mục Deepsort từ: https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch thay đổi các thông số chạy lệnh command nên giờ tùy chỉnh phần code mình mò nhưng vẫn không ra. Có bác nào rành về cái này cho mình xin hỏi tìm hiểu với ạ. Mình cảm ơn.",,,,,
"Mọi người cho em hỏi chút ạ. Em đang học theo sách Mathematics for Machine Learning. Ở chương SVD có bài tìm xấp xỉ rank-1 của 1 ma trận A, theo em đc biết thì nó được tính theo công thức σi.ui.vi^T. Vậy còn trường hợp đề bài hỏi xấp xỉ rank 2 của ma trận thì tính sao ạ ? Em xin cám ơn.","Mọi người cho em hỏi chút ạ. Em đang học theo sách Mathematics for Machine Learning. Ở chương SVD có bài tìm xấp xỉ rank-1 của 1 ma trận A, theo em đc biết thì nó được tính theo công thức σi.ui.vi^T. Vậy còn trường hợp đề bài hỏi xấp xỉ rank 2 của ma trận thì tính sao ạ ? Em xin cám ơn.",,,,,
"Em chào mọi người, em đang làm đồ án về phát hiện tương đồng trong văn bản tiếng Việt, em chưa có kinh nghiệm nhiều trong NLP nên mong được các anh chị có kinh nghiệm chia sẻ hướng làm hay là các model hay thuật toán nào tốt với ạ. Em cảm ơn","Em chào mọi người, em đang làm đồ án về phát hiện tương đồng trong văn bản tiếng Việt, em chưa có kinh nghiệm nhiều trong NLP nên mong được các anh chị có kinh nghiệm chia sẻ hướng làm hay là các model hay thuật toán nào tốt với ạ. Em cảm ơn",,,,,
"Xin hỏi diễn đàn có bạn nào quan tâm tới việc nghiên cứu phân loại ElectroEncephaloGram (EEG) không? Mình có đang chuyển dữ liệu EEG về dạng Spectrogram (mỗi window là 10 giây, sampling rate=250, mình dùng thư viện librosa) mà sao nó chạy rất lâu, mình thử dùng cả thư viện multiprocessing nhưng kiểm tra CPU dùng rất ít. Mình đoán để extract hết dữ liệu sẽ tốn cả 10 ngày chạy. Có bạn nào có kinh nghiệp góp ý tips/trick/treat vụ này cho mình được không?
Tập dữ liệu Epilepsy của Temple University tại đây","Xin hỏi diễn đàn có bạn nào quan tâm tới việc nghiên cứu phân loại ElectroEncephaloGram (EEG) không? Mình có đang chuyển dữ liệu EEG về dạng Spectrogram (mỗi window là 10 giây, sampling rate=250, mình dùng thư viện librosa) mà sao nó chạy rất lâu, mình thử dùng cả thư viện multiprocessing nhưng kiểm tra CPU dùng rất ít. Mình đoán để extract hết dữ liệu sẽ tốn cả 10 ngày chạy. Có bạn nào có kinh nghiệp góp ý tips/trick/treat vụ này cho mình được không? Tập dữ liệu Epilepsy của Temple University tại đây",,,,,
"Các bác cho em hỏi chút với ạ. Em đang nghiên cứu PhoBert và có lên trang github của họ nhưng vẫn chưa hiểu PhoBert dùng để làm gì?
Em đang hình dung PhoBert như là Word2vec để mã hóa câu thành vector đúng ko ah?
Em ko thấy thêm tác dùng gì mong các bác chỉ giáo.",Các bác cho em hỏi chút với ạ. Em đang nghiên cứu PhoBert và có lên trang github của họ nhưng vẫn chưa hiểu PhoBert dùng để làm gì? Em đang hình dung PhoBert như là Word2vec để mã hóa câu thành vector đúng ko ah? Em ko thấy thêm tác dùng gì mong các bác chỉ giáo.,,,,,
"Hello m.n,
Đối với ae A.I newbie việc thực hành là vô cùng quan trọng, mình xin giới thiệu một số PROJECTS có source code và dataset để ae tham khảo.!
Thank all.","Hello m.n, Đối với ae A.I newbie việc thực hành là vô cùng quan trọng, mình xin giới thiệu một số PROJECTS có source code và dataset để ae tham khảo.! Thank all.",,,,,
"Chào các anh em. Hôm nay chúng ta sẽ cùng tiếp cận một bài toán tiếp theo trong Series AI in Banking là ""Thử làm model dự đoán khách hàng rời bỏ dịch vụ (Customer Churn Prediction) "".
Hi vọng giúp được anh em mới học tìm hiểu thêm một món mới!","Chào các anh em. Hôm nay chúng ta sẽ cùng tiếp cận một bài toán tiếp theo trong Series AI in Banking là ""Thử làm model dự đoán khách hàng rời bỏ dịch vụ (Customer Churn Prediction) "". Hi vọng giúp được anh em mới học tìm hiểu thêm một món mới!",,,,,
"Như mọi người có thể đã biết, Graph Neural Networks hiện đang là một trong những chủ đề nghiên cứu rất được quan tâm trên thế giới, được ứng dụng thành công trong nhiều lĩnh vực như NLP, computer vision, và drug discovery. Mình xin chia sẻ với mọi người về Graph Transformer mà nhóm mình đã bắt đầu phát triển từ 3 năm trước tới hiện tại. Paper sẽ được trình bày tại TheWebConf WWW '22 Poster and Demo track: https://arxiv.org/pdf/1909.11855v13.pdf
Code: https://github.com/daiquocnguyen/Graph-Transformer
Abstract:
We introduce a transformer-based GNN model, named UGformer, to learn graph representations. In particular, we present two UGformer variants, wherein the first variant (publicized in September 2019) is to leverage the transformer on a set of sampled neighbors for each input node, while the second (publicized in May 2021) is to leverage the transformer on all input nodes. Experimental results demonstrate that the first UGformer variant achieves state-of-the-art accuracies on benchmark datasets for graph classification in both inductive setting and unsupervised transductive setting; and the second UGformer variant obtains state-of-the-art accuracies for inductive text classification.","Như mọi người có thể đã biết, Graph Neural Networks hiện đang là một trong những chủ đề nghiên cứu rất được quan tâm trên thế giới, được ứng dụng thành công trong nhiều lĩnh vực như NLP, computer vision, và drug discovery. Mình xin chia sẻ với mọi người về Graph Transformer mà nhóm mình đã bắt đầu phát triển từ 3 năm trước tới hiện tại. Paper sẽ được trình bày tại TheWebConf WWW '22 Poster and Demo track: https://arxiv.org/pdf/1909.11855v13.pdf Code: https://github.com/daiquocnguyen/Graph-Transformer Abstract: We introduce a transformer-based GNN model, named UGformer, to learn graph representations. In particular, we present two UGformer variants, wherein the first variant (publicized in September 2019) is to leverage the transformer on a set of sampled neighbors for each input node, while the second (publicized in May 2021) is to leverage the transformer on all input nodes. Experimental results demonstrate that the first UGformer variant achieves state-of-the-art accuracies on benchmark datasets for graph classification in both inductive setting and unsupervised transductive setting; and the second UGformer variant obtains state-of-the-art accuracies for inductive text classification.",,,,,
"Chào mọi người, hiện em đang muốn fine tuning mô hình arcface trên tập dữ liệu của mình nhưng gặp một số vấn đề về việc load pretrained model. Link github sử dụng:
https://github.com/auroua/InsightFace_TF/blob/master/train_nets.py
Dữ liệu đi qua resnet model sau đó qua hàm arcface_loss (hình 1). Em muốn sử dụng lại trọng số của resnet và thay đổi số lớp ouput của arcface_loss nên em chuyển phần restore pretrained lên phía trên (hình 2) thì code vẫn chạy được nhưng khi em sửa một số thứ để xem embedding_tensor = test_net.outputs có là không đổi (để kiểm tra đã load trọng số resnet thành công hay chưa) khi cho 1 ảnh khuôn mặt đi qua mạng thì nếu để nguyên code cũ thì không đổi nhưng khi chuyển lên phía trên thì lại khác nhau.
Có anh/chị/bạn nào đã biết về việc fine tuning với TensorFlow 1.4 1.6, TensorLayer 1.7 cho em hỏi cách mình fine tuning model với ạ, do trong github đó xài phiên bản này ạ. Em cảm ơn.","Chào mọi người, hiện em đang muốn fine tuning mô hình arcface trên tập dữ liệu của mình nhưng gặp một số vấn đề về việc load pretrained model. Link github sử dụng: https://github.com/auroua/InsightFace_TF/blob/master/train_nets.py Dữ liệu đi qua resnet model sau đó qua hàm arcface_loss (hình 1). Em muốn sử dụng lại trọng số của resnet và thay đổi số lớp ouput của arcface_loss nên em chuyển phần restore pretrained lên phía trên (hình 2) thì code vẫn chạy được nhưng khi em sửa một số thứ để xem embedding_tensor = test_net.outputs có là không đổi (để kiểm tra đã load trọng số resnet thành công hay chưa) khi cho 1 ảnh khuôn mặt đi qua mạng thì nếu để nguyên code cũ thì không đổi nhưng khi chuyển lên phía trên thì lại khác nhau. Có anh/chị/bạn nào đã biết về việc fine tuning với TensorFlow 1.4 1.6, TensorLayer 1.7 cho em hỏi cách mình fine tuning model với ạ, do trong github đó xài phiên bản này ạ. Em cảm ơn.",,,,,
"[structure from motion - Colmap]
Ko liên quan lắm đến ML nhưng liên quan đến CV ạ. Các anh chị có ai đã dùng Colmap thì cho em inb hỏi chút. Câu hỏi cụ thể của em (em dùng colmap command line, ko dùng dc GUI)
em chạy colmap automatic-reconstruction mà nó ko generate dense model, chỉ có sparse directory dc tạo ra. Có ai đã dùng tool này và gặp hiện tượng này ko ạ?
estimated camera intrinsics từ colmap automatic-reconstruction và colmap feature-extractor khác hẳn nhau. Code phần automatic_reconstructor của em trên bash như dưới ảnh 1. Colmap feature-extractor tương tự, ảnh 2
 Làm sao để visualize point cloud từ sparse and dense model    của Colmap? (preferably bằng script, ko phải bằng hit-and-click GUI của Colmap, vì em có gần 3k bộ ảnh).
Em muốn set initial guess cho intrinsic and extrinsic camera parameter for sfm model, thì làm như thế nào ạ? Em đọc doc thấy bảo là phải modify .db file generated nhưng  db file em ko thấy cái cột nào cho focal length với principal point của camera hết thì sửa vào đâu ?
Em cảm ơn ạ","[structure from motion - Colmap] Ko liên quan lắm đến ML nhưng liên quan đến CV ạ. Các anh chị có ai đã dùng Colmap thì cho em inb hỏi chút. Câu hỏi cụ thể của em (em dùng colmap command line, ko dùng dc GUI) em chạy colmap automatic-reconstruction mà nó ko generate dense model, chỉ có sparse directory dc tạo ra. Có ai đã dùng tool này và gặp hiện tượng này ko ạ? estimated camera intrinsics từ colmap automatic-reconstruction và colmap feature-extractor khác hẳn nhau. Code phần automatic_reconstructor của em trên bash như dưới ảnh 1. Colmap feature-extractor tương tự, ảnh 2 Làm sao để visualize point cloud từ sparse and dense model của Colmap? (preferably bằng script, ko phải bằng hit-and-click GUI của Colmap, vì em có gần 3k bộ ảnh). Em muốn set initial guess cho intrinsic and extrinsic camera parameter for sfm model, thì làm như thế nào ạ? Em đọc doc thấy bảo là phải modify .db file generated nhưng db file em ko thấy cái cột nào cho focal length với principal point của camera hết thì sửa vào đâu ? Em cảm ơn ạ",,,,,
"Hello m.n,
Chắc ai làm việc với A.i cũng từng tốn khá nhiều thời gian để tinh chỉnh model cho fit với data, hơn 80% thời gian chúng ta dành cho việc data preparation, clean data, 20% còn lại cho Model và dường như ít người quan tâm đến điều đó. Vậy tại sao ngay từ đầu chúng ta không tập trung tinh lọc input cho A.i application?
em xin giới thiệu 1 bài viết về DATA-CENTRIC APPROACH và MODEL-CENTRIC APPROACH.
Thanks all.","Hello m.n, Chắc ai làm việc với A.i cũng từng tốn khá nhiều thời gian để tinh chỉnh model cho fit với data, hơn 80% thời gian chúng ta dành cho việc data preparation, clean data, 20% còn lại cho Model và dường như ít người quan tâm đến điều đó. Vậy tại sao ngay từ đầu chúng ta không tập trung tinh lọc input cho A.i application? em xin giới thiệu 1 bài viết về DATA-CENTRIC APPROACH và MODEL-CENTRIC APPROACH. Thanks all.",,,,,
Các bạn có thể tham gia một số khóa học Machine Learning miễn phí ở đây,Các bạn có thể tham gia một số khóa học Machine Learning miễn phí ở đây,,,,,
VietAI công bố dự án nghiên cứu MTet: Mô hình dịch máy Anh-Việt đa lĩnh vực tối ưu với dữ liệu lớn nhất tại Việt Nam! Chi tiết xem phía dưới.,VietAI công bố dự án nghiên cứu MTet: Mô hình dịch máy Anh-Việt đa lĩnh vực tối ưu với dữ liệu lớn nhất tại Việt Nam! Chi tiết xem phía dưới.,,,,,
"Chào anh/ chị và các bạn,
Mình hiện đang làm về lĩnh vực tài chính, đang bắt đầu tìm hiểu về ML. Các bạn có giới thiệu cho mình sách về ML cho beginners, giải thích những khái niệm và 1 vài models cơ bản về ML, một cách đơn giản, có hệ thống và dễ hiểu đc không ạ, liên quan đến lĩnh vực kinh tế thì càng tốt? Cám ơn các bạn đã chia sẻ.","Chào anh/ chị và các bạn, Mình hiện đang làm về lĩnh vực tài chính, đang bắt đầu tìm hiểu về ML. Các bạn có giới thiệu cho mình sách về ML cho beginners, giải thích những khái niệm và 1 vài models cơ bản về ML, một cách đơn giản, có hệ thống và dễ hiểu đc không ạ, liên quan đến lĩnh vực kinh tế thì càng tốt? Cám ơn các bạn đã chia sẻ.",,,,,
"Trong máu của chúng ta có it nhất 3 loại tế bào: tế bào hồng cầu (RBC), tế bào bạch cầu (WBC) và tế bào tiểu cầu (platelets). Trong đó, hồng cầu (RBC) chiếm số lượng nhiều nhất, có nhiệm vụ vận chuyển khí Oxy (O2) đến phổi và các mô, đi khắp cơ thể, đồng thời tiếp thu các chất thải và khí cabonic (CO2) về phổi để đào thải. Nếu thiếu hồng cầu (thiếu máu), con người sẽ thấy mệt mỏi và yếu sức.
Tế bào bạch cầu (WBC) là những phần quan trọng của hệ thống miễn dịch của chúng ta và chúng bảo vệ cơ thể chống lại nhiễm trùng bằng cách loại bỏ vi rút, vi khuẩn, ký sinh trùng và nấm. Số lượng loại bạch cầu và tổng số lượng bạch cầu cung cấp thông tin quan trọng về tình trạng sức khỏe của chúng ta. Các bệnh như bệnh bạch cầu, AIDS, bệnh tự miễn, suy giảm miễn dịch, bệnh máu có thể được chẩn đoán dựa trên số lượng bạch cầu.
Tiểu cầu (Platelets) là những mảnh tế bào nhỏ, có chức năng cầm máu, tạo cục máu đông, bịt các vết thương ở thành mạch máu. Khi lượng tiểu cầu quá thấp, các cơ quan nội tạng và não bộ có thể bị xuất huyết.
Do đó, việc xác định và đếm được số lượng hồng cầu, bạch cầu và tiểu cầu trong một mẫu máu là nhiệm vụ quan trọng. Nó giúp các bác sỹ chuẩn đoán cách bệnh nan y, đồng thời trợ giúp đưa ra các phác đồ điều trị hiệu quả nhất.
Bằng việc sử dụng công nghệ AI, chúng ta có thể thiết kế ra phần mềm kết nối trực tiếp với các loại kính hiển vi điện tử, cho phép xác định chính xác số lượng các loại tế bào máu trong thời gian thực (real-time). Trong bài hướng dẫn này, chúng tôi sử dụng dữ liệu máu từ trang robolfow.com và sử dụng phần mềm ANS ODHUB (anscenter.com) để chứng minh tính khả thi của công nghệ AI trong lĩnh vực y khoa.
Với khả năng chuẩn đoán, nhận dạng và xác định được vật thể ở mức tế bào, nếu sử dụng phương pháp tương tự cho xét nghiệm COVID19, chúng ta có thể tạo ra hệ thống xét nghiệm nhanh (30s) với độ chính xác cao (trên 90%) nếu chúng ta có đủ mẫu dữ liệu để huấn luyện hệ thống AI. Nếu các bạn thấy bài này có ích cho cộng đồng, hãy giúp chúng tôi chia sẻ nó. Biết đâu sẽ có các nhà nghiên cứu, các công ty hay các cơ quan nhà nước ứng dụng phương pháp này như một giải pháp thay thế cho xét nghiệm COVID hiệu quả.
Chúng tôi sẽ chia sẻ cách thiết kế hệ thống AI bằng ANS ODHUB được tích hợp với công nghệ điện toán đám mây trên hệ thống chủ (ANSCloud) trên kênh Youtube của ANSCENTER.","Trong máu của chúng ta có it nhất 3 loại tế bào: tế bào hồng cầu (RBC), tế bào bạch cầu (WBC) và tế bào tiểu cầu (platelets). Trong đó, hồng cầu (RBC) chiếm số lượng nhiều nhất, có nhiệm vụ vận chuyển khí Oxy (O2) đến phổi và các mô, đi khắp cơ thể, đồng thời tiếp thu các chất thải và khí cabonic (CO2) về phổi để đào thải. Nếu thiếu hồng cầu (thiếu máu), con người sẽ thấy mệt mỏi và yếu sức. Tế bào bạch cầu (WBC) là những phần quan trọng của hệ thống miễn dịch của chúng ta và chúng bảo vệ cơ thể chống lại nhiễm trùng bằng cách loại bỏ vi rút, vi khuẩn, ký sinh trùng và nấm. Số lượng loại bạch cầu và tổng số lượng bạch cầu cung cấp thông tin quan trọng về tình trạng sức khỏe của chúng ta. Các bệnh như bệnh bạch cầu, AIDS, bệnh tự miễn, suy giảm miễn dịch, bệnh máu có thể được chẩn đoán dựa trên số lượng bạch cầu. Tiểu cầu (Platelets) là những mảnh tế bào nhỏ, có chức năng cầm máu, tạo cục máu đông, bịt các vết thương ở thành mạch máu. Khi lượng tiểu cầu quá thấp, các cơ quan nội tạng và não bộ có thể bị xuất huyết. Do đó, việc xác định và đếm được số lượng hồng cầu, bạch cầu và tiểu cầu trong một mẫu máu là nhiệm vụ quan trọng. Nó giúp các bác sỹ chuẩn đoán cách bệnh nan y, đồng thời trợ giúp đưa ra các phác đồ điều trị hiệu quả nhất. Bằng việc sử dụng công nghệ AI, chúng ta có thể thiết kế ra phần mềm kết nối trực tiếp với các loại kính hiển vi điện tử, cho phép xác định chính xác số lượng các loại tế bào máu trong thời gian thực (real-time). Trong bài hướng dẫn này, chúng tôi sử dụng dữ liệu máu từ trang robolfow.com và sử dụng phần mềm ANS ODHUB (anscenter.com) để chứng minh tính khả thi của công nghệ AI trong lĩnh vực y khoa. Với khả năng chuẩn đoán, nhận dạng và xác định được vật thể ở mức tế bào, nếu sử dụng phương pháp tương tự cho xét nghiệm COVID19, chúng ta có thể tạo ra hệ thống xét nghiệm nhanh (30s) với độ chính xác cao (trên 90%) nếu chúng ta có đủ mẫu dữ liệu để huấn luyện hệ thống AI. Nếu các bạn thấy bài này có ích cho cộng đồng, hãy giúp chúng tôi chia sẻ nó. Biết đâu sẽ có các nhà nghiên cứu, các công ty hay các cơ quan nhà nước ứng dụng phương pháp này như một giải pháp thay thế cho xét nghiệm COVID hiệu quả. Chúng tôi sẽ chia sẻ cách thiết kế hệ thống AI bằng ANS ODHUB được tích hợp với công nghệ điện toán đám mây trên hệ thống chủ (ANSCloud) trên kênh Youtube của ANSCENTER.",,,,,
"các Anh/Chị/ bạn cho mình hỏi : trong Q-learning, khi nào mình sẽ dừng việc train, có phương pháp nào đánh giá kết quả training của Q learning không
Xin cám ơn","các Anh/Chị/ bạn cho mình hỏi : trong Q-learning, khi nào mình sẽ dừng việc train, có phương pháp nào đánh giá kết quả training của Q learning không Xin cám ơn",,,,,
"KÍNH CHÀO TUẦN MỚI CÁC BÁC! NHIỀU ANH EM KHI LÀM CÁC BÀI TOÁN PHÂN LỚP VÀ SẼ GẶP PHẢI IMBALANCED DATA. EM CHIA SẺ VÀI TIP ĐỂ ANH EM XỬ LÝ CÁI MÓN KHÙ KHOẰM NÀY.
Hi vọng giúp được các anh em mới học có hướng tham khảo!",KÍNH CHÀO TUẦN MỚI CÁC BÁC! NHIỀU ANH EM KHI LÀM CÁC BÀI TOÁN PHÂN LỚP VÀ SẼ GẶP PHẢI IMBALANCED DATA. EM CHIA SẺ VÀI TIP ĐỂ ANH EM XỬ LÝ CÁI MÓN KHÙ KHOẰM NÀY. Hi vọng giúp được các anh em mới học có hướng tham khảo!,,,,,
Cùng thử tìm hiểu và viết AI tự động tô màu cho ảnh đen trắng :),Cùng thử tìm hiểu và viết AI tự động tô màu cho ảnh đen trắng :),,,,,
"Em chào mọi người,
Hiện tại em đang có một vấn đề như sau: Kiểm tra chính tả một câu mà người dùng nhập vào, sau đó đề xuất ra câu mới nếu như câu của người dùng sai chính tả. Như google ấy ạ.
Bài toán của em là ở phạm vi trong thương mại điện tử, ví dụ như người dùng nhập sai tên sản phẩm trên thanh search thì mình sẽ đề xuất sửa lại từ đó cho họ. Hiện tại cách tiếp cận của em là tạo tập từ điển riêng về TMDT bằng cách cào dữ liệu từ tiki. Sau đó sử dụng SymSpell để sửa từ ạ.
Anh chị có thể cho em hỏi thêm về các cách tiếp cận khác hay cải tiến cho bài toán và có thể áp dụng vào thực tế được không ạ.
Em xin cảm ơn mọi người.","Em chào mọi người, Hiện tại em đang có một vấn đề như sau: Kiểm tra chính tả một câu mà người dùng nhập vào, sau đó đề xuất ra câu mới nếu như câu của người dùng sai chính tả. Như google ấy ạ. Bài toán của em là ở phạm vi trong thương mại điện tử, ví dụ như người dùng nhập sai tên sản phẩm trên thanh search thì mình sẽ đề xuất sửa lại từ đó cho họ. Hiện tại cách tiếp cận của em là tạo tập từ điển riêng về TMDT bằng cách cào dữ liệu từ tiki. Sau đó sử dụng SymSpell để sửa từ ạ. Anh chị có thể cho em hỏi thêm về các cách tiếp cận khác hay cải tiến cho bài toán và có thể áp dụng vào thực tế được không ạ. Em xin cảm ơn mọi người.",,,,,
"TÌM NGƯỜI CÙNG HỌC ML CƠ BẢN
Mình có kinh nghiệm làm NLP (hồi chưa có deep learning) và code nhiều năm, giờ muốn học cơ bản về ML và tự cài đặt các giải thuật từ đầu. 

Tuộc  muốn tìm người giỏi toán + muốn học code để đồng hành vì  quen code rồi, cứ nhìn thấy công thức toán là buồn ngủ, nên cần 01 đồng đội diễn giải công thức toán bằng văn nói hoặc dạng giải thuật. Bù lại thì Tuộc có rất nhiều kinh nghiệm code, từ backend, front-end, mobile apps ... Có thể trợ giúp bạn ở mảng thực chiến.

Ngoài ra Tuộc cũng muốn tìm 01 bạn có kinh nghiệm lập trình hệ thống (low-level) để tối ưu hóa khi triển khai ứng dụng. Trước mắt là nền Web thông qua web-assembly, để mình có ngay những app / demo trực quan, sinh động.

Chi tiết tại https://github.com/telexyz/ml4coders","TÌM NGƯỜI CÙNG HỌC ML CƠ BẢN Mình có kinh nghiệm làm NLP (hồi chưa có deep learning) và code nhiều năm, giờ muốn học cơ bản về ML và tự cài đặt các giải thuật từ đầu. Tuộc muốn tìm người giỏi toán + muốn học code để đồng hành vì quen code rồi, cứ nhìn thấy công thức toán là buồn ngủ, nên cần 01 đồng đội diễn giải công thức toán bằng văn nói hoặc dạng giải thuật. Bù lại thì Tuộc có rất nhiều kinh nghiệm code, từ backend, front-end, mobile apps ... Có thể trợ giúp bạn ở mảng thực chiến. Ngoài ra Tuộc cũng muốn tìm 01 bạn có kinh nghiệm lập trình hệ thống (low-level) để tối ưu hóa khi triển khai ứng dụng. Trước mắt là nền Web thông qua web-assembly, để mình có ngay những app / demo trực quan, sinh động. Chi tiết tại https://github.com/telexyz/ml4coders",,,,,
các bạn lưu link github tài liệu về Toán học giúp hiểu sâu về machine learning,các bạn lưu link github tài liệu về Toán học giúp hiểu sâu về machine learning,,,,,
"#Sharing
Chia sẻ về quan điểm ""Khởi nghiệp"" đến từ Chief AI Officer của FPT Software. Bạn nghĩ khởi nghiệp về AI tại Việt Nam khó hay dễ?","Chia sẻ về quan điểm ""Khởi nghiệp"" đến từ Chief AI Officer của FPT Software. Bạn nghĩ khởi nghiệp về AI tại Việt Nam khó hay dễ?",#Sharing,,,,
"Mời các bạn tham dự cuộc thi để thử nghiệm các backbones SOTA hiện nay.
[Cuộc thi tháng 2-2022: Dog vs Cat classification]
I. Giới thiệu chung:
Hiện tại có rất nhiều các backbones khác nhau được ứng dụng trong các tác vụ của thị giác máy tính. Cuộc thi này nhằm tạo điều kiện để bạn ứng dụng những backbone này vào điều kiện thực tiễn để kiểm tra mức độ hiệu quả của chúng như thế nào đối với dữ liệu kích thước nhỏ.
II. Mục tiêu của cuộc thi: Phân loại ảnh chó và mèo trong đó tập huấn luyện gồm 25.000 hình ảnh có nhãn và tập kiểm tra gồm 8.000 hình ảnh chưa có nhãn được sử dụng để đánh giá xếp hạng.
III. Các gợi ý:
Bạn có thể sử dụng cách tiếp cận theo Data Centric và Model Centric cho cuộc thi này:
1. Data Centric: Sử dụng các thuật toán augmentation như Rotation, Random Crop, Scale, Bright Contrast, Mixup, CutMix, Label Smoothing,…
2. Model Centric:
2.1. Thay đổi kiến trúc: Có rất nhiều backbone có thể được sử dụng trong cuộc thi này:
- Các kiến trúc CNN: EfficientNet, ConvNeXt, ResNet, ResNext, MobileNet, AlexNet, InceptionNet, DenseNet,…
- Các kiến trúc họ Transformer: ViT, DeiT, CoAtNet, SwinTransformer, MobileViT
- Các kiến trúc khác: MLP-Mixer, PatchConvNet, Neural Architecture Search,…
Lưu ý không phải mô hình kích thước càng lớn càng tốt bởi thường dễ dẫn tới overfitting.
2.2. Thay đổi phương pháp huấn luyện:
- Sử dụng Transfer Learning
- Sử dụng Knowledge Distillation
- Thay đổi chiến lược Optimization
- Thử nghiệm các Loss function khác nhau
Vì dữ liệu của cuộc thi tương đối nhỏ nên tạo điều kiện cho ai cũng có thể tham gia, kể cả không có GPU. Các bạn có thể sử dụng google colab hoặc kaggle GPU để huấn luyện mô hình.
IV. Thời hạn cuộc thi:
Cuộc thi được bắt đầu vào 22:22 22.22 ngày 22/2/2022. Kết thúc vào ngày 22/3/2022
V. Các thông tin chi tiết cuộc thi xem tại:
https://www.kaggle.com/c/dog-vs-cat-classification/overview
Chúc các bạn thành công với các ý tưởng tại cuộc thi này.","Mời các bạn tham dự cuộc thi để thử nghiệm các backbones SOTA hiện nay. [Cuộc thi tháng 2-2022: Dog vs Cat classification] I. Giới thiệu chung: Hiện tại có rất nhiều các backbones khác nhau được ứng dụng trong các tác vụ của thị giác máy tính. Cuộc thi này nhằm tạo điều kiện để bạn ứng dụng những backbone này vào điều kiện thực tiễn để kiểm tra mức độ hiệu quả của chúng như thế nào đối với dữ liệu kích thước nhỏ. II. Mục tiêu của cuộc thi: Phân loại ảnh chó và mèo trong đó tập huấn luyện gồm 25.000 hình ảnh có nhãn và tập kiểm tra gồm 8.000 hình ảnh chưa có nhãn được sử dụng để đánh giá xếp hạng. III. Các gợi ý: Bạn có thể sử dụng cách tiếp cận theo Data Centric và Model Centric cho cuộc thi này: 1. Data Centric: Sử dụng các thuật toán augmentation như Rotation, Random Crop, Scale, Bright Contrast, Mixup, CutMix, Label Smoothing,… 2. Model Centric: 2.1. Thay đổi kiến trúc: Có rất nhiều backbone có thể được sử dụng trong cuộc thi này: - Các kiến trúc CNN: EfficientNet, ConvNeXt, ResNet, ResNext, MobileNet, AlexNet, InceptionNet, DenseNet,… - Các kiến trúc họ Transformer: ViT, DeiT, CoAtNet, SwinTransformer, MobileViT - Các kiến trúc khác: MLP-Mixer, PatchConvNet, Neural Architecture Search,… Lưu ý không phải mô hình kích thước càng lớn càng tốt bởi thường dễ dẫn tới overfitting. 2.2. Thay đổi phương pháp huấn luyện: - Sử dụng Transfer Learning - Sử dụng Knowledge Distillation - Thay đổi chiến lược Optimization - Thử nghiệm các Loss function khác nhau Vì dữ liệu của cuộc thi tương đối nhỏ nên tạo điều kiện cho ai cũng có thể tham gia, kể cả không có GPU. Các bạn có thể sử dụng google colab hoặc kaggle GPU để huấn luyện mô hình. IV. Thời hạn cuộc thi: Cuộc thi được bắt đầu vào 22:22 22.22 ngày 22/2/2022. Kết thúc vào ngày 22/3/2022 V. Các thông tin chi tiết cuộc thi xem tại: https://www.kaggle.com/c/dog-vs-cat-classification/overview Chúc các bạn thành công với các ý tưởng tại cuộc thi này.",,,,,
"Hi mọi người,
Đợt trước mình thấy bạn nào đó up solution của cuộc thi RSNA-MICCAI Brain Tumor Radiogenomic Classification
https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/code
mà mình tìm mãi không thấy . Ai còn giữ link không, cho mình xin lại với.
Cảm ơn cả nhà","Hi mọi người, Đợt trước mình thấy bạn nào đó up solution của cuộc thi RSNA-MICCAI Brain Tumor Radiogenomic Classification https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/code mà mình tìm mãi không thấy . Ai còn giữ link không, cho mình xin lại với. Cảm ơn cả nhà",,,,,
"Em chào mọi người,Anh em trong group đã có ai làm việc với Clova AI chưa ạ (đã tự thực hiện training cho tập dữ liệu riêng), nếu đã làm xin hãy comment hoặc cho em thông tin liên lạc. Em xin phép được liên hệ cà phê cà pháo để học hỏi một chút ạ!Hoặc nếu đường xá xa xôi thì em xin được inbox hỏi cũng được ạ.Em đã thực hiện chạy các demo và thấy con Clova này ngon nhất trong các con OCR mà em đã thử, em cũng đang gặp vấn đề là không đọc được số thẻ ngân hàng của Việt Nam (dập nổi). Dùng Yolo và tensorflow trainig thì đối tượng bé quá nên không nhận được (loss khi training tensorflow luôn luôn > 1.5, chán không buồn test)Link: https://github.com/clovaai/deep-text-recognition-benchmarkDemo: https://demo.ocr.clova.ai/Em cám ơn!Skype: tran.van.tho.tinb","Em chào mọi người,Anh em trong group đã có ai làm việc với Clova AI chưa ạ (đã tự thực hiện training cho tập dữ liệu riêng), nếu đã làm xin hãy comment hoặc cho em thông tin liên lạc. Em xin phép được liên hệ cà phê cà pháo để học hỏi một chút ạ!Hoặc nếu đường xá xa xôi thì em xin được inbox hỏi cũng được ạ.Em đã thực hiện chạy các demo và thấy con Clova này ngon nhất trong các con OCR mà em đã thử, em cũng đang gặp vấn đề là không đọc được số thẻ ngân hàng của Việt Nam (dập nổi). Dùng Yolo và tensorflow trainig thì đối tượng bé quá nên không nhận được (loss khi training tensorflow luôn luôn > 1.5, chán không buồn test)Link: https://github.com/clovaai/deep-text-recognition-benchmarkDemo: https://demo.ocr.clova.ai/Em cám ơn!Skype: tran.van.tho.tinb",,,,,
"Kính chào các bác trong Group ML Cơ bản,
Hôm nay em đang thử làm về món này nên em xin mạnh dạn chia sẻ video clip về việc điều khiển game bằng Mediapipe Pose và Pyautogui.
Món này lập trình cho các bé chơi để tăng cường vận động khá hay và hữu ích!
Mong giúp được các bạn mới học. Chúc các bạn thành công!","Kính chào các bác trong Group ML Cơ bản, Hôm nay em đang thử làm về món này nên em xin mạnh dạn chia sẻ video clip về việc điều khiển game bằng Mediapipe Pose và Pyautogui. Món này lập trình cho các bé chơi để tăng cường vận động khá hay và hữu ích! Mong giúp được các bạn mới học. Chúc các bạn thành công!",,,,,
"Hiện tại em đang làm đồ án về nhận diện té ngã ở người, mọi người có tài liệu tham khảo hoặc video hướng dẫn cho em xin với ạ, Em cảm ơn !","Hiện tại em đang làm đồ án về nhận diện té ngã ở người, mọi người có tài liệu tham khảo hoặc video hướng dẫn cho em xin với ạ, Em cảm ơn !",,,,,
"Hiện tại em đang làm đồ án về nhận diện té ngã ở người, mọi người có tài liệu tham khảo hoặc video hướng dẫn cho em xin với ạ, Em cảm ơn ạ !","Hiện tại em đang làm đồ án về nhận diện té ngã ở người, mọi người có tài liệu tham khảo hoặc video hướng dẫn cho em xin với ạ, Em cảm ơn ạ !",,,,,
"Chào cả nhà!
Chia sẻ với mọi người một bài viết đang được khá nhiều quan tâm trên reddit về Machine Learning. Mình mạo muội dịch và share cho anh em. Mong mọi người sẽ thấy thú vị 😃",Chào cả nhà! Chia sẻ với mọi người một bài viết đang được khá nhiều quan tâm trên reddit về Machine Learning. Mình mạo muội dịch và share cho anh em. Mong mọi người sẽ thấy thú vị,,,,,
"#hỏiđáp
#reinforcementlearning
Em chào các anh, chị. Hiện tại em đang nghiên cứu về reinforcement learning và thắc mắc về cách để đánh giá thuật toán này. Ví dụ bài toán tìm kho báu : Thuật toán sẽ tìm ra đường đi đước kho báu nhanh nhất và đi qua ít bẫy nhất, nhưng số điểm kho báu, bẫy là do mình quy định. Vậy thì ta đánh giá thuật toán này tối ưu hay không dựa trên tiêu chí nào ạ, cách chọn hệ số gamma và điểm thưởng, môi trường như thế nào để phù hợp với bài toàn, khi sử dụng các thuật toán khác nhau để giải quyết bài toán thì làm sao biết được thuật toán nào tốt hơn. Mong mọi người giúp em giải đáp thắc mắc. Em cảm ơn nhiều
(ảnh minh họa)","Em chào các anh, chị. Hiện tại em đang nghiên cứu về reinforcement learning và thắc mắc về cách để đánh giá thuật toán này. Ví dụ bài toán tìm kho báu : Thuật toán sẽ tìm ra đường đi đước kho báu nhanh nhất và đi qua ít bẫy nhất, nhưng số điểm kho báu, bẫy là do mình quy định. Vậy thì ta đánh giá thuật toán này tối ưu hay không dựa trên tiêu chí nào ạ, cách chọn hệ số gamma và điểm thưởng, môi trường như thế nào để phù hợp với bài toàn, khi sử dụng các thuật toán khác nhau để giải quyết bài toán thì làm sao biết được thuật toán nào tốt hơn. Mong mọi người giúp em giải đáp thắc mắc. Em cảm ơn nhiều (ảnh minh họa)",#hỏiđáp	#reinforcementlearning,,,,
"Xin chào mọi người !
Bên mình đang phát triển android mobile app nhận diện chữ (7 segment) real-time qua camera. Ai đã làm rồi cho mình hỏi có thư viện nào có sẵn không ạ? Hay có model nào tương tự để train.",Xin chào mọi người ! Bên mình đang phát triển android mobile app nhận diện chữ (7 segment) real-time qua camera. Ai đã làm rồi cho mình hỏi có thư viện nào có sẵn không ạ? Hay có model nào tương tự để train.,,,,,
"Hôm qua mình có thảo luận một chút về chủ đề dịch thuật ngữ ở post của anh Khánh. Có nhiều thứ mình nghĩ nhưng không tiện chia sẻ trong phạm vi của phần bình luận, nên hôm nay mình blog một bài về chủ đề này.","Hôm qua mình có thảo luận một chút về chủ đề dịch thuật ngữ ở post của anh Khánh. Có nhiều thứ mình nghĩ nhưng không tiện chia sẻ trong phạm vi của phần bình luận, nên hôm nay mình blog một bài về chủ đề này.",,,,,
"Xin chào mọi người !
Em chưa biết nhiều về code nên có tìm hiểu qua Teachable Machine của Google và cũng train được model.
Hiện em thấy ở phần Tensorflow.js code snippets Javascript nó chỉ có code mẫu dùng Webcam để làm nguồn đầu vào hình ảnh.
Em muốn sửa lại khi bấm Start thì chọn files từ máy tính thì sửa làm sao ạ? Mọi người có code mẫu hoặc sửa giúp em đoạn ấy em gởi ít cf ạ.
Em xin cảm ơn !",Xin chào mọi người ! Em chưa biết nhiều về code nên có tìm hiểu qua Teachable Machine của Google và cũng train được model. Hiện em thấy ở phần Tensorflow.js code snippets Javascript nó chỉ có code mẫu dùng Webcam để làm nguồn đầu vào hình ảnh. Em muốn sửa lại khi bấm Start thì chọn files từ máy tính thì sửa làm sao ạ? Mọi người có code mẫu hoặc sửa giúp em đoạn ấy em gởi ít cf ạ. Em xin cảm ơn !,,,,,
"CS W182 / 282A at UC Berkeley | Designing, Visualizing, and Understanding Deep Neural Networks
Design principles and best practices: design motifs that work well in particular domains, structure optimization and parameter optimization.
Visualizing deep networks. Exploring the training and use of deep networks with visualization tools.
Understanding deep networks. Methods with formal guarantees: generative and adversarial models, tensor factorization.
[Course Link]
https://youtu.be/ghOgcYuBMmw","CS W182 / 282A at UC Berkeley | Designing, Visualizing, and Understanding Deep Neural Networks Design principles and best practices: design motifs that work well in particular domains, structure optimization and parameter optimization. Visualizing deep networks. Exploring the training and use of deep networks with visualization tools. Understanding deep networks. Methods with formal guarantees: generative and adversarial models, tensor factorization. [Course Link] https://youtu.be/ghOgcYuBMmw",,,,,
"#AI_share
Chào cả nhà!
Share với mọi người library C++ Machine learning build từ scratch. Cụ thể hơn thì thư viện này gồm hơn 13k dòng code trải dài nhiều topic từ thống kê, đại số tuyến tính, phân tích số và tất nhiên, học máy và deep learning.
Happy learning!
https://github.com/novak-99/MLPP","Chào cả nhà! Share với mọi người library C++ Machine learning build từ scratch. Cụ thể hơn thì thư viện này gồm hơn 13k dòng code trải dài nhiều topic từ thống kê, đại số tuyến tính, phân tích số và tất nhiên, học máy và deep learning. Happy learning! https://github.com/novak-99/MLPP",#AI_share,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2020 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2020 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"Gần đây mình có chia sẻ khá nhiều về Graph Neural Network liên quan tới bài toán phân loại ảnh. Câu hỏi đặt ra rằng liệu GNNs có thể giải quyết quyết bài toán như object detection hay object verification hay không? Tình cờ, mình thấy có codebase này tập hợp các chủ để liên quan giữa GNNs ~ object detection/verification tại đây chia sẻ khá nhiều về Graph Neural Network liên quan tới bài toán phân loại ảnh. Câu hỏi đặt ra rằng liệu GNNs có thể giải quyết quyết bài toán như object detection hay object verification hay không? Tình cờ, mình thấy có codebase này tập hợp các chủ để liên quan giữa GNNs ~ object detection/verification tại đây https://github.com/Thinklab-SJTU/ThinkMatch
Cơ bản ý tưởng hiện nay cho chủ đề này là họ dùng CNN để tạo input data, rồi đưa vào GNNs.
Ps. Gần đây mình có chia sẻ một số bài viết về GNN~ Object classification, Ý tưởng tiếp cận của mình có khác biệt là chuyển thẳng input data là dạng ảnh thành Graph-structured data cho quá trình huấn luyện bài toán Graph classification (qua GNN models)","Gần đây mình có chia sẻ khá nhiều về Graph Neural Network liên quan tới bài toán phân loại ảnh. Câu hỏi đặt ra rằng liệu GNNs có thể giải quyết quyết bài toán như object detection hay object verification hay không? Tình cờ, mình thấy có codebase này tập hợp các chủ để liên quan giữa GNNs ~ object detection/verification tại đây chia sẻ khá nhiều về Graph Neural Network liên quan tới bài toán phân loại ảnh. Câu hỏi đặt ra rằng liệu GNNs có thể giải quyết quyết bài toán như object detection hay object verification hay không? Tình cờ, mình thấy có codebase này tập hợp các chủ để liên quan giữa GNNs ~ object detection/verification tại đây https://github.com/Thinklab-SJTU/ThinkMatch Cơ bản ý tưởng hiện nay cho chủ đề này là họ dùng CNN để tạo input data, rồi đưa vào GNNs. Ps. Gần đây mình có chia sẻ một số bài viết về GNN~ Object classification, Ý tưởng tiếp cận của mình có khác biệt là chuyển thẳng input data là dạng ảnh thành Graph-structured data cho quá trình huấn luyện bài toán Graph classification (qua GNN models)",,,,,
"Kính chào các bác, chào anh em, hôm nay em đang học phần LSTM nên em mạnh dạn làm video tìm hiểu cách sử dụng Mediapipe Pose kèm với LSTM Model để nhận diện hành vi con người.
Hi vọng mang lại những kiến thức bổ ích cho anh em mới học ạ.","Kính chào các bác, chào anh em, hôm nay em đang học phần LSTM nên em mạnh dạn làm video tìm hiểu cách sử dụng Mediapipe Pose kèm với LSTM Model để nhận diện hành vi con người. Hi vọng mang lại những kiến thức bổ ích cho anh em mới học ạ.",,,,,
"Em chào mọi người ạ,
Mọi người cho e hỏi một chút là đối với các bài toán nhận dạng chuỗi dùng deep learning ( Ví dụ bài toán nhận dạng người đang thực hiện hành động , thì đầu vào là 1 chuỗi hành đông ), thì mình xử lý dữ liệu đầu vào như thế nào ạ. Vì yêu cầu đầu vào của mạng thì tất cả các chuỗi phải cùng input, trong khi đó, mỗi hành động có thể diễn ra nhanh chậm khác nhau.
Vì vậy, theo mọi người thì cách xử lý đầu vào như thế nào thì sẽ hiệu quả nhất ạ.
Em cảm ơn mọi người nhiều.","Em chào mọi người ạ, Mọi người cho e hỏi một chút là đối với các bài toán nhận dạng chuỗi dùng deep learning ( Ví dụ bài toán nhận dạng người đang thực hiện hành động , thì đầu vào là 1 chuỗi hành đông ), thì mình xử lý dữ liệu đầu vào như thế nào ạ. Vì yêu cầu đầu vào của mạng thì tất cả các chuỗi phải cùng input, trong khi đó, mỗi hành động có thể diễn ra nhanh chậm khác nhau. Vì vậy, theo mọi người thì cách xử lý đầu vào như thế nào thì sẽ hiệu quả nhất ạ. Em cảm ơn mọi người nhiều.",,,,,
"Em xin chào mọi người ạ.
Hiện tại em đang làm bài toán Speaker Diarization (phân cực người nói trong một đoạn audio) với tiếng Việt. Về mô hình thì qua tìm hiểu em đang thử theo hai hướng là pyannote.audio và uis-rnn của Google. Cả 2 mô hình đều có pretrained cho tiếng Anh nhưng khi em thử nghiệm cho tiếng Việt thì lại không tốt. Còn dữ liệu cho tiếng Việt thì em đã tìm rất lâu nhưng vẫn không thấy.
Mọi người cho em hỏi là hiện tại ai đã có bộ dữ liệu cho bài toán này với giọng nói tiếng Việt chưa ạ?
Và nếu trong nhóm đã có ai từng làm bài toán này rồi thì có thể cho em xin lời khuyên về cách chọn mô hình cũng như huấn luyện được không ạ ? Em cảm ơn",Em xin chào mọi người ạ. Hiện tại em đang làm bài toán Speaker Diarization (phân cực người nói trong một đoạn audio) với tiếng Việt. Về mô hình thì qua tìm hiểu em đang thử theo hai hướng là pyannote.audio và uis-rnn của Google. Cả 2 mô hình đều có pretrained cho tiếng Anh nhưng khi em thử nghiệm cho tiếng Việt thì lại không tốt. Còn dữ liệu cho tiếng Việt thì em đã tìm rất lâu nhưng vẫn không thấy. Mọi người cho em hỏi là hiện tại ai đã có bộ dữ liệu cho bài toán này với giọng nói tiếng Việt chưa ạ? Và nếu trong nhóm đã có ai từng làm bài toán này rồi thì có thể cho em xin lời khuyên về cách chọn mô hình cũng như huấn luyện được không ạ ? Em cảm ơn,,,,,
"Anh em save Github kho tàng các thuật toán được phát triển bằng python, java, C và rất nhiều ngôn ngữ lập trình khác","Anh em save Github kho tàng các thuật toán được phát triển bằng python, java, C và rất nhiều ngôn ngữ lập trình khác",,,,,
"Như vậy là cuốn sách đã được chia sẻ công khai. Chúng tôi sẽ tìm kiếm chủ nhân của file này và có giải pháp pháp lý phù hợp.
Các bạn có thể ủng hộ sách bản quyền tại https://handson-ml.mlbvn.org/.",Như vậy là cuốn sách đã được chia sẻ công khai. Chúng tôi sẽ tìm kiếm chủ nhân của file này và có giải pháp pháp lý phù hợp. Các bạn có thể ủng hộ sách bản quyền tại https://handson-ml.mlbvn.org/.,,,,,
"em chào anh chị ạ, lời đầu tiên em xin chúc anh chị và gia đình năm mới sức khỏe, may mắn và thành công.
Hiện em mới bắt đầu học tập mảng data/ai (gà mờ), nên có 1 vài câu hỏi mong anh chị trả lời và định hướng giúp:
1. Em đang phân vân giữa Data Sience và AI Engineer, anh có thể cho em biết sự khác biệt giữa 2 ngành này được không ạ? Em đặc biệt yêu thích về thuật toán, model và cách giải quyết vấn đề mà cả 2 đều có
2. Dựa theo kinh nghiệm và hiểu biết của anh chị, anh chị có thể chỉ điểm cho em 1 vài công ty phù hợp được không ạ? Em chú trọng hơn phần phát triển lâu dài, học được gì, đường phát triển sau này, đặc biệt em cần mentor để chỉ đường
3. Em cần những gì để có thể đạt được ngưỡng yêu cầu của các công ty, ít nhất là vị trí fresher? Hiện tại em đang theo học khóa AI Engineer của IBM (đã được 50%), sau đó em dự định học thêm khóa Data Sience của IBM và khóa chuyên sâu về toán hơn của Stanford","em chào anh chị ạ, lời đầu tiên em xin chúc anh chị và gia đình năm mới sức khỏe, may mắn và thành công. Hiện em mới bắt đầu học tập mảng data/ai (gà mờ), nên có 1 vài câu hỏi mong anh chị trả lời và định hướng giúp: 1. Em đang phân vân giữa Data Sience và AI Engineer, anh có thể cho em biết sự khác biệt giữa 2 ngành này được không ạ? Em đặc biệt yêu thích về thuật toán, model và cách giải quyết vấn đề mà cả 2 đều có 2. Dựa theo kinh nghiệm và hiểu biết của anh chị, anh chị có thể chỉ điểm cho em 1 vài công ty phù hợp được không ạ? Em chú trọng hơn phần phát triển lâu dài, học được gì, đường phát triển sau này, đặc biệt em cần mentor để chỉ đường 3. Em cần những gì để có thể đạt được ngưỡng yêu cầu của các công ty, ít nhất là vị trí fresher? Hiện tại em đang theo học khóa AI Engineer của IBM (đã được 50%), sau đó em dự định học thêm khóa Data Sience của IBM và khóa chuyên sâu về toán hơn của Stanford",,,,,
"Em chào mọi người, cho em hỏi trong mô hình Transformers gốc thì phần Multihead-attention được tính toán như thế nào vậy ạ. em đã tham khảo một số nguồn và hiểu như sau nhưng chưa biết đúng hay sai mong mọi người giải đáp ạ:
Cách hiểu 1:
Bước 1: Cho input embeeding (5x512) đi qua 1 Linear và thu được 3 ma trận Q, K, V (512 x 512)
Bước 2: Chia nhỏ Q, K, V thành 8 phần (theo cột) rồi tính self-attention cho từng phần đó và cuối cùng concat chúng tạo được một ma trận Z
Bước 3: Cho Z qua Linear để thu được kết quả cuối cùng (5x512)
Cách hiểu 2:
Bước 1: Cho input embeeding (5x512) đi qua 1 Linear và thu được 3 ma trận Q, K, V (512 x 512)
Bước 2: Tiếp tục cho Q, K, V qua 8 Linear tức mỗi lần sẽ thu được một cặp 3 ma trận và tính self-attention cho 3 cặp ma trận này và output sẽ ra được 8 cặp 512 x 64, rồi cuối cùng concate chúng để được Z (512x512)
Bước 3: Cho Z qua Linear để thu được output là (5x512)
Rất mong được mọi người giải đáp ạ, em xin chân thành cảm ơn!","Em chào mọi người, cho em hỏi trong mô hình Transformers gốc thì phần Multihead-attention được tính toán như thế nào vậy ạ. em đã tham khảo một số nguồn và hiểu như sau nhưng chưa biết đúng hay sai mong mọi người giải đáp ạ: Cách hiểu 1: Bước 1: Cho input embeeding (5x512) đi qua 1 Linear và thu được 3 ma trận Q, K, V (512 x 512) Bước 2: Chia nhỏ Q, K, V thành 8 phần (theo cột) rồi tính self-attention cho từng phần đó và cuối cùng concat chúng tạo được một ma trận Z Bước 3: Cho Z qua Linear để thu được kết quả cuối cùng (5x512) Cách hiểu 2: Bước 1: Cho input embeeding (5x512) đi qua 1 Linear và thu được 3 ma trận Q, K, V (512 x 512) Bước 2: Tiếp tục cho Q, K, V qua 8 Linear tức mỗi lần sẽ thu được một cặp 3 ma trận và tính self-attention cho 3 cặp ma trận này và output sẽ ra được 8 cặp 512 x 64, rồi cuối cùng concate chúng để được Z (512x512) Bước 3: Cho Z qua Linear để thu được output là (5x512) Rất mong được mọi người giải đáp ạ, em xin chân thành cảm ơn!",,,,,
"[GauGAN2 - Nvidia Canvas]
Bạn đang có ý tưởng vẽ hoặc đơn giản là những khối hình nhưng lại muốn có một bức tranh hoàn chỉnh? phần mềm Nvidia Canvas chính là lựa chọn cho bạn.
Gần đây Nvidia Canvas cho ra mắt mạng GauGAN2 với ảnh sinh ra tăng 4 lần độ phân giải so với các mô hình trước đây. Canvas giúp bạn biến những nét vẽ về nước, cỏ, tuyết, núi,... được hòa quyện với nhau thành ảnh phong cảnh tự nhiên. Hơn thế nữa thì phần mềm Canvas đang miễn phí nên mọi người có thể tải về dùng thử.
Phần mềm Canvas: https://www.nvidia.com/en-us/studio/canvas/
Youtube: https://www.youtube.com/watch?v=wKztRskmsig
Post: https://blogs.nvidia.com/blog/2022/01/04/studio-canvas-update-gaugan2-ces/","[GauGAN2 - Nvidia Canvas] Bạn đang có ý tưởng vẽ hoặc đơn giản là những khối hình nhưng lại muốn có một bức tranh hoàn chỉnh? phần mềm Nvidia Canvas chính là lựa chọn cho bạn. Gần đây Nvidia Canvas cho ra mắt mạng GauGAN2 với ảnh sinh ra tăng 4 lần độ phân giải so với các mô hình trước đây. Canvas giúp bạn biến những nét vẽ về nước, cỏ, tuyết, núi,... được hòa quyện với nhau thành ảnh phong cảnh tự nhiên. Hơn thế nữa thì phần mềm Canvas đang miễn phí nên mọi người có thể tải về dùng thử. Phần mềm Canvas: https://www.nvidia.com/en-us/studio/canvas/ Youtube: https://www.youtube.com/watch?v=wKztRskmsig Post: https://blogs.nvidia.com/blog/2022/01/04/studio-canvas-update-gaugan2-ces/",,,,,
"xin chào mọi người, mình muốn hỏi mọi người một số sách hoặc nguồn học và thực hành về OCR hay, mong mọi người chỉ giúp ạ, tks mọi người","xin chào mọi người, mình muốn hỏi mọi người một số sách hoặc nguồn học và thực hành về OCR hay, mong mọi người chỉ giúp ạ, tks mọi người",,,,,
"Mình mới học ML nên còn it kinh nghiệm, mong các bạn chỉ giúp. Mình có 1 model regresssion ( 50% train, 25% test va 25% validation. ) Loss va val_loss ra như vậy và RMSE gần ~ 0.01. Như vậy là sao nhỉ. Cảm ơn các bạn","Mình mới học ML nên còn it kinh nghiệm, mong các bạn chỉ giúp. Mình có 1 model regresssion ( 50% train, 25% test va 25% validation. ) Loss va val_loss ra như vậy và RMSE gần ~ 0.01. Như vậy là sao nhỉ. Cảm ơn các bạn",,,,,
"Tìm giá trị mong đợi (expected value) và sai số (variance)
Hiện tại, mình có T = {t1, t2, t3, t4, t5} = {10.5, 10.6, 17.2, 21.0, 26.1}. Mình muốn tìm expected value <T> và variance cho đại lượng T.
Vì mình amature về Machine Learning nên rất mong các bạn hưỡng dẫn giúp mình. Mình có một vài thắc mắc sau:
1/ Giá trị trung bình và sai số tính theo <T> = 1/5 * sum(ti) và varT = sqrt(dt1^2+dt2^2 +.... dt5^2) nó khác thế nào so với giá trị tính theo Mote-Carlo approximation?
2/ Nếu mình tính theo Monte-Carlo approximation thì mình có thể tính như thế nào ạ?
3/ Nếu mình giả sử T có giá trị trong vùng T = [tmin, tmax] = [10.5, 26.1], thì mình có thể tính giá trị expected và var theo Gaussian distribution phải không ạ?
Mình rất mong sự giúp đỡ của các bạn. Mình cảm ơn các bạn.
Chúc các bạn sang năm mới nhiều sức khỏe, tài lộc, may mắn và bình an nhé!","Tìm giá trị mong đợi (expected value) và sai số (variance) Hiện tại, mình có T = {t1, t2, t3, t4, t5} = {10.5, 10.6, 17.2, 21.0, 26.1}. Mình muốn tìm expected value <T> và variance cho đại lượng T. Vì mình amature về Machine Learning nên rất mong các bạn hưỡng dẫn giúp mình. Mình có một vài thắc mắc sau: 1/ Giá trị trung bình và sai số tính theo <T> = 1/5 * sum(ti) và varT = sqrt(dt1^2+dt2^2 +.... dt5^2) nó khác thế nào so với giá trị tính theo Mote-Carlo approximation? 2/ Nếu mình tính theo Monte-Carlo approximation thì mình có thể tính như thế nào ạ? 3/ Nếu mình giả sử T có giá trị trong vùng T = [tmin, tmax] = [10.5, 26.1], thì mình có thể tính giá trị expected và var theo Gaussian distribution phải không ạ? Mình rất mong sự giúp đỡ của các bạn. Mình cảm ơn các bạn. Chúc các bạn sang năm mới nhiều sức khỏe, tài lộc, may mắn và bình an nhé!",,,,,
"Khai bút đầu Xuân với bài giới thiệu (của Chris Hughes) về thư viện TIMM (cho các bạn làm computer vision bằng PyTorch) với >16k sao. Theo quan điểm của mình TIMM là thư viện MẠNH NHẤT trong lĩnh vực computer vision, có người còn so sánh nó là HuggingFace của CV.
Năm mới chúc các bạn gặt hái được nhiều thành công trong công việc.","Khai bút đầu Xuân với bài giới thiệu (của Chris Hughes) về thư viện TIMM (cho các bạn làm computer vision bằng PyTorch) với >16k sao. Theo quan điểm của mình TIMM là thư viện MẠNH NHẤT trong lĩnh vực computer vision, có người còn so sánh nó là HuggingFace của CV. Năm mới chúc các bạn gặt hái được nhiều thành công trong công việc.",,,,,
"Chúc mừng năm mới ACE MLCB nhé. Chúc Loss của ACE ko như trong hình nhé :)).
PS. WANDB ""hiểm ghớm"", thấy trước được ""tương lai"" và đặt cái tên exp là ""tough-universe"" hợp v~ (LoL).","Chúc mừng năm mới ACE MLCB nhé. Chúc Loss của ACE ko như trong hình nhé :)). PS. WANDB ""hiểm ghớm"", thấy trước được ""tương lai"" và đặt cái tên exp là ""tough-universe"" hợp v~ (LoL).",,,,,
"#chiase
Nhân dịp năm mới mình xin gửi lời chúc tới toàn thể mọi người trong nhóm mọi điều tốt đẹp nhất.
Mình mới viết 1 bài về chủ đề crawl dữ liệu các trang web với nội dung được tạo bởi javascript sử dụng thư viện chromedp của golang, đây là 1 thư viện còn mới đối với mọi người. mục tiêu là vượt qua bước đăng nhập của 2 trang web việc làm itviec, vietnamworks và google và bóc tách phần dữ liệu bị ẩn đi khi chưa đăng nhập.
Hi vọng bài viết này mọi người sẽ có thêm 1 lựa chọn nữa để crawl web có javascript 😁😁😁","Nhân dịp năm mới mình xin gửi lời chúc tới toàn thể mọi người trong nhóm mọi điều tốt đẹp nhất. Mình mới viết 1 bài về chủ đề crawl dữ liệu các trang web với nội dung được tạo bởi javascript sử dụng thư viện chromedp của golang, đây là 1 thư viện còn mới đối với mọi người. mục tiêu là vượt qua bước đăng nhập của 2 trang web việc làm itviec, vietnamworks và google và bóc tách phần dữ liệu bị ẩn đi khi chưa đăng nhập. Hi vọng bài viết này mọi người sẽ có thêm 1 lựa chọn nữa để crawl web có javascript",#chiase,,,,
"Có bạn nào là SAS user không? Mình có một vài code free để học và thi lấy chứng chỉ SAS Certification cho Data Science Program, hạn đến 12/2022. Đây là prize của mình khi thi competition, bạn nào cần thì mình tặng lại vì mình không có nhu cầu sử dụng. Certification bao gồm:
Data Curation Professional
Advanced Analytics Professional
AI & Machine Learning Professional 
Link tham khảo: https://www.sas.com/en_us/training/academy-data-science.html
Vì SAS không phổ biến nên nếu bạn nào thực sự cần thì liên hệ mình. ","Có bạn nào là SAS user không? Mình có một vài code free để học và thi lấy chứng chỉ SAS Certification cho Data Science Program, hạn đến 12/2022. Đây là prize của mình khi thi competition, bạn nào cần thì mình tặng lại vì mình không có nhu cầu sử dụng. Certification bao gồm: Data Curation Professional Advanced Analytics Professional AI & Machine Learning Professional Link tham khảo: https://www.sas.com/en_us/training/academy-data-science.html Vì SAS không phổ biến nên nếu bạn nào thực sự cần thì liên hệ mình.",,,,,
"Em chào mọi người, bản chất của bức ảnh này là gì ạ?? Em biết được sơ sơ nhiêu đây:
 L1 và L2 là mô hình Lasso Regression và Ridge Regression giúp tránh hiện tượng overfitting. 
Hàm Loss của L1:  loss = error(y, y^) + lamda*sum(abs(các theta))
Hàm Loss của L2:  loss = error(y, y^) + lamda*sum(square(các theta))
Lượng cộng thêm thì gọi là penalty. Em rất muốn biết tại sao L1 lại là hình thoi ạ","Em chào mọi người, bản chất của bức ảnh này là gì ạ?? Em biết được sơ sơ nhiêu đây: L1 và L2 là mô hình Lasso Regression và Ridge Regression giúp tránh hiện tượng overfitting. Hàm Loss của L1: loss = error(y, y^) + lamda*sum(abs(các theta)) Hàm Loss của L2: loss = error(y, y^) + lamda*sum(square(các theta)) Lượng cộng thêm thì gọi là penalty. Em rất muốn biết tại sao L1 lại là hình thoi ạ",,,,,
"Chào mọi người ...trong quá trình build một mô hình phân loại thì em có gặp vấn đề sau.
Khi trainning thì em tính accuracy được 73%
Còn khi testting trên chính tập vừa train thì khi trong code của em có model.eval() thì accuracy đạt 39%.
Còn khi không có model.eval()
thì accuracy lại đạt 72% bám sát 73% lúc trainning.
Mọi người cho em hỏi là tại sao vậy ạ . Em cảm ơn mọi người",Chào mọi người ...trong quá trình build một mô hình phân loại thì em có gặp vấn đề sau. Khi trainning thì em tính accuracy được 73% Còn khi testting trên chính tập vừa train thì khi trong code của em có model.eval() thì accuracy đạt 39%. Còn khi không có model.eval() thì accuracy lại đạt 72% bám sát 73% lúc trainning. Mọi người cho em hỏi là tại sao vậy ạ . Em cảm ơn mọi người,,,,,
"Kính chào các bác. Nhân dịp đang nghiên cứu em mạnh dạn chia sẻ cùng anh em bài về Model Quantization giúp model chạy ngon hơn trên các thiết bị Pi, Jetson Nano...
Năm cũ sắp qua, năm mới sắp đến. Chúc các bác một năm thành công và hạnh phúc!","Kính chào các bác. Nhân dịp đang nghiên cứu em mạnh dạn chia sẻ cùng anh em bài về Model Quantization giúp model chạy ngon hơn trên các thiết bị Pi, Jetson Nano... Năm cũ sắp qua, năm mới sắp đến. Chúc các bác một năm thành công và hạnh phúc!",,,,,
"Trang machinelearningcoban bị lỗi sao em dùng đt vô ok mà pc thì đứt, báo lỗi em google chỉ là clear Ssl, delete cookies caches rồi nhưng vẫn vậy.","Trang machinelearningcoban bị lỗi sao em dùng đt vô ok mà pc thì đứt, báo lỗi em google chỉ là clear Ssl, delete cookies caches rồi nhưng vẫn vậy.",,,,,
"Mọi người ơi, mấy hôm nay em không vào được website Machine Learning Cơ Bản nữa ạ.
Không biết có ai gặp tình trạng giống em không, hình như thông báo là đã chuyển sang trang mới hoặc trang này không tồn tại @@
Em cảm ơn cả nhà ạ!","Mọi người ơi, mấy hôm nay em không vào được website Machine Learning Cơ Bản nữa ạ. Không biết có ai gặp tình trạng giống em không, hình như thông báo là đã chuyển sang trang mới hoặc trang này không tồn tại @@ Em cảm ơn cả nhà ạ!",,,,,
"Mọi người cho em hỏi con Macbook M1 pro có train model nhanh hơn Colab free không ạ? Em định mua một con laptop để làm việc, mọi người cho em lời khuyên nhé! Ngân sách không quan trọng vì có người tài trợ mua cho em ạ. Cảm ơn mọi người!","Mọi người cho em hỏi con Macbook M1 pro có train model nhanh hơn Colab free không ạ? Em định mua một con laptop để làm việc, mọi người cho em lời khuyên nhé! Ngân sách không quan trọng vì có người tài trợ mua cho em ạ. Cảm ơn mọi người!",,,,,
"Đi lang thang trên mạng gặp được một nguồn tổng hợp hơn 800 khoá học miễn phí về Computer Science, Data Science & Machine Learning, ... có vẻ khá hay, các bạn lưu lại để tham khảo dần dần nhé.
https://github.com/Developer-Y/cs-video-courses
P/S: Nhiều quá đôi khi cũng không tốt, nhưng thôi kệ cứ sưu tầm về đã 😛","Đi lang thang trên mạng gặp được một nguồn tổng hợp hơn 800 khoá học miễn phí về Computer Science, Data Science & Machine Learning, ... có vẻ khá hay, các bạn lưu lại để tham khảo dần dần nhé. https://github.com/Developer-Y/cs-video-courses P/S: Nhiều quá đôi khi cũng không tốt, nhưng thôi kệ cứ sưu tầm về đã",,,,,
"Chào cả nhà,
Mình mới làm ít video chia sẻ về Machine Learning, rất mong mọi người đóng ý kiến để các video tiếp theo hoàn thiện tốt hơn:
https://www.youtube.com/playlist?list=PLWBrqglnjNl17CA7H7RGr9uHAbDKbwRKq
Cảm ơn cả nhà nhiều","Chào cả nhà, Mình mới làm ít video chia sẻ về Machine Learning, rất mong mọi người đóng ý kiến để các video tiếp theo hoàn thiện tốt hơn: https://www.youtube.com/playlist?list=PLWBrqglnjNl17CA7H7RGr9uHAbDKbwRKq Cảm ơn cả nhà nhiều",,,,,
Bài viết về dự báo “tương lai” GNNs trong năm 2022 của M. Bronstein và Veličković. Bài dài tới 44 phút đọc nhưng đáng đọc,Bài viết về dự báo “tương lai” GNNs trong năm 2022 của M. Bronstein và Veličković. Bài dài tới 44 phút đọc nhưng đáng đọc,,,,,
"[Label Studio – 7.4k star*]
Label Studio là gì?
Label Studio là một công cụ label dữ liệu mã nguồn mở. Nó cho phép bạn gán nhãn các loại dữ liệu như âm thanh, hình ảnh, văn bản, video và chuỗi thời gian với giao diện người dùng đơn giản, dễ hiểu và dễ dàng xuất sang các định dạng mô hình khác nhau.
Đặc biệt, bạn có thể thiết lập mô hình Machine Learning với Label Studio hay tích hợp Label Studio với các công cụ hiện có của bạn.
Chi tiết mọi người xem thêm ở đây:
Web: https://labelstud.io/
Github: https://github.com/heartexlabs/label-studio","[Label Studio – 7.4k star*] Label Studio là gì? Label Studio là một công cụ label dữ liệu mã nguồn mở. Nó cho phép bạn gán nhãn các loại dữ liệu như âm thanh, hình ảnh, văn bản, video và chuỗi thời gian với giao diện người dùng đơn giản, dễ hiểu và dễ dàng xuất sang các định dạng mô hình khác nhau. Đặc biệt, bạn có thể thiết lập mô hình Machine Learning với Label Studio hay tích hợp Label Studio với các công cụ hiện có của bạn. Chi tiết mọi người xem thêm ở đây: Web: https://labelstud.io/ Github: https://github.com/heartexlabs/label-studio",,,,,
"Em chào mọi người, em đang thực hiện so sánh chất lượng mô hình em phát triển với các mô hình từ 1 số bài báo khác nhau. Để so sánh chất lượng, em xây dựng các tập dữ liệu khác nhau, trong mỗi tập dữ liệu đều có training set, validation set và test set. Em cho mô hình của em học trên training set sau đó cho early stopping với validation set rồi evaluate trên test set để thu được các độ đo chất lượng mô hình. Với các mô hình khác, em cũng lặp lại quy trình trên, nhưng có 1 mô hình train rất lâu, tuy nhiên lại có file weight của mô hình khi train với tập dữ liệu trong bài báo gốc. Để tiết kiệm thì em có nên sử dụng file weight đó để load luôn mô hình rồi đánh giá trên tập test em đã xây dựng không ạ? Em nghĩ em có câu trả lời là rất có thể là không, vì có thể file weight cho mô hình từ bài báo đó chỉ thể hiện rằng mô hình có thể fit rất tốt với training set trong bài báo đó chứ nếu đánh giá luôn trên test set mà em xây dựng thì có thể lại rất kém. Em thấy rằng trong các bài báo, người ta thường train và test các mô hình trên cùng tập dữ liệu (cả training và test set), liệu còn có những cách nào hay cách khác em vừa nêu ra liên quan đến load pretrained model có hợp lý không ạ. (ở đây em chỉ hỏi so sánh về performance metrics như accuracy, f1-score, auroc, auprc giữa các mô hình ạ). Em cảm ơn mọi người.

PS: Ảnh em lấy mang tính chất minh họa ạ","Em chào mọi người, em đang thực hiện so sánh chất lượng mô hình em phát triển với các mô hình từ 1 số bài báo khác nhau. Để so sánh chất lượng, em xây dựng các tập dữ liệu khác nhau, trong mỗi tập dữ liệu đều có training set, validation set và test set. Em cho mô hình của em học trên training set sau đó cho early stopping với validation set rồi evaluate trên test set để thu được các độ đo chất lượng mô hình. Với các mô hình khác, em cũng lặp lại quy trình trên, nhưng có 1 mô hình train rất lâu, tuy nhiên lại có file weight của mô hình khi train với tập dữ liệu trong bài báo gốc. Để tiết kiệm thì em có nên sử dụng file weight đó để load luôn mô hình rồi đánh giá trên tập test em đã xây dựng không ạ? Em nghĩ em có câu trả lời là rất có thể là không, vì có thể file weight cho mô hình từ bài báo đó chỉ thể hiện rằng mô hình có thể fit rất tốt với training set trong bài báo đó chứ nếu đánh giá luôn trên test set mà em xây dựng thì có thể lại rất kém. Em thấy rằng trong các bài báo, người ta thường train và test các mô hình trên cùng tập dữ liệu (cả training và test set), liệu còn có những cách nào hay cách khác em vừa nêu ra liên quan đến load pretrained model có hợp lý không ạ. (ở đây em chỉ hỏi so sánh về performance metrics như accuracy, f1-score, auroc, auprc giữa các mô hình ạ). Em cảm ơn mọi người. PS: Ảnh em lấy mang tính chất minh họa ạ",,,,,
"Trong lúc nghiên cứu về represention learning, mình có phát hiện ra phương pháp của mình cho feature visualization như bên dưới và đạt được perfomance tốt hơn baseline. Theo hình thì feature của mình more uniformity. Kết quả thực nghiệm cho performance tốt hơn. Nhưng dạng phân phối features như này có tác dụng gì tới downstream tasks. Mình không thể lý giải sâu được. Xin phép mọi người chỉ bảo thêm ạ?","Trong lúc nghiên cứu về represention learning, mình có phát hiện ra phương pháp của mình cho feature visualization như bên dưới và đạt được perfomance tốt hơn baseline. Theo hình thì feature của mình more uniformity. Kết quả thực nghiệm cho performance tốt hơn. Nhưng dạng phân phối features như này có tác dụng gì tới downstream tasks. Mình không thể lý giải sâu được. Xin phép mọi người chỉ bảo thêm ạ?",,,,,
"em xin chào mọi người, em có chút thắc mắc phần này (ảnh bên dưới) khi đọc cuốn hands-on ML.
cụ thể là tác giả có nói: giả sử xác suất tung đồng xu có mặt ngửa là 51%, mặt sấp là 49%. Nếu tung 1000 lần thì sẽ có khoảng 510 mặt ngửa và 490 mặt sấp. Và sau càng nhiều lần tung thì xác suất để mặt ngửa càng tăng (10000 lần là 97%) nhưng trên đồ thị thì biểu diễn k đúng lắm. em đang chưa hiểu rõ phần này, rất mong được mọi người giúp đỡ, em cảm ơn ạ!","em xin chào mọi người, em có chút thắc mắc phần này (ảnh bên dưới) khi đọc cuốn hands-on ML. cụ thể là tác giả có nói: giả sử xác suất tung đồng xu có mặt ngửa là 51%, mặt sấp là 49%. Nếu tung 1000 lần thì sẽ có khoảng 510 mặt ngửa và 490 mặt sấp. Và sau càng nhiều lần tung thì xác suất để mặt ngửa càng tăng (10000 lần là 97%) nhưng trên đồ thị thì biểu diễn k đúng lắm. em đang chưa hiểu rõ phần này, rất mong được mọi người giúp đỡ, em cảm ơn ạ!",,,,,
"[SHARING - MACHINE LEARNING IN QUANTITATIVE TRADING PROJECT]
Hello mọi người, mình có nghiên cứu Machine Learning trong Trading khá lâu và có làm 1 project để ứng dụng Machine Learning trong quy trình nghiên cứu trên 1 thị trường thực tế. Quy trình trong project sẽ có nhiều bước từ Data Cleaning, Feature Engineering, Machine Learning modeling, Alpha Factor Analysis, Porfolio Optimization và Backtesting. Từ đó các bạn có thể tự phát triển, xây dựng chiến lược cho riêng mình. Nếu bạn nào có hứng thú muốn tìm hiểu thêm thì có thể liên hệ mình nhé.","[SHARING - MACHINE LEARNING IN QUANTITATIVE TRADING PROJECT] Hello mọi người, mình có nghiên cứu Machine Learning trong Trading khá lâu và có làm 1 project để ứng dụng Machine Learning trong quy trình nghiên cứu trên 1 thị trường thực tế. Quy trình trong project sẽ có nhiều bước từ Data Cleaning, Feature Engineering, Machine Learning modeling, Alpha Factor Analysis, Porfolio Optimization và Backtesting. Từ đó các bạn có thể tự phát triển, xây dựng chiến lược cho riêng mình. Nếu bạn nào có hứng thú muốn tìm hiểu thêm thì có thể liên hệ mình nhé.",,,,,
"Em chào mn ạ. Em mới tìm hiểu về machine learning. Trong b10 logistic regression trên blog machine learning cơ bản https://machinelearningcoban.com/2017/01/27/logisticregression/
em có thấy anh Tiệp khi code lại thuật toán, em thấy anh sử dụng mix_id data và tối ưu hàm mất mát cho từng điểm 1. Em có thử thay bằng hàm mất mát của trung bình tổng các data (vd tổng mất mát của m data rồi chia cho m), về bản chất em cảm giác không sai khác nhau lắm, cũng có những tài liệu viết hàm mất mát dưới dạng này nhưng kết quả thử của em lại ra không tốt nữa ạ. Vậy không biết lỗi là ở đâu, mong mọi người chỉ dẫn ạ. Em cảm ơn.","Em chào mn ạ. Em mới tìm hiểu về machine learning. Trong b10 logistic regression trên blog machine learning cơ bản https://machinelearningcoban.com/2017/01/27/logisticregression/ em có thấy anh Tiệp khi code lại thuật toán, em thấy anh sử dụng mix_id data và tối ưu hàm mất mát cho từng điểm 1. Em có thử thay bằng hàm mất mát của trung bình tổng các data (vd tổng mất mát của m data rồi chia cho m), về bản chất em cảm giác không sai khác nhau lắm, cũng có những tài liệu viết hàm mất mát dưới dạng này nhưng kết quả thử của em lại ra không tốt nữa ạ. Vậy không biết lỗi là ở đâu, mong mọi người chỉ dẫn ạ. Em cảm ơn.",,,,,
"[CoAtNet - Marrying convolution and attention for all data sizes]
Transformer đang đạt được sự quan tâm đáng kể trong Computer Vision nhưng vẫn chưa hoàn toàn vượt qua được các kiến trúc CNN tốt nhất. Gần đây trong bài báo CoAtNet của nhóm tác giả nổi tiếng đến từ google là anh Quốc Lê và Mingxing Tan đã đưa ra một sự kết hợp giữa Convolution với Attention. Đây là một ý tưởng đơn giản nhưng độc đáo mà mình bị hấp dẫn và cảm thấy thú vị. Hiện tại thì mô hình tốt nhất của kiến trúc này là CoAtNet-7 đang xếp top 1 trên Leader Board của ImageNet, bỏ xa ConvNeXt, EfficientNetV2-L và ViT:
https://paperswithcode.com/sota/image-classification-on-imagenet
Mình cũng dành thời gian phân tích và implement lại mô hình này gần đây và hôm nay muốn review lại những ý chính của paper tới mọi người. Qua đó, mọi người có thể có thêm ý tưởng mới trong research cũng như apply vào các project của công ty,
1. Capture một vài tính chất quan trọng của CNN và Transformer.
1.1. Tính chất của CNN:
- Translation Equivariance: Sự dịch chuyển của một vật thể trên ảnh đầu vào so với đầu ra là bất biến trên một mạng CNN. Nhờ tính chất này mà CNN có thể học tốt hơn với ít dữ liệu thông qua Augumentation.
- Local Receptive Field: Các đặc trưng được CNN tổng hợp là dựa trên các local region của ảnh và những đặc trưng này có sự biến đổi theo độ sâu. Ở những level đầu là những đặc trưng bậc thấp mang tính phổ biến như các nét dọc, ngang, chéo và được tổng hợp thành những đặc trưng bậc cao giúp nhận diện class ở những layer sâu hơn.
1.2. Các tính chất của Transformer:
- Global context: Transformer ban đầu được xây dựng là cho sequential data chẳng hạn như text data với core building block là Multi-head Attention. Nhờ đó nó có khả năng trích xuất thông tin global context rất tốt. Một feature token học được từ mạng là được tổng hợp từ toàn bộ các patches trên ảnh.
- Global Receptive Field: Transformer hoạt động như một Global Receptive Field trong khi CNN hoạt động dựa trên Local Receptive Field. Tính chất này khiến Transformer có xu hướng dễ gặp phải overfitting vì dữ liệu của bạn khi thực hiện các Hard Augmentation như Rotation với góc lớn, Random Shuffling, CutMix thì Transformer vẫn có thể nhận ra được. Đó cũng là nguyên nhân khiến Transformer sẽ hoạt động tốt hơn so với CNN khi huấn luyện trên các bộ dữ liệu kích thước cực lớn (lên tới vài trăm triệu images) như ImageNet-21K, JFT-300M nhưng lại kém hơn trên ImageNet-1K (chỉ 1 triệu images).
- Layer’s Features Similarity: Các đặc trưng mà Transformer học được có sự tương đồng giữa first layer và last layer, trong khi CNN thì khác biệt mạnh giữa chúng.
3. Ý tưởng của bài báo:
Kết hợp đồng thời cả Convolution và Attention trong cùng một block và stack chúng trong một end-to-end network. Khi áp dụng kiến trúc Transformer có thể khiến cho số lượng tham số của mô hình tăng lên rất lớn. Như vậy chi phí tính toán sẽ được cân nhắc trong thiết kế của kiến trúc này. Một số điều chỉnh đã được thực hiện:
• Downsampling để giảm thiểu parameters.
• Hạn chế Attention trong Transformer về local region.
• Thay thế GeLU bằng ReLU activation hoặc bỏ bớt các Non-Linear Activation để giảm thiểu chi phí tính toán.
Kiến trúc cuối cùng: Ở những layers đầu là mạng CNN có tác dụng trích lọc đặc trưng trên local receptive field, những layer cuối cùng sử dụng attention + feed forward.
[Conv(3,3)] x 2 → [Conv(1,1) +DConv(3,3)+Conv(1,1)] x L1→ [Conv(1,1) +DConv(3,3)+Conv(1,1)] x L2 → [Rel-Attention + FFN] x L3 → [Rel-Attention + FFN] x L4 → Global Pool→FC → Output
4. Kết quả:
- Nhờ sự kết hợp với CNN mà trên những bộ dữ liệu kích thước nhỏ có thể vượt qua SOTA model là ViT.
- Mức độ cạnh tranh so với các based CNN network có cùng kích cỡ như EfficientNet-V2, NFNets.
- Vượt xa họ các kiến trúc attentions based Nets.
5. Suy nghĩ của tôi:
CoAtNet đã phá vỡ các quan niệm truyền thống khi huấn luyện các kiến trúc Transformer based trên dữ liệu image đó là cần dữ liệu lớn để đạt được SOTA. Điều này có ý nghĩa quan trọng trong các dự án thực tiễn vì không phải khi nào chúng ta cũng đủ vài chục hay thậm chí vài trăm triệu ảnh để huấn luyện. Ví dụ như với dữ liệu tốn kém chi phí gán nhãn như xử lý ảnh y tế chẳng hạn. Ý tưởng thiết kế những block kết hợp cả Convolution và Attention có thể phù hợp với dữ liệu dạng hình ảnh trong tương lai, thứ mà yêu cầu sự liên kết không gian 2 chiều chặt chẽ hơn so với trên văn bản.","[CoAtNet - Marrying convolution and attention for all data sizes] Transformer đang đạt được sự quan tâm đáng kể trong Computer Vision nhưng vẫn chưa hoàn toàn vượt qua được các kiến trúc CNN tốt nhất. Gần đây trong bài báo CoAtNet của nhóm tác giả nổi tiếng đến từ google là anh Quốc Lê và Mingxing Tan đã đưa ra một sự kết hợp giữa Convolution với Attention. Đây là một ý tưởng đơn giản nhưng độc đáo mà mình bị hấp dẫn và cảm thấy thú vị. Hiện tại thì mô hình tốt nhất của kiến trúc này là CoAtNet-7 đang xếp top 1 trên Leader Board của ImageNet, bỏ xa ConvNeXt, EfficientNetV2-L và ViT: https://paperswithcode.com/sota/image-classification-on-imagenet Mình cũng dành thời gian phân tích và implement lại mô hình này gần đây và hôm nay muốn review lại những ý chính của paper tới mọi người. Qua đó, mọi người có thể có thêm ý tưởng mới trong research cũng như apply vào các project của công ty, 1. Capture một vài tính chất quan trọng của CNN và Transformer. 1.1. Tính chất của CNN: - Translation Equivariance: Sự dịch chuyển của một vật thể trên ảnh đầu vào so với đầu ra là bất biến trên một mạng CNN. Nhờ tính chất này mà CNN có thể học tốt hơn với ít dữ liệu thông qua Augumentation. - Local Receptive Field: Các đặc trưng được CNN tổng hợp là dựa trên các local region của ảnh và những đặc trưng này có sự biến đổi theo độ sâu. Ở những level đầu là những đặc trưng bậc thấp mang tính phổ biến như các nét dọc, ngang, chéo và được tổng hợp thành những đặc trưng bậc cao giúp nhận diện class ở những layer sâu hơn. 1.2. Các tính chất của Transformer: - Global context: Transformer ban đầu được xây dựng là cho sequential data chẳng hạn như text data với core building block là Multi-head Attention. Nhờ đó nó có khả năng trích xuất thông tin global context rất tốt. Một feature token học được từ mạng là được tổng hợp từ toàn bộ các patches trên ảnh. - Global Receptive Field: Transformer hoạt động như một Global Receptive Field trong khi CNN hoạt động dựa trên Local Receptive Field. Tính chất này khiến Transformer có xu hướng dễ gặp phải overfitting vì dữ liệu của bạn khi thực hiện các Hard Augmentation như Rotation với góc lớn, Random Shuffling, CutMix thì Transformer vẫn có thể nhận ra được. Đó cũng là nguyên nhân khiến Transformer sẽ hoạt động tốt hơn so với CNN khi huấn luyện trên các bộ dữ liệu kích thước cực lớn (lên tới vài trăm triệu images) như ImageNet-21K, JFT-300M nhưng lại kém hơn trên ImageNet-1K (chỉ 1 triệu images). - Layer’s Features Similarity: Các đặc trưng mà Transformer học được có sự tương đồng giữa first layer và last layer, trong khi CNN thì khác biệt mạnh giữa chúng. 3. Ý tưởng của bài báo: Kết hợp đồng thời cả Convolution và Attention trong cùng một block và stack chúng trong một end-to-end network. Khi áp dụng kiến trúc Transformer có thể khiến cho số lượng tham số của mô hình tăng lên rất lớn. Như vậy chi phí tính toán sẽ được cân nhắc trong thiết kế của kiến trúc này. Một số điều chỉnh đã được thực hiện: • Downsampling để giảm thiểu parameters. • Hạn chế Attention trong Transformer về local region. • Thay thế GeLU bằng ReLU activation hoặc bỏ bớt các Non-Linear Activation để giảm thiểu chi phí tính toán. Kiến trúc cuối cùng: Ở những layers đầu là mạng CNN có tác dụng trích lọc đặc trưng trên local receptive field, những layer cuối cùng sử dụng attention + feed forward. [Conv(3,3)] x 2 → [Conv(1,1) +DConv(3,3)+Conv(1,1)] x L1→ [Conv(1,1) +DConv(3,3)+Conv(1,1)] x L2 → [Rel-Attention + FFN] x L3 → [Rel-Attention + FFN] x L4 → Global Pool→FC → Output 4. Kết quả: - Nhờ sự kết hợp với CNN mà trên những bộ dữ liệu kích thước nhỏ có thể vượt qua SOTA model là ViT. - Mức độ cạnh tranh so với các based CNN network có cùng kích cỡ như EfficientNet-V2, NFNets. - Vượt xa họ các kiến trúc attentions based Nets. 5. Suy nghĩ của tôi: CoAtNet đã phá vỡ các quan niệm truyền thống khi huấn luyện các kiến trúc Transformer based trên dữ liệu image đó là cần dữ liệu lớn để đạt được SOTA. Điều này có ý nghĩa quan trọng trong các dự án thực tiễn vì không phải khi nào chúng ta cũng đủ vài chục hay thậm chí vài trăm triệu ảnh để huấn luyện. Ví dụ như với dữ liệu tốn kém chi phí gán nhãn như xử lý ảnh y tế chẳng hạn. Ý tưởng thiết kế những block kết hợp cả Convolution và Attention có thể phù hợp với dữ liệu dạng hình ảnh trong tương lai, thứ mà yêu cầu sự liên kết không gian 2 chiều chặt chẽ hơn so với trên văn bản.",,,,,
"[Label Studio – 7.4k star*]
Label Studio là gì?
Label Studio là một công cụ label dữ liệu mã nguồn mở. Nó cho phép bạn gán nhãn các loại dữ liệu như âm thanh, hình ảnh, văn bản, video và chuỗi thời gian với giao diện người dùng đơn giản, dễ hiểu và dễ dàng xuất sang các định dạng mô hình khác nhau.
Đặc biệt, bạn có thể thiết lập mô hình Machine Learning với Label Studio hay tích hợp Label Studio với các công cụ hiện có của bạn.
Chi tiết mọi người xem thêm ở đây:
Web: https://labelstud.io/
Github: https://github.com/heartexlabs/label-studio","[Label Studio – 7.4k star*] Label Studio là gì? Label Studio là một công cụ label dữ liệu mã nguồn mở. Nó cho phép bạn gán nhãn các loại dữ liệu như âm thanh, hình ảnh, văn bản, video và chuỗi thời gian với giao diện người dùng đơn giản, dễ hiểu và dễ dàng xuất sang các định dạng mô hình khác nhau. Đặc biệt, bạn có thể thiết lập mô hình Machine Learning với Label Studio hay tích hợp Label Studio với các công cụ hiện có của bạn. Chi tiết mọi người xem thêm ở đây: Web: https://labelstud.io/ Github: https://github.com/heartexlabs/label-studio",,,,,
"Kính chào các bác, nhân có bạn hỏi về xử lý Image Message trong Chatbot nên em xin mạnh dạn chia sẻ cùng cả nhà luôn.
Hi vọng giúp được anh em mới học cách đọc ảnh, đưa qua model và trả lời người dùng.","Kính chào các bác, nhân có bạn hỏi về xử lý Image Message trong Chatbot nên em xin mạnh dạn chia sẻ cùng cả nhà luôn. Hi vọng giúp được anh em mới học cách đọc ảnh, đưa qua model và trả lời người dùng.",,,,,
A weird Italian method to do programming,A weird Italian method to do programming,,,,,
"[AI News – ConvNeXt]
Convolution is not dead: A ConvNet for the 2020s
Những năm 2020, nhận dạng hình ảnh bắt đầu bùng nổ với sự ra đời của Vision Transformers (ViT) nhanh chóng thay thế ConvNet, trở thành mô hình phân loại hình ảnh hiện đại nhất.
Tuy nhiên gần đây, một nhóm nghiên cứu đã khám phá ra một nhóm các mô hình ConvNet thuần túy gọi chung là ConvNeXt. ConvNeXt được xây dựng từ các ConvNet tiêu chuẩn, vượt trội so với Transformers về độ chính xác và khả năng mở rộng, đạt top 1 của Imagenet, vượt trội hơn Swin Transformers về COCO detection và ADE20K segmentation nhưng vẫn giữ được tính đơn giản và hiệu quả của ConvNets tiêu chuẩn.
Code Pytorch triển khai của ConvNeXt đã được Facebookresearch public.
Mọi người tham khảo thêm ở đây:
Paper: https://arxiv.org/abs/2201.03545
Github: https://github.com/facebookresearch/ConvNeXt
Youtube: https://www.youtube.com/watch?v=WvKsMI4Iemk","[AI News – ConvNeXt] Convolution is not dead: A ConvNet for the 2020s Những năm 2020, nhận dạng hình ảnh bắt đầu bùng nổ với sự ra đời của Vision Transformers (ViT) nhanh chóng thay thế ConvNet, trở thành mô hình phân loại hình ảnh hiện đại nhất. Tuy nhiên gần đây, một nhóm nghiên cứu đã khám phá ra một nhóm các mô hình ConvNet thuần túy gọi chung là ConvNeXt. ConvNeXt được xây dựng từ các ConvNet tiêu chuẩn, vượt trội so với Transformers về độ chính xác và khả năng mở rộng, đạt top 1 của Imagenet, vượt trội hơn Swin Transformers về COCO detection và ADE20K segmentation nhưng vẫn giữ được tính đơn giản và hiệu quả của ConvNets tiêu chuẩn. Code Pytorch triển khai của ConvNeXt đã được Facebookresearch public. Mọi người tham khảo thêm ở đây: Paper: https://arxiv.org/abs/2201.03545 Github: https://github.com/facebookresearch/ConvNeXt Youtube: https://www.youtube.com/watch?v=WvKsMI4Iemk",,,,,
"Chào mng!
Em đang tập tành Deep Learning. Cụ thể em đang code lại U-Net với bộ dataset các tế bào. Em nhận thấy đây không phải là bài toán phân loại bình thường nên không thể dùng các loss function bth.
Cụ thể label có 95% pixel thuộc class 1 (trắng) và 5% thuộc class 0 (đen).
Nếu output layer là 'Tất cả các pixel thuộc class 1' thì accury là 95%.
Mng đã từng làm bài toán này đề xuất cho em loss function phù hợp với ạ.",Chào mng! Em đang tập tành Deep Learning. Cụ thể em đang code lại U-Net với bộ dataset các tế bào. Em nhận thấy đây không phải là bài toán phân loại bình thường nên không thể dùng các loss function bth. Cụ thể label có 95% pixel thuộc class 1 (trắng) và 5% thuộc class 0 (đen). Nếu output layer là 'Tất cả các pixel thuộc class 1' thì accury là 95%. Mng đã từng làm bài toán này đề xuất cho em loss function phù hợp với ạ.,,,,,
Một thư viện mới công bố giúp việc train models nhanh hơn rất nhiều mà không làm giảm accuracy quá nhiều,Một thư viện mới công bố giúp việc train models nhanh hơn rất nhiều mà không làm giảm accuracy quá nhiều,,,,,
"Petar Veličković là ngôi sao đang lên trong lĩnh vực nghiên cứu Graph Neural Networks. Hiện anh đang làm việc tại DeepMind. Dưới đây là bài giảng giới thiệu về GNN kèm theo bài tập trong colab. Lưu ý: lần này code sử dụng TensorFlow
Bài giảng trên YouTube tại đây → goo.gle/3rt4l1B",Petar Veličković là ngôi sao đang lên trong lĩnh vực nghiên cứu Graph Neural Networks. Hiện anh đang làm việc tại DeepMind. Dưới đây là bài giảng giới thiệu về GNN kèm theo bài tập trong colab. Lưu ý: lần này code sử dụng TensorFlow Bài giảng trên YouTube tại đây → goo.gle/3rt4l1B,,,,,
"[SHARING - DOCKER FOR MACHINE LEARNING / DATA SCIENCE PROJECT]
Hello mọi người, tuy những công việc liên quan tới Develop & Operation thường sẽ cho team DevOps phụ trách nhưng ở một vài công ty vẫn yêu cầu AI engineer và Data Scientist phải biết những kiến thức cơ bản về Docker để deploy project. 
Trong quá trình tự học về Docker, mình thấy rất ít tài liệu hướng dẫn Docker dành riêng cho Data Scientist, nên trong quá trình vừa học vừa làm mình có record lại để các bạn bên AI / DS có thể tiếp cận Docker 1 cách dễ dàng. 

link-github:  https://github.com/DatacollectorVN/Docker-Tutorial

p/s 1: Mình sẽ liên tục update những mini project tiếp về Docker lên repo. 
p/s 2: Này cũng chỉ là quá trình mình vừa làm vừa học và record lại nên sẽ có chỗ mình hiểu sai, rất mong nhận được feedback của mọi người.","[SHARING - DOCKER FOR MACHINE LEARNING / DATA SCIENCE PROJECT] Hello mọi người, tuy những công việc liên quan tới Develop & Operation thường sẽ cho team DevOps phụ trách nhưng ở một vài công ty vẫn yêu cầu AI engineer và Data Scientist phải biết những kiến thức cơ bản về Docker để deploy project. Trong quá trình tự học về Docker, mình thấy rất ít tài liệu hướng dẫn Docker dành riêng cho Data Scientist, nên trong quá trình vừa học vừa làm mình có record lại để các bạn bên AI / DS có thể tiếp cận Docker 1 cách dễ dàng. link-github: https://github.com/DatacollectorVN/Docker-Tutorial p/s 1: Mình sẽ liên tục update những mini project tiếp về Docker lên repo. p/s 2: Này cũng chỉ là quá trình mình vừa làm vừa học và record lại nên sẽ có chỗ mình hiểu sai, rất mong nhận được feedback của mọi người.",,,,,
"Em chào mn ạ, mn ai có kinh nghiệm giúp e với😄
Mô hình CNN của e gặp phải hiện tượng là luôn dự đoán tất cả các class vào một class duy nhất. Tức là nếu e train nhận diện 5 class thì mô hình luôn dự đoán nó là class 1.
- E đã kiểm tra và chắc chắn việc load dữ liệu, tiền xử lí dữ liệu là đúng, nhãn và dữ liệu cũng đã tương ứng
- hàm loss e chọn là categorical_crossentropy
- layer Dense cuối cùng đã đúng số unit đầu ra (activation function là softmax)
- e cũng đã thử 1 số loại khởi tạo khác nhau và các activation function khác nhau ở các layer để chắc chắn k phải do việc khởi tạo ban đầu (hoặc do dying ReLU,..)
-learning rate e cũng đã thử những giá trị từ rất nhỏ cho đến rất lớn (1e-1 đến 1e-9) để đảm bảo k phải do vấn đề lựa chọn lr 😐
- số lượng data của mỗi class là như nhau
- bonus thêm là nếu e giữ nguyên đầu ra là 5 units (phân loại 5 class) và chỉ cho mô hình học dữ liệu của một class bất kì 1,3,5,..) thì mô hình lại dự đoán đúng
- e cũng đã thử thay đổi batch_size và thuật toán tối ưu (Adam, SGD) nhưng kết quả vẫn k khác
- framework e sử dụng là keras
- mô hình này là e tham khảo và tác giả của mô hình đã chạy đc nó với cùng bộ dữ liệu (số class nhiều hơn) và cho kết quả tốt (e có mail hỏi tác giả nhưng chưa đc reply :)))
😔E check cả tuần nay mà chưa thành công rồi ạ. E cảm ơn mn nhiều","Em chào mn ạ, mn ai có kinh nghiệm giúp e với Mô hình CNN của e gặp phải hiện tượng là luôn dự đoán tất cả các class vào một class duy nhất. Tức là nếu e train nhận diện 5 class thì mô hình luôn dự đoán nó là class 1. - E đã kiểm tra và chắc chắn việc load dữ liệu, tiền xử lí dữ liệu là đúng, nhãn và dữ liệu cũng đã tương ứng - hàm loss e chọn là categorical_crossentropy - layer Dense cuối cùng đã đúng số unit đầu ra (activation function là softmax) - e cũng đã thử 1 số loại khởi tạo khác nhau và các activation function khác nhau ở các layer để chắc chắn k phải do việc khởi tạo ban đầu (hoặc do dying ReLU,..) -learning rate e cũng đã thử những giá trị từ rất nhỏ cho đến rất lớn (1e-1 đến 1e-9) để đảm bảo k phải do vấn đề lựa chọn lr - số lượng data của mỗi class là như nhau - bonus thêm là nếu e giữ nguyên đầu ra là 5 units (phân loại 5 class) và chỉ cho mô hình học dữ liệu của một class bất kì 1,3,5,..) thì mô hình lại dự đoán đúng - e cũng đã thử thay đổi batch_size và thuật toán tối ưu (Adam, SGD) nhưng kết quả vẫn k khác - framework e sử dụng là keras - mô hình này là e tham khảo và tác giả của mô hình đã chạy đc nó với cùng bộ dữ liệu (số class nhiều hơn) và cho kết quả tốt (e có mail hỏi tác giả nhưng chưa đc reply :))) E check cả tuần nay mà chưa thành công rồi ạ. E cảm ơn mn nhiều",,,,,
"Có ai làm ở Google cho e hỏi với :D
Làm thế nào để google lens phân biệt được khi nào query_image là một object, 1 loại animal, hay một landmark nào đó? Có phải Google dùng google graph knowledge hay domain classification để làm việc này?","Có ai làm ở Google cho e hỏi với :D Làm thế nào để google lens phân biệt được khi nào query_image là một object, 1 loại animal, hay một landmark nào đó? Có phải Google dùng google graph knowledge hay domain classification để làm việc này?",,,,,
"Em chào mọi người,
Em là sv năm nhất ngành khmt, em muốn hỏi là học ML nên bắt đầu từ đâu ạ, đã có bạn trong group hỏi nên bắt đầu ntn rồi nhưng câu trả lời là học các môn giải tích, dstt, xstk nhưng chưa ai nói về việc sau khi học những nền tảng đó thì nên tiếp tục học gì ạ nên em muốn hỏi
Sau khi mình học các môn về toán rồi thì mình nên kiếm nguồn tài liệu nào để học tiếp ạ
Em cảm ơn mng","Em chào mọi người, Em là sv năm nhất ngành khmt, em muốn hỏi là học ML nên bắt đầu từ đâu ạ, đã có bạn trong group hỏi nên bắt đầu ntn rồi nhưng câu trả lời là học các môn giải tích, dstt, xstk nhưng chưa ai nói về việc sau khi học những nền tảng đó thì nên tiếp tục học gì ạ nên em muốn hỏi Sau khi mình học các môn về toán rồi thì mình nên kiếm nguồn tài liệu nào để học tiếp ạ Em cảm ơn mng",,,,,
"Chào các anh các chị, chả là em đang có 1 bộ dữ liệu ảnh với 2 class và em đang muốn sử dụng t-sne để phân tích, xem 2 class có khác nhau quá không; nhưng em vẫn chưa biết bắt đầu từ đâu nên mọi người có thể giúp em các bước làm được không ạ?
Em cảm ơn","Chào các anh các chị, chả là em đang có 1 bộ dữ liệu ảnh với 2 class và em đang muốn sử dụng t-sne để phân tích, xem 2 class có khác nhau quá không; nhưng em vẫn chưa biết bắt đầu từ đâu nên mọi người có thể giúp em các bước làm được không ạ? Em cảm ơn",,,,,
#hyperparameters,,#hyperparameters,,,,
"Em chào mọi người ,thầy đưa cho slide thuật toán kmeans và cho bài tập này ạ, em chưa biết cách giải quyết thế nào mọi người có thể giải thích giúp em như thế nào với ạ. Em cảm ơn
Phân cụm dữ liệu với bảng dữ liệu dưới đây","Em chào mọi người ,thầy đưa cho slide thuật toán kmeans và cho bài tập này ạ, em chưa biết cách giải quyết thế nào mọi người có thể giải thích giúp em như thế nào với ạ. Em cảm ơn Phân cụm dữ liệu với bảng dữ liệu dưới đây",,,,,
"Xin chào mọi người, em hiện đang có một tập dữ liệu vê số ca nhiễm Covid hằng ngày và tập dữ liệu tổng số lượt tiêm vắc xin từ trước đến nay hằng ngày. Em muốn tìm mối quan hệ giữa hai tập dữ liệu trên để mô tả sự ảnh hưởng của số lượt tiêm vắc xin đến số ca nhiễm, mọi người cho em hỏi có bộ đo hay mô hình nào để diễn đạt mối quan hệ trên không ạ, em xin cảm ơn.","Xin chào mọi người, em hiện đang có một tập dữ liệu vê số ca nhiễm Covid hằng ngày và tập dữ liệu tổng số lượt tiêm vắc xin từ trước đến nay hằng ngày. Em muốn tìm mối quan hệ giữa hai tập dữ liệu trên để mô tả sự ảnh hưởng của số lượt tiêm vắc xin đến số ca nhiễm, mọi người cho em hỏi có bộ đo hay mô hình nào để diễn đạt mối quan hệ trên không ạ, em xin cảm ơn.",,,,,
"Chào mọi người, em mới chỉ bắt đầu học về ML và đang thử làm 1 project deepfake về nó. Mọi người có thể cho em hỏi, em đang tham khảo tài liệu trên git trong file model.th. Có phải file này đã được train rồi không ạ(hình bên phải là file em mở bằng notepad)? Em có thể tìm được mô hình gốc của file này để mình tự train không ạ?","Chào mọi người, em mới chỉ bắt đầu học về ML và đang thử làm 1 project deepfake về nó. Mọi người có thể cho em hỏi, em đang tham khảo tài liệu trên git trong file model.th. Có phải file này đã được train rồi không ạ(hình bên phải là file em mở bằng notepad)? Em có thể tìm được mô hình gốc của file này để mình tự train không ạ?",,,,,
"[MaSSP 2022 TUYỂN MENTORS]
Ra đời từ năm 2016, trải qua 6 năm hoạt động, MaSSP luôn tự hào là một trong những trại hè được yêu thích nhất dành cho các bạn trẻ yêu khoa học. Để tạo nên thành công của MaSSP ngày hôm nay, không thể thiếu vai trò của các mentor - những người trực tiếp giảng dạy, truyền đam mê khoa học cho các bạn học sinh. Họ chính là những ngọn hải đăng, soi sáng và dẫn đường cho những con thuyền băng ra biển lớn.
Đối với trại hè năm nay, BTC MaSSP 2022 chính thức thông báo tuyển mentor đối với 2 môn học: Data Science và Architecture.
Chi tiết đơn ứng tuyển và JD các anh chị và các bạn truy cập bài viết phía dưới ạ.
Cảm ơn mọi người!","[MaSSP 2022 TUYỂN MENTORS] Ra đời từ năm 2016, trải qua 6 năm hoạt động, MaSSP luôn tự hào là một trong những trại hè được yêu thích nhất dành cho các bạn trẻ yêu khoa học. Để tạo nên thành công của MaSSP ngày hôm nay, không thể thiếu vai trò của các mentor - những người trực tiếp giảng dạy, truyền đam mê khoa học cho các bạn học sinh. Họ chính là những ngọn hải đăng, soi sáng và dẫn đường cho những con thuyền băng ra biển lớn. Đối với trại hè năm nay, BTC MaSSP 2022 chính thức thông báo tuyển mentor đối với 2 môn học: Data Science và Architecture. Chi tiết đơn ứng tuyển và JD các anh chị và các bạn truy cập bài viết phía dưới ạ. Cảm ơn mọi người!",,,,,
"Chào các anh chị, em hiện đang bắt đầu nghiên cứu về Text mining đánh giá sự thành công của các Tiền ảo thông qua các Whitepaper (Em chỉ làm phục vụ mục đích học tập). Em có tìm hiểu nhưng thấy có khá ít các tài liệu liên quan. Em không biết có anh chị nào đã và đang nghiên cứu mảng này có thể cho em liên hệ để học hỏi không ạ? Em xin cảm ơn ạ!","Chào các anh chị, em hiện đang bắt đầu nghiên cứu về Text mining đánh giá sự thành công của các Tiền ảo thông qua các Whitepaper (Em chỉ làm phục vụ mục đích học tập). Em có tìm hiểu nhưng thấy có khá ít các tài liệu liên quan. Em không biết có anh chị nào đã và đang nghiên cứu mảng này có thể cho em liên hệ để học hỏi không ạ? Em xin cảm ơn ạ!",,,,,
"[Face Anti-Spoofing][Dataset]

Bạn nào nghiên cứu về Face Anti-Spoofing hoặc các topic liên quan có thể tham khảo bộ dataset mới này (CelebA-Spoof). Bộ dữ liệu được xây dựng dựa trên dataset CelebA ( giới thiệu trong ""Deep Learning Face Attributes in the Wild""; ICCV 2015)

CelebA-Spoof bao gồm 625,000++ ảnh của 10 000 người. Hy vọng sẽ là nguồn dữ liệu tốt cho các bạn tham khảo và nghiên cứu.

Link Google Drive: https://drive.google.com/drive/folders/1OW_1bawO79pRqdVEVmBzp8HSxdSwln_Z

Link paper: ""CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations""; ECCV 2020
 https://arxiv.org/pdf/2007.12342.pdf","[Face Anti-Spoofing][Dataset] Bạn nào nghiên cứu về Face Anti-Spoofing hoặc các topic liên quan có thể tham khảo bộ dataset mới này (CelebA-Spoof). Bộ dữ liệu được xây dựng dựa trên dataset CelebA ( giới thiệu trong ""Deep Learning Face Attributes in the Wild""; ICCV 2015) CelebA-Spoof bao gồm 625,000++ ảnh của 10 000 người. Hy vọng sẽ là nguồn dữ liệu tốt cho các bạn tham khảo và nghiên cứu. Link Google Drive: https://drive.google.com/drive/folders/1OW_1bawO79pRqdVEVmBzp8HSxdSwln_Z Link paper: ""CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations""; ECCV 2020 https://arxiv.org/pdf/2007.12342.pdf",,,,,
"#PCA
Em chào mọi người,
Hiện tại em đang có bài tập về tìm PC1 và variance như hình dưới với covariance cho trước không biết là đề có nhầm lẫn gì không, việc tìm variance thì không có gì đặc biệt nhưng với việc tính PC1 thì em ko tìm ra giải pháp nào tốt được khi không có features matrix có ai có giải pháp nào giúp em không ạ ? em xin cảm ơn mọi người giúp đỡ","Em chào mọi người, Hiện tại em đang có bài tập về tìm PC1 và variance như hình dưới với covariance cho trước không biết là đề có nhầm lẫn gì không, việc tìm variance thì không có gì đặc biệt nhưng với việc tính PC1 thì em ko tìm ra giải pháp nào tốt được khi không có features matrix có ai có giải pháp nào giúp em không ạ ? em xin cảm ơn mọi người giúp đỡ",#PCA,,,,
"Chào buổi tối các bác! Hôm nay em đổi món tìm hiểu về DevOps nên mạnh dạn giới thiệu với anh em một chút về DevOps pipeline dùng để triển khai với các baid toán Python , ML . Hi vọng giúp được các anh em mới học.","Chào buổi tối các bác! Hôm nay em đổi món tìm hiểu về DevOps nên mạnh dạn giới thiệu với anh em một chút về DevOps pipeline dùng để triển khai với các baid toán Python , ML . Hi vọng giúp được các anh em mới học.",,,,,
"Chào mọi người, em đang tìm hiểu về Attention ạ. Tuy nhiên thì có một chỗ em thắc mắc là bước tìm sự tương quan . Ví dụ như trong câu "" I study at school"" thì việc xét sự tương quan giữa những từ trong câu rồi đánh trọng số cao dựa theo tiêu chí nào vậy ạ? Hơn nữa, em muốn hiểu rõ ý nghĩa của các ma trận Q,K,V là gì ạ. Mong mọi người giải đáp giúp em ạ. Em xin chân thành cảm ơn !!!","Chào mọi người, em đang tìm hiểu về Attention ạ. Tuy nhiên thì có một chỗ em thắc mắc là bước tìm sự tương quan . Ví dụ như trong câu "" I study at school"" thì việc xét sự tương quan giữa những từ trong câu rồi đánh trọng số cao dựa theo tiêu chí nào vậy ạ? Hơn nữa, em muốn hiểu rõ ý nghĩa của các ma trận Q,K,V là gì ạ. Mong mọi người giải đáp giúp em ạ. Em xin chân thành cảm ơn !!!",,,,,
"Khi nào nên học Machine Learning ???
Em chào mọi người,
Hiện tại em đang là SV năm 1 ngành KHMT.
Ở kì đầu tiên thì em có học ở trường về Intro Python, Intro C, giải tích 1 và Program Design - Abstraction (Java) + giải tích 2 (sem 2 năm 1)
Em có thử đọc qua 1 số tài liệu về Machine Learning như quyển ML Yearning nhưng em thấy nó khá nặng với nền tảng hiện tại em học.
Cho em hỏi các anh chị đã học Deep Learning, Machine Learning và AI thì thường pathway của anh chị là thế nào để có thể tiếp thu hiệu quả học phần này không. Cũng như trong 3 cái trên thì nên tiếp cận mảng nào đầu tiên.
Nếu được anh chị có thể recommend em vài tài liệu thích hợp cho beginner để tìm hiểu không?
Em cảm ơn.","Khi nào nên học Machine Learning ??? Em chào mọi người, Hiện tại em đang là SV năm 1 ngành KHMT. Ở kì đầu tiên thì em có học ở trường về Intro Python, Intro C, giải tích 1 và Program Design - Abstraction (Java) + giải tích 2 (sem 2 năm 1) Em có thử đọc qua 1 số tài liệu về Machine Learning như quyển ML Yearning nhưng em thấy nó khá nặng với nền tảng hiện tại em học. Cho em hỏi các anh chị đã học Deep Learning, Machine Learning và AI thì thường pathway của anh chị là thế nào để có thể tiếp thu hiệu quả học phần này không. Cũng như trong 3 cái trên thì nên tiếp cận mảng nào đầu tiên. Nếu được anh chị có thể recommend em vài tài liệu thích hợp cho beginner để tìm hiểu không? Em cảm ơn.",,,,,
"Hi mọi người,
Em đang thắc mắc là sau khi có 1 spam filtering model rồi thì làm sao để integrate nó với mailbox ạ (google mail,..)? Em search google không ra được gì có ích lắm nên mong mọi người giúp đỡ","Hi mọi người, Em đang thắc mắc là sau khi có 1 spam filtering model rồi thì làm sao để integrate nó với mailbox ạ (google mail,..)? Em search google không ra được gì có ích lắm nên mong mọi người giúp đỡ",,,,,
"IEEE WCCI 2022 Competition on Meta-learning from Learning Curves is now open
PRIZES: $1,000
Closing date: February 10
Keywords: AutoML, Meta-learning, Reinforcement Learning, Learning Curves
---------------------
Dear Meta-learning Enthusiasts,
Following the great success of the NeurIPS 2021 meta-learning challenge, we are organizing a new challenge on meta-learning from learning curves.
Opening: January 6
End: February 10
More information on our page: https://metalearning.chalearn.org/
The organizing team
---------------------
Medium article:
https://medium.com/@hungnm.vnu/meta-learning-from-learning-curves-ieee-wcci-2022-competition-5e1932742644","IEEE WCCI 2022 Competition on Meta-learning from Learning Curves is now open PRIZES: $1,000 Closing date: February 10 Keywords: AutoML, Meta-learning, Reinforcement Learning, Learning Curves --------------------- Dear Meta-learning Enthusiasts, Following the great success of the NeurIPS 2021 meta-learning challenge, we are organizing a new challenge on meta-learning from learning curves. Opening: January 6 End: February 10 More information on our page: https://metalearning.chalearn.org/ The organizing team --------------------- Medium article: https://medium.com/@hungnm.vnu/meta-learning-from-learning-curves-ieee-wcci-2022-competition-5e1932742644",,,,,
"Team Train4Ever xin chia sẻ solution đứng thứ 7 (gold medal) trong cuộc thi Sartorius Cell Instance Segmentation. Đây là một cuộc thi về instance segmentation - các đối tượng cùng dù cùng 1 class cũng sẽ được phân biệt với nhau.
Giải thích solution: https://www.kaggle.com/c/sartorius-cell-instance-segmentation/discussion/298002
Code: https://github.com/gallegi/T4E_Sartorius_Cell_InstanceSegmentation
Mong được học hỏi từ góp ý của mọi người.
Special thanks to teammates: Khánh Vũ Duy Nhật Trường Bùi Đàm Trọng Tuyên",Team Train4Ever xin chia sẻ solution đứng thứ 7 (gold medal) trong cuộc thi Sartorius Cell Instance Segmentation. Đây là một cuộc thi về instance segmentation - các đối tượng cùng dù cùng 1 class cũng sẽ được phân biệt với nhau. Giải thích solution: https://www.kaggle.com/c/sartorius-cell-instance-segmentation/discussion/298002 Code: https://github.com/gallegi/T4E_Sartorius_Cell_InstanceSegmentation Mong được học hỏi từ góp ý của mọi người. Special thanks to teammates: Khánh Vũ Duy Nhật Trường Bùi Đàm Trọng Tuyên,,,,,
"Chào các bạn trong forum, mình đang làm một project nhỏ về điểm danh nhân viên trong công ty bằng khuôn mặt, ý tưởng là quét camera để detect khuôn mặt (face localization) rồi decode khuôn mặt đó bằng một mạng CNN, so sánh với DB để xem đó là ai (face recognition). Phần face recognition tạm thời ổn, nhưng phần face detection thì chỉ nhận dang được khi khuôn mặt đối diện camera, nếu xoay đi các hướng hoặc bị che mất một phần thì fail (mình đã test với HOG và HAAR). Các cao nhân ai có kinh nghiệm vụ face detection này làm ơn giúp mình với. Xin cảm ơn!
Edit: Mình có tìm được một paper của Yahoo khá hay https://arxiv.org/pdf/1502.02766.pdf nhưng không thấy implementation của nó.","Chào các bạn trong forum, mình đang làm một project nhỏ về điểm danh nhân viên trong công ty bằng khuôn mặt, ý tưởng là quét camera để detect khuôn mặt (face localization) rồi decode khuôn mặt đó bằng một mạng CNN, so sánh với DB để xem đó là ai (face recognition). Phần face recognition tạm thời ổn, nhưng phần face detection thì chỉ nhận dang được khi khuôn mặt đối diện camera, nếu xoay đi các hướng hoặc bị che mất một phần thì fail (mình đã test với HOG và HAAR). Các cao nhân ai có kinh nghiệm vụ face detection này làm ơn giúp mình với. Xin cảm ơn! Edit: Mình có tìm được một paper của Yahoo khá hay https://arxiv.org/pdf/1502.02766.pdf nhưng không thấy implementation của nó.",,,,,
"Loạt bài giảng trong học kì mùa thu 2021 của ĐH Stanford về Graph Neural Networks, kèm cả slides, và Colab.","Loạt bài giảng trong học kì mùa thu 2021 của ĐH Stanford về Graph Neural Networks, kèm cả slides, và Colab.",,,,,
"Chào mn, cho mình hỏi trong nhóm có ai làm hướng nghiên cứu về normalizing flow, invertiable neural network không? Cho mình pm hỏi 1 số thứ được không ạ.","Chào mn, cho mình hỏi trong nhóm có ai làm hướng nghiên cứu về normalizing flow, invertiable neural network không? Cho mình pm hỏi 1 số thứ được không ạ.",,,,,
"Chào mọi người ạ,
Hiện em đang đọc quyển ESL, đọc đến trang 74, 75 thì tác giả có đề cập đến khái niệm L1 arc length trong context của Lasso Regression và LAR. Thì em đang chưa hiểu khái niệm của L1 arc length có ý nghĩa là gì, em đang phỏng đoán nó là độ dài đường biên trên miền của beta dưới ràng buộc ||beta||1 <= t, và mỗi L1 arc length sẽ tương ứng với 1 giá trị t, và mỗi giá trị t lại tương ứng với một solution của beta; hay nói cách khác thì profile của beta là một hàm của L1 arc length. Nhưng hiểu như vậy có vẻ chỉ make sense với Lasso, còn với LAR thì không có ràng buộc t nào lên ||beta||1, em đang nghĩ khi đó chưa chắc profile của beta đã là một hàm của L1 arc length. Em không biết mình có hiểu sai chỗ nào không, hi vọng mọi người giải đáp ạ.
Sách có thể truy cập tại: https://hastie.su.domains/Papers/ESLII.pdf","Chào mọi người ạ, Hiện em đang đọc quyển ESL, đọc đến trang 74, 75 thì tác giả có đề cập đến khái niệm L1 arc length trong context của Lasso Regression và LAR. Thì em đang chưa hiểu khái niệm của L1 arc length có ý nghĩa là gì, em đang phỏng đoán nó là độ dài đường biên trên miền của beta dưới ràng buộc ||beta||1 <= t, và mỗi L1 arc length sẽ tương ứng với 1 giá trị t, và mỗi giá trị t lại tương ứng với một solution của beta; hay nói cách khác thì profile của beta là một hàm của L1 arc length. Nhưng hiểu như vậy có vẻ chỉ make sense với Lasso, còn với LAR thì không có ràng buộc t nào lên ||beta||1, em đang nghĩ khi đó chưa chắc profile của beta đã là một hàm của L1 arc length. Em không biết mình có hiểu sai chỗ nào không, hi vọng mọi người giải đáp ạ. Sách có thể truy cập tại: https://hastie.su.domains/Papers/ESLII.pdf",,,,,
"#Ask #Attention
Chào mọi người, em đang tìm hiểu về Attention ạ. Tuy nhiên thì có một chỗ em thắc mắc là bước tìm sự tương quan . Ví dụ như trong câu "" I study at school"" thì việc xét sự tương quan giữa những từ trong câu rồi đánh trọng số cao dựa theo tiêu chí nào vậy ạ?
Hơn nữa, em muốn hiểu rõ ý nghĩa của Query,Key,Value là gì ạ. Mong mọi người giải đáp giúp em ạ. Em xin chân thành cảm ơn !!!","Chào mọi người, em đang tìm hiểu về Attention ạ. Tuy nhiên thì có một chỗ em thắc mắc là bước tìm sự tương quan . Ví dụ như trong câu "" I study at school"" thì việc xét sự tương quan giữa những từ trong câu rồi đánh trọng số cao dựa theo tiêu chí nào vậy ạ? Hơn nữa, em muốn hiểu rõ ý nghĩa của Query,Key,Value là gì ạ. Mong mọi người giải đáp giúp em ạ. Em xin chân thành cảm ơn !!!",#Ask	#Attention,,,,
Em đang tìm hiểu về nhận diện đối tượng và gặp vấn đề về bộ dữ liệu ạ. Mọi người có ai đã làm qua cho em hỏi là có bộ dữ liệu nhận diện khuôn mặt nào mà đủ tốt để sử dụng cho nhận diện và cho việc tách lỗ tai (sử dụng ear detection) (em cần thông tin này ạ). Em cảm ơn.,Em đang tìm hiểu về nhận diện đối tượng và gặp vấn đề về bộ dữ liệu ạ. Mọi người có ai đã làm qua cho em hỏi là có bộ dữ liệu nhận diện khuôn mặt nào mà đủ tốt để sử dụng cho nhận diện và cho việc tách lỗ tai (sử dụng ear detection) (em cần thông tin này ạ). Em cảm ơn.,,,,,
"Xin chào mọi người.
Hiện tại mình đang làm đề tài về fact verification. Mình có tìm tool về reverse image search - từ ảnh, tìm website chứa ảnh đó để thu về toàn bộ thông tin trên trang web. Mình có tìm thấy Google Reverse Image Search, nhưng hiện tại API này đã ngưng. Mình có tìm thấy SerpAPI nhưng phí của SerpAPI hơi đắt. Mình muốn hỏi có bạn nào có biết tool hay code github nào hỗ trợ vấn đề này không ạ.
Mình xin cảm ơn ạ.","Xin chào mọi người. Hiện tại mình đang làm đề tài về fact verification. Mình có tìm tool về reverse image search - từ ảnh, tìm website chứa ảnh đó để thu về toàn bộ thông tin trên trang web. Mình có tìm thấy Google Reverse Image Search, nhưng hiện tại API này đã ngưng. Mình có tìm thấy SerpAPI nhưng phí của SerpAPI hơi đắt. Mình muốn hỏi có bạn nào có biết tool hay code github nào hỗ trợ vấn đề này không ạ. Mình xin cảm ơn ạ.",,,,,
"Chào mn, em đang đọc 1 blog post về language model GPT-3. Trong phần limitation ở ảnh dưới có ghi ""GPT-3 là model autoregressive, không phải bi-direct như BERT, vậy nên phù hợp với các task ""in-context"" learning-based hơn là các task cần fine tuning model""
Em đang không hiểu câu này, ""in-context"" learning-based là thế nào, một số task ví dụ và tại sao model autoregressive lại phù hợp với nó.
Mong mn giải đáp giúp ạ, em cảm ơn.
Link bài viết:
https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/","Chào mn, em đang đọc 1 blog post về language model GPT-3. Trong phần limitation ở ảnh dưới có ghi ""GPT-3 là model autoregressive, không phải bi-direct như BERT, vậy nên phù hợp với các task ""in-context"" learning-based hơn là các task cần fine tuning model"" Em đang không hiểu câu này, ""in-context"" learning-based là thế nào, một số task ví dụ và tại sao model autoregressive lại phù hợp với nó. Mong mn giải đáp giúp ạ, em cảm ơn. Link bài viết: https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/",,,,,
"Em chào mọi người ạ.
Hiện tại em đang làm project bài toán nhận diện hành động của người dựa trên thông tin khung xương. Em đã làm xong bước training model để nhận dạng hành động.
Model em sử dụng là mạng GCN, đầu vào của mạng là chuỗi chỉ thực hiện 1 hành động ( ví dụ hành động writing thì sẽ đưa vào mạng là chuỗi frame khung xương thực hiện hành động writing).
Bây giờ, em muốn có thể nhận dạng chuỗi thực hiện gồm nhiều hành động trong video thực ( như video bên dưới ), ý tưởng của em là có thể sử dụng cửa sổ trượt để áp model vào. Tuy nhiên, có một vấn đề là trong phần training model đó em không thực hiện training với chuỗi đầu vào có nhãn là no action. Mà trong video thực tế, thì có thể người thực hiện có những quãng nghỉ ( no action) ở những thời điểm bất kì.
Vì vậy, em có một câu hỏi mọi người giải đáp, đó là làm cách nào để mình có thể phân tách được giữa các hành động với những quãng no-action không ạ ( Em ko có dữ liệu no action để training). Liệu mình có thể dựa trên rule-based với chuỗi đầu vào để phát hiện no-action ko ạ, hay bắt buộc phải kiếm dữ liệu no action ạ.
Rất mong nhận được sự giúp đỡ của mọi người. Em cảm ơn mọi người nhiều","Em chào mọi người ạ. Hiện tại em đang làm project bài toán nhận diện hành động của người dựa trên thông tin khung xương. Em đã làm xong bước training model để nhận dạng hành động. Model em sử dụng là mạng GCN, đầu vào của mạng là chuỗi chỉ thực hiện 1 hành động ( ví dụ hành động writing thì sẽ đưa vào mạng là chuỗi frame khung xương thực hiện hành động writing). Bây giờ, em muốn có thể nhận dạng chuỗi thực hiện gồm nhiều hành động trong video thực ( như video bên dưới ), ý tưởng của em là có thể sử dụng cửa sổ trượt để áp model vào. Tuy nhiên, có một vấn đề là trong phần training model đó em không thực hiện training với chuỗi đầu vào có nhãn là no action. Mà trong video thực tế, thì có thể người thực hiện có những quãng nghỉ ( no action) ở những thời điểm bất kì. Vì vậy, em có một câu hỏi mọi người giải đáp, đó là làm cách nào để mình có thể phân tách được giữa các hành động với những quãng no-action không ạ ( Em ko có dữ liệu no action để training). Liệu mình có thể dựa trên rule-based với chuỗi đầu vào để phát hiện no-action ko ạ, hay bắt buộc phải kiếm dữ liệu no action ạ. Rất mong nhận được sự giúp đỡ của mọi người. Em cảm ơn mọi người nhiều",,,,,
"Chào mọi người,
Mình đến từ team NTBN bao gồm 1 thành viên là mình Nguyễn Nhật Hoàng. Mình xin chia sẻ solution mình đã áp dụng để giành được top 1 private test tại Zalo AI Challenge 2021 task 5k compliance. Mình cảm thấy rất may mắn khi đạt được top1 này, điểm mấu chốt có lẽ là do việc lựa chọn mô hình và tuning tham số.
Giải pháp của mình bao gồm :
1. Chuẩn bị dữ liệu
Sử dụng yolov5x để detect person, sửa nhãn những bức hình có lượng người <=1 nhưng có nhãn distance là 0.
Stratified CV 5-fold
2. Huấn luyện
Mô hình mình sử dụng là Swin Transformer large 193M parameter.
Optimizer: AdamW ( weight_decay= 1e-5).
Image size: 384x384.
Training 20 epochs có early stopping.
Augmentation: Color jitter, random erasing, mixup, cutmix, and random augment from timm.
Mình tập trung vào tuning các tham số drop path rate, base learning rate, warmup learning rate và min learning rate.
Toàn bộ các tham số có trong file config và trong file bash chạy huấn luyện từng fold.
3. Dự đoán
Với mask và distance mình tính trung bình kết của của 5 folds.
Kết quả dự đoán 5K là kết hợp mask và distance với ngưỡng mask là 0.5 và ngưỡng distance là 0.44
Toàn bộ source code từ clean data đến train và inference mình đã để hết trong repo để mọi người tham khảo
Cảm ơn mọi người đã quan tâm.","Chào mọi người, Mình đến từ team NTBN bao gồm 1 thành viên là mình Nguyễn Nhật Hoàng. Mình xin chia sẻ solution mình đã áp dụng để giành được top 1 private test tại Zalo AI Challenge 2021 task 5k compliance. Mình cảm thấy rất may mắn khi đạt được top1 này, điểm mấu chốt có lẽ là do việc lựa chọn mô hình và tuning tham số. Giải pháp của mình bao gồm : 1. Chuẩn bị dữ liệu Sử dụng yolov5x để detect person, sửa nhãn những bức hình có lượng người <=1 nhưng có nhãn distance là 0. Stratified CV 5-fold 2. Huấn luyện Mô hình mình sử dụng là Swin Transformer large 193M parameter. Optimizer: AdamW ( weight_decay= 1e-5). Image size: 384x384. Training 20 epochs có early stopping. Augmentation: Color jitter, random erasing, mixup, cutmix, and random augment from timm. Mình tập trung vào tuning các tham số drop path rate, base learning rate, warmup learning rate và min learning rate. Toàn bộ các tham số có trong file config và trong file bash chạy huấn luyện từng fold. 3. Dự đoán Với mask và distance mình tính trung bình kết của của 5 folds. Kết quả dự đoán 5K là kết hợp mask và distance với ngưỡng mask là 0.5 và ngưỡng distance là 0.44 Toàn bộ source code từ clean data đến train và inference mình đã để hết trong repo để mọi người tham khảo Cảm ơn mọi người đã quan tâm.",,,,,
"Em chào mọi người, mọi người có ai đang dùng colab pro+ mà đợt dạo này allocate chỉ được P100 hay T4 không ạ, em từ lúc mua đến giờ chưa allocate V100 hay A100 mà chỉ được 2 máy trên. Em muốn thuê GPU mà có thể thử nghiệm làm được vài tuần, mọi người có thể gợi ý dịch vụ mọi người đang dùng thấy tốt không ạ.
Em cảm ơn mọi người ạ.","Em chào mọi người, mọi người có ai đang dùng colab pro+ mà đợt dạo này allocate chỉ được P100 hay T4 không ạ, em từ lúc mua đến giờ chưa allocate V100 hay A100 mà chỉ được 2 máy trên. Em muốn thuê GPU mà có thể thử nghiệm làm được vài tuần, mọi người có thể gợi ý dịch vụ mọi người đang dùng thấy tốt không ạ. Em cảm ơn mọi người ạ.",,,,,
"Mn cho e hỏi chút với ạ! Có ai đã từng làm việc với bộ dataset NTU-60/120 chưa ạ? Vì bộ này quá lớn nên e k tải về để training đc, mn có biết link nào chứa subset của bộ này hoặc có cách nào lấy 1 phần nhỏ của nó k ạ?
Em cảm ơn mn nhiều ❤️","Mn cho e hỏi chút với ạ! Có ai đã từng làm việc với bộ dataset NTU-60/120 chưa ạ? Vì bộ này quá lớn nên e k tải về để training đc, mn có biết link nào chứa subset của bộ này hoặc có cách nào lấy 1 phần nhỏ của nó k ạ? Em cảm ơn mn nhiều",,,,,
"Mình mới publish 1 bài về trích xuất dữ liệu từ hóa đơn. Ý tưởng khá đơn giản và dễ dàng reproduce, dựa trên thông tin về từ khóa cho các trường dữ liệu (ví dụ: invoice number, invoice date,...), kiểu dữ liệu có cấu trúc (date, phone number, VAT number,...), entities (NER, ORG, LOC), address (sử dụng thư viện libpostal), và vị trí tương quan giữa các blocks. Các bạn có thể tham khảo trên link này:
https://authors.elsevier.com/c/1eJ-o3I06IZDW7
Link đọc và download miễn phí cho đến 16/02/2022.","Mình mới publish 1 bài về trích xuất dữ liệu từ hóa đơn. Ý tưởng khá đơn giản và dễ dàng reproduce, dựa trên thông tin về từ khóa cho các trường dữ liệu (ví dụ: invoice number, invoice date,...), kiểu dữ liệu có cấu trúc (date, phone number, VAT number,...), entities (NER, ORG, LOC), address (sử dụng thư viện libpostal), và vị trí tương quan giữa các blocks. Các bạn có thể tham khảo trên link này: https://authors.elsevier.com/c/1eJ-o3I06IZDW7 Link đọc và download miễn phí cho đến 16/02/2022.",,,,,
"Are you working on/interesting in multi-agent learning with strategic agents? Please consider submitting your work to our workshop at AAMAS 2022:
https://minbiaohan.github.io/LSA/index.html
Important dates:
Paper Submission deadline: 30 January 2022
Notification of Acceptance: 27 February 2022
More details on the website.",Are you working on/interesting in multi-agent learning with strategic agents? Please consider submitting your work to our workshop at AAMAS 2022: https://minbiaohan.github.io/LSA/index.html Important dates: Paper Submission deadline: 30 January 2022 Notification of Acceptance: 27 February 2022 More details on the website.,,,,,
"Mình có tổng hợp một repository các trang research paper ML, DL, AI, Data; về task, datasets, idea, state-of-the-art dành cho những bạn mới bắt đầu (mình cũng vậy) tìm kiếm các nguồn tài liệu chất lượng. Hi vọng mọi người sẽ thích và chúc mừng năm mới anh Tiệp và mọi người trong group MLCB!
Note: Mọi người recommend research site thì comment ở dưới để mình add vào nhé, cám ơn mọi người !","Mình có tổng hợp một repository các trang research paper ML, DL, AI, Data; về task, datasets, idea, state-of-the-art dành cho những bạn mới bắt đầu (mình cũng vậy) tìm kiếm các nguồn tài liệu chất lượng. Hi vọng mọi người sẽ thích và chúc mừng năm mới anh Tiệp và mọi người trong group MLCB! Note: Mọi người recommend research site thì comment ở dưới để mình add vào nhé, cám ơn mọi người !",,,,,
Em xin chào thầy cô và mọi người ạ! Em có đang tìm hiểu về thuật toán phân cụm K-means và tìm cách **THÊM RÀNG BUỘC ĐƯỜNG KÍNH CỤM** vào! Mọi người nếu ai biết thì cho em xin gợi ý được không ạ? Em cảm ơn rất nhiều ạ!,Em xin chào thầy cô và mọi người ạ! Em có đang tìm hiểu về thuật toán phân cụm K-means và tìm cách **THÊM RÀNG BUỘC ĐƯỜNG KÍNH CỤM** vào! Mọi người nếu ai biết thì cho em xin gợi ý được không ạ? Em cảm ơn rất nhiều ạ!,,,,,
"#ask #pandas
Em có một bảng pandas 1 có các dữ liệu bị trùng, và muốn biến đổi nó thành bảng 2 (như hình minh hoạ). Em có thử dùng các hàm groupby nhưng không như mong muốn. Em cũng nghĩ đến việc define một hàm xử lý rồi apply cho nó nhưng vẫn chưa biết xử lý thế nào. Hy vọng mọi người gợi ý hướng đi hoặc cho em vài keyword để xử lý vấn đề này ạ.
Link notebook và data bên dưới nếu mọi người muốn xem kỹ hơn:
Notebook: https://github.com/Brian-Doan/movie-rs/blob/main/movie_rs.ipynb
Data: https://drive.google.com/drive/folders/1V6IO3GX4lPKDrnVZ2v1KnTHdmbjeQQVX?usp=sharing","Em có một bảng pandas 1 có các dữ liệu bị trùng, và muốn biến đổi nó thành bảng 2 (như hình minh hoạ). Em có thử dùng các hàm groupby nhưng không như mong muốn. Em cũng nghĩ đến việc define một hàm xử lý rồi apply cho nó nhưng vẫn chưa biết xử lý thế nào. Hy vọng mọi người gợi ý hướng đi hoặc cho em vài keyword để xử lý vấn đề này ạ. Link notebook và data bên dưới nếu mọi người muốn xem kỹ hơn: Notebook: https://github.com/Brian-Doan/movie-rs/blob/main/movie_rs.ipynb Data: https://drive.google.com/drive/folders/1V6IO3GX4lPKDrnVZ2v1KnTHdmbjeQQVX?usp=sharing",#ask	#pandas,,,,
"[Deep Learning List Reading]
Đọc sách là một trong những cách tốt nhất để hiểu nền tảng của Machine Learning và Deep Learning. Thông qua sách có thể cung cấp cho bạn sự hiểu biết lý thuyết cần thiết để giúp bạn học các khái niệm mới nhanh hơn trong tương lai. Bên dưới là danh sách tuyển chọn những cuốn sách về Deep Learning năm 2021 dành cho beginner của admin. Xin được chia sẻ tới bạn đọc:
1. AI and Machine Learning for Coders: Tác giả Laurence Moroney, hiện đang làm việc tại google. Cuốn này được forewarded bởi Andrew Ng
https://www.oreilly.com/library/view/ai-and-machine/9781492078180/
2. Deep Learning with Python, Second Edition: Cuốn này của François Chollet, cha đẻ của tensorflow-keras.
https://www.manning.com/books/deep-learning-with-python-second-edition
3. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition: Cuốn này của Aurélien Géron, được đánh giá là một trong những cuốn sách hay nhất về Machine Learning. Phần I đã được dịch ra bản Tiếng Việt bởi nhóm dịch thuật Machine Learning Cơ bản.
https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/
4. Deep Learning: Cuốn sách gối đầu của bao thế hệ sinh viên về Deep Learning của ba tác giả Ian Goodfellow, Yoshua Bengio and Aaron Courville. Để thúc đẩy sự phát triển cộng đồng, bản ebook của cuốn sách đã được tác giả public. Sách hiện đã có bản dịch tiếng Việt của nhóm DLBOOKVN.
https://www.deeplearningbook.org/
5. Neural Networks and Deep Learning: Cuốn này của ‪Michael Nielsen, một nhà nghiên cứu nổi tiếng về AI. Cuốn này trình bày khá sâu về lý thuyết giống như cuốn Deep Learning.
http://neuralnetworksanddeeplearning.com/
6. Learning TensorFlow.js: Cuốn này được viết bởi Gant Laborde và được forewarded bởi Laurence Moroney, tác giả của cuốn sách thứ nhất.
https://www.oreilly.com/library/view/learning-tensorflowjs/9781492090786/
7. Deep Learning with JavaScript: Được viết bởi nhóm tác giả Shanqing Cai, Stanley Bileschi, Eric D. Nielsen with Francois Chollet nhằm hướng dẫn cách phát triển các ứng dụng Deep Learning trên javascript. Phù hợp với web developer muốn ứng dụng thêm AI.
https://www.manning.com/books/deep-learning-with-javascript
8. Natural Language Processing with Transformers: Nếu muốn học về các ứng dụng trong NLP và kiến trúc Transformers thì đây là cuốn sách hết sức tuyệt vời.
https://www.oreilly.com/library/view/natural-language-processing/9781098103231/
----------------------------------------------------------------------------
Nhằm củng cố kiến thức về Deep Learning, bạn đọc có thể đăng kí course 3 - Deep Learning khai giảng 16/1/2021 theo đường link: https://forms.gle/HveenGg54Gc8yj238","[Deep Learning List Reading] Đọc sách là một trong những cách tốt nhất để hiểu nền tảng của Machine Learning và Deep Learning. Thông qua sách có thể cung cấp cho bạn sự hiểu biết lý thuyết cần thiết để giúp bạn học các khái niệm mới nhanh hơn trong tương lai. Bên dưới là danh sách tuyển chọn những cuốn sách về Deep Learning năm 2021 dành cho beginner của admin. Xin được chia sẻ tới bạn đọc: 1. AI and Machine Learning for Coders: Tác giả Laurence Moroney, hiện đang làm việc tại google. Cuốn này được forewarded bởi Andrew Ng https://www.oreilly.com/library/view/ai-and-machine/9781492078180/ 2. Deep Learning with Python, Second Edition: Cuốn này của François Chollet, cha đẻ của tensorflow-keras. https://www.manning.com/books/deep-learning-with-python-second-edition 3. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition: Cuốn này của Aurélien Géron, được đánh giá là một trong những cuốn sách hay nhất về Machine Learning. Phần I đã được dịch ra bản Tiếng Việt bởi nhóm dịch thuật Machine Learning Cơ bản. https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ 4. Deep Learning: Cuốn sách gối đầu của bao thế hệ sinh viên về Deep Learning của ba tác giả Ian Goodfellow, Yoshua Bengio and Aaron Courville. Để thúc đẩy sự phát triển cộng đồng, bản ebook của cuốn sách đã được tác giả public. Sách hiện đã có bản dịch tiếng Việt của nhóm DLBOOKVN. https://www.deeplearningbook.org/ 5. Neural Networks and Deep Learning: Cuốn này của ‪Michael Nielsen, một nhà nghiên cứu nổi tiếng về AI. Cuốn này trình bày khá sâu về lý thuyết giống như cuốn Deep Learning. http://neuralnetworksanddeeplearning.com/ 6. Learning TensorFlow.js: Cuốn này được viết bởi Gant Laborde và được forewarded bởi Laurence Moroney, tác giả của cuốn sách thứ nhất. https://www.oreilly.com/library/view/learning-tensorflowjs/9781492090786/ 7. Deep Learning with JavaScript: Được viết bởi nhóm tác giả Shanqing Cai, Stanley Bileschi, Eric D. Nielsen with Francois Chollet nhằm hướng dẫn cách phát triển các ứng dụng Deep Learning trên javascript. Phù hợp với web developer muốn ứng dụng thêm AI. https://www.manning.com/books/deep-learning-with-javascript 8. Natural Language Processing with Transformers: Nếu muốn học về các ứng dụng trong NLP và kiến trúc Transformers thì đây là cuốn sách hết sức tuyệt vời. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/ ---------------------------------------------------------------------------- Nhằm củng cố kiến thức về Deep Learning, bạn đọc có thể đăng kí course 3 - Deep Learning khai giảng 16/1/2021 theo đường link: https://forms.gle/HveenGg54Gc8yj238",,,,,
Bookmarked.,Bookmarked.,,,,,
"chào mọi người, em muốn học video khóa cs231n của năm 2020 hoặc 2021 thì kiếm ở đâu được ạ, em tìm trên google thấy toàn video từ năm 2017","chào mọi người, em muốn học video khóa cs231n của năm 2020 hoặc 2021 thì kiếm ở đâu được ạ, em tìm trên google thấy toàn video từ năm 2017",,,,,
"Đây là bài viết thú vị về tổng kết những thành tựu về nghiên cứu Graph Neural Networks trong năm 2021 cũng như dự đoán xu thế về GNN trong năm 2022 https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0
Trong bài viết này có câu trả lời của một số bạn hỏi về độ sâu, độ rộng khi thiết kế GNNs mà trong bài giảng tại Viện Toán Cao cấp của Gs Nguyễn Hùng Sơn vào ngày 30/12/2021. Có một số nghiên cứu chỉ ra rằng GNNs chỉ cần từ 2-4 layers là ổn, càng nhiều layers càng làm giảm hiệu năng của mô hình.
Với kinh nghiệm cá nhân, việc train GNNs không cần quá quan tâm tới việc tinh chỉnh hyperparameters. Mình thấy GNNs sẽ hội tụ tốt với hàm ADAM, Learning rate ~0.001 (không cần Learning scheduler). Mình thử dùng một số hàm Optimizer khác kết hợp cùng với scheduler thì models rất khó hội tụ!
Một trong những khó khăn nữa mà có một số bạn có hỏi riêng đó là nhận diện đúng dạng bài toán như (sub)Graph classification, node classification, link prediction,…. Từ đó mới chuẩn bị đúng dạng dữ liệu, build đúng models, viết đúng hàm train loop, chọn đúng phương pháp metrics trong quá trình training và testing phases. Gần đây, Mình biết có 1 bài báo rất rất “tốt” nhưng bị retracted chỉ vì mắc sai lầm về nhận dạng bài toán. Rất tiếc cho nhóm nghiên cứu tiên phong đó!
Chúc các bạn năm mới bình an và đạt được nhiều thành tựu","Đây là bài viết thú vị về tổng kết những thành tựu về nghiên cứu Graph Neural Networks trong năm 2021 cũng như dự đoán xu thế về GNN trong năm 2022 https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0 Trong bài viết này có câu trả lời của một số bạn hỏi về độ sâu, độ rộng khi thiết kế GNNs mà trong bài giảng tại Viện Toán Cao cấp của Gs Nguyễn Hùng Sơn vào ngày 30/12/2021. Có một số nghiên cứu chỉ ra rằng GNNs chỉ cần từ 2-4 layers là ổn, càng nhiều layers càng làm giảm hiệu năng của mô hình. Với kinh nghiệm cá nhân, việc train GNNs không cần quá quan tâm tới việc tinh chỉnh hyperparameters. Mình thấy GNNs sẽ hội tụ tốt với hàm ADAM, Learning rate ~0.001 (không cần Learning scheduler). Mình thử dùng một số hàm Optimizer khác kết hợp cùng với scheduler thì models rất khó hội tụ! Một trong những khó khăn nữa mà có một số bạn có hỏi riêng đó là nhận diện đúng dạng bài toán như (sub)Graph classification, node classification, link prediction,…. Từ đó mới chuẩn bị đúng dạng dữ liệu, build đúng models, viết đúng hàm train loop, chọn đúng phương pháp metrics trong quá trình training và testing phases. Gần đây, Mình biết có 1 bài báo rất rất “tốt” nhưng bị retracted chỉ vì mắc sai lầm về nhận dạng bài toán. Rất tiếc cho nhóm nghiên cứu tiên phong đó! Chúc các bạn năm mới bình an và đạt được nhiều thành tựu",,,,,
"em xin chào thầy cô và mọi người ạ. mọi người cho em hỏi ai đã từng train yolov4 trên colab mà gặp hiện tượng khoảng tầm cứ sau 1500 epochs thì loss nó bị về -nan tất như thế này chưa ạ. em đã thử vài bộ datasets format của yolo và tham khảo một số trang như miai và làm theo giống y hệt mà k đc ạ. rất mong được mọi người giúp đỡ ạ!
link folder em train ở đây ạ: https://drive.google.com/drive/folders/1YGNi2xa1DEYt6Vl2alEyaLkbgcZQsegj?usp=sharing
#yolov4 #objectdetection",em xin chào thầy cô và mọi người ạ. mọi người cho em hỏi ai đã từng train yolov4 trên colab mà gặp hiện tượng khoảng tầm cứ sau 1500 epochs thì loss nó bị về -nan tất như thế này chưa ạ. em đã thử vài bộ datasets format của yolo và tham khảo một số trang như miai và làm theo giống y hệt mà k đc ạ. rất mong được mọi người giúp đỡ ạ! link folder em train ở đây ạ: https://drive.google.com/drive/folders/1YGNi2xa1DEYt6Vl2alEyaLkbgcZQsegj?usp=sharing,#yolov4	#objectdetection,,,,
"Bản dich cuốn sách học sâu đã mở để các bạn pre-order. Chúng tôi dự định chỉ in khoảng 500-1000 cuốn, tùy vào số lượng đặt trước mà chúng tôi nhận được. Do đó, để đảm bảo bạn có được một phiên bản của cuốn sách, hãy nhanh chân đặt hàng. Giá chỉ có 349.000 cho cuốn sách 650 trang.","Bản dich cuốn sách học sâu đã mở để các bạn pre-order. Chúng tôi dự định chỉ in khoảng 500-1000 cuốn, tùy vào số lượng đặt trước mà chúng tôi nhận được. Do đó, để đảm bảo bạn có được một phiên bản của cuốn sách, hãy nhanh chân đặt hàng. Giá chỉ có 349.000 cho cuốn sách 650 trang.",,,,,
"Kính chào các bác, trước em làm mấy bài train YOLO các phiên bản rồi. Lần này mình đang học phần SSD nên mạnh dàn làm clip chia sẻ cùng các bạn mới học.
Hi vọng giúp được các bạn đang học phần này ạ!","Kính chào các bác, trước em làm mấy bài train YOLO các phiên bản rồi. Lần này mình đang học phần SSD nên mạnh dàn làm clip chia sẻ cùng các bạn mới học. Hi vọng giúp được các bạn đang học phần này ạ!",,,,,
"Chào các bạn,
Chúng mình đến từ team 3PU - 3 PhD Students in USA. Sau đây chúng mình xin chia sẻ solution của chúng mình cho task Legal Text Retrieval (Top 1 Public Leaderboard - Top 1 Private Leaderboard)
Ý tưởng: Xếp hạng điểm số cho các câu trong legal corpus, đánh giá độ giống nhau dựa trên cosine similarity score
Training:
Step 1: Fine-tuning masked language models dựa vào corpus mà ban tổ chức cung cấp. Ở đây, corpus sẽ bao gồm các điều luật + các câu hỏi trong training questions và public test questions. Settings sẽ như sau:
Model 1: Vibert base
Model 2: PhoBert Large
Model 3: PhoBert Large + Condenser
Model 4: PhoBert Large + Co-condenser.
Step 2: Training Sentence Transformer + Contrastive loss. Positive samples là các câu trả lời từ training data, negative sample là top-k câu trả lời từ BM25. Sau đây là setting cho các mô hình
Model 1: ViBert base - negative sentence pairs: Top 50 sentences từ BM 25.
Model 2: PhoBert Large - negative sentence pairs : Top 20 sentences từ BM 25
Model 3: PhoBert Large + Condenser - negative sentence pairs: top 20 sentences từ BM 25
Model 4: PhoBert Large + Co-condenser - negative sentence pairs: top 20 sentences từ BM 25
Step 3: Hard negative mining: Dùng 4 models trên dự đoán top 20 cặp trong training data có cos-sim scores cao nhất cho từng model. Sau đó lưu lại các cặp câu này để training tiếp round 2 cho sentence transformer.
Step 4: Training Sentence Transformer + contrastive loss từ dữ liệu được sinh ra ở step 3.
Step 5: Ensemble 4 x Sentence Transformer + BM25 cho từng câu hỏi :
Mình sẽ dùng 4 models ST để tính cosine similarity scores của từng câu hỏi với tất cả các câu có trong legal corpus.
Weighted Ensemble 4 x Sentence Transformer:
bert_score = ∑ w_i * cos_sim_model_i
Bm_25_score = bm25 score của từng câu hỏi
Final score = Bm_25_score * bert_score
Pick các câu có cosine similarity score trong khoảng [max_score - 2.6, max_score]
Post-processing:
Loại bỏ 1 số trường hợp sai điều luật nd-, nđ-cp (chữ đ ở 1 dạng kí tự khác), nd-cp -> nd-cp
09/2014/ttlt-btp-tandtc-vksndtc -> 09/2014/ttlt-btp-tandtc-vksndtc-btc.
Chỉ lấy tối đa 5 câu trong khoảng [max_score - 2.6, max_score].
Source-code được public tại: https://github.com/CuongNN218/zalo_ltr_2021
Lê Tuấn Dũng
P/s: mình sẽ sửa chú thích chi tiết hơn trong thời gian tới.","Chào các bạn, Chúng mình đến từ team 3PU - 3 PhD Students in USA. Sau đây chúng mình xin chia sẻ solution của chúng mình cho task Legal Text Retrieval (Top 1 Public Leaderboard - Top 1 Private Leaderboard) Ý tưởng: Xếp hạng điểm số cho các câu trong legal corpus, đánh giá độ giống nhau dựa trên cosine similarity score Training: Step 1: Fine-tuning masked language models dựa vào corpus mà ban tổ chức cung cấp. Ở đây, corpus sẽ bao gồm các điều luật + các câu hỏi trong training questions và public test questions. Settings sẽ như sau: Model 1: Vibert base Model 2: PhoBert Large Model 3: PhoBert Large + Condenser Model 4: PhoBert Large + Co-condenser. Step 2: Training Sentence Transformer + Contrastive loss. Positive samples là các câu trả lời từ training data, negative sample là top-k câu trả lời từ BM25. Sau đây là setting cho các mô hình Model 1: ViBert base - negative sentence pairs: Top 50 sentences từ BM 25. Model 2: PhoBert Large - negative sentence pairs : Top 20 sentences từ BM 25 Model 3: PhoBert Large + Condenser - negative sentence pairs: top 20 sentences từ BM 25 Model 4: PhoBert Large + Co-condenser - negative sentence pairs: top 20 sentences từ BM 25 Step 3: Hard negative mining: Dùng 4 models trên dự đoán top 20 cặp trong training data có cos-sim scores cao nhất cho từng model. Sau đó lưu lại các cặp câu này để training tiếp round 2 cho sentence transformer. Step 4: Training Sentence Transformer + contrastive loss từ dữ liệu được sinh ra ở step 3. Step 5: Ensemble 4 x Sentence Transformer + BM25 cho từng câu hỏi : Mình sẽ dùng 4 models ST để tính cosine similarity scores của từng câu hỏi với tất cả các câu có trong legal corpus. Weighted Ensemble 4 x Sentence Transformer: bert_score = ∑ w_i * cos_sim_model_i Bm_25_score = bm25 score của từng câu hỏi Final score = Bm_25_score * bert_score Pick các câu có cosine similarity score trong khoảng [max_score - 2.6, max_score] Post-processing: Loại bỏ 1 số trường hợp sai điều luật nd-, nđ-cp (chữ đ ở 1 dạng kí tự khác), nd-cp -> nd-cp 09/2014/ttlt-btp-tandtc-vksndtc -> 09/2014/ttlt-btp-tandtc-vksndtc-btc. Chỉ lấy tối đa 5 câu trong khoảng [max_score - 2.6, max_score]. Source-code được public tại: https://github.com/CuongNN218/zalo_ltr_2021 Lê Tuấn Dũng P/s: mình sẽ sửa chú thích chi tiết hơn trong thời gian tới.",,,,,
"Kính chào các bác! Anh em ta nhiều khi train xong model, phân tích xong một vấn đề về data và muốn share cho người khác sử dụng thì sẽ phải biết FrontEnd, BackEnd để dev ra một trang web.
Bây giờ thì với Streamlit, anh em không cần phải vất vả nửa, dựng lên trang web chỉ trong 1 phút mà thôi.
Nhân dịp đang tìm hiểu về vấn đề này nên mình mạnh dạn làm video chia sẻ, hi vọng giúp được các bạn!","Kính chào các bác! Anh em ta nhiều khi train xong model, phân tích xong một vấn đề về data và muốn share cho người khác sử dụng thì sẽ phải biết FrontEnd, BackEnd để dev ra một trang web. Bây giờ thì với Streamlit, anh em không cần phải vất vả nửa, dựng lên trang web chỉ trong 1 phút mà thôi. Nhân dịp đang tìm hiểu về vấn đề này nên mình mạnh dạn làm video chia sẻ, hi vọng giúp được các bạn!",,,,,
"Em chào mọi người trong nhóm
Hiện tại em có đang làm 1 bài toán , yêu cầu đặt ra là phải crawl được tối thiểu 500mb data , mọi người có thể cho em biết 1 vài nguồn được không ạ . Em cảm ơn mọi người","Em chào mọi người trong nhóm Hiện tại em có đang làm 1 bài toán , yêu cầu đặt ra là phải crawl được tối thiểu 500mb data , mọi người có thể cho em biết 1 vài nguồn được không ạ . Em cảm ơn mọi người",,,,,
Dạ em chào mọi người ạ. Hiện em đang làm project về chatbot. Hướng của em đang đi là dùng các model pre train trên Hugging Face như dialogpt. Liệu em fine tuning có sử dụng cho tiếng việt được không ạ. Xin mọi người cho ý kiến. Em thấy trên mạng chưa có code làm này nên hơi lo!!!!,Dạ em chào mọi người ạ. Hiện em đang làm project về chatbot. Hướng của em đang đi là dùng các model pre train trên Hugging Face như dialogpt. Liệu em fine tuning có sử dụng cho tiếng việt được không ạ. Xin mọi người cho ý kiến. Em thấy trên mạng chưa có code làm này nên hơi lo!!!!,,,,,
"Chúc mừng Blog MLCB được 5 tuổi (theo giờ Mỹ). Khi tạo commit đầu tiên vào repo https://github.com/tiepvupsu/tiepvupsu.github.io, mình không thể ngờ lại đi cùng MLCB xa đến thế. Cảm ơn các bạn đã góp phần duy trì cộng đồng này.","Chúc mừng Blog MLCB được 5 tuổi (theo giờ Mỹ). Khi tạo commit đầu tiên vào repo https://github.com/tiepvupsu/tiepvupsu.github.io, mình không thể ngờ lại đi cùng MLCB xa đến thế. Cảm ơn các bạn đã góp phần duy trì cộng đồng này.",,,,,
Mình thử nghiệm cho thành viên đăng bài ẩn danh. Các bạn có thể thoải mái đặt câu hỏi mà không sợ bị đánh giá. Tất nhiên các câu hỏi vẫn phải theo nội quy của nhóm.,Mình thử nghiệm cho thành viên đăng bài ẩn danh. Các bạn có thể thoải mái đặt câu hỏi mà không sợ bị đánh giá. Tất nhiên các câu hỏi vẫn phải theo nội quy của nhóm.,,,,,
"Chào mọi người!
Em đang thực hiện một đồ án nhỏ và muốn sử dụng Keras để tăng cường dữ liệu cho tập dữ liệu của mình.
Mọi người có thể cho em hỏi tại sao em thực hiện như đoạn code dưới nhưng chỉ xuất hiện kết quả là ""Found 10 images belonging to 2 classes."" nhưng không thấy các ảnh mới được tạo ra là vì sao ạ?","Chào mọi người! Em đang thực hiện một đồ án nhỏ và muốn sử dụng Keras để tăng cường dữ liệu cho tập dữ liệu của mình. Mọi người có thể cho em hỏi tại sao em thực hiện như đoạn code dưới nhưng chỉ xuất hiện kết quả là ""Found 10 images belonging to 2 classes."" nhưng không thấy các ảnh mới được tạo ra là vì sao ạ?",,,,,
"Xin chào mọi người,
Hiện tại em đang tìm hiểu về Logical Analysis of Data (LAD). Mục đích của em là sử dụng LAD để generate các patterns trong tín hiệu
dao động. Em có đọc báo và tìm hiểu thì có được biết đến cbmLAD software nhưng mọi thông tin rất hạn chế. Em đang chưa tìm được cách xây dựng mô hình và phương pháp để implement LAD với python hay C++.
Em rất mong được mọi người giúp đỡ.
Em cảm ơn!","Xin chào mọi người, Hiện tại em đang tìm hiểu về Logical Analysis of Data (LAD). Mục đích của em là sử dụng LAD để generate các patterns trong tín hiệu dao động. Em có đọc báo và tìm hiểu thì có được biết đến cbmLAD software nhưng mọi thông tin rất hạn chế. Em đang chưa tìm được cách xây dựng mô hình và phương pháp để implement LAD với python hay C++. Em rất mong được mọi người giúp đỡ. Em cảm ơn!",,,,,
"Em xin chào cả nhà, cho em hỏi một chút về dịch máy trong NLP
Em có làm theo phương pháp nêu trong bài báo này https://aclanthology.org/2020.acl-main.144, và áp dụng với cặp ngôn ngữ Anh-Việt, thì thấy kết quả có chút cải tiến, tuy nhiên em không hiểu tại sao nó lại tốt hơn.
Cụ thể: Bài báo này nói về việc edit ngữ liệu trước khi huấn luyện mô hình dịch máy
- Dữ liệu ban đầu => huấn luyện => mô hình Base
- Dữ liệu ban đầu => edit => huấn luyện => mô hình X
Khi test: mô hình X cho ra điểm BLEU tốt hơn với mô hình Base.
Trong quá trình edit, ngữ liệu bị sửa lại chút xíu, cụ thể là câu nguồn sẽ được nối với 1 câu đích nếu thoả điều kiện cho trước (xem ảnh sẽ rõ hơn)
Anh/chị nào có kinh nghiệm có thể giải thích cho em biết tại sao trong quá trình edit, việc ghép nối câu theo kiểu của tác giả đa phần lại giúp mô hình của mình được tốt hơn nhỉ?
Em cảm ơn nhiều ạ
#NLP","Em xin chào cả nhà, cho em hỏi một chút về dịch máy trong NLP Em có làm theo phương pháp nêu trong bài báo này https://aclanthology.org/2020.acl-main.144, và áp dụng với cặp ngôn ngữ Anh-Việt, thì thấy kết quả có chút cải tiến, tuy nhiên em không hiểu tại sao nó lại tốt hơn. Cụ thể: Bài báo này nói về việc edit ngữ liệu trước khi huấn luyện mô hình dịch máy - Dữ liệu ban đầu => huấn luyện => mô hình Base - Dữ liệu ban đầu => edit => huấn luyện => mô hình X Khi test: mô hình X cho ra điểm BLEU tốt hơn với mô hình Base. Trong quá trình edit, ngữ liệu bị sửa lại chút xíu, cụ thể là câu nguồn sẽ được nối với 1 câu đích nếu thoả điều kiện cho trước (xem ảnh sẽ rõ hơn) Anh/chị nào có kinh nghiệm có thể giải thích cho em biết tại sao trong quá trình edit, việc ghép nối câu theo kiểu của tác giả đa phần lại giúp mô hình của mình được tốt hơn nhỉ? Em cảm ơn nhiều ạ",#NLP,,,,
"Chào mọi người, em đang đọc một số tài liệu về phân rã ma trận và gặp một trường hợp mà em chưa rõ lắm, ""the eigenvectors of a discrete line are the cosine and sinusoidal functions"" -> ý nói ở đây là ""các vector riêng của một đường rời rạc là hàm cos/sin"". Các cao thủ có thể chỉ giáo giúp em vì sao lại như vậy được không ạ","Chào mọi người, em đang đọc một số tài liệu về phân rã ma trận và gặp một trường hợp mà em chưa rõ lắm, ""the eigenvectors of a discrete line are the cosine and sinusoidal functions"" -> ý nói ở đây là ""các vector riêng của một đường rời rạc là hàm cos/sin"". Các cao thủ có thể chỉ giáo giúp em vì sao lại như vậy được không ạ",,,,,
"xin chào mọi người ạ, cho em hỏi trong group mình có ai từng đọc qua 2 cuốn này chưa có thể cho em chút review được không ạ, và nên chọn cuốn nào ạ. mục tiêu em hướng tới là Engineering ạ. em cũng có một chút kinh nghiệm về ML rồi nhưng muốn đọc lại để hiểu sâu hơn mặt toán học và phân xác suất thống kê ạ. cảm ơn mọi người ạ!","xin chào mọi người ạ, cho em hỏi trong group mình có ai từng đọc qua 2 cuốn này chưa có thể cho em chút review được không ạ, và nên chọn cuốn nào ạ. mục tiêu em hướng tới là Engineering ạ. em cũng có một chút kinh nghiệm về ML rồi nhưng muốn đọc lại để hiểu sâu hơn mặt toán học và phân xác suất thống kê ạ. cảm ơn mọi người ạ!",,,,,
"Newbie_pytorch_samecode_same_data_difference_result
ae cho mình hỏi chút mình có model học tốt ở local nhưng lên colab pro thì không học được gì là hiện tượng gì nhỉ?
Edit: mng có giải pháp khắc phục không?
https://stackoverflow.com/questions/66446056/different-results-on-google-colab-than-local",Newbie_pytorch_samecode_same_data_difference_result ae cho mình hỏi chút mình có model học tốt ở local nhưng lên colab pro thì không học được gì là hiện tượng gì nhỉ? Edit: mng có giải pháp khắc phục không? https://stackoverflow.com/questions/66446056/different-results-on-google-colab-than-local,,,,,
"Xin chào cả nhà, cho em hỏi một chút về Dịch máy.
Em có làm theo phương pháp nêu trong bài báo này https://aclanthology.org/2020.acl-main.144, và áp dụng với cặp ngôn ngữ Anh-Việt, thì thấy kết quả có chút cải tiến, tuy nhiên em không hiểu tại sao nó lại tốt hơn.
Cụ thể: Bài báo này nói về việc edit ngữ liệu trước khi huấn luyện mô hình dịch máy
- Dữ liệu ban đầu => huấn luyện => mô hình Base
- Dữ liệu ban đầu => edit => huấn luyện => mô hình X
Khi test: mô hình X cho ra điểm BLEU tốt hơn với mô hình Base.
Trong quá trình edit, ngữ liệu bị sửa lại chút xíu, cụ thể là câu nguồn sẽ được nối với 1 câu đích nếu thoả điều kiện cho trước (xem ảnh sẽ rõ hơn)
Bạn nào có kinh nghiệm có thể giải thích cho mình biết tại sao trong quá trình edit, việc ghép nối câu theo kiểu của tác giả đa phần lại giúp mô hình được tốt hơn nhỉ?
Lưu ý: quá trình edit thì em có dùng SBERT để so sánh với SIF của bài báo gốc, nhưng việc này không ảnh hưởng đến câu hỏi đưa ra.","Xin chào cả nhà, cho em hỏi một chút về Dịch máy. Em có làm theo phương pháp nêu trong bài báo này https://aclanthology.org/2020.acl-main.144, và áp dụng với cặp ngôn ngữ Anh-Việt, thì thấy kết quả có chút cải tiến, tuy nhiên em không hiểu tại sao nó lại tốt hơn. Cụ thể: Bài báo này nói về việc edit ngữ liệu trước khi huấn luyện mô hình dịch máy - Dữ liệu ban đầu => huấn luyện => mô hình Base - Dữ liệu ban đầu => edit => huấn luyện => mô hình X Khi test: mô hình X cho ra điểm BLEU tốt hơn với mô hình Base. Trong quá trình edit, ngữ liệu bị sửa lại chút xíu, cụ thể là câu nguồn sẽ được nối với 1 câu đích nếu thoả điều kiện cho trước (xem ảnh sẽ rõ hơn) Bạn nào có kinh nghiệm có thể giải thích cho mình biết tại sao trong quá trình edit, việc ghép nối câu theo kiểu của tác giả đa phần lại giúp mô hình được tốt hơn nhỉ? Lưu ý: quá trình edit thì em có dùng SBERT để so sánh với SIF của bài báo gốc, nhưng việc này không ảnh hưởng đến câu hỏi đưa ra.",,,,,
"có anh chị hay bạn nào có kinh nghiệm cắt ảnh trong video có thể cho em (mình) cách cắt ảnh sao cho đảm bảo được không bỏ sót object vì video thì có lúc người quay sẽ có đoạn quay nhanh và có đoạn quay chậm, nếu cắt thủ công cứ 20 frame rồi cắt thì lúc đoạn người quay lia máy thì sẽ bị sót frame mình cần . nhưng vẫn đảm bảo được không cắt quá nhiều frame nhỉ. em(mình) cảm ơn ạ .","có anh chị hay bạn nào có kinh nghiệm cắt ảnh trong video có thể cho em (mình) cách cắt ảnh sao cho đảm bảo được không bỏ sót object vì video thì có lúc người quay sẽ có đoạn quay nhanh và có đoạn quay chậm, nếu cắt thủ công cứ 20 frame rồi cắt thì lúc đoạn người quay lia máy thì sẽ bị sót frame mình cần . nhưng vẫn đảm bảo được không cắt quá nhiều frame nhỉ. em(mình) cảm ơn ạ .",,,,,
"Chào mọi người, 

Mình đến từ team Bơ cuộc thi Zalo AI - Legal text (top 3 public test) đợt vừa rồi.  Code mọi người có thể tham khảo ở đây:
https://github.com/phuongnm-bkhn/legal_text_retrieval

Nhìn chung, phương pháp khá đơn giản: 
B1: sinh negative samples dựa trên điểm cosine của tfidf, và bm25 score 
B2: fine-tune pretrained PhoBERT model (+ NlpHUST model) 

-- 
ps: cảm ơn vì sự đóng góp của  Đỗ Đình Trường và lab của mình ở JAIST: ""NGUYEN lab""
https://www.jaist.ac.jp/is/labs/nguyen-lab/home/","Chào mọi người, Mình đến từ team Bơ cuộc thi Zalo AI - Legal text (top 3 public test) đợt vừa rồi. Code mọi người có thể tham khảo ở đây: https://github.com/phuongnm-bkhn/legal_text_retrieval Nhìn chung, phương pháp khá đơn giản: B1: sinh negative samples dựa trên điểm cosine của tfidf, và bm25 score B2: fine-tune pretrained PhoBERT model (+ NlpHUST model) -- ps: cảm ơn vì sự đóng góp của Đỗ Đình Trường và lab của mình ở JAIST: ""NGUYEN lab"" https://www.jaist.ac.jp/is/labs/nguyen-lab/home/",,,,,
anh chị trong nhóm có kinh nghiệm có thể cho em keyword và source code về model có thể chọn ra được frame tốt nhất khi đưa vào tập ảnh hoặc 1 video ngắn kiểu như ảnh dưới không ạ. em cảm ơn ạ,anh chị trong nhóm có kinh nghiệm có thể cho em keyword và source code về model có thể chọn ra được frame tốt nhất khi đưa vào tập ảnh hoặc 1 video ngắn kiểu như ảnh dưới không ạ. em cảm ơn ạ,,,,,
"Chào mọi người, em là sinh viên đã ra trường 1 năm, hiện đang đi làm tại một công ty về AI, ML. Em có dự định học lên thạc sĩ ngành Khoa học máy tính vì em thấy ngành này liên quan nhất tới AI, ML. Nhưng em chưa biết nên học trường nào tốt, học phí và thời gian học ra sao nên muốn xin các anh chị đi trước lời khuyên ạ.
Em chỉ có thể học ở Hà Nội vì vẫn còn hợp đồng làm việc tại công ty. Em cảm ơn.","Chào mọi người, em là sinh viên đã ra trường 1 năm, hiện đang đi làm tại một công ty về AI, ML. Em có dự định học lên thạc sĩ ngành Khoa học máy tính vì em thấy ngành này liên quan nhất tới AI, ML. Nhưng em chưa biết nên học trường nào tốt, học phí và thời gian học ra sao nên muốn xin các anh chị đi trước lời khuyên ạ. Em chỉ có thể học ở Hà Nội vì vẫn còn hợp đồng làm việc tại công ty. Em cảm ơn.",,,,,
"Chào mọi người,
Em đang làm một website Airbnb và đang muốn tích hợp máy học vào web như là là mf một hệ thống gợi ý , hệ thống tìm kiếm tốt hơn và hệ thống phân loại bình luận nhưng do lần đầu làm website và không biết bắt đầu từ đâu nên mong mọi người giúp em gợi ý với ạ.
Website được xây dựng Backend bới Nodejs Expressjs , Database là Mongodb và em có dùng Graphql để cho Client gọi Api tương tác với Server ạ. Em cũng có kiến thức về máy học và Python ạ nhưng chỉ dừng ở mức cơ bản, và hơi đơn giản ạ.
Em cảm ơn mọi người đã đọc","Chào mọi người, Em đang làm một website Airbnb và đang muốn tích hợp máy học vào web như là là mf một hệ thống gợi ý , hệ thống tìm kiếm tốt hơn và hệ thống phân loại bình luận nhưng do lần đầu làm website và không biết bắt đầu từ đâu nên mong mọi người giúp em gợi ý với ạ. Website được xây dựng Backend bới Nodejs Expressjs , Database là Mongodb và em có dùng Graphql để cho Client gọi Api tương tác với Server ạ. Em cũng có kiến thức về máy học và Python ạ nhưng chỉ dừng ở mức cơ bản, và hơi đơn giản ạ. Em cảm ơn mọi người đã đọc",,,,,
Join cuộc thi để cùng nhau xây dựng mô hình auto trading cho thị trường chứng khoán Việt Nam nào các bạn.,Join cuộc thi để cùng nhau xây dựng mô hình auto trading cho thị trường chứng khoán Việt Nam nào các bạn.,,,,,
"Mọi người ơi cho em hỏi về cách deploy một project Python với ạ.
Em đã hoàn thiện việc training một model, và cũng đã viết code xử lý logic (lấy input, xử lý output, call API đến server, ...) nhưng tất cả đều sử dụng Python. Giờ em muốn đóng gói project kèm với các dependencies và đưa lên một máy chủ sử dụng hệ điều hành Windows thì sử dụng cách nào là tốt nhất ạ?
Em đã có tìm hiểu qua Docker nhưng Docker hỗ trợ không tốt với hệ điều hành Windows nếu muốn sử dụng GPU.","Mọi người ơi cho em hỏi về cách deploy một project Python với ạ. Em đã hoàn thiện việc training một model, và cũng đã viết code xử lý logic (lấy input, xử lý output, call API đến server, ...) nhưng tất cả đều sử dụng Python. Giờ em muốn đóng gói project kèm với các dependencies và đưa lên một máy chủ sử dụng hệ điều hành Windows thì sử dụng cách nào là tốt nhất ạ? Em đã có tìm hiểu qua Docker nhưng Docker hỗ trợ không tốt với hệ điều hành Windows nếu muốn sử dụng GPU.",,,,,
"Chào mọi người em có tham gia Zalo AI challenge task NLP nhưng kết quả rất thấp, không biết anh/chị tham gia có thể chia sẻ solution để em tham khảo và nghiên cứu được không, em cảm ơn ạ","Chào mọi người em có tham gia Zalo AI challenge task NLP nhưng kết quả rất thấp, không biết anh/chị tham gia có thể chia sẻ solution để em tham khảo và nghiên cứu được không, em cảm ơn ạ",,,,,
Mọi người cho em hỏi về phần cài đặt của thuật toán ID3 trong Bài 34: Decision Trees (1): Iterative Dichotomiser 3 của anh Tiệp trên web Machine Learning Cơ Bản. Chỗ này tại sao lại kiểm tra điều kiện node.entropy < self.min_gain ạ? Ý nghĩa của bước kiểm tra này là gì? Em cảm ơn ạ!,Mọi người cho em hỏi về phần cài đặt của thuật toán ID3 trong Bài 34: Decision Trees (1): Iterative Dichotomiser 3 của anh Tiệp trên web Machine Learning Cơ Bản. Chỗ này tại sao lại kiểm tra điều kiện node.entropy < self.min_gain ạ? Ý nghĩa của bước kiểm tra này là gì? Em cảm ơn ạ!,,,,,
"Chào cả nhà. Hôm qua em có làm một clip về train model trên GPU cùa Mac M1. Hôm nay em xin giới thiệu một clip về train model trên GPU của Mac Intel.
Mong giúp được các bạn đang học ạ!",Chào cả nhà. Hôm qua em có làm một clip về train model trên GPU cùa Mac M1. Hôm nay em xin giới thiệu một clip về train model trên GPU của Mac Intel. Mong giúp được các bạn đang học ạ!,,,,,
"Chào mọi người ạ,
Em muốn đưa một model classification âm thanh lên điện thoại android qua android studio ạ, không biết ai đã làm rồi thì có thể cho em xin kinh nghiệm không ạ như là dùng framework gi ạ. Em cám ơn ạ","Chào mọi người ạ, Em muốn đưa một model classification âm thanh lên điện thoại android qua android studio ạ, không biết ai đã làm rồi thì có thể cho em xin kinh nghiệm không ạ như là dùng framework gi ạ. Em cám ơn ạ",,,,,
"Hi m.n, a Tiệp,
Tối qua e vừa xem xong video của a Mark, có tìm hiểu xem thì thấy có nói sử dụng detectron( object detection) và alphapose( pose estimation).
Cho e hỏi ngoài 2 algorithms đó ra, thì còn có thuật toán nào khác trong PJ này ko ạ!
E cảm ơn ạ!","Hi m.n, a Tiệp, Tối qua e vừa xem xong video của a Mark, có tìm hiểu xem thì thấy có nói sử dụng detectron( object detection) và alphapose( pose estimation). Cho e hỏi ngoài 2 algorithms đó ra, thì còn có thuật toán nào khác trong PJ này ko ạ! E cảm ơn ạ!",,,,,
"Em có tìm hiểu một bài báo để báo cáo thì em không hiểu mấy con số trong hình .104,6,9
Các chỉ số alt.atheism ... là gì ạ.
Các số ở đường chéo càng cao thì tính chính xác càng đúng ạ.
Mong được mọi người giúp đỡ","Em có tìm hiểu một bài báo để báo cáo thì em không hiểu mấy con số trong hình .104,6,9 Các chỉ số alt.atheism ... là gì ạ. Các số ở đường chéo càng cao thì tính chính xác càng đúng ạ. Mong được mọi người giúp đỡ",,,,,
"xin phép mọi người em mới học về spark cho em hỏi một xíu ạ
đề bài là cho một list và nó sẽ sử lý tính toán để cộng các số bên trong list đó
trong video mẫu em thấy được họ cộng 2 số đầu, số thứ 3 sẽ được gán vào hàng chờ để tiếp tục cộng trong bước tiếp theo
trong code thực tế em chạy thì em lại thấy rằng nó gộp các cặp 2 số liên tiếp và cộng đúng theo tiêu chí parallelize chứ không tuần tự
code giống nhau 100% và 2 phiên bản spark là 3.0.1 và 3.0.3
vậy không biết mọi người có thể giải thích cho em tại sao được không ạ
đoạn em thắc mắc ở cuối ạ chỗ có 2 hàng số","xin phép mọi người em mới học về spark cho em hỏi một xíu ạ đề bài là cho một list và nó sẽ sử lý tính toán để cộng các số bên trong list đó trong video mẫu em thấy được họ cộng 2 số đầu, số thứ 3 sẽ được gán vào hàng chờ để tiếp tục cộng trong bước tiếp theo trong code thực tế em chạy thì em lại thấy rằng nó gộp các cặp 2 số liên tiếp và cộng đúng theo tiêu chí parallelize chứ không tuần tự code giống nhau 100% và 2 phiên bản spark là 3.0.1 và 3.0.3 vậy không biết mọi người có thể giải thích cho em tại sao được không ạ đoạn em thắc mắc ở cuối ạ chỗ có 2 hàng số",,"#math, #Q&A",,,
"Kính chào các bác, em có đang tìm hiểu về cách train model Tensorflow/Keras trên Macbook Pro M1. Em mạnh dạn làm clip để chia sẻ cùng các bạn đang vướng phần này.
Xin cảm ơn các bác đã xem và ủng hộ.","Kính chào các bác, em có đang tìm hiểu về cách train model Tensorflow/Keras trên Macbook Pro M1. Em mạnh dạn làm clip để chia sẻ cùng các bạn đang vướng phần này. Xin cảm ơn các bác đã xem và ủng hộ.",,,,,
"Em chào mọi người ạ.
Em hiện đang làm 1 project cuối kì về nhận dạng hành động dùng thông tin khung xương ( DÙng openpose hoặc mediapipe để detect khung xương của người rồi nhận dạng), output mong muốn của e là ra được 1 video mà có thể nhận dạng được hành động trong video gồm nhiều hành động như trong video e gửi.
HIện tại e làm xong hướng về nhận dạng rồi ( tức là có 1 chuỗi khung xương gồm thông tin của 1 hành động -> train model để nhận dạng ra hành động đó) .
E có thắc mắc mong mọi người giải đáp đó là Bây giờ mình áp dụng model đó vào video gồm nhiều hành động diễn ra thì mình có thể áp dụng algorithm nào, flow nhận dạng như thế nào để nhận dạng chuỗi nhiều hành động đó không ạ ( vì model trước đó là chỉ dùng với từng hành động ).
Em xin chân thành cảm ơn mọi người","Em chào mọi người ạ. Em hiện đang làm 1 project cuối kì về nhận dạng hành động dùng thông tin khung xương ( DÙng openpose hoặc mediapipe để detect khung xương của người rồi nhận dạng), output mong muốn của e là ra được 1 video mà có thể nhận dạng được hành động trong video gồm nhiều hành động như trong video e gửi. HIện tại e làm xong hướng về nhận dạng rồi ( tức là có 1 chuỗi khung xương gồm thông tin của 1 hành động -> train model để nhận dạng ra hành động đó) . E có thắc mắc mong mọi người giải đáp đó là Bây giờ mình áp dụng model đó vào video gồm nhiều hành động diễn ra thì mình có thể áp dụng algorithm nào, flow nhận dạng như thế nào để nhận dạng chuỗi nhiều hành động đó không ạ ( vì model trước đó là chỉ dùng với từng hành động ). Em xin chân thành cảm ơn mọi người",,,,,
"[ML] Xin hỏi cả nhà về blog MLCB bài về MNIST, em có chạy `pip install python-mnist` thành công, nhưng sau đó `from mnist import MNIST` thì bị báo lỗi là `cannot import name 'MNIST' from 'mnist'`. Em chạy trên ipynb thì báo là `ERROR: No matching distribution found for python-mnist` Xin hỏi ai đã học phần đấy rồi mà chạy được thì chỉ dẫn giúp mình với.","[ML] Xin hỏi cả nhà về blog MLCB bài về MNIST, em có chạy `pip install python-mnist` thành công, nhưng sau đó `from mnist import MNIST` thì bị báo lỗi là `cannot import name 'MNIST' from 'mnist'`. Em chạy trên ipynb thì báo là `ERROR: No matching distribution found for python-mnist` Xin hỏi ai đã học phần đấy rồi mà chạy được thì chỉ dẫn giúp mình với.",,,,,
"Cho em chào các anh các chị thầy cô ạ, em đang làm bài và có phần nghiên cứu về dùng ML dữ đoán timeseries thì theo như model em có các điểm mà data trong model tăng vọt như này thì em loại bỏ khỏi data để khi validate con số nó sẽ đẹp hơn được không ạ ?, và cho em hỏi với mô hình như này mình nên chạy model nào để có kết quả tối ưu em đã chạy thử FBProphet, Arima, Sarima sai số RMSE khá lớn còn MAPE thì dưới <7% ạ ! Em cám ơn nhiều ạ.","Cho em chào các anh các chị thầy cô ạ, em đang làm bài và có phần nghiên cứu về dùng ML dữ đoán timeseries thì theo như model em có các điểm mà data trong model tăng vọt như này thì em loại bỏ khỏi data để khi validate con số nó sẽ đẹp hơn được không ạ ?, và cho em hỏi với mô hình như này mình nên chạy model nào để có kết quả tối ưu em đã chạy thử FBProphet, Arima, Sarima sai số RMSE khá lớn còn MAPE thì dưới <7% ạ ! Em cám ơn nhiều ạ.",,,,,
"Dạ mọi người cho em hỏi về ý nghĩa một số metric trong face recognition với ạ.
TAR@10^-6 FAR
Phần identification
false negative identification rate (FNIR) và false positive identification rate (FPIR) đánh giá trên Open-set face identification
Em cảm ơn.",Dạ mọi người cho em hỏi về ý nghĩa một số metric trong face recognition với ạ. TAR@10^-6 FAR Phần identification false negative identification rate (FNIR) và false positive identification rate (FPIR) đánh giá trên Open-set face identification Em cảm ơn.,,,,,
"EM chào anh chị và thầy cô ạ.
Em đang đọc bài 26 có chỗ đây, chỗ số ""2"" em tô vàng ý ạ. Anh chị có thể cho em hỏi số 2 ở đây có ý nghĩa gì được không ạ ;<<","EM chào anh chị và thầy cô ạ. Em đang đọc bài 26 có chỗ đây, chỗ số ""2"" em tô vàng ý ạ. Anh chị có thể cho em hỏi số 2 ở đây có ý nghĩa gì được không ạ ;<<",,,,,
"Tiệp cho mình chia sẻ lớp học online miễn phí về oundation of Machine Learning, Data Science (dạy bằng Tiếng Việt) cho các bạn quan tâm nhé (Thông tin đầy đủ các bạn có thể đọc trong page mà mình đính kèm phía dưới). Thời gian sớm nhất mình tính tổ chức lớp học là vào hè năm sau. Nếu các bạn cũng muốn tham gia vào quá trình giảng dạy (soạn lectures, etc.) thì chúng mình có thể thảo luận thêm.","Tiệp cho mình chia sẻ lớp học online miễn phí về oundation of Machine Learning, Data Science (dạy bằng Tiếng Việt) cho các bạn quan tâm nhé (Thông tin đầy đủ các bạn có thể đọc trong page mà mình đính kèm phía dưới). Thời gian sớm nhất mình tính tổ chức lớp học là vào hè năm sau. Nếu các bạn cũng muốn tham gia vào quá trình giảng dạy (soạn lectures, etc.) thì chúng mình có thể thảo luận thêm.",,,,,
"Mình xin chia sẻ tiếp part 2 từ bài viết tuần trước. Trong bài này mình xin gợi ý chủ quan các vấn đề sau:
1. Con đường học Machine Learning bài bản dựa vào chương trình Master Data Science ở trường Aalto (Ngồi ở nhà tự học nhưng kiến thức vẫn ngang với trình độ Master Phần Lan-EU)
2. Lộ trình học Data Science để áp dụng vào Marketing, Logistics-Supply chain cho các bạn có background kinh tế
3. Xu hướng tuyển dụng và ngành mũi nhọn của Phần Lan (đúng hơn là miền nam Phần Lan: Helsinki, Espoo, Turku)
4. Chia sẻ nguồn học Data Science với giá 0 đồng cho tất cả mọi người với tiêu chuẩn Phần Lan (1 trong nhưng nền giáo dục tốt nhất thế giới)
Rất mong được nhận nhiều sự đóng góp ý kiến của các bạn. Mình xin cám ơn!","Mình xin chia sẻ tiếp part 2 từ bài viết tuần trước. Trong bài này mình xin gợi ý chủ quan các vấn đề sau: 1. Con đường học Machine Learning bài bản dựa vào chương trình Master Data Science ở trường Aalto (Ngồi ở nhà tự học nhưng kiến thức vẫn ngang với trình độ Master Phần Lan-EU) 2. Lộ trình học Data Science để áp dụng vào Marketing, Logistics-Supply chain cho các bạn có background kinh tế 3. Xu hướng tuyển dụng và ngành mũi nhọn của Phần Lan (đúng hơn là miền nam Phần Lan: Helsinki, Espoo, Turku) 4. Chia sẻ nguồn học Data Science với giá 0 đồng cho tất cả mọi người với tiêu chuẩn Phần Lan (1 trong nhưng nền giáo dục tốt nhất thế giới) Rất mong được nhận nhiều sự đóng góp ý kiến của các bạn. Mình xin cám ơn!",,,,,
"Mình đang làm 1 task về read captcha, mình có xem các project trên git thì các data train đều thuộc cùng 1 type. nhưng cty mình hiện tại muốn gộp nhiều type lại với nhau kiểu là bỏ bất kỳ hình captcha nào vào cũng phải nhận diện được chữ . hiện tại mình vẫn chưa có solution cụ thể cho trường hợp này mong mọi người cho mình một số idea,có project hay paper nào đã work như vậy thì càng tốt. Mình xin cảm ơn. mình có kèm theo 1 số hình ảnh để mn xem các type capcha khác nhau .","Mình đang làm 1 task về read captcha, mình có xem các project trên git thì các data train đều thuộc cùng 1 type. nhưng cty mình hiện tại muốn gộp nhiều type lại với nhau kiểu là bỏ bất kỳ hình captcha nào vào cũng phải nhận diện được chữ . hiện tại mình vẫn chưa có solution cụ thể cho trường hợp này mong mọi người cho mình một số idea,có project hay paper nào đã work như vậy thì càng tốt. Mình xin cảm ơn. mình có kèm theo 1 số hình ảnh để mn xem các type capcha khác nhau .",,,,,
"Các bạn học sinh có thể đăng ký nhanh [1] để nhận đc book miễn phí từ tác giả nhé.
[1] https://docs.google.com/forms/d/10T3Ty9klzsQTPqEy76WhCps0yeAyv3ZmH0VynjmsMVo/viewform?edit_requested=true",Các bạn học sinh có thể đăng ký nhanh [1] để nhận đc book miễn phí từ tác giả nhé. [1] https://docs.google.com/forms/d/10T3Ty9klzsQTPqEy76WhCps0yeAyv3ZmH0VynjmsMVo/viewform?edit_requested=true,,,,,
"[Income classification solution sharing]
Income classification là một cuộc thi liên quan tới phân loại trên dữ liệu dạng bảng. Mục tiêu đó là dựa vào thông tin liên quan tới công dân, bạn hãy giúp chính phủ dự đoán xem thu nhập của công dân đó có vượt quá 50k USD hay không. Ở sự kiện Data Quest sắp tới đây chúng ta hãy cùng nhau theo dõi những chia sẻ từ các bạn thí sinh đã đạt giải về các kĩ thuật hữu ích đã được các bạn áp dụng như thế nào.
----------------------------------------
Income classification is a competition relating to a classification task on the tabular dataset. The target is to use the citizen information, you help the government make a prediction whether a citizen has an annual income is greater than 50k USD? In the next Data Quest meeting, you have a chance to catch up on delicate solution sharing from the winners via bellow meeting:
Meeting room: Income Classification Solution Sharing
Friday, December 17 · 7:30 – 8:30pm
Google Meet joining info
Video call link: https://meet.google.com/oyv-qmdk-ftp","[Income classification solution sharing] Income classification là một cuộc thi liên quan tới phân loại trên dữ liệu dạng bảng. Mục tiêu đó là dựa vào thông tin liên quan tới công dân, bạn hãy giúp chính phủ dự đoán xem thu nhập của công dân đó có vượt quá 50k USD hay không. Ở sự kiện Data Quest sắp tới đây chúng ta hãy cùng nhau theo dõi những chia sẻ từ các bạn thí sinh đã đạt giải về các kĩ thuật hữu ích đã được các bạn áp dụng như thế nào. ---------------------------------------- Income classification is a competition relating to a classification task on the tabular dataset. The target is to use the citizen information, you help the government make a prediction whether a citizen has an annual income is greater than 50k USD? In the next Data Quest meeting, you have a chance to catch up on delicate solution sharing from the winners via bellow meeting: Meeting room: Income Classification Solution Sharing Friday, December 17 · 7:30 – 8:30pm Google Meet joining info Video call link: https://meet.google.com/oyv-qmdk-ftp",,,,,
"Chào anh/chị/các bạn
Hiện tại em đang làm một đồ án về nhận diện biển số xe máy Việt Nam (biển số 2 dòng) do em tìm hiểu về Machine Learning mới đây thôi nên có một chút khó khăn nên rất cần các anh/chị/các bạn giúp đỡ.
Theo những gì em tìm hiểu về những việc cần làm là preprocess ảnh trước (chuyển ảnh xám, nhị phân hóa ảnh thành ảnh trắng đen) sau đó sẽ lọc tất cả các contours trong ảnh và nhận diện biển số bằng contours có 4 cạnh và có diện tích lớn nhất trong ảnh. Nhưng khi xác định contours thì em có một chút khó khăn do có một vài biển số xe viền bên ngoài người ta sẽ viền một lớp kim loại (hình 1a) hoặc có viền đen nhưng bị ánh sáng chiếu vào làm đường viền bị chá (hình 2a) khiến cho đường viền không rõ nét hoặc không nhận thấy đường viền làm cho quá trình lọc contours gặp chút khó khăn, các contours sẽ bị đứt khúc nên không phát hiện được contours bằng phương pháp nhận diện contours 4 cạnh có diện tích lớn nhất (hình contours bị đứt khúc: hình 1b, 2b).Còn các hình có đường viền đen rõ và không bị ánh sáng phản chiếu như vầy thì em đã nhận diện được (hình 3).
Anh/chị/các bạn cho em hỏi là có cách nào khắc phục trường hợp này(làm cho contour đường viền biển số nó liền mạch) không ạ?
Em xin cảm ơn!","Chào anh/chị/các bạn Hiện tại em đang làm một đồ án về nhận diện biển số xe máy Việt Nam (biển số 2 dòng) do em tìm hiểu về Machine Learning mới đây thôi nên có một chút khó khăn nên rất cần các anh/chị/các bạn giúp đỡ. Theo những gì em tìm hiểu về những việc cần làm là preprocess ảnh trước (chuyển ảnh xám, nhị phân hóa ảnh thành ảnh trắng đen) sau đó sẽ lọc tất cả các contours trong ảnh và nhận diện biển số bằng contours có 4 cạnh và có diện tích lớn nhất trong ảnh. Nhưng khi xác định contours thì em có một chút khó khăn do có một vài biển số xe viền bên ngoài người ta sẽ viền một lớp kim loại (hình 1a) hoặc có viền đen nhưng bị ánh sáng chiếu vào làm đường viền bị chá (hình 2a) khiến cho đường viền không rõ nét hoặc không nhận thấy đường viền làm cho quá trình lọc contours gặp chút khó khăn, các contours sẽ bị đứt khúc nên không phát hiện được contours bằng phương pháp nhận diện contours 4 cạnh có diện tích lớn nhất (hình contours bị đứt khúc: hình 1b, 2b).Còn các hình có đường viền đen rõ và không bị ánh sáng phản chiếu như vầy thì em đã nhận diện được (hình 3). Anh/chị/các bạn cho em hỏi là có cách nào khắc phục trường hợp này(làm cho contour đường viền biển số nó liền mạch) không ạ? Em xin cảm ơn!",,,,,
"Chào mọi người. Hiện mình đang gặp một bài toán như này. Mong chỉ dẫn hoặc gợi ý hướng đi ,giải pháp từ mọi người. Mình cảm ơn trước.
Bài toán có input là tên sản phẩm trên trang thương mại điện tử(tiếng Anh). Output là các tags của sản phẩm đó. Ví dụ như brand, category, cho nam hay nữ, …
Data hiện mình đang có khoảng hơn 10k rows mình tự gán nhãn.","Chào mọi người. Hiện mình đang gặp một bài toán như này. Mong chỉ dẫn hoặc gợi ý hướng đi ,giải pháp từ mọi người. Mình cảm ơn trước. Bài toán có input là tên sản phẩm trên trang thương mại điện tử(tiếng Anh). Output là các tags của sản phẩm đó. Ví dụ như brand, category, cho nam hay nữ, … Data hiện mình đang có khoảng hơn 10k rows mình tự gán nhãn.",,,,,
"Dạ thưa anh chị ạ, em hơi không hiểu chỗ đoạn bôi vàng ý ạ.
Việc biểu diễn ma trận A dưới dạng đó em thấy lạ quá 😢
Với cả từ ""phụ thuộc"" mang hàm ý gì vậy ạ
Em xin cảm ơn.","Dạ thưa anh chị ạ, em hơi không hiểu chỗ đoạn bôi vàng ý ạ. Việc biểu diễn ma trận A dưới dạng đó em thấy lạ quá Với cả từ ""phụ thuộc"" mang hàm ý gì vậy ạ Em xin cảm ơn.",,,,,
Em chào ac trong nhóm. Hiện em xin ac trong nhóm tư vấn giúp em các keyword liên quan đến IOT và ML. Hoặc xin vài lời khuyên của ac ạ.,Em chào ac trong nhóm. Hiện em xin ac trong nhóm tư vấn giúp em các keyword liên quan đến IOT và ML. Hoặc xin vài lời khuyên của ac ạ.,,,,,
"Chào mọi người, vừa qua team SDSV_AICR của mình đã đạt top 1 chung cuộc cuộc thi MC-OCR tại hội thảo RIVF 2021. Mình có chia sẻ lại toàn bộ quá trình giải quyết 2 tasks (1) đánh giá chất lượng hóa đơn và (2) trích xuất thông tin hóa đơn trên github https://github.com/ndcuong91/MC_OCR . Mọi người tham khảo và góp ý nhé :D","Chào mọi người, vừa qua team SDSV_AICR của mình đã đạt top 1 chung cuộc cuộc thi MC-OCR tại hội thảo RIVF 2021. Mình có chia sẻ lại toàn bộ quá trình giải quyết 2 tasks (1) đánh giá chất lượng hóa đơn và (2) trích xuất thông tin hóa đơn trên github https://github.com/ndcuong91/MC_OCR . Mọi người tham khảo và góp ý nhé :D",,,,,
"Gần đây, phương pháp score matching có một bước đột phá khi sử dụng ý tưởng tình cờ giống với mô hình diffusion, đưa ra kết quả khả quan cho mô hình sinh. Tương tự như vậy, mô hình diffusion cũng đưa ra kết quả xấp xỉ các mô hình kiểu GAN khi sử dụng ý tưởng của denoise score matching. Hai mô hình này sau đó đã được tổng quát hóa từ góc nhìn phương trình vi phân ngẫu nhiên, trở thành SOTA cho sinh dữ liệu ảnh. Mình có ghi chép lại một chút về các mô hình trên, mong nhận được ý kiến của mọi người về hướng đi mới này.","Gần đây, phương pháp score matching có một bước đột phá khi sử dụng ý tưởng tình cờ giống với mô hình diffusion, đưa ra kết quả khả quan cho mô hình sinh. Tương tự như vậy, mô hình diffusion cũng đưa ra kết quả xấp xỉ các mô hình kiểu GAN khi sử dụng ý tưởng của denoise score matching. Hai mô hình này sau đó đã được tổng quát hóa từ góc nhìn phương trình vi phân ngẫu nhiên, trở thành SOTA cho sinh dữ liệu ảnh. Mình có ghi chép lại một chút về các mô hình trên, mong nhận được ý kiến của mọi người về hướng đi mới này.",,,,,
"[𝗢𝗣𝗘𝗡 𝗦𝗢𝗨𝗥𝗖𝗘 𝗗𝗔𝗧𝗔] DANeS - BỘ DỮ LIỆU MỞ GỒM 500,000 BÀI BÁO ĐIỆN TỬ TIẾNG VIỆT
DANeS là một bộ dữ liệu mở về văn bản, gồm ~ 500.000 bài báo điện tử tiếng Việt. Các bài báo sẽ bao gồm tiêu đề, URL, mô tả tổng quan từng bài báo và được dán nhãn tích cực/tiêu cực/trung tính dựa trên nội dung tiêu đề.
𝗧𝗿𝗮̣𝗻𝗴 𝘁𝗵𝗮́𝗶: DANeS hiện vẫn đang trong quá trình dán nhãn và rất cần sự tham gia mạnh mẽ từ cộng đồng. Link Github sẽ là nơi cập nhật dữ liệu thô và đã được dán nhãn từ https://tool.dataset.vn/projects/752/details. Tần suất cập nhật dự kiến sẽ là 1 tháng/lần.
𝗕𝗮̉𝗻 𝗾𝘂𝘆𝗲̂̀𝗻: Dữ liệu là tên các bài báo thuộc về tác giả đã đặt cho bài báo của mình. Các dữ liệu phái sinh từ phần mềm https://tool.dataset.vn được cấp phép bởi giấy CC-BY. Phần mã nguồn mở cho mô hình học máy thuộc bản quyền MIT.
-------------------
※ Toàn bộ dữ liệu thô sẽ được công khai tại địa chỉ sau:
https://github.com/dataset-vn/DANeS
※ Để nhận dữ liệu đã được dán nhãn mới nhất, bạn vui lòng thực hiện các bước sau:
Bước 1: Truy cập bộ dữ liệu qua đường link dưới đây:
📥 Link: https://tool.dataset.vn/projects/752/details
Bước 2: Ấn “Tham gia” dự án và đợi quản trị viên duyệt yêu cầu của bạn.
Bước 3: Tải và sử dụng một phần kho dữ liệu đã được dán nhãn bởi DATASET.
-------------------
📌 Về 𝗗𝗔𝗧𝗔𝗦𝗘𝗧 .𝗝𝗦𝗖: https://www.facebook.com/dataset.vn
Sứ mệnh của DATASET là trở thành nền tảng dữ liệu ""nguồn lực cộng đồng"" tiên phong tại Việt Nam, hỗ trợ các cá nhân, tổ chức trong việc ứng dụng khoa học dữ liệu để giải quyết các bài toán của xã hội. Với nền tảng phần mềm mạnh mẽ và cộng đồng xử lý dữ liệu đông đảo, DATASET mong muốn đưa đến cho đối tác một giải pháp toàn diện và chất lượng, phù hợp với đặc thù của thị trường công nghệ Việt Nam và thế giới.
📌 Về 𝗔𝗜𝗩 𝗚𝗿𝗼𝘂𝗽: https://www.facebook.com/aivgroup.jsc/
AIV Group hướng đến việc ứng dụng những tiến bộ về công nghệ, đặc biệt là Trí tuệ nhân tạo (AI), Điện toán đám mây (Cloud Computing), Dữ liệu lớn (Big Data) để số hoá, hiện đại hoá các quy trình sản xuất và tiêu thụ thông tin đã tồn tại lâu đời trong xã hội Việt Nam, đồng thời góp phần giải quyết những vấn đề mới phát sinh trong lĩnh vực truyền thông do mặt trái của công nghệ như: vấn nạn tin giả, hình ảnh, video được cắt ghép tự động…
Nguồn: Dataset.vn - Data Crowdsourcing Platform
_______
THÔNG TIN LIÊN HỆ:
DATASET .JSC
Tầng 3, Golden Land Building, Số 275 Nguyễn Trãi, Thanh Xuân, Hà Nội
📍 SĐT: 0984420826
📍 Facebook: https://www.facebook.com/dataset.vn
📍 Email: info@dataset.vn
📍 Website: dataset.vn","[ ] DANeS - BỘ DỮ LIỆU MỞ GỒM 500,000 BÀI BÁO ĐIỆN TỬ TIẾNG VIỆT DANeS là một bộ dữ liệu mở về văn bản, gồm ~ 500.000 bài báo điện tử tiếng Việt. Các bài báo sẽ bao gồm tiêu đề, URL, mô tả tổng quan từng bài báo và được dán nhãn tích cực/tiêu cực/trung tính dựa trên nội dung tiêu đề. ̣ ́: DANeS hiện vẫn đang trong quá trình dán nhãn và rất cần sự tham gia mạnh mẽ từ cộng đồng. Link Github sẽ là nơi cập nhật dữ liệu thô và đã được dán nhãn từ https://tool.dataset.vn/projects/752/details. Tần suất cập nhật dự kiến sẽ là 1 tháng/lần. ̉ ̂̀: Dữ liệu là tên các bài báo thuộc về tác giả đã đặt cho bài báo của mình. Các dữ liệu phái sinh từ phần mềm https://tool.dataset.vn được cấp phép bởi giấy CC-BY. Phần mã nguồn mở cho mô hình học máy thuộc bản quyền MIT. ------------------- ※ Toàn bộ dữ liệu thô sẽ được công khai tại địa chỉ sau: https://github.com/dataset-vn/DANeS ※ Để nhận dữ liệu đã được dán nhãn mới nhất, bạn vui lòng thực hiện các bước sau: Bước 1: Truy cập bộ dữ liệu qua đường link dưới đây: Link: https://tool.dataset.vn/projects/752/details Bước 2: Ấn “Tham gia” dự án và đợi quản trị viên duyệt yêu cầu của bạn. Bước 3: Tải và sử dụng một phần kho dữ liệu đã được dán nhãn bởi DATASET. ------------------- Về .: https://www.facebook.com/dataset.vn Sứ mệnh của DATASET là trở thành nền tảng dữ liệu ""nguồn lực cộng đồng"" tiên phong tại Việt Nam, hỗ trợ các cá nhân, tổ chức trong việc ứng dụng khoa học dữ liệu để giải quyết các bài toán của xã hội. Với nền tảng phần mềm mạnh mẽ và cộng đồng xử lý dữ liệu đông đảo, DATASET mong muốn đưa đến cho đối tác một giải pháp toàn diện và chất lượng, phù hợp với đặc thù của thị trường công nghệ Việt Nam và thế giới. Về : https://www.facebook.com/aivgroup.jsc/ AIV Group hướng đến việc ứng dụng những tiến bộ về công nghệ, đặc biệt là Trí tuệ nhân tạo (AI), Điện toán đám mây (Cloud Computing), Dữ liệu lớn (Big Data) để số hoá, hiện đại hoá các quy trình sản xuất và tiêu thụ thông tin đã tồn tại lâu đời trong xã hội Việt Nam, đồng thời góp phần giải quyết những vấn đề mới phát sinh trong lĩnh vực truyền thông do mặt trái của công nghệ như: vấn nạn tin giả, hình ảnh, video được cắt ghép tự động… Nguồn: Dataset.vn - Data Crowdsourcing Platform _______ THÔNG TIN LIÊN HỆ: DATASET .JSC Tầng 3, Golden Land Building, Số 275 Nguyễn Trãi, Thanh Xuân, Hà Nội SĐT: 0984420826 Facebook: https://www.facebook.com/dataset.vn Email: info@dataset.vn Website: dataset.vn",,,,,
"[PCA - Principle Component Analysis]
Các model Machine Learning được huấn luyện trên dữ liệu lớn thường khá tốn kém về chi phí phí huấn luyện và đôi khi hay xảy ra hiện tượng overfitting do quá nhiều biến đầu vào. PCA là thuật toán cho phép giảm chiều dữ liệu từ không gian cao chiều xuống không gian thấp chiều mà vẫn giữ được những đặc tính tốt của dữ liệu gốc thông qua phương pháp phân tích suy biến. Phương pháp PCA là môt trong những phương pháp có tính ứng dụng cao trong Machine Learning như giảm chiều dữ liệu, nén ảnh, visualize phân bố của dữ liệu trong không gian 2D, 3D.
Cùng tìm hiểu về phương pháp PCA trong chương tiếp theo của Machine Learning Algorithm to Practice.
-----------------------------------------------------------------------------
Machine Learning models trained on big data are often quite expensive in terms of training costs, and sometimes overfitting can occur due to too many input variables. PCA is an algorithm that allows to reduce the data dimensionality from high-dimensional space to low-dimensional space while preserving the good components of the original data through Singular Decomposition Analysis method. The PCA method is one of the highly applicable methods in Machine Learning such as data dimensionality reduction, image compression, and visualization of the distribution of data in 2D and 3D space. Let learn about PCA theory in the next chapter of Machine Learning Algorithms to Practices book.
https://phamdinhkhanh.github.io/deep.../ch_ml/index_PCA.html","[PCA - Principle Component Analysis] Các model Machine Learning được huấn luyện trên dữ liệu lớn thường khá tốn kém về chi phí phí huấn luyện và đôi khi hay xảy ra hiện tượng overfitting do quá nhiều biến đầu vào. PCA là thuật toán cho phép giảm chiều dữ liệu từ không gian cao chiều xuống không gian thấp chiều mà vẫn giữ được những đặc tính tốt của dữ liệu gốc thông qua phương pháp phân tích suy biến. Phương pháp PCA là môt trong những phương pháp có tính ứng dụng cao trong Machine Learning như giảm chiều dữ liệu, nén ảnh, visualize phân bố của dữ liệu trong không gian 2D, 3D. Cùng tìm hiểu về phương pháp PCA trong chương tiếp theo của Machine Learning Algorithm to Practice. ----------------------------------------------------------------------------- Machine Learning models trained on big data are often quite expensive in terms of training costs, and sometimes overfitting can occur due to too many input variables. PCA is an algorithm that allows to reduce the data dimensionality from high-dimensional space to low-dimensional space while preserving the good components of the original data through Singular Decomposition Analysis method. The PCA method is one of the highly applicable methods in Machine Learning such as data dimensionality reduction, image compression, and visualization of the distribution of data in 2D and 3D space. Let learn about PCA theory in the next chapter of Machine Learning Algorithms to Practices book. https://phamdinhkhanh.github.io/deep.../ch_ml/index_PCA.html",,,,,
"Chào mọi người,
Mình đang phát triển ứng dụng xử lý ảnh trên C# và đang gặp bài toán OCR cho dữ liệu như đính kèm. Hiện tại đang sủ dụng opencv - tesseract nhưng chỉ cho kết quả tốt nhất là 154623/,69300) và 16407,5/.69300). Ko biết trên C# có thư viện open-source nào cho kết quả tốt hơn ko mọi người?","Chào mọi người, Mình đang phát triển ứng dụng xử lý ảnh trên C# và đang gặp bài toán OCR cho dữ liệu như đính kèm. Hiện tại đang sủ dụng opencv - tesseract nhưng chỉ cho kết quả tốt nhất là 154623/,69300) và 16407,5/.69300). Ko biết trên C# có thư viện open-source nào cho kết quả tốt hơn ko mọi người?",,,,,
"Dạ cho e hỏi với ạ. Trong thuật toán K-means, ví dụ nếu khoảng cách của 1 con gà với 1 con vịt với trung tâm cùng =4 thì cái nhóm đó chọn con nào vậy ạ?
Em xin cảm ơn","Dạ cho e hỏi với ạ. Trong thuật toán K-means, ví dụ nếu khoảng cách của 1 con gà với 1 con vịt với trung tâm cùng =4 thì cái nhóm đó chọn con nào vậy ạ? Em xin cảm ơn",,,,,
Tiệp cho mình chia sẻ thêm một follow-up post hôm kia dành cho các bạn có sự quan tâm về học tiến sĩ các ngành liên quan AI nhé. Một điều mà Việt Nam mình còn thiếu là một cộng đồng vững mạnh các chuyên gia chuyên sâu về AI mặc dù chúng ta không hề thiếu kỹ sư giỏi. Hy vọng bài post sẽ mang một số thông tin cần thiết cho các bạn.,Tiệp cho mình chia sẻ thêm một follow-up post hôm kia dành cho các bạn có sự quan tâm về học tiến sĩ các ngành liên quan AI nhé. Một điều mà Việt Nam mình còn thiếu là một cộng đồng vững mạnh các chuyên gia chuyên sâu về AI mặc dù chúng ta không hề thiếu kỹ sư giỏi. Hy vọng bài post sẽ mang một số thông tin cần thiết cho các bạn.,,,,,
"Có lẽ đây là post sau về GNN trong tác vụ phân loại ảnh trong y học (vì tránh làm phiền admins và cộng đồng phải nghe mãi 1 câu chuyện.
Trong 2 Stt trước đây về bài toán phân loại (1) lao phổi từ ảnh X-quang tại đây
(https://www.facebook.com/groups/machinelearningcoban/permalink/1354190071705063/)
và (2) ảnh Optical Coherence Tomography tại đây (https://www.facebook.com/groups/machinelearningcoban/permalink/1351401011983969/), kết quả rất khả quan.
Trong post này mình muốn kiểm tra xem GNN liệu có hoạt động tốt trên dữ liệu âm thanh trong y học hay không (dữ liệu âm thanh có đặc tính theo chuỗi thời gian, và thường được chuyển thành dạng ảnh cho bài toán phân loại).
Một dự án điển hình khác là phân loại tiếng ho của bệnh nhân Covid-19 đã kết thúc tại đây (https://www.facebook.com/groups/1264976217251463), nhưng kết quả impractical, hay chí ít đi xa hơn về mặt nghiên cứu, khi minhd được biết là Accuracy trên private test set của nhà tổ chức chỉ hơn giá trị random một chút (>70%).
Dữ liệu của nghiên cứu này rất rất khó nếu tiếp cận theo hướng Machine Learning (SVM, XBoost,...), Deep Learning (xử lí gián tiếp thông qua xử lí ảnh với CNN & Transformer, hay hướng xử lí âm thanh thuần túy). Khi mình review phần Related Works, không có nghiên cứu đạt được kết quả có thể nghĩ tới việc practical trong thực tế!
Vậy nên mình sẽ demo hướng tiếp cận GNN trong phân loại âm thanh với dữ liệu của bài báo có tên ""A Respiratory Sound Database for the Development of Automated Classification"" tại đây (https://sci-hub.se/10.1007/978-981-10-7419-6_6), dữ liệu mở tại đây (https://bhichallenge.med.auth.gr/). Dữ liệu này phân loại tiếng nghe phổi của bệnh nhân, và khi review tài liệu thì task này rất rất khó, chỉ đạt accuracy ~80% mà thôi. Tuy nhiên nếu dùng GNN, bài toán được giải quyết triệt để, ít nhất trong phạm vi dữ liệu này. Từ đó, nó mở ra hướng mới rất triển vọng. Các bạn có thể xem chi tiết kết quả ở hình bên dưới!","Có lẽ đây là post sau về GNN trong tác vụ phân loại ảnh trong y học (vì tránh làm phiền admins và cộng đồng phải nghe mãi 1 câu chuyện. Trong 2 Stt trước đây về bài toán phân loại (1) lao phổi từ ảnh X-quang tại đây (https://www.facebook.com/groups/machinelearningcoban/permalink/1354190071705063/) và (2) ảnh Optical Coherence Tomography tại đây (https://www.facebook.com/groups/machinelearningcoban/permalink/1351401011983969/), kết quả rất khả quan. Trong post này mình muốn kiểm tra xem GNN liệu có hoạt động tốt trên dữ liệu âm thanh trong y học hay không (dữ liệu âm thanh có đặc tính theo chuỗi thời gian, và thường được chuyển thành dạng ảnh cho bài toán phân loại). Một dự án điển hình khác là phân loại tiếng ho của bệnh nhân Covid-19 đã kết thúc tại đây (https://www.facebook.com/groups/1264976217251463), nhưng kết quả impractical, hay chí ít đi xa hơn về mặt nghiên cứu, khi minhd được biết là Accuracy trên private test set của nhà tổ chức chỉ hơn giá trị random một chút (>70%). Dữ liệu của nghiên cứu này rất rất khó nếu tiếp cận theo hướng Machine Learning (SVM, XBoost,...), Deep Learning (xử lí gián tiếp thông qua xử lí ảnh với CNN & Transformer, hay hướng xử lí âm thanh thuần túy). Khi mình review phần Related Works, không có nghiên cứu đạt được kết quả có thể nghĩ tới việc practical trong thực tế! Vậy nên mình sẽ demo hướng tiếp cận GNN trong phân loại âm thanh với dữ liệu của bài báo có tên ""A Respiratory Sound Database for the Development of Automated Classification"" tại đây (https://sci-hub.se/10.1007/978-981-10-7419-6_6), dữ liệu mở tại đây (https://bhichallenge.med.auth.gr/). Dữ liệu này phân loại tiếng nghe phổi của bệnh nhân, và khi review tài liệu thì task này rất rất khó, chỉ đạt accuracy ~80% mà thôi. Tuy nhiên nếu dùng GNN, bài toán được giải quyết triệt để, ít nhất trong phạm vi dữ liệu này. Từ đó, nó mở ra hướng mới rất triển vọng. Các bạn có thể xem chi tiết kết quả ở hình bên dưới!",,,,,
"Chào mọi người,
Mình tìm cách chạy yolov5 trên jetson nano,.. đã convert sang file yolov5.trt , khi chạy code để load model theo link : https://github.com/.../SemanticSeg.../tutorial-runtime.ipynb thì bị lỗi như hình bên dưới (trong quá trình convert có thông báo plugin successed)
(tensorrt 8.0.1.6)
Mọi người nào có phương pháp nào để detect object cụ thể sau khi đã xuất được file .trt không ạ ?
Mong được mọi người hỗ trợ.
Mình cảm ơn.","Chào mọi người, Mình tìm cách chạy yolov5 trên jetson nano,.. đã convert sang file yolov5.trt , khi chạy code để load model theo link : https://github.com/.../SemanticSeg.../tutorial-runtime.ipynb thì bị lỗi như hình bên dưới (trong quá trình convert có thông báo plugin successed) (tensorrt 8.0.1.6) Mọi người nào có phương pháp nào để detect object cụ thể sau khi đã xuất được file .trt không ạ ? Mong được mọi người hỗ trợ. Mình cảm ơn.",,,,,
"Chào m.n ạ!
M,n ai đã từng triển khai một mô hình CV pytorch như Mask RCNN, Faster RCNN, ... trên Netframework của C# chưa ạ. Có thể cho em xin git tham khảo hoặc hướng triển khai được không!
Thanks m.n!","Chào m.n ạ! M,n ai đã từng triển khai một mô hình CV pytorch như Mask RCNN, Faster RCNN, ... trên Netframework của C# chưa ạ. Có thể cho em xin git tham khảo hoặc hướng triển khai được không! Thanks m.n!",,,,,
"Cho em hỏi em đã cài GPU cho tensorflow nhưng khi train thì tốc độ vẫn không cải thiện dùng vram của gpu full
Em dùng :
Tensorflow 2.7
CUDA toolkit 11.2
Cudnn 8.1
đã set path đầy đủ rồi ạ.
Em người cho em giải pháp khắc phục với ạ, em cảm ơn","Cho em hỏi em đã cài GPU cho tensorflow nhưng khi train thì tốc độ vẫn không cải thiện dùng vram của gpu full Em dùng : Tensorflow 2.7 CUDA toolkit 11.2 Cudnn 8.1 đã set path đầy đủ rồi ạ. Em người cho em giải pháp khắc phục với ạ, em cảm ơn",,,,,
"Em chào mọi người. Em đang thiết kế một model để tạo hành động cho bot trong game đặt bom đối kháng. Đầu vào của model em dự định sẽ là một ma trận map game mà mỗi phần tử trong ma trận có thể là các vật cản hoặc vị trí của các bomb, đối thủ, bonus,.... Model sẽ có nhiệm vụ xử lý các đầu vào và đưa ra quyết định cho bot trong 5 hành động sau: Up, Down, Left, Right, Set Bomb. Mọi người cho em ý kiến về việc chọn model để thực hiện yêu cầu đó, Cách tổ chức và gắn nhãn dữ liệu. Em định xây dựng model MLP với đầu vào là các map được flattening thành các vector shape(d, 1) và đầu ra là 5 classifier như mô tả ở trên, hoặc model CNN xử lý các ma trận map đầu vào bằng các tầng tích chập để học được kỹ hơn các đặc trưng của map, còn classifier vẫn như MLP với đầu ra 5 lớp.
Em cám ơn bất kỳ đóng góp nào của mọi người.","Em chào mọi người. Em đang thiết kế một model để tạo hành động cho bot trong game đặt bom đối kháng. Đầu vào của model em dự định sẽ là một ma trận map game mà mỗi phần tử trong ma trận có thể là các vật cản hoặc vị trí của các bomb, đối thủ, bonus,.... Model sẽ có nhiệm vụ xử lý các đầu vào và đưa ra quyết định cho bot trong 5 hành động sau: Up, Down, Left, Right, Set Bomb. Mọi người cho em ý kiến về việc chọn model để thực hiện yêu cầu đó, Cách tổ chức và gắn nhãn dữ liệu. Em định xây dựng model MLP với đầu vào là các map được flattening thành các vector shape(d, 1) và đầu ra là 5 classifier như mô tả ở trên, hoặc model CNN xử lý các ma trận map đầu vào bằng các tầng tích chập để học được kỹ hơn các đặc trưng của map, còn classifier vẫn như MLP với đầu ra 5 lớp. Em cám ơn bất kỳ đóng góp nào của mọi người.",,,,,
"Quỹ Đổi mới Sáng tạo Vingroup (VINIF) sẽ chính thức công bố danh sách các dự án nghiên cứu Khoa học - Công nghệ được nhận tài trợ năm 2021. Là năm thứ ba triển khai, chương trình đã thu hút số lượng kỷ lục với 211 hồ sơ đề xuất, từ đó, tiến hành thẩm định khắt khe và kỹ lưỡng để chọn ra 20 dự án tiêu biểu.
Tại sự kiện, Quỹ VINIF cũng sẽ sơ kết hai năm hoạt động của Chương trình Tài trợ Dự án Khoa học - Công nghệ.
Hãy cùng đón đợi những nhóm nghiên cứu được nhận tài trợ năm nay, đồng thời, cùng VINIF điểm lại những cột mốc đáng nhớ trên chặng đường đồng hành cùng các nhà khoa học Việt.
#VINIF #KHCN","Quỹ Đổi mới Sáng tạo Vingroup (VINIF) sẽ chính thức công bố danh sách các dự án nghiên cứu Khoa học - Công nghệ được nhận tài trợ năm 2021. Là năm thứ ba triển khai, chương trình đã thu hút số lượng kỷ lục với 211 hồ sơ đề xuất, từ đó, tiến hành thẩm định khắt khe và kỹ lưỡng để chọn ra 20 dự án tiêu biểu. Tại sự kiện, Quỹ VINIF cũng sẽ sơ kết hai năm hoạt động của Chương trình Tài trợ Dự án Khoa học - Công nghệ. Hãy cùng đón đợi những nhóm nghiên cứu được nhận tài trợ năm nay, đồng thời, cùng VINIF điểm lại những cột mốc đáng nhớ trên chặng đường đồng hành cùng các nhà khoa học Việt.",#VINIF	#KHCN,,,,
Dạ em chào mọi người. Mọi người cho em hỏi có ai trong đây biết web chưa dataset của ảnh UAV mà về cây hoa không ạ. Ai biết xin chỉ dùm em với ạ. Em xin cảm ơn!,Dạ em chào mọi người. Mọi người cho em hỏi có ai trong đây biết web chưa dataset của ảnh UAV mà về cây hoa không ạ. Ai biết xin chỉ dùm em với ạ. Em xin cảm ơn!,,,,,
"Gần đây mình có đăng một số threads về Graph Neural Networks, trong đó có việc ứng dụng GNN trong phân loại ảnh y học tại đây https://www.facebook.com/groups/machinelearningcoban/?multi_permalinks=1351401011983969&comment_id=1351448025312601&notif_id=1638517817496670&notif_t=feedback_reaction_generic&ref=notif
Vì kết quả too good to be true nên mình nghi ngờ, do đó có làm thêm bài toán phân loại bệnh lao phổi dựa trên ảnh X quang vùng ngực dựa trên bài báo của nhóm mình: ""Detection of tuberculosis from chest X-ray images: Boosting the performance with vision transformer and transfer learning"" và source code tại đây: https://github.com/linhduongtuan/Tuberculosis_ChestXray_Classifier.
Kết quả dựa trên GNN cho bài toán phân loại Lao phổi (Turberculosis) vs ảnh không viêm phổi vs ảnh viêm phổi do tác nhân khác, rất khả quan và vượt qua nghiên cứu mình đã công bố trước đây. Cái hay của GNN là nó có thể khắc phục việc lệch (imbalance) của dataset rất tốt!
Tới giờ mình có thể tương đối tự tin nói rằng GNN cho bài toán phân loại ảnh y học sẽ là hướng đi mới, giải quyết những khó khăn trước đây mà CNNs hay Transformers chưa giải quyết được!
PROTIP: Việc build GNN models không có gì khó khăn, nhưng quá trình tiền xử lí ảnh với edge detection rồi chuyển nó thành Graph data là một thách thức!
TIP 1: Quá trình train GNN models rất đơn giản và tốn ít tài nguyên, hoàn toàn có thể dùng Colab Free hay thậm trí dùng CPU
TIP 2: Việc settings hyperparameters để tìm điểm tối ưu cũng rất đơn giản, càng phức tạp hóa việc thiết lập tham số, càng cho kết quả không như mong muốn!
Tạm kết: GNNs có lẽ là tương lai của nhiều lĩnh vực, đặc biệt trong computational Biology/Chemistry/Physics/etc","Gần đây mình có đăng một số threads về Graph Neural Networks, trong đó có việc ứng dụng GNN trong phân loại ảnh y học tại đây https://www.facebook.com/groups/machinelearningcoban/?multi_permalinks=1351401011983969&comment_id=1351448025312601&notif_id=1638517817496670&notif_t=feedback_reaction_generic&ref=notif Vì kết quả too good to be true nên mình nghi ngờ, do đó có làm thêm bài toán phân loại bệnh lao phổi dựa trên ảnh X quang vùng ngực dựa trên bài báo của nhóm mình: ""Detection of tuberculosis from chest X-ray images: Boosting the performance with vision transformer and transfer learning"" và source code tại đây: https://github.com/linhduongtuan/Tuberculosis_ChestXray_Classifier. Kết quả dựa trên GNN cho bài toán phân loại Lao phổi (Turberculosis) vs ảnh không viêm phổi vs ảnh viêm phổi do tác nhân khác, rất khả quan và vượt qua nghiên cứu mình đã công bố trước đây. Cái hay của GNN là nó có thể khắc phục việc lệch (imbalance) của dataset rất tốt! Tới giờ mình có thể tương đối tự tin nói rằng GNN cho bài toán phân loại ảnh y học sẽ là hướng đi mới, giải quyết những khó khăn trước đây mà CNNs hay Transformers chưa giải quyết được! PROTIP: Việc build GNN models không có gì khó khăn, nhưng quá trình tiền xử lí ảnh với edge detection rồi chuyển nó thành Graph data là một thách thức! TIP 1: Quá trình train GNN models rất đơn giản và tốn ít tài nguyên, hoàn toàn có thể dùng Colab Free hay thậm trí dùng CPU TIP 2: Việc settings hyperparameters để tìm điểm tối ưu cũng rất đơn giản, càng phức tạp hóa việc thiết lập tham số, càng cho kết quả không như mong muốn! Tạm kết: GNNs có lẽ là tương lai của nhiều lĩnh vực, đặc biệt trong computational Biology/Chemistry/Physics/etc",,,,,
"Rất vui khi được giới thiệu hướng tiếp cận mới cho bài toán phân loại ảnh y học.
Trước đây, bài toán phân loại ảnh chủ yếu dựa vào kiến trúc mạng CNNs. Trong 2 năm qua, việc ứng dụng kiến trúc Attention/Transformers vào bài toán xử lí ảnh cũng được cộng đồng quốc tế đặc biệt quan tâm.
Tuy nhiên, trong vòng 5 năm qua, kiến trúc Graph Neural Networks (GNN) ngày một phát triển và có những ứng dụng hữu ích vào dữ liệu khác nhau.
Câu hỏi đặt ra là liệu GNNs có thể giải quyết bài toán phân loại ảnh trong y học được không? Thật may, với một số thí nghiệm độc lập mình làm, GNNs động cực kì hiệu quả cho bài toán phân loại các ảnh y khoa khác nhau như Chest X-ray, Optical Coherence Tomography (OCT), Mammography,...
Sau đây là kết quả mình muốn giới thiệu việc dùng GNN để phân loại ảnh OCT tại bài báo ""Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning"" (https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867418301545%3Fshowall%3Dtrue) , dataset tại đây https://data.mendeley.com/datasets/rscbjbr9sj/3. Có 1 lưu ý rằng, nếu dataset này với phiên bản 1 và 2, các models với kiến trúc CNNs hoạt động rất rất tốt với accuracy trên test set > 99%. Tuy nhiên với dataset phiên bản 3 như trong đường dẫn trên, mình đã thử cả CNNs và transformers thì accuracy trên test set <96%, điều này khiến mình khá thất vọng!!!
Một điều thú vị nữa với GNNs là tổng thời gian tiền xử lí ảnh, và huấn luyện chỉ tầm 10h so với vài ngày nếu dùng CNNs hay Transformers! Mà thời gian huấn luyện models chỉ chiếm 10 phút trên RTX 3090, và models hội tụ rất rất nhanh ở ngay những chu kì huấn luyện đầu tiên. Thêm vào đó, phần lớn thời gian còn lại là để biến ảnh thành dạng Graph data (mình dùng 1 Xeon E5 2680v4 với 64G RAM). Lưu ý, để xử lí số ảnh trên, bạn sẽ bị tràn RAM, nhưng giải pháp khắc phục là tăng dung lượng phân vùng SWAP của ổ cứng trong hệ điều hành Ubuntu lên >100G sẽ an toàn! Kết quả cuối cùng accuracy trên tất cả các tập như training, validation và đặc biệt là test set đều HOÀN HẢO!!!
Rất mong nhận được ý kiến chia sẻ, đóng góp, nếu có hợp tác nghiên cứu thì càng tốt!","Rất vui khi được giới thiệu hướng tiếp cận mới cho bài toán phân loại ảnh y học. Trước đây, bài toán phân loại ảnh chủ yếu dựa vào kiến trúc mạng CNNs. Trong 2 năm qua, việc ứng dụng kiến trúc Attention/Transformers vào bài toán xử lí ảnh cũng được cộng đồng quốc tế đặc biệt quan tâm. Tuy nhiên, trong vòng 5 năm qua, kiến trúc Graph Neural Networks (GNN) ngày một phát triển và có những ứng dụng hữu ích vào dữ liệu khác nhau. Câu hỏi đặt ra là liệu GNNs có thể giải quyết bài toán phân loại ảnh trong y học được không? Thật may, với một số thí nghiệm độc lập mình làm, GNNs động cực kì hiệu quả cho bài toán phân loại các ảnh y khoa khác nhau như Chest X-ray, Optical Coherence Tomography (OCT), Mammography,... Sau đây là kết quả mình muốn giới thiệu việc dùng GNN để phân loại ảnh OCT tại bài báo ""Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning"" (https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867418301545%3Fshowall%3Dtrue) , dataset tại đây https://data.mendeley.com/datasets/rscbjbr9sj/3. Có 1 lưu ý rằng, nếu dataset này với phiên bản 1 và 2, các models với kiến trúc CNNs hoạt động rất rất tốt với accuracy trên test set > 99%. Tuy nhiên với dataset phiên bản 3 như trong đường dẫn trên, mình đã thử cả CNNs và transformers thì accuracy trên test set <96%, điều này khiến mình khá thất vọng!!! Một điều thú vị nữa với GNNs là tổng thời gian tiền xử lí ảnh, và huấn luyện chỉ tầm 10h so với vài ngày nếu dùng CNNs hay Transformers! Mà thời gian huấn luyện models chỉ chiếm 10 phút trên RTX 3090, và models hội tụ rất rất nhanh ở ngay những chu kì huấn luyện đầu tiên. Thêm vào đó, phần lớn thời gian còn lại là để biến ảnh thành dạng Graph data (mình dùng 1 Xeon E5 2680v4 với 64G RAM). Lưu ý, để xử lí số ảnh trên, bạn sẽ bị tràn RAM, nhưng giải pháp khắc phục là tăng dung lượng phân vùng SWAP của ổ cứng trong hệ điều hành Ubuntu lên >100G sẽ an toàn! Kết quả cuối cùng accuracy trên tất cả các tập như training, validation và đặc biệt là test set đều HOÀN HẢO!!! Rất mong nhận được ý kiến chia sẻ, đóng góp, nếu có hợp tác nghiên cứu thì càng tốt!",,,,,
"Xin mến chào các anh chị,
Em đang tự học phân tích dữ liệu, hiện em có một vấn đề liên quan đến kết hợp beeswarm plot và boxplot. 
Call beswarm thì bình thường không có gì (Hình chấm xanh), sau đó em tiếp tục call boxplot  thì kết quả mong muốn là cái hình có  beeswarm plot và boxplot.
Nhưng hiện tại error xảy ra khi em thêm parameter ""add = T"", parameter này cho phép chúng ta kết hợp boxplot với current plot. Error: ""Error in xypolygon(xx, yy, lty = ""blank"", col = boxfill[i]): plot.new has not been called yet""
Em đã thử tìm kiếm trên gg về lỗi này nhưng chưa có nhiều thông tin lắm và em cũng đang đăng lên stackoverfollow
https://stackoverflow.com/questions/70277890/beewarm-boxplot-plot-new-has-not-been-called-yet
Trong quá trình chờ đợi các anh chị hỗ trợ em sẽ cố gắng tìm kiếm thêm thông tin về lỗi này.
Xin lỗi vì sự bất tiện
Mến chào các anh chị.","Xin mến chào các anh chị, Em đang tự học phân tích dữ liệu, hiện em có một vấn đề liên quan đến kết hợp beeswarm plot và boxplot. Call beswarm thì bình thường không có gì (Hình chấm xanh), sau đó em tiếp tục call boxplot thì kết quả mong muốn là cái hình có beeswarm plot và boxplot. Nhưng hiện tại error xảy ra khi em thêm parameter ""add = T"", parameter này cho phép chúng ta kết hợp boxplot với current plot. Error: ""Error in xypolygon(xx, yy, lty = ""blank"", col = boxfill[i]): plot.new has not been called yet"" Em đã thử tìm kiếm trên gg về lỗi này nhưng chưa có nhiều thông tin lắm và em cũng đang đăng lên stackoverfollow https://stackoverflow.com/questions/70277890/beewarm-boxplot-plot-new-has-not-been-called-yet Trong quá trình chờ đợi các anh chị hỗ trợ em sẽ cố gắng tìm kiếm thêm thông tin về lỗi này. Xin lỗi vì sự bất tiện Mến chào các anh chị.",,,,,
"Hướng dẫn Edge AI với TensorFlow Lite và Raspberry Pi
Chào mọi người! Mình mới đăng 1 video series trên kênh YouTube của TensorFlow với 4 video về object detection model trên Raspberry Pi bằng TensorFlow Lite. Model này có độ chính xác cao nhưng vẫn chạy khá nhanh (đạt khoảng 6 fps) trên CPU của Raspberry Pi 4. Các video này bao gồm đầy đủ các use case bạn thường gặp khi deploy Edge AI:
1. Download model đã train sẵn từ TensorFlow Hub và tích hợp ngay vào app
2. Tự train model trên tập dữ liệu của mình để phục vụ những ứng dụng mà pretrained models không đáp ứng được.
3. Chọn kiến trúc mô hình (model architecture) nào để phù hợp với nhu cầu của từng ứng dụng.
4. Tăng tốc độ chạy mô hình bằng cách dùng Coral Egde TPU.

Link để xem trên YouTube: goo.gle/3dtM4dU

Nếu mọi người thấy hữu ích thì mình sẽ làm thêm bản tiếng Việt nữa nhé. Nếu các bạn muốn có tutorial gì về Edge AI thì comment lại để mình sẽ tiếp tục làm các video mới trong tương lai. :)",Hướng dẫn Edge AI với TensorFlow Lite và Raspberry Pi Chào mọi người! Mình mới đăng 1 video series trên kênh YouTube của TensorFlow với 4 video về object detection model trên Raspberry Pi bằng TensorFlow Lite. Model này có độ chính xác cao nhưng vẫn chạy khá nhanh (đạt khoảng 6 fps) trên CPU của Raspberry Pi 4. Các video này bao gồm đầy đủ các use case bạn thường gặp khi deploy Edge AI: 1. Download model đã train sẵn từ TensorFlow Hub và tích hợp ngay vào app 2. Tự train model trên tập dữ liệu của mình để phục vụ những ứng dụng mà pretrained models không đáp ứng được. 3. Chọn kiến trúc mô hình (model architecture) nào để phù hợp với nhu cầu của từng ứng dụng. 4. Tăng tốc độ chạy mô hình bằng cách dùng Coral Egde TPU. Link để xem trên YouTube: goo.gle/3dtM4dU Nếu mọi người thấy hữu ích thì mình sẽ làm thêm bản tiếng Việt nữa nhé. Nếu các bạn muốn có tutorial gì về Edge AI thì comment lại để mình sẽ tiếp tục làm các video mới trong tương lai. :),,,,,
"Follow-up thảo luận của mình với anh Long trong post trước, để gắn kết cộng đồng AI Việt Nam hơn, một trong các kế hoạch sắp tới là tổ chức một web seminar thuần Việt về các chủ đề bên khoa học dữ liệu, học máy, thống kê, và trí tuệ nhân tạo. Ngôn ngữ của seminar sẽ hoàn toàn là tiếng Việt (mặc dù slides có thể là tiếng Anh) để các bạn có thể theo dõi dễ dàng hơn. Các speakers sẽ rất đa dạng, từ các bạn đang học những năm cuối ở đại học, các bạn đang làm ngoài industry, các sinh viên cao học, các nhà nghiên cứu ở các industry lab, hay các giáo sư đại học.
Thông tin về mỗi seminar sẽ được cập nhật tại các pages/ blogs để cho mọi người tiện theo dõi.","Follow-up thảo luận của mình với anh Long trong post trước, để gắn kết cộng đồng AI Việt Nam hơn, một trong các kế hoạch sắp tới là tổ chức một web seminar thuần Việt về các chủ đề bên khoa học dữ liệu, học máy, thống kê, và trí tuệ nhân tạo. Ngôn ngữ của seminar sẽ hoàn toàn là tiếng Việt (mặc dù slides có thể là tiếng Anh) để các bạn có thể theo dõi dễ dàng hơn. Các speakers sẽ rất đa dạng, từ các bạn đang học những năm cuối ở đại học, các bạn đang làm ngoài industry, các sinh viên cao học, các nhà nghiên cứu ở các industry lab, hay các giáo sư đại học. Thông tin về mỗi seminar sẽ được cập nhật tại các pages/ blogs để cho mọi người tiện theo dõi.",,,,,
"Em chào mọi người ạ.
HIện tại em có làm một đề tài về nhận dạng hành động dựa trên thông tin khung xương ( là tập 20-25 khớp xương theo thời gian thu được từ kinect). Vì trước đó phần model có bạn làm rồi, tuy nhiên, phần model để nhận dạng thì đầu vào là các đoạn hành động đã được phân tách sẵn từ bộ CSDL, cho vào model để suy ra nhãn hoạt động,
Tuy nhiên, với chuỗi video khung xương thực hiện liên tục nhiều hành động thì cần bài toán phân đoạn/phân vùng (segment) hành động trước ạ, em đã thử một số cách như cửa sổ trượt, hoặc dùng biểu đồ năng lượng short time energy, nhưng vẫn không hiệu quả lắm. Vậy mọi người ai đã làm về hướng phân đoạn hành động dựa trên khung xương có thể suggest cho e 1 số keyword, phương pháp để có thể tự động phân đoạn được không ạ ( phục vụ cho online action recognition).
Em xin chân thành cảm ơn mọi người.","Em chào mọi người ạ. HIện tại em có làm một đề tài về nhận dạng hành động dựa trên thông tin khung xương ( là tập 20-25 khớp xương theo thời gian thu được từ kinect). Vì trước đó phần model có bạn làm rồi, tuy nhiên, phần model để nhận dạng thì đầu vào là các đoạn hành động đã được phân tách sẵn từ bộ CSDL, cho vào model để suy ra nhãn hoạt động, Tuy nhiên, với chuỗi video khung xương thực hiện liên tục nhiều hành động thì cần bài toán phân đoạn/phân vùng (segment) hành động trước ạ, em đã thử một số cách như cửa sổ trượt, hoặc dùng biểu đồ năng lượng short time energy, nhưng vẫn không hiệu quả lắm. Vậy mọi người ai đã làm về hướng phân đoạn hành động dựa trên khung xương có thể suggest cho e 1 số keyword, phương pháp để có thể tự động phân đoạn được không ạ ( phục vụ cho online action recognition). Em xin chân thành cảm ơn mọi người.",,,,,
#transformer,,#transformer,,,,
"Em chào mọi người. Mn cho em hỏi là bài phân tách 2 giọng nói lồng vào nhau trong 1 bản ghi âm dùng Association rule trong Unsupervised learning được không ạ?
Cụ thể là cái ví dụ từ 5h30s của bài này ạ
https://www.youtube.com/watch?v=CCoQ49NASQ8&t=53s
Mọi người giúp em với. Em xin cảm ơn nhiều!",Em chào mọi người. Mn cho em hỏi là bài phân tách 2 giọng nói lồng vào nhau trong 1 bản ghi âm dùng Association rule trong Unsupervised learning được không ạ? Cụ thể là cái ví dụ từ 5h30s của bài này ạ https://www.youtube.com/watch?v=CCoQ49NASQ8&t=53s Mọi người giúp em với. Em xin cảm ơn nhiều!,,,,,
Dành cho những ai quan tâm đến bản dịch sách Deep Learning ạ.,Dành cho những ai quan tâm đến bản dịch sách Deep Learning ạ.,,,,,
"Em xin chào mọi người. Em đang là sinh viên năm 3 ngành Điện tử Viễn thông. Em có định hướng theo ngành Data Science sau khi ra trường. Không biết trong group của mình có anh chị nào cũng background Điện tử Viễn thông mà đi làm về Data Science không ạ, em rất mong được tham khảo lộ trình tự học của các anh chị ạ.
Ngoài ra, các anh chị có thể cho em xin gợi ý về các đề tài có ứng dụng Data Science mà em có thể chọn để làm cho Đồ án cũng như Luận văn tốt nghiệp ngành Điện tử Viễn thông không ạ?
Em xin cảm ơn mọi người nhiều ạ.","Em xin chào mọi người. Em đang là sinh viên năm 3 ngành Điện tử Viễn thông. Em có định hướng theo ngành Data Science sau khi ra trường. Không biết trong group của mình có anh chị nào cũng background Điện tử Viễn thông mà đi làm về Data Science không ạ, em rất mong được tham khảo lộ trình tự học của các anh chị ạ. Ngoài ra, các anh chị có thể cho em xin gợi ý về các đề tài có ứng dụng Data Science mà em có thể chọn để làm cho Đồ án cũng như Luận văn tốt nghiệp ngành Điện tử Viễn thông không ạ? Em xin cảm ơn mọi người nhiều ạ.",,,,,
Mời các bạn tham khảo. Admin của trang này rất xịn nhé.,Mời các bạn tham khảo. Admin của trang này rất xịn nhé.,,,,,
"Đã bao giờ bạn tự hỏi AI là gì và tại sao lại bùng nổ trong những năm gần đây? AI mang lại giá trị như thế nào cho doanh nghiệp? Machine Learning và Deep Learning là gì? ...
Để trả lời cho những câu hỏi này, các bạn có thể tham khảo bài tổng hợp rất hay của McKinsey dưới đây.","Đã bao giờ bạn tự hỏi AI là gì và tại sao lại bùng nổ trong những năm gần đây? AI mang lại giá trị như thế nào cho doanh nghiệp? Machine Learning và Deep Learning là gì? ... Để trả lời cho những câu hỏi này, các bạn có thể tham khảo bài tổng hợp rất hay của McKinsey dưới đây.",,,,,
"Mọi người cho em hỏi, nếu một thuật toán machine learning mà cho ra kết quả có ma trận confusion với % cao thế này thì mô hình đó được đánh giá train tốt ko ạ.? hay là rơi và trường hợp overfit ạ. E xin cảm ơn.","Mọi người cho em hỏi, nếu một thuật toán machine learning mà cho ra kết quả có ma trận confusion với % cao thế này thì mô hình đó được đánh giá train tốt ko ạ.? hay là rơi và trường hợp overfit ạ. E xin cảm ơn.",,,,,
"Chào các bạn, mình đang có một tập dữ liệu về các gói cước viễn thông (data, thoại, sms, hoặc hỗn hợp) và doanh thu, số lượng bán được của từng loại gói cước tại các vùng trong cả nước (tỉnh/TP, quận/huyện, miền). Mỗi gói cước thường được triên khai trong một khoảng thời gian nào đó (ví dụ: gói cước A triển khai từ 1/2021 - 6/2021,...).
Hiện mình muốn sử dụng tập dữ liệu này để đưa ra dự đoán là khi công ty đưa ra một gói cước thì nên tập trung triển khai ở thị trường (tỉnh/huyện/vùng) nào là phù hợp nhất dựa vào các số liệu của các gói cước đã triển khai trước đó. Mình nghĩ đây là bài toán hồi quy nhưng mình chưa có kinh nghiệm đối với bài toán này nên chưa biết nên sử dụng giải thuật nào cho phù hợp hoặc từ khóa để tìm tài liệu tham khảo. Vì vậy mình post lên đây để nhờ các bạn, các anh chị cho lời khuyên giúp về giải thuật thích hợp cho bài toán này hoặc từ khóa để tìm kiếm thêm.
Xin cảm ơn. :)","Chào các bạn, mình đang có một tập dữ liệu về các gói cước viễn thông (data, thoại, sms, hoặc hỗn hợp) và doanh thu, số lượng bán được của từng loại gói cước tại các vùng trong cả nước (tỉnh/TP, quận/huyện, miền). Mỗi gói cước thường được triên khai trong một khoảng thời gian nào đó (ví dụ: gói cước A triển khai từ 1/2021 - 6/2021,...). Hiện mình muốn sử dụng tập dữ liệu này để đưa ra dự đoán là khi công ty đưa ra một gói cước thì nên tập trung triển khai ở thị trường (tỉnh/huyện/vùng) nào là phù hợp nhất dựa vào các số liệu của các gói cước đã triển khai trước đó. Mình nghĩ đây là bài toán hồi quy nhưng mình chưa có kinh nghiệm đối với bài toán này nên chưa biết nên sử dụng giải thuật nào cho phù hợp hoặc từ khóa để tìm tài liệu tham khảo. Vì vậy mình post lên đây để nhờ các bạn, các anh chị cho lời khuyên giúp về giải thuật thích hợp cho bài toán này hoặc từ khóa để tìm kiếm thêm. Xin cảm ơn. :)",,,,,
"Xin chào mọi người ^^ Hôm nay mình rất phấn khích khi giới thiệu với những anh em bài báo Nature Scientific Report (h5-index 200 rank #26 citation in Google Scholar) của mình với tựa đề ""Self-controlling Photonic-on-Chip Networks With Deep Reinforcement Learning "" thuộc nhánh con của Nature family, một hãng công bố báo lớn nhất trên thế giới về khoa học. Bài báo tập trung việc tìm ra cấu trúc mạng quang tử mới và tối ưu các yếu tố vật lý sao cho transmission loss, power consumption, routing time giữa các nốt quang là nhỏ nhất bằng việc sử dụng giải thuật Reinforcement Learning mới do nhóm đề xuất đó là Multi Sample Discovery. Hi vọng anh em nào có thời gian có thể tìm hiểu =))
Bài báo được tài trợ bởi quỹ #VinIF và thuộc nhóm nghiên cứu của PTIT, VinAi Research, StonyBrook University USA.
Link here : https://www.nature.com/articles/s41598-021-02583-7?fbclid=IwAR3ZFoDelt9-lQhuaL-a4grt-cMfMcSYV72_ZVwdPTFK_LOrz1whASNZAtM","Xin chào mọi người ^^ Hôm nay mình rất phấn khích khi giới thiệu với những anh em bài báo Nature Scientific Report (h5-index 200 rank citation in Google Scholar) của mình với tựa đề ""Self-controlling Photonic-on-Chip Networks With Deep Reinforcement Learning "" thuộc nhánh con của Nature family, một hãng công bố báo lớn nhất trên thế giới về khoa học. Bài báo tập trung việc tìm ra cấu trúc mạng quang tử mới và tối ưu các yếu tố vật lý sao cho transmission loss, power consumption, routing time giữa các nốt quang là nhỏ nhất bằng việc sử dụng giải thuật Reinforcement Learning mới do nhóm đề xuất đó là Multi Sample Discovery. Hi vọng anh em nào có thời gian có thể tìm hiểu =)) Bài báo được tài trợ bởi quỹ và thuộc nhóm nghiên cứu của PTIT, VinAi Research, StonyBrook University USA. Link here : https://www.nature.com/articles/s41598-021-02583-7?fbclid=IwAR3ZFoDelt9-lQhuaL-a4grt-cMfMcSYV72_ZVwdPTFK_LOrz1whASNZAtM",#26	#VinIF,,,,
"Xin chào mọi người, mọi người cho mình muốn xin review về 2 cuốn này, mình thắc mắc là nên đọc cuốn nào (do 2 cuốn khá dài nên mình tính đọc một trong hai). Mình làm chủ yếu bên Deep Learning nên nhu cầu mình thiên về hướng đọc để hiểu hơn về toán và các thuật toán ML trước đây hơn.
Nếu được mong mọi người giới thiệu mình một số sách về Statistic nói về toán nhiều và công thức đọc dễ hiểu một xíu (Mình có đọc All of Statistics A Concise Course in Statistical Inference, nhưng tới tầm chương 16 là mình đọc bắt đầu thấy mơ hồ).
Mình xin cám ơn mọi người.","Xin chào mọi người, mọi người cho mình muốn xin review về 2 cuốn này, mình thắc mắc là nên đọc cuốn nào (do 2 cuốn khá dài nên mình tính đọc một trong hai). Mình làm chủ yếu bên Deep Learning nên nhu cầu mình thiên về hướng đọc để hiểu hơn về toán và các thuật toán ML trước đây hơn. Nếu được mong mọi người giới thiệu mình một số sách về Statistic nói về toán nhiều và công thức đọc dễ hiểu một xíu (Mình có đọc All of Statistics A Concise Course in Statistical Inference, nhưng tới tầm chương 16 là mình đọc bắt đầu thấy mơ hồ). Mình xin cám ơn mọi người.",,,,,
"Một trong những ứng dụng hay và thú vị nhất trong những năm gần đây là Neural Style Transfer.
Nhờ có ứng dụng này, chúng ta có thể mô phỏng phong cách nghệ thuật của những hoạ sĩ huyền thoại như Picasso, Van Gogh hay của bất kỳ một hoạ sĩ nào khác.
Trong bài hôm nay, hãy cũng tìm hiểu sơ lược về Neural Style Transfer và cách áp dụng nó để hồi sinh những phong cách hội hoạ nổi tiếng trên thế giới.","Một trong những ứng dụng hay và thú vị nhất trong những năm gần đây là Neural Style Transfer. Nhờ có ứng dụng này, chúng ta có thể mô phỏng phong cách nghệ thuật của những hoạ sĩ huyền thoại như Picasso, Van Gogh hay của bất kỳ một hoạ sĩ nào khác. Trong bài hôm nay, hãy cũng tìm hiểu sơ lược về Neural Style Transfer và cách áp dụng nó để hồi sinh những phong cách hội hoạ nổi tiếng trên thế giới.",,,,,
"Xin chào mọi người,
Em đang tìm hiểu về Non-Subsampled Shearlet Transform.
Em đọc công thức này mà không hiểu cách tinh toán ma trận sau để lấy được output 🙁
Các anh trong nhóm đã làm phần này cho em hỏi cách tính toán trên ma trận để lấy được output hoặc cho em xin tài liệu phần này tiếng Việt ạ.
Em xin cảm ơn mọi người","Xin chào mọi người, Em đang tìm hiểu về Non-Subsampled Shearlet Transform. Em đọc công thức này mà không hiểu cách tinh toán ma trận sau để lấy được output Các anh trong nhóm đã làm phần này cho em hỏi cách tính toán trên ma trận để lấy được output hoặc cho em xin tài liệu phần này tiếng Việt ạ. Em xin cảm ơn mọi người",,,,,
"Thấy nhiều bạn trẻ hay khóc khi hỏi, mình mới thêm nội quy này.","Thấy nhiều bạn trẻ hay khóc khi hỏi, mình mới thêm nội quy này.",,,,,
"Xin chào mọi người.
Em đang thực hiện bài toán hồi quy với mạng 2 lớp ẩn. Sau khi huấn luyện mô hình thì muốn đánh giá tầm quan trọng của các biến đầu vào bằng thuật toán garson và thuật toán trọng số kết nối(CW). Em đọc một số tài liệu hướng dẫn thì thấy 2 thuật toán này hướng dẫn cho mạng có một lớp ẩn. nên không biết với số lớp ẩn lớn hơn 2 thì 2 thuật toán này có áp dụng được không?. Nếu được nhờ mọi người chỉ dẫn giúp em với. em mới học về về Machine Learning chưa biết nhiều mong mọi người giúp đỡ nhiều.",Xin chào mọi người. Em đang thực hiện bài toán hồi quy với mạng 2 lớp ẩn. Sau khi huấn luyện mô hình thì muốn đánh giá tầm quan trọng của các biến đầu vào bằng thuật toán garson và thuật toán trọng số kết nối(CW). Em đọc một số tài liệu hướng dẫn thì thấy 2 thuật toán này hướng dẫn cho mạng có một lớp ẩn. nên không biết với số lớp ẩn lớn hơn 2 thì 2 thuật toán này có áp dụng được không?. Nếu được nhờ mọi người chỉ dẫn giúp em với. em mới học về về Machine Learning chưa biết nhiều mong mọi người giúp đỡ nhiều.,,,,,
"Cho mình hỏi bạn nào biết trong Tensorrt thì có ba định dạng file là : .trt , .engine và .plan. Vậy chúng khác nhau thế nào? ai biết chỉ giùm nhé. Cảm ơn nhiều","Cho mình hỏi bạn nào biết trong Tensorrt thì có ba định dạng file là : .trt , .engine và .plan. Vậy chúng khác nhau thế nào? ai biết chỉ giùm nhé. Cảm ơn nhiều",,,,,
"hello ae
mình tìm thấy cheatsheet tổng hợp về AI, NN, ML, DL và Big Data khá hay và xịn xò nên share cho mn cùng tìm hiểu 😁
nguồn: LearnDataScience","hello ae mình tìm thấy cheatsheet tổng hợp về AI, NN, ML, DL và Big Data khá hay và xịn xò nên share cho mn cùng tìm hiểu nguồn: LearnDataScience",,,,,
"Có ai đã từng gặp lỗi này khi dùng shap để phân tích feature chưa ạ, search mãi k ra :(","Có ai đã từng gặp lỗi này khi dùng shap để phân tích feature chưa ạ, search mãi k ra :(",,,,,
"#aivivn #timeseries

AIviVN xin trân trọng thông báo cuộc thi thứ tư. Một cuộc thi về time series và có tổng giải thưởng là 15 triệu đồng.

Cảm ơn một công ty giấu tên đã cung cấp dữ liệu và giải thưởng cho cuộc thi :).
https://www.aivivn.com/contests/4",AIviVN xin trân trọng thông báo cuộc thi thứ tư. Một cuộc thi về time series và có tổng giải thưởng là 15 triệu đồng. Cảm ơn một công ty giấu tên đã cung cấp dữ liệu và giải thưởng cho cuộc thi :). https://www.aivivn.com/contests/4,#aivivn	#timeseries,,,,
"🔻[𝐑𝐞𝐠𝐢𝐬𝐭𝐫𝐚𝐭𝐢𝐨𝐧 𝐎𝐩𝐞𝐧] 𝐃𝐀𝐓𝐀-𝐂𝐄𝐍𝐓𝐑𝐈𝐂 𝐆𝐎 𝐁𝐀𝐂𝐊 𝐓𝐎 𝐁𝐀𝐒𝐈𝐂𝐒 ‼️
TECH INNOVATORS #5: GẶP GỠ BỘ ÓC THIÊN TÀI DEEP LEARNING- ANDREW NG
Andrew Ng - Người được xem là thiên tài Deep Learning, một trong người đi đầu trong lĩnh vực Trí tuệ nhân tạo. Ông là Co-founder của Google Brains & Coursera và sáng lập của Landing Ai.
Ngoài ra, sự kiện có sự hiện diện của:
- Thạc sĩ Khoa học máy tính Huyen Chip - Giảng viên kiêm nhiệm tại Đại học Stanford, Hoa Kỳ & Top 5 trong danh sách Top Voices về lĩnh vực Khoa học Dữ liệu và Trí Thông minh Nhân tạo do LinkedIn bình chọn năm 2020
-  Ms. Lynn He - Chuyên gia AI quốc tế tại Deeplearning.AI & Cựu Applied Machine Learning Intensive tại Google, Hoa Kỳ
📌 Bạn sẽ nhận được gì khi tham gia Tech Innovators #5
- Andrew Ng, Huyen Chip và Lynn He thảo luận về những bài toán Data, cách tiếp cận Data-centric, Data Automation trong câu chuyện công nghệ.
Tại sao chúng ta nên trở lại với nền tảng Data cốt lõi? Liệu bạn đang chỉ quan tâm đến việc phác thảo mô hình AI & Machine Learning mà quên đi những yếu tố cốt lõi tạo ra một ""kiến trúc kiên cố""? 
Data-centric là hướng tiếp cận tập trung một kiến trúc nơi dữ liệu là tài sản cốt lõi và lâu dài. Với Data-centric model, mô hình dữ liệu là nền móng đầu tiên trong việc triển khai bất kỳ ứng dụng hay AI model nào và sẽ yếu tố tồn tại và có giá trị lâu dài.
✍️ Đăng ký tham dự và đặt câu hỏi cho khách mời tại link: https://bit.ly/Register_TechInnovators5
Đăng ký và đặt câu hỏi ngay , để có cơ hội trở thành người trực tiếp trò chuyện cùng những bộ óc thiên tài ngành Ai.","[ ] - ‼ TECH INNOVATORS GẶP GỠ BỘ ÓC THIÊN TÀI DEEP LEARNING- ANDREW NG Andrew Ng - Người được xem là thiên tài Deep Learning, một trong người đi đầu trong lĩnh vực Trí tuệ nhân tạo. Ông là Co-founder của Google Brains & Coursera và sáng lập của Landing Ai. Ngoài ra, sự kiện có sự hiện diện của: - Thạc sĩ Khoa học máy tính Huyen Chip - Giảng viên kiêm nhiệm tại Đại học Stanford, Hoa Kỳ & Top 5 trong danh sách Top Voices về lĩnh vực Khoa học Dữ liệu và Trí Thông minh Nhân tạo do LinkedIn bình chọn năm 2020 - Ms. Lynn He - Chuyên gia AI quốc tế tại Deeplearning.AI & Cựu Applied Machine Learning Intensive tại Google, Hoa Kỳ Bạn sẽ nhận được gì khi tham gia Tech Innovators - Andrew Ng, Huyen Chip và Lynn He thảo luận về những bài toán Data, cách tiếp cận Data-centric, Data Automation trong câu chuyện công nghệ. Tại sao chúng ta nên trở lại với nền tảng Data cốt lõi? Liệu bạn đang chỉ quan tâm đến việc phác thảo mô hình AI & Machine Learning mà quên đi những yếu tố cốt lõi tạo ra một ""kiến trúc kiên cố""? Data-centric là hướng tiếp cận tập trung một kiến trúc nơi dữ liệu là tài sản cốt lõi và lâu dài. Với Data-centric model, mô hình dữ liệu là nền móng đầu tiên trong việc triển khai bất kỳ ứng dụng hay AI model nào và sẽ yếu tố tồn tại và có giá trị lâu dài. Đăng ký tham dự và đặt câu hỏi cho khách mời tại link: https://bit.ly/Register_TechInnovators5 Đăng ký và đặt câu hỏi ngay , để có cơ hội trở thành người trực tiếp trò chuyện cùng những bộ óc thiên tài ngành Ai.",#5:	#5,,,,
"Em xin chào mọi người, em đang chạy SVM Multiclass Classification cho một bộ data về cảm biến khí. Em muốn vẽ các đường support vector sau khi chạy ra kết quả. Cho em hỏi có cách nào để vẽ không vậy ạ? Em xin cảm ơn mọi người.
Nguồn ảnh : https://github.com/cran/gensvm","Em xin chào mọi người, em đang chạy SVM Multiclass Classification cho một bộ data về cảm biến khí. Em muốn vẽ các đường support vector sau khi chạy ra kết quả. Cho em hỏi có cách nào để vẽ không vậy ạ? Em xin cảm ơn mọi người. Nguồn ảnh : https://github.com/cran/gensvm",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 11/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.",,,,,
"Em xin chào mọi người, em hiện chỉ mới đang tập tành về Machine Learning. Em có một câu hỏi liên quan đến cơ sở dữ liệu cho 1 project ML là:
Tại sao khi xây dựng một dự án Machine Learning chúng ta lại cần phải có một cơ sở dữ liệu, có phải máy sẽ học bằng cách nhìn vào tập cơ sở dữ liệu đó để có thể rút ra được kinh nghiệm và từ đó training được model không ạ?
Nếu cơ sở dữ liệu chúng ta sử dụng là các ảnh thang xám nhưng đầu vào chúng ta đưa vô hệ thống ML là ảnh màu (có độ sáng tối khác với CSDL) thì chúng ta cần phải khắc phục thế nào?
Em xin cảm ơn các anh chị ạ!","Em xin chào mọi người, em hiện chỉ mới đang tập tành về Machine Learning. Em có một câu hỏi liên quan đến cơ sở dữ liệu cho 1 project ML là: Tại sao khi xây dựng một dự án Machine Learning chúng ta lại cần phải có một cơ sở dữ liệu, có phải máy sẽ học bằng cách nhìn vào tập cơ sở dữ liệu đó để có thể rút ra được kinh nghiệm và từ đó training được model không ạ? Nếu cơ sở dữ liệu chúng ta sử dụng là các ảnh thang xám nhưng đầu vào chúng ta đưa vô hệ thống ML là ảnh màu (có độ sáng tối khác với CSDL) thì chúng ta cần phải khắc phục thế nào? Em xin cảm ơn các anh chị ạ!",,,,,
"Có anh em nào tự áp dụng ML/DL vào ứng dụng thực tế cho mình xin ít kinh nghiệm với, hiện tại BE mình có php8, FE thì là reactjs, mobile là React native.
Muốn áp dụng ML vào dự án (giả sử 1 tính năng nào đó) thì nên dùng cách nào khả thi vậy ạ ?
1 - build model.h5 ở local rồi dùng tensorflowjs để load lên/predict
2 - dựng thêm server python để xử lý
3 - phương án khác","Có anh em nào tự áp dụng ML/DL vào ứng dụng thực tế cho mình xin ít kinh nghiệm với, hiện tại BE mình có php8, FE thì là reactjs, mobile là React native. Muốn áp dụng ML vào dự án (giả sử 1 tính năng nào đó) thì nên dùng cách nào khả thi vậy ạ ? 1 - build model.h5 ở local rồi dùng tensorflowjs để load lên/predict 2 - dựng thêm server python để xử lý 3 - phương án khác",,,,,
"Chào mọi người.
Em đang tìm hiểu cách giải quyết bài toán tối ưu theo cách maximum a posterior cho thuật toán Naive Bayes Classifier. Em muốn hỏi rằng hàm prior (biến đổi từ posterior) của phương pháp MAP) và hàm prior của hàm mục tiêu Naive Bayes có giống nhau không ạ?
Em cảm ơn ạ.",Chào mọi người. Em đang tìm hiểu cách giải quyết bài toán tối ưu theo cách maximum a posterior cho thuật toán Naive Bayes Classifier. Em muốn hỏi rằng hàm prior (biến đổi từ posterior) của phương pháp MAP) và hàm prior của hàm mục tiêu Naive Bayes có giống nhau không ạ? Em cảm ơn ạ.,,,,,
"Chào ace,
Mô hình của mình sau khi đánh giá thì trên tập test lỗi bé hơn tập train, accuracy tập test cũng tốt hơn như hình. Vậy mình cần điều chỉnh gì ko ạ? Cùng bộ dữ liệu này, các mô hình khác của mình đều thấy trên test ko tốt bằng train ạ.
Cám ơn mn!","Chào ace, Mô hình của mình sau khi đánh giá thì trên tập test lỗi bé hơn tập train, accuracy tập test cũng tốt hơn như hình. Vậy mình cần điều chỉnh gì ko ạ? Cùng bộ dữ liệu này, các mô hình khác của mình đều thấy trên test ko tốt bằng train ạ. Cám ơn mn!",,,,,
"Chào mọi người, mình hiện tại đang làm về bài toán liên quan tới gene expression. Mình vẫn đang thắc mắc là dữ liệu về bài toán này thì mình nên tiếp cận theo microarray hay là rna-seq? Đối với rna-seq thì file dữ liệu mình thu được là fastq hay là file cuối cùng đã được xử lý và chuẩn hóa rồi nhỉ? Mong mn ai có kinh nghiệm giải đáp giúp mình.","Chào mọi người, mình hiện tại đang làm về bài toán liên quan tới gene expression. Mình vẫn đang thắc mắc là dữ liệu về bài toán này thì mình nên tiếp cận theo microarray hay là rna-seq? Đối với rna-seq thì file dữ liệu mình thu được là fastq hay là file cuối cùng đã được xử lý và chuẩn hóa rồi nhỉ? Mong mn ai có kinh nghiệm giải đáp giúp mình.",,,,,
"💥💥 TECH INNOVATORS #5: DATA-CENTRIC GO BACK TO BASICS.
⏰ Thời gian: 9h30-11h00 ngày 02/12/2021 tại Fanpage FPT Software.
✍️ Đăng ký ngay và đặt câu hỏi dành cho các diễn giả của TECH INNOVATORS: GO BACK TO BASICS: https://bit.ly/Register_TechInnovators5
👥 Sự kiện có sự góp mặt của:
- Prof. ANDREW NG - nhà khoa học máy tính và doanh nhân công nghệ người Mỹ hoạt động trong lĩnh vực học máy và trí tuệ nhân tạo. Ông là nhà đồng sáng lập kiêm người đứng đầu Google Brain. Andrew Ng được mệnh danh là người có bộ óc thiên tài về Deep learning và người tạo nên đột phá với Data-Centric, dữ liệu được ông đánh giá là nguồn thức ăn cho AI.
Andrew Ng cũng là một trong những nhà khoa học máy tính nổi tiếng được giới phát triển AI vô cùng kính nể. Tạp chí TIME từng vinh danh Andrew là một trong 100 người có ảnh hưởng nhất năm 2012.
- Nguyễn Thị Khánh Huyền (HUYEN CHIP) - Thạc sĩ ngành khoa học máy tính tại Đại học Stanford, đứng thứ 5 trong danh sách Top Voices về lĩnh vực Khoa học Dữ liệu và Trí Thông minh Nhân tạo do LinkedIn bình chọn năm 2020.
Từng có kinh nghiệm làm việc tại các công ty công nghệ nổi tiếng như Netflix, NVIDIA; hay đảm nhiệm vị trí Kỹ sư Machine Learning tại Snorkel AI. Hiện tại, Huyen Chip đang là giảng viên kiêm nhiệm tại Đại học Stanford, Mỹ.
- LYNN HE - Chuyên gia AI quốc tế, đồng thời đảm nhận vị trí chuyên gia kỹ thuật tại Deeplearning.ai. Lynn He lần đầu tiên tiếp xúc với AI thông qua một chương trình học máy ứng dụng chuyên sâu tại Google và kể từ đó, chị đã thực hiện nghiên cứu AI về các chủ đề phát hiện các mô típ trong bộ dữ liệu hội họa cho đến quy định và chính sách.","TECH INNOVATORS DATA-CENTRIC GO BACK TO BASICS. ⏰ Thời gian: 9h30-11h00 ngày 02/12/2021 tại Fanpage FPT Software. Đăng ký ngay và đặt câu hỏi dành cho các diễn giả của TECH INNOVATORS: GO BACK TO BASICS: https://bit.ly/Register_TechInnovators5 Sự kiện có sự góp mặt của: - Prof. ANDREW NG - nhà khoa học máy tính và doanh nhân công nghệ người Mỹ hoạt động trong lĩnh vực học máy và trí tuệ nhân tạo. Ông là nhà đồng sáng lập kiêm người đứng đầu Google Brain. Andrew Ng được mệnh danh là người có bộ óc thiên tài về Deep learning và người tạo nên đột phá với Data-Centric, dữ liệu được ông đánh giá là nguồn thức ăn cho AI. Andrew Ng cũng là một trong những nhà khoa học máy tính nổi tiếng được giới phát triển AI vô cùng kính nể. Tạp chí TIME từng vinh danh Andrew là một trong 100 người có ảnh hưởng nhất năm 2012. - Nguyễn Thị Khánh Huyền (HUYEN CHIP) - Thạc sĩ ngành khoa học máy tính tại Đại học Stanford, đứng thứ 5 trong danh sách Top Voices về lĩnh vực Khoa học Dữ liệu và Trí Thông minh Nhân tạo do LinkedIn bình chọn năm 2020. Từng có kinh nghiệm làm việc tại các công ty công nghệ nổi tiếng như Netflix, NVIDIA; hay đảm nhiệm vị trí Kỹ sư Machine Learning tại Snorkel AI. Hiện tại, Huyen Chip đang là giảng viên kiêm nhiệm tại Đại học Stanford, Mỹ. - LYNN HE - Chuyên gia AI quốc tế, đồng thời đảm nhận vị trí chuyên gia kỹ thuật tại Deeplearning.ai. Lynn He lần đầu tiên tiếp xúc với AI thông qua một chương trình học máy ứng dụng chuyên sâu tại Google và kể từ đó, chị đã thực hiện nghiên cứu AI về các chủ đề phát hiện các mô típ trong bộ dữ liệu hội họa cho đến quy định và chính sách.",#5:,,,,
"Chào mọi người, em đang làm việc với dataset của em là 1 large numpy array 3D kích thước (11187, 2000, 1024) (có thể coi mỗi data point hay sample trong dataset là 1 ma trận kích thước 2000x1024). Để có thể load dữ liệu vào mô hình, em ghi numpy array dataset trên ra file HDF5 rồi thực hiện đọc vào tạo ra 1 HDF5 Dataset object, sau đó tạo 1 generator function để có thể iterate qua HDF5 dataset object như trên, cuối cùng pass vào tf.data input pipeline bởi tf.data.Dataset.from_generator. Sau đó có thể fetch vào trong functional API model mà em đã khởi tạo cho mô hình với method fit. Tuy nhiên trong quá trình làm, em để ý thấy 1 số điều em muốn hỏi mọi ngườiarray dataset trên ra file HDF5 rồi thực hiện đọc vào tạo ra 1 HDF5 Dataset object, sau đó tạo 1 generator function để có thể iterate qua HDF5 dataset object như trên, cuối cùng pass vào tf.data input pipeline bởi tf.data.Dataset.from_generator. Sau đó có thể fetch vào trong functional API model mà em đã khởi tạo cho mô hình với method fit. Em đang train trong colab Pro+ với V100 chế độ High-RAM.Tuy nhiên trong quá trình làm, em để ý thấy 1 số điều em muốn hỏi mọi người
1. Em có thử viết training custom loop với GradientTape để làm phép so sánh với method fit của functional, có vẻ như training custom loop chậm hơn và kết quả không giống so với method fit. Mà em tra một số kết quả thì thấy method fit built-in của tensorflow khá chậm nên mới đi viết training custom loop. Vậy nên dùng method fit hay implement subclass model với GradientTape để cải thiện tốc độ ạ (em đã có set các NVIDIA GPU flags cho tối ưu tốc độ)
2. Hard-disk của Colab có tăng lên khá nhiều khi em train mô hình, tăng vài chục Gb Disk tuy nhiên vẫn train được mô hình. Em có tra thử nhưng chưa ra được vấn đề này ạ, mọi người có ai gặp vấn đề này không ạ.
3. Đối với dữ liệu kích thước rất lớn, em biết là nên tạo generator cho dữ liệu, và để có thể chuẩn bị tốt quá trình input data cho model thì với tensorflow họ recommed tf.data input pipeline. Ở đây để pass generator function em tạo cho dữ liệu (tạo iterator cho từng điểm dữ liệu rồi next qua từng iterator) vào tf.data thì em dùng tf.data.Dataset.from_generator tuy nhiên method lại được nói trong docs của tensorflow là nên hạn chế sử dụng và có tốc độ khá chậm đọc dữ liệu. Vậy thì để pass generator vào được tf.data input pipeline thì hoặc fetch vào mô hình một cách hiệu quả thì nên dùng gì ạ
Em cảm ơn mọi người đã đọc tới đây, 3 câu hỏi của em khá dài tuy nhiên em cũng cố gắng diễn đạt concise nhất có thể context và problem của em. Em xin cảm ơn ạ.","Chào mọi người, em đang làm việc với dataset của em là 1 large numpy array 3D kích thước (11187, 2000, 1024) (có thể coi mỗi data point hay sample trong dataset là 1 ma trận kích thước 2000x1024). Để có thể load dữ liệu vào mô hình, em ghi numpy array dataset trên ra file HDF5 rồi thực hiện đọc vào tạo ra 1 HDF5 Dataset object, sau đó tạo 1 generator function để có thể iterate qua HDF5 dataset object như trên, cuối cùng pass vào tf.data input pipeline bởi tf.data.Dataset.from_generator. Sau đó có thể fetch vào trong functional API model mà em đã khởi tạo cho mô hình với method fit. Tuy nhiên trong quá trình làm, em để ý thấy 1 số điều em muốn hỏi mọi ngườiarray dataset trên ra file HDF5 rồi thực hiện đọc vào tạo ra 1 HDF5 Dataset object, sau đó tạo 1 generator function để có thể iterate qua HDF5 dataset object như trên, cuối cùng pass vào tf.data input pipeline bởi tf.data.Dataset.from_generator. Sau đó có thể fetch vào trong functional API model mà em đã khởi tạo cho mô hình với method fit. Em đang train trong colab Pro+ với V100 chế độ High-RAM.Tuy nhiên trong quá trình làm, em để ý thấy 1 số điều em muốn hỏi mọi người 1. Em có thử viết training custom loop với GradientTape để làm phép so sánh với method fit của functional, có vẻ như training custom loop chậm hơn và kết quả không giống so với method fit. Mà em tra một số kết quả thì thấy method fit built-in của tensorflow khá chậm nên mới đi viết training custom loop. Vậy nên dùng method fit hay implement subclass model với GradientTape để cải thiện tốc độ ạ (em đã có set các NVIDIA GPU flags cho tối ưu tốc độ) 2. Hard-disk của Colab có tăng lên khá nhiều khi em train mô hình, tăng vài chục Gb Disk tuy nhiên vẫn train được mô hình. Em có tra thử nhưng chưa ra được vấn đề này ạ, mọi người có ai gặp vấn đề này không ạ. 3. Đối với dữ liệu kích thước rất lớn, em biết là nên tạo generator cho dữ liệu, và để có thể chuẩn bị tốt quá trình input data cho model thì với tensorflow họ recommed tf.data input pipeline. Ở đây để pass generator function em tạo cho dữ liệu (tạo iterator cho từng điểm dữ liệu rồi next qua từng iterator) vào tf.data thì em dùng tf.data.Dataset.from_generator tuy nhiên method lại được nói trong docs của tensorflow là nên hạn chế sử dụng và có tốc độ khá chậm đọc dữ liệu. Vậy thì để pass generator vào được tf.data input pipeline thì hoặc fetch vào mô hình một cách hiệu quả thì nên dùng gì ạ Em cảm ơn mọi người đã đọc tới đây, 3 câu hỏi của em khá dài tuy nhiên em cũng cố gắng diễn đạt concise nhất có thể context và problem của em. Em xin cảm ơn ạ.",,,,,
"Chào mọi người
Em đang nghiên cứu phân loại, phân cụm khách hàng bằng các thuật toán học không giám sát FCM và PFCM với kết quả dữ liệu của thuật toán RFM. Sau khi chạy phân cụm được khách hàng với 2 thuật toán FCM và PFCM xong rồi. Nhưng em không biết làm sao để đánh giá được thuật toán FCM và PFCM cái nào tốt hơn. Em có xem 1 số cách đánh giá khi chạy qua các thuật toán chỉ số đánh giá như PC or XB, mà em lại đang không nắm rõ được cách đánh giá của nó. Nên em mong muốn có cao nhân nào trong group nghiên cứu mảng này truyền đạo giúp em. Em sẽ cảm ơn và hậu tạ ạ","Chào mọi người Em đang nghiên cứu phân loại, phân cụm khách hàng bằng các thuật toán học không giám sát FCM và PFCM với kết quả dữ liệu của thuật toán RFM. Sau khi chạy phân cụm được khách hàng với 2 thuật toán FCM và PFCM xong rồi. Nhưng em không biết làm sao để đánh giá được thuật toán FCM và PFCM cái nào tốt hơn. Em có xem 1 số cách đánh giá khi chạy qua các thuật toán chỉ số đánh giá như PC or XB, mà em lại đang không nắm rõ được cách đánh giá của nó. Nên em mong muốn có cao nhân nào trong group nghiên cứu mảng này truyền đạo giúp em. Em sẽ cảm ơn và hậu tạ ạ",,,,,
"Xin chào mọi người, hiện tại em đang giải quyết một bài toán liên quan đến tính khoảng cách giữa người với người, dựa trên input có góc chụp là góc ngang. Vì là góc ngang nên sẽ khó xác định được khoảng cách, nên em muốn bằng làm cách nào đó để có thể chuyển đổi hình ảnh này sang góc chụp từ trên xuống (top-view), output không phải là một hình ảnh mà là tập hợp điểm trên đồ thị, mỗi điểm đại diện cho một cá thể người.
Để giải thích rõ ràng hơn thì vấn đề của em gần giống như công nghệ GOAL LINE trong bóng đá, mọi người có thể xem đoạn từ 2:41 -> 2:45 (https://youtu.be/plu8zxhU3Cw?t=161). Trong video thì mọi người thấy camera sẽ chuyển từ góc chụp ngang sang góc chụp từ trên xuống.
Mục đích em làm vậy là bởi vì em muốn áp dụng DBSCAN clustering để giải quyết bài toán giãn cách xã hội, nhưng lại gặp phải vấn đề phải phân biệt được những người ở gần camera và những người ở xa camera.
Em có một cách làm khá ""ngô nghê"" và không mấy hiệu quả: Vì là góc chụp ngang nên hoàn toàn xác định được vị trí của người đó theo trục Ox, còn chiều dọc thì em sẽ detect human head và tính diện tích của bounding box, nếu diện tích càng nhỏ nghĩa là người đó đang ở xa camera, nên giá trị trên trục Oy sẽ lớn.
Không biết mọi người có một phương pháp hiệu quả nào hơn không ạ. Em xin chân thành cảm ơn!

p.s: 
Trong hình 1 là em sử dụng pre-trained model trên mạng nên detect human head còn xót ""1 cái đầu"" không detect được (có thể tập dataset không có nhiều hình đeo khẩu trang)
Hình 2 là kết quả mà em xử lý nhưng vẫn còn chưa chính xác.","Xin chào mọi người, hiện tại em đang giải quyết một bài toán liên quan đến tính khoảng cách giữa người với người, dựa trên input có góc chụp là góc ngang. Vì là góc ngang nên sẽ khó xác định được khoảng cách, nên em muốn bằng làm cách nào đó để có thể chuyển đổi hình ảnh này sang góc chụp từ trên xuống (top-view), output không phải là một hình ảnh mà là tập hợp điểm trên đồ thị, mỗi điểm đại diện cho một cá thể người. Để giải thích rõ ràng hơn thì vấn đề của em gần giống như công nghệ GOAL LINE trong bóng đá, mọi người có thể xem đoạn từ 2:41 -> 2:45 (https://youtu.be/plu8zxhU3Cw?t=161). Trong video thì mọi người thấy camera sẽ chuyển từ góc chụp ngang sang góc chụp từ trên xuống. Mục đích em làm vậy là bởi vì em muốn áp dụng DBSCAN clustering để giải quyết bài toán giãn cách xã hội, nhưng lại gặp phải vấn đề phải phân biệt được những người ở gần camera và những người ở xa camera. Em có một cách làm khá ""ngô nghê"" và không mấy hiệu quả: Vì là góc chụp ngang nên hoàn toàn xác định được vị trí của người đó theo trục Ox, còn chiều dọc thì em sẽ detect human head và tính diện tích của bounding box, nếu diện tích càng nhỏ nghĩa là người đó đang ở xa camera, nên giá trị trên trục Oy sẽ lớn. Không biết mọi người có một phương pháp hiệu quả nào hơn không ạ. Em xin chân thành cảm ơn! p.s: Trong hình 1 là em sử dụng pre-trained model trên mạng nên detect human head còn xót ""1 cái đầu"" không detect được (có thể tập dataset không có nhiều hình đeo khẩu trang) Hình 2 là kết quả mà em xử lý nhưng vẫn còn chưa chính xác.",,,,,
"Chào mọi người. Em đang làm bài toán liên quan tới chỉnh sửa ảnh và muốn thử các encoder khác ngoài StyleGAN, e4e và ALAE. Vì em thấy StyleGAN encode lâu quá :< còn 2 cái còn lại thì reconstructred image khác khá nhiều so với input. Mong anh chị chia sẻ thêm các hướng khác ạ. Nếu k thì em có thể làm gì để cải thiện kết quả từ 3 encoder kia được ạ.
Em cảm ơn ạ.","Chào mọi người. Em đang làm bài toán liên quan tới chỉnh sửa ảnh và muốn thử các encoder khác ngoài StyleGAN, e4e và ALAE. Vì em thấy StyleGAN encode lâu quá :< còn 2 cái còn lại thì reconstructred image khác khá nhiều so với input. Mong anh chị chia sẻ thêm các hướng khác ạ. Nếu k thì em có thể làm gì để cải thiện kết quả từ 3 encoder kia được ạ. Em cảm ơn ạ.",,,,,
"[Hỏi đáp]
#bigdata
chào mọi người, em đăng bài không liên quan đến ML, nhưng liên quan đến việc xây dựng và quản lý data, theo em cũng khá thú vị, mong được mọi người cho lời khuyên ạ.
1. hằng ngày em nhận được data dưới dạng ""static table"" như hình dưới, với dạng key-value là name-code. Nhưng có ngày code tương ứng của name sẽ thay đổi.
Em mong muốn tìm cách xây dựng một ""dynamic table"" như thế nào để có thể dễ dàng tìm code cũ, code mới, và thời gian thay đổi một cách hiệu quả trong trường hợp dữ liệu lớn ạ.
2. Em đang tìm giải pháp để có thể quản lý các data phi cấu trúc và đặc biệt (ví dụ như tai nạn thị trường, tin tức, ngoại lệ trong giao dịch, những thay đổi trong quy định, ...).
Em cần làm sao để tích hợp những data phi cấu trúc như trên với những data bình thường cho mục đích phân tích dữ liệu một cách hiệu quả nhất.
Thật sự em không phải là người làm database, nên gặp vấn đề có chút lúng túng. Mong được mọi người đóng góp và cho lời khuyên, em trân trọng cảm ơn ạ.","[Hỏi đáp] chào mọi người, em đăng bài không liên quan đến ML, nhưng liên quan đến việc xây dựng và quản lý data, theo em cũng khá thú vị, mong được mọi người cho lời khuyên ạ. 1. hằng ngày em nhận được data dưới dạng ""static table"" như hình dưới, với dạng key-value là name-code. Nhưng có ngày code tương ứng của name sẽ thay đổi. Em mong muốn tìm cách xây dựng một ""dynamic table"" như thế nào để có thể dễ dàng tìm code cũ, code mới, và thời gian thay đổi một cách hiệu quả trong trường hợp dữ liệu lớn ạ. 2. Em đang tìm giải pháp để có thể quản lý các data phi cấu trúc và đặc biệt (ví dụ như tai nạn thị trường, tin tức, ngoại lệ trong giao dịch, những thay đổi trong quy định, ...). Em cần làm sao để tích hợp những data phi cấu trúc như trên với những data bình thường cho mục đích phân tích dữ liệu một cách hiệu quả nhất. Thật sự em không phải là người làm database, nên gặp vấn đề có chút lúng túng. Mong được mọi người đóng góp và cho lời khuyên, em trân trọng cảm ơn ạ.",#bigdata,,,,
"Chào mọi người:
Em có bài toán sau nhưng chưa biết cách áp dụng giải thuật recommendation: content base,
collaborative filtering thế nào, mong người cho em hướng đi.
Bài toán của em là cần tìm người cho dự án. Mỗi dự án có các yêu cầu về role,
về kỹ năng. Mỗi nhân viên có skill, expected skill có level, role có level, hobby,
softskill và một vài thông tin cơ bản nữa.
Hiện tại em không biết cách xây dựng feature thế nào.
Mong mọi người giúp đỡ, em cám ơn
Em bổ sung thêm: hiện tại, em nghĩ có thể mô hình các dữ liệu của em thiêu dạng vector đại diện cho mỗi nhân viên như sau,
em có 91 skill, em tạo 1 vector skill 91 chiều, 1 vector role có 10 chiều, một vector về level giá trị (từ 0 - 9) tại ví trị có skill .., cứ như vậy đến hết thông tin về kỹ năng mêm,
sở thích để mã hóa, xong rồi làm thế nào để liên hệ với kết quả là phân vào project nào thì em chưa nghĩ ra, em suy nghĩ tư tưởng từ bài toán recommendation của movie imdb.","Chào mọi người: Em có bài toán sau nhưng chưa biết cách áp dụng giải thuật recommendation: content base, collaborative filtering thế nào, mong người cho em hướng đi. Bài toán của em là cần tìm người cho dự án. Mỗi dự án có các yêu cầu về role, về kỹ năng. Mỗi nhân viên có skill, expected skill có level, role có level, hobby, softskill và một vài thông tin cơ bản nữa. Hiện tại em không biết cách xây dựng feature thế nào. Mong mọi người giúp đỡ, em cám ơn Em bổ sung thêm: hiện tại, em nghĩ có thể mô hình các dữ liệu của em thiêu dạng vector đại diện cho mỗi nhân viên như sau, em có 91 skill, em tạo 1 vector skill 91 chiều, 1 vector role có 10 chiều, một vector về level giá trị (từ 0 - 9) tại ví trị có skill .., cứ như vậy đến hết thông tin về kỹ năng mêm, sở thích để mã hóa, xong rồi làm thế nào để liên hệ với kết quả là phân vào project nào thì em chưa nghĩ ra, em suy nghĩ tư tưởng từ bài toán recommendation của movie imdb.",,,,,
"Chào mọi người,
Mình có thắc mắc là cái model LSTM mình train trên Python rồi lưu thành file .h5 nhưng chuyển qua MATLAB thì import không được các layer LSTM (các layer khác như dense hoặc dropout thì được). Có bạn nào gặp vấn đề tương tự không?","Chào mọi người, Mình có thắc mắc là cái model LSTM mình train trên Python rồi lưu thành file .h5 nhưng chuyển qua MATLAB thì import không được các layer LSTM (các layer khác như dense hoặc dropout thì được). Có bạn nào gặp vấn đề tương tự không?",,,,,
"Em chào anh chị và mọi người ạ. Em là sinh viên năm nhất, thầy có cho nhóm em làm về đề tài này (môn Đại số tuyến tính) ạ. Anh chị có thể cho em xin ít tài liệu về đề tài này không ạ ^^. Em xin chân thành cảm ơn
p/s: em học Điện điện tử với lại chưa biết nhiều về code. không biết có làm kịp đề tài này trong 2 tuần tới không nữa ạ 😢😢","Em chào anh chị và mọi người ạ. Em là sinh viên năm nhất, thầy có cho nhóm em làm về đề tài này (môn Đại số tuyến tính) ạ. Anh chị có thể cho em xin ít tài liệu về đề tài này không ạ ^^. Em xin chân thành cảm ơn p/s: em học Điện điện tử với lại chưa biết nhiều về code. không biết có làm kịp đề tài này trong 2 tuần tới không nữa ạ",,,,,
"Hi everyone, I'm pleased to share with you a Machine learning course for begineer with relaxing background music. This provide a new way to enjoy your self-education journey. This playlist will be updated daily with new relaxing parts. Thank you and enjoy!","Hi everyone, I'm pleased to share with you a Machine learning course for begineer with relaxing background music. This provide a new way to enjoy your self-education journey. This playlist will be updated daily with new relaxing parts. Thank you and enjoy!",,,,,
"#newbie
Em chào mọi người. Em có một câu hỏi cần mọi người giải đáp giúp. Trong lúc huấn luyện MLP, Em có sử dụng mini-batch trong SGD để cập nhật các trọng số và bias. Giả sử ta có MỘT hàm phi tuyến áp dụng cho từng minibatch mỗi lúc. Việc này sẽ dẫn đến những vấn đề gì ạ?
Vì khi em sử dụng mini-batch thì hiệu quả của huấn luyện không tăng lên bao nhiêu so với khi thực hiện với toàn bộ dữ liệu.
Dữ liệu của em là dữ liệu tạo giả định với C lớp cùng phân phối gauss.","Em chào mọi người. Em có một câu hỏi cần mọi người giải đáp giúp. Trong lúc huấn luyện MLP, Em có sử dụng mini-batch trong SGD để cập nhật các trọng số và bias. Giả sử ta có MỘT hàm phi tuyến áp dụng cho từng minibatch mỗi lúc. Việc này sẽ dẫn đến những vấn đề gì ạ? Vì khi em sử dụng mini-batch thì hiệu quả của huấn luyện không tăng lên bao nhiêu so với khi thực hiện với toàn bộ dữ liệu. Dữ liệu của em là dữ liệu tạo giả định với C lớp cùng phân phối gauss.",#newbie,,,,
"Hi mọi người, mọi người cho em xin các dự án Machine Learning hoặc Deep Learning liên quan đến Marketing với ạ!

Em cảm ơn ☺️","Hi mọi người, mọi người cho em xin các dự án Machine Learning hoặc Deep Learning liên quan đến Marketing với ạ! Em cảm ơn",,,,,
"Em chào mọi người. Em đạng tính ROC_AUC với ACCURACY nhưng nó có 1 vài batch bị lệch như bên dưới. Với trường hợp này thì phải xử lý như thế nào ạ? Mọi người cho em hỏi thêm là có hướng dẫn nào tính tay cái ROC_AUC không, có ví dụ càng tốt ạ. Em xin cảm ơn!","Em chào mọi người. Em đạng tính ROC_AUC với ACCURACY nhưng nó có 1 vài batch bị lệch như bên dưới. Với trường hợp này thì phải xử lý như thế nào ạ? Mọi người cho em hỏi thêm là có hướng dẫn nào tính tay cái ROC_AUC không, có ví dụ càng tốt ạ. Em xin cảm ơn!",,,,,
"Em đang có một số thắc mắc về phần Linear discriminant analysis (LDA) trong cuốn sách ML cơ bản. Mong nhận được sự giúp đỡ giải tháp thắc mắc của các anh chị trong nhóm ạ.
1. Ví dụ tập dữ liệu gồm 2 lớp (xanh và đỏ) thì thuật toán LDA sẽ tìm một đường thẳng để khi chiếu các điểm dữ liệu lên thì sẽ được các hình chiếu của 2 lớp này phân tách tuyến tính (linearly separable), như vậy hình chiếu của các điểm nằm trên đường thẳng này vẫn thuộc không gian 2 chiều ban đầu. Nhưng khi sử dụng thư viện sklearn để fit dữ liệu và transform tập X thì kết quả chỉ là các điểm dữ liệu 1 chiều - điều này đúng với lý thuyết anh Tiệp viết rằng chiều của ko gian mới luôn <= C-1 (C là số class). Vậy có mối liên hệ nào giữa các hình chiếu và kết quả transform bên dưới như em vừa trình bày ạ.
2. ma trận w tìm được ở đây là một bộ hệ số biểu diễn cho một đường thẳng hay nó là 1 điểm thuộc đường thẳng tìm được ạ.
Em cảm ơn và rất muốn được giải đáp thắc mắc.","Em đang có một số thắc mắc về phần Linear discriminant analysis (LDA) trong cuốn sách ML cơ bản. Mong nhận được sự giúp đỡ giải tháp thắc mắc của các anh chị trong nhóm ạ. 1. Ví dụ tập dữ liệu gồm 2 lớp (xanh và đỏ) thì thuật toán LDA sẽ tìm một đường thẳng để khi chiếu các điểm dữ liệu lên thì sẽ được các hình chiếu của 2 lớp này phân tách tuyến tính (linearly separable), như vậy hình chiếu của các điểm nằm trên đường thẳng này vẫn thuộc không gian 2 chiều ban đầu. Nhưng khi sử dụng thư viện sklearn để fit dữ liệu và transform tập X thì kết quả chỉ là các điểm dữ liệu 1 chiều - điều này đúng với lý thuyết anh Tiệp viết rằng chiều của ko gian mới luôn <= C-1 (C là số class). Vậy có mối liên hệ nào giữa các hình chiếu và kết quả transform bên dưới như em vừa trình bày ạ. 2. ma trận w tìm được ở đây là một bộ hệ số biểu diễn cho một đường thẳng hay nó là 1 điểm thuộc đường thẳng tìm được ạ. Em cảm ơn và rất muốn được giải đáp thắc mắc.",,,,,
"Em chào mng, hiện tại em muốn tìm hiểu về ứng dụng của PCA trong hồi quy tuyến tính em muốn tham khảo ý kiến và xin một số tài liệu liên quan được không. Em cám ơn ^^","Em chào mng, hiện tại em muốn tìm hiểu về ứng dụng của PCA trong hồi quy tuyến tính em muốn tham khảo ý kiến và xin một số tài liệu liên quan được không. Em cám ơn ^^",,,,,
"Em chào mọi người, em là sinh viên đang làm đề tài về ""Sửa lỗi chính tả tiếng Việt"" có xét đến ngữ cảnh của từ trong câu. Em đã chạy thử nghiệm với SerpAPI của google, SymSpell (cần bộ từ điển tần suất bigram nhưng mà em chưa tìm được), XLMRoBERTa và Hierarchical Transformer Encoders for Vietnamese Spelling Correction (https://arxiv.org/abs/2105.13578), em cũng tham khảo BARTpho (em làm theo mô tả và chạy trên Google Colab nhưng vẫn không hiểu sao là trong AutoTokenizer lại không tìm thấy BartphoTokenizer). Em có câu hỏi muốn tham khảo ý kiến của mọi người:
Em có gặp lỗi chính tả trong câu ""vi phạm nếu klho 6ng tố giác"". Trong đó ""klho 6ng"" là từ mong muốn sửa lỗi chính tả thành từ ""không"". Em có sử dụng những công cụ nêu trên và theo như em tìm hiểu và chạy thử nghiệm thì họ phát hiện và correction cho từng từ ""klho"" và ""6ng"". Câu hỏi của em là có cách nào để xử lý cho vấn đề này chưa ạ, hay có thể gợi ý cho em hướng để xử lý được không ạ?
P/s: Em cũng muốn xin mọi người thêm tài liệu tham khảo của mọi người về vấn đề ""Sửa lỗi chính tả tiếng Việt""
Em cảm ơn mọi người đã dành thời gian!","Em chào mọi người, em là sinh viên đang làm đề tài về ""Sửa lỗi chính tả tiếng Việt"" có xét đến ngữ cảnh của từ trong câu. Em đã chạy thử nghiệm với SerpAPI của google, SymSpell (cần bộ từ điển tần suất bigram nhưng mà em chưa tìm được), XLMRoBERTa và Hierarchical Transformer Encoders for Vietnamese Spelling Correction (https://arxiv.org/abs/2105.13578), em cũng tham khảo BARTpho (em làm theo mô tả và chạy trên Google Colab nhưng vẫn không hiểu sao là trong AutoTokenizer lại không tìm thấy BartphoTokenizer). Em có câu hỏi muốn tham khảo ý kiến của mọi người: Em có gặp lỗi chính tả trong câu ""vi phạm nếu klho 6ng tố giác"". Trong đó ""klho 6ng"" là từ mong muốn sửa lỗi chính tả thành từ ""không"". Em có sử dụng những công cụ nêu trên và theo như em tìm hiểu và chạy thử nghiệm thì họ phát hiện và correction cho từng từ ""klho"" và ""6ng"". Câu hỏi của em là có cách nào để xử lý cho vấn đề này chưa ạ, hay có thể gợi ý cho em hướng để xử lý được không ạ? P/s: Em cũng muốn xin mọi người thêm tài liệu tham khảo của mọi người về vấn đề ""Sửa lỗi chính tả tiếng Việt"" Em cảm ơn mọi người đã dành thời gian!",,,,,
Chào mọi người. E có câu hỏi như sau. Hnay e có tìm hiểu về khái niệm là Tied bias và Untied bias dùng để add bias cho CNN. Nhưng thực sự chưa hiểu rõ lắm. Ai có thể giải thích giúp e được không ạ.,Chào mọi người. E có câu hỏi như sau. Hnay e có tìm hiểu về khái niệm là Tied bias và Untied bias dùng để add bias cho CNN. Nhưng thực sự chưa hiểu rõ lắm. Ai có thể giải thích giúp e được không ạ.,,,,,
"Hi everyone, hồi 2018, FPT AI public 1 dataset speech 30 giờ đã qua xử lý, giờ mình tìm lại không thấy đâu nữa. Trong nhóm ai còn lưu cho mình xin lại với. Many thanks","Hi everyone, hồi 2018, FPT AI public 1 dataset speech 30 giờ đã qua xử lý, giờ mình tìm lại không thấy đâu nữa. Trong nhóm ai còn lưu cho mình xin lại với. Many thanks",,,,,
"Hiện nay, việc huấn luyện một mô hình phát hiện đối tượng càng ngày càng trở nên dễ dàng hơn.
Hãy cùng thử tìm hiểu các bước huấn luyện một mô hình Object Detection cho một đối tượng mới (gấu trúc) dựa trên YOLOv5 trong bài hôm nay.
P/S: bài hướng dẫn có kèm theo Google Colab Notebook để tiện cho việc tham khảo 🙂","Hiện nay, việc huấn luyện một mô hình phát hiện đối tượng càng ngày càng trở nên dễ dàng hơn. Hãy cùng thử tìm hiểu các bước huấn luyện một mô hình Object Detection cho một đối tượng mới (gấu trúc) dựa trên YOLOv5 trong bài hôm nay. P/S: bài hướng dẫn có kèm theo Google Colab Notebook để tiện cho việc tham khảo",,,,,
Chào mọi người. Cho em hỏi không biết đã có ai trong forum mình đã từng thành công trong việc convert model resnet từ Pytorch sang TensorRT chưa ạ?,Chào mọi người. Cho em hỏi không biết đã có ai trong forum mình đã từng thành công trong việc convert model resnet từ Pytorch sang TensorRT chưa ạ?,,,,,
"Mọi người cho em hỏi là khi clone một dự án từ github về mà dự án đấy sử dụng tensorflow 1x, mình nên ưu tiên sử dụng luôn tensorflow 1x như của tác giả và bổ sung thêm các tính năng để phù hợp với bài toán của mình hay là sử dụng tensorflow phiên bản mới nhất ạ. Em đã sử dụng tensorflow 2.6.0 thay cho phiên bản tensorflow 1x nhưng phải sửa lại rất nhiều trong mã nguồn ạ.
Em cảm ơn mọi người ạ","Mọi người cho em hỏi là khi clone một dự án từ github về mà dự án đấy sử dụng tensorflow 1x, mình nên ưu tiên sử dụng luôn tensorflow 1x như của tác giả và bổ sung thêm các tính năng để phù hợp với bài toán của mình hay là sử dụng tensorflow phiên bản mới nhất ạ. Em đã sử dụng tensorflow 2.6.0 thay cho phiên bản tensorflow 1x nhưng phải sửa lại rất nhiều trong mã nguồn ạ. Em cảm ơn mọi người ạ",,,,,
Dạ em chào mọi người. E đang tìm hiểu về R studio. Anh/chị có thể giúp em phân nhỏ dữ liệu trong tập training thành 2 tập con với ạ. (Kiểu như chia nhiều lớp từ bộ dữ liệu ban đầu). Em cảm ơn ạ,Dạ em chào mọi người. E đang tìm hiểu về R studio. Anh/chị có thể giúp em phân nhỏ dữ liệu trong tập training thành 2 tập con với ạ. (Kiểu như chia nhiều lớp từ bộ dữ liệu ban đầu). Em cảm ơn ạ,,,,,
"Chào cả nhà, em xin tổng kết quá trình làm OCR bằng Tesseract và nhận được kết quả ko tốt ah. Em đã thử nhậ n dạng tiếng Việt bằng cả bộ font train sẵn của VietOCR, font train riêng cua em nhưng tất cả đều nhận không thành công.
Bác nào làm món này cho em hỏi :
Có phải em có vấn đề về threshold không ah? Chứ em ko nghĩ Tesseract nhận ngu như thế.
Có bác nào có phương án OCR nào ngon hơn cho em cái keyword em search được không ah?
Em đã cài cái CTC-OCR nhưng không thành công ah.
Ảnh minh họa: Là ảnh có liên quan, ảnh 1 là ảnh em trích xuất từ ảnh gốc, chuyển sang grayscale. Ảnh 2 là em áp threshold để lấy chữ ah.

// Em đã xóa bài cũ để tránh loãng diễn đàn. ","Chào cả nhà, em xin tổng kết quá trình làm OCR bằng Tesseract và nhận được kết quả ko tốt ah. Em đã thử nhậ n dạng tiếng Việt bằng cả bộ font train sẵn của VietOCR, font train riêng cua em nhưng tất cả đều nhận không thành công. Bác nào làm món này cho em hỏi : Có phải em có vấn đề về threshold không ah? Chứ em ko nghĩ Tesseract nhận ngu như thế. Có bác nào có phương án OCR nào ngon hơn cho em cái keyword em search được không ah? Em đã cài cái CTC-OCR nhưng không thành công ah. Ảnh minh họa: Là ảnh có liên quan, ảnh 1 là ảnh em trích xuất từ ảnh gốc, chuyển sang grayscale. Ảnh 2 là em áp threshold để lấy chữ ah. // Em đã xóa bài cũ để tránh loãng diễn đàn.",,,,,
"Chào mọi người, hiện nhóm em đang tìm hiểu về nhận dạng phương ngữ tiếng Việt thì biết được một bộ dữ liệu tên VDSPEC nhưng tìm rất nhiều nơi trên mạng nhưng không có. Mọi người có kinh nghiệm về bộ dữ liệu hoặc đã tìm hiểu có thể cho em xin thêm thông tin. Cảm ơn mọi người trước. 😁","Chào mọi người, hiện nhóm em đang tìm hiểu về nhận dạng phương ngữ tiếng Việt thì biết được một bộ dữ liệu tên VDSPEC nhưng tìm rất nhiều nơi trên mạng nhưng không có. Mọi người có kinh nghiệm về bộ dữ liệu hoặc đã tìm hiểu có thể cho em xin thêm thông tin. Cảm ơn mọi người trước.",,,,,
"Chào ace,
Mình có tập train và test đã chia sẵn, học dùng MLP hồi quy. Giờ muốn vẽ train loss và test loss. Trong sklearn có hướng dẫn, nhưng chỉ áp dụng với train_test_split chứ mình chưa tìm được vẽ test loss với tập test riêng.
Xin các cao nhân giúp đỡ, mình cám ơn!","Chào ace, Mình có tập train và test đã chia sẵn, học dùng MLP hồi quy. Giờ muốn vẽ train loss và test loss. Trong sklearn có hướng dẫn, nhưng chỉ áp dụng với train_test_split chứ mình chưa tìm được vẽ test loss với tập test riêng. Xin các cao nhân giúp đỡ, mình cám ơn!",,,,,
"Bài không liên quan đến machine learning mà liên quan đến cách đặt câu hỏi đặc biệt khi nhắn tin riêng.
Các bạn đừng đặt câu hỏi “có ai biết cái x này không em hỏi” trên group hoặc nhắn riêng mình hỏi “anh ơi em hỏi cái này được không”. Nên tôn trọng thời gian của chính mình và người được hỏi, trừ khi có chuyện thầm kín.","Bài không liên quan đến machine learning mà liên quan đến cách đặt câu hỏi đặc biệt khi nhắn tin riêng. Các bạn đừng đặt câu hỏi “có ai biết cái x này không em hỏi” trên group hoặc nhắn riêng mình hỏi “anh ơi em hỏi cái này được không”. Nên tôn trọng thời gian của chính mình và người được hỏi, t rừ khi có chuyện thầm kín.",,,,,
Đây là buổi nói chuyện của mình ngày mới đây liên quan đến dữ liệu và các công việc liên quan. Hy vọng có ích cho nhiều bạn.,Đây là buổi nói chuyện của mình ngày mới đây liên quan đến dữ liệu và các công việc liên quan. Hy vọng có ích cho nhiều bạn.,,,,,
"Chào các bác. Em đang học đến phần chatbot Rasa kết nối với Zalo nên mạnh dạn làm clip chia sẻ cùng các bạn mới học hỏi về chatbot.
Mong giúp được mọi người chút ít. Xin cảm ơn cả nhà!",Chào các bác. Em đang học đến phần chatbot Rasa kết nối với Zalo nên mạnh dạn làm clip chia sẻ cùng các bạn mới học hỏi về chatbot. Mong giúp được mọi người chút ít. Xin cảm ơn cả nhà!,,,,,
"Chào mọi người, mình đang nghiên cứu về speech to text nhưng đang gặp vấn đề về data
Mình có tìm hiểu về bộ dữ liệu VDSPEC trong paper ""Automatic identification of Vietnamese speech"" của TS. Phạm Ngọc Hưng.
Không biết mọi người trong group có ai biết cách để xin bộ dữ liệu này không ạ
thanks all !!!","Chào mọi người, mình đang nghiên cứu về speech to text nhưng đang gặp vấn đề về data Mình có tìm hiểu về bộ dữ liệu VDSPEC trong paper ""Automatic identification of Vietnamese speech"" của TS. Phạm Ngọc Hưng. Không biết mọi người trong group có ai biết cách để xin bộ dữ liệu này không ạ thanks all !!!",,,,,
"Em chào mọi người ạ
Mọi người cho em hỏi trong nhóm đã từng có ai viết mail để xin code của một bài báo nào đó chưa ạ. Nếu có mọi người có thể cho em xin ít kinh nghiệm và văn mẫu không ạ, và xác suất thành công là bao nhiêu ạ. Do phần đề tài của e thì thấy chỉ có một số ít paper chứ không nhiều.
Em xin chân thành cảm ơn mọi người.","Em chào mọi người ạ Mọi người cho em hỏi trong nhóm đã từng có ai viết mail để xin code của một bài báo nào đó chưa ạ. Nếu có mọi người có thể cho em xin ít kinh nghiệm và văn mẫu không ạ, và xác suất thành công là bao nhiêu ạ. Do phần đề tài của e thì thấy chỉ có một số ít paper chứ không nhiều. Em xin chân thành cảm ơn mọi người.",,,,,
"Câu hỏi về thiết kế hệ thống human detection (surveillance camera) trong công ty.
Chào cả nhà, em đang cần xử lý bài toán human detection (object detection) sử dụng công nghệ xử lý ảnh object detection (yolo, ssd, rcnn, ...). Ở trong phạm vi các phòng của tòa nhà công ty (phòng họp, phòng ăn, phòng sinh hoạt, hành lang,...). Yêu cầu chính xác không cần tuyệt đối, chỉ cần ước lượng mật độ người trong các khung giờ hành chính.
Bài toán của em liên quan đến việc engineer các model và thiết kế hệ thống phù hợp để deploy, do chưa có kinh nghiệm và google thì quá nhiều thông tin nên em muốn hỏi kinh nghiệm trong group của mình.
Giữa edge (jetson, ncs2, coral,...), on-premise server (máy chủ tự mua) và public server (aws, gcp, azure,...) thì giá cả như thế nào và lựa chọn nào là hợp lý cho hệ thống nhỏ, vừa, lớn?
Nếu team chưa có kinh nghiệm thì nên bắt đầu từ đâu? Em định sẽ sử dụng free public cloud để đánh giá trước khi quyết định đầu tư nhưng sợ không kiểm soát được do không có kinh nghiệm sẽ bị charge phí.
Cảm ơn anh Tiệp đã duyệt bài.
#objectdetection #yolo #GPU #deeplearning #systemengineer","Câu hỏi về thiết kế hệ thống human detection (surveillance camera) trong công ty. Chào cả nhà, em đang cần xử lý bài toán human detection (object detection) sử dụng công nghệ xử lý ảnh object detection (yolo, ssd, rcnn, ...). Ở trong phạm vi các phòng của tòa nhà công ty (phòng họp, phòng ăn, phòng sinh hoạt, hành lang,...). Yêu cầu chính xác không cần tuyệt đối, chỉ cần ước lượng mật độ người trong các khung giờ hành chính. Bài toán của em liên quan đến việc engineer các model và thiết kế hệ thống phù hợp để deploy, do chưa có kinh nghiệm và google thì quá nhiều thông tin nên em muốn hỏi kinh nghiệm trong group của mình. Giữa edge (jetson, ncs2, coral,...), on-premise server (máy chủ tự mua) và public server (aws, gcp, azure,...) thì giá cả như thế nào và lựa chọn nào là hợp lý cho hệ thống nhỏ, vừa, lớn? Nếu team chưa có kinh nghiệm thì nên bắt đầu từ đâu? Em định sẽ sử dụng free public cloud để đánh giá trước khi quyết định đầu tư nhưng sợ không kiểm soát được do không có kinh nghiệm sẽ bị charge phí. Cảm ơn anh Tiệp đã duyệt bài.",#objectdetection	#yolo	#GPU	#deeplearning	#systemengineer,,,,
"Ngày mai mình cùng các chuyên gia bên FSOFT AI chia sẻ với các bạn về công việc và các kỹ năng cần thiết trong ngành dữ liệu.
Mời các bạn tham gia và đặt câu hỏi.
Các bạn có thể xem thêm thông tin trong post gốc.",Ngày mai mình cùng các chuyên gia bên FSOFT AI chia sẻ với các bạn về công việc và các kỹ năng cần thiết trong ngành dữ liệu. Mời các bạn tham gia và đặt câu hỏi. Các bạn có thể xem thêm thông tin trong post gốc.,,,,,
"Có lẽ trong forum Machine Learning Cơ bản có nhiều người quan tâm tới Graph Neural Networks và các ứng dụng của nó. Trên thực tế có nhiều bài toán không thể giải tốt nếu dùng các models như CNN, RNN, LSTM, hay Transformers. Do vậy GNN và các biến thể của nó có thể sẽ là giải pháp! Tiện đây mình thấy có bài giới thiệu tổng quan khá thú vị về GNN tại đây https://distill.pub/2021/gnn-intro/. Hi vọng nó giúp ích với mọi người khi mở rộng kiến thức. Nếu bạn nào quan tâm có thể tham khảo Codebase có tên DGL, nó hỗ trợ cho cả TensorFlow, PyTorch và MXNet, nhưng chưa thấy hỗ trợ JAX. Hướng dẫn sử dụng DGL tại đây https://docs.dgl.ai/tutorials/blitz/index.html. Bên cạnh thư viện DGL, còn 2 thư viện khác là torch-geometric, và có lần mình giới thiệu thư viện torchdrug (do nhóm NC ở MILA viết).","Có lẽ trong forum Machine Learning Cơ bản có nhiều người quan tâm tới Graph Neural Networks và các ứng dụng của nó. Trên thực tế có nhiều bài toán không thể giải tốt nếu dùng các models như CNN, RNN, LSTM, hay Transformers. Do vậy GNN và các biến thể của nó có thể sẽ là giải pháp! Tiện đây mình thấy có bài giới thiệu tổng quan khá thú vị về GNN tại đây https://distill.pub/2021/gnn-intro/. Hi vọng nó giúp ích với mọi người khi mở rộng kiến thức. Nếu bạn nào quan tâm có thể tham khảo Codebase có tên DGL, nó hỗ trợ cho cả TensorFlow, PyTorch và MXNet, nhưng chưa thấy hỗ trợ JAX. Hướng dẫn sử dụng DGL tại đây https://docs.dgl.ai/tutorials/blitz/index.html. Bên cạnh thư viện DGL, còn 2 thư viện khác là torch-geometric, và có lần mình giới thiệu thư viện torchdrug (do nhóm NC ở MILA viết).",,,,,
"Chào các ace,
Em dùng MLPClassifier, khi thay đổi các tham số để tìm tham số tối ưu, thì có hiện thông báo: ""The optimization hasn't converged yet"". Dù vậy, AUC và AUPR tốt hơn khi ko hiện thông báo đó.
Cho em hỏi là trường hợp nào thì thì tốt hơn ạ, theo kết quả AUC và AUPR hay sao ạ?
Em cám ơn mn!","Chào các ace, Em dùng MLPClassifier, khi thay đổi các tham số để tìm tham số tối ưu, thì có hiện thông báo: ""The optimization hasn't converged yet"". Dù vậy, AUC và AUPR tốt hơn khi ko hiện thông báo đó. Cho em hỏi là trường hợp nào thì thì tốt hơn ạ, theo kết quả AUC và AUPR hay sao ạ? Em cám ơn mn!",,,,,
"Dạ em chào tất cả mọi người ạ , hiện tại em đang gặp 1 số vấn đề trong lúc cài đặt thư viện Tensorflow , khi cài xong thì em import vào nó báo ra lỗi như vậy . Em đã thực hiện fix bằng cách cài đặt CUDA toolkit ,vv nhưng khi cài CUDA toolkit thì nó lại Fail . Máy em hiện tại chỉ có 1 Card Onboard của Intel . Không biết khi cài cái thư viện này nó có cần yêu cầu Card rời hay như thế nào không , em cũng mới đang tìm hiểu nên cần sự giúp đỡ của mọi người , có cách nào để khắc phục , hoặc bây giờ em nên bắt đầu cài lại từ đâu ạ ? Em xin chân thành cảm ơn.","Dạ em chào tất cả mọi người ạ , hiện tại em đang gặp 1 số vấn đề trong lúc cài đặt thư viện Tensorflow , khi cài xong thì em import vào nó báo ra lỗi như vậy . Em đã thực hiện fix bằng cách cài đặt CUDA toolkit ,vv nhưng khi cài CUDA toolkit thì nó lại Fail . Máy em hiện tại chỉ có 1 Card Onboard của Intel . Không biết khi cài cái thư viện này nó có cần yêu cầu Card rời hay như thế nào không , em cũng mới đang tìm hiểu nên cần sự giúp đỡ của mọi người , có cách nào để khắc phục , hoặc bây giờ em nên bắt đầu cài lại từ đâu ạ ? Em xin chân thành cảm ơn.",,,,,
,nan,,,,,
"Xin chào các bạn trong group.
Mình đang có bài toán là thông qua images để nhận diện phân loại các biển báo giao thông và có thể bước tiếp theo là nhận định xem các biển báo đó còn đạt yêu cầu sử dụng hay không. Ví dụ như độ tương phản có đủ không, hình dạng có bị biến dạng hay không.,,,. Mình chưa có kinh nghiệm về lĩnh vực này lắm rất mong mọi người chỉ dẫn cho nguồn (cả data và code/solution) để mình có thể tham khảo. Mình dùng python.
Trân trọng cảm ơn và chúc mọi người một tuần vui vẻ.","Xin chào các bạn trong group. Mình đang có bài toán là thông qua images để nhận diện phân loại các biển báo giao thông và có thể bước tiếp theo là nhận định xem các biển báo đó còn đạt yêu cầu sử dụng hay không. Ví dụ như độ tương phản có đủ không, hình dạng có bị biến dạng hay không.,,,. Mình chưa có kinh nghiệm về lĩnh vực này lắm rất mong mọi người chỉ dẫn cho nguồn (cả data và code/solution) để mình có thể tham khảo. Mình dùng python. Trân trọng cảm ơn và chúc mọi người một tuần vui vẻ.",,,,,
"[Data Quest 3 - Advanced Labeling Technology]
☑️Mình xin chia sẻ với các bạn video và các tài liệu mà mình thực hiện tuần trước trong mini-seminar được tổ chức bởi ban admin TowardDataScience về các kĩ thuật gán nhãn hiện đại trong huấn luyện những mô hình học có giám sát trong học máy. Đây là những kiến thức mình đã tổng hợp trong quá trình đọc các papers, blogs và theo dõi các kênh khoa học liên quan.
☑️Nội dung chính của buổi thuyết trình hướng tới ba kĩ thuật chính giúp tận dụng nguồn tài nguyên về dữ liệu chưa được gán nhãn nhằm củng cố tính robust cho mô hình. Bao gồm những phương pháp sau:
1. Semi-supervised Learning: Huấn luyện mô hình kết hợp giữa học có giám sát và không giám sát.
2. Active Learning: Kĩ thuật lựa chọn mẫu thông minh cho dự án học máy.
3. Weak Supervision: Ứng dụng các phương pháp chuyên gia và các mô hình học có giám sát yếu trong gán nhãn tự động. Ví dụ demo trên phân loại nội dung từ Snorkel AI startup.
☑️Để thuận tiện cho các bạn thấy được hiệu quả của các phương pháp trong việc cải thiện hiệu suất của mô hình, mình cung cấp thêm những notebooks thực hành tương ứng với các phương pháp khác nhau. Hi vọng rằng những nội dung từ buổi mini-seminar sẽ mang lại thông tin hữu ích và góp phần xây dựng cộng đồng AI Việt Nam vững mạnh hơn.
☑️Tài liệu:
- Slide: https://docs.google.com/presentation/d/1MYztes2PGiBmzYdcoeBrvLbIxRPorD4nLp-shw1K32c/edit?usp=sharing
- Active Learning:
https://colab.research.google.com/drive/1m3K9-_u468O9hA_4S-uTbbafiXAPxt86?usp=sharing
- Weak Supervision:
https://colab.research.google.com/drive/1uwO-hkIohAj0D9hgpw0S8bAS6mLACxD_?usp=sharing
- Link video:
https://www.youtube.com/watch?v=EB1gCJ6zMz4","[Data Quest 3 - Advanced Labeling Technology] Mình xin chia sẻ với các bạn video và các tài liệu mà mình thực hiện tuần trước trong mini-seminar được tổ chức bởi ban admin TowardDataScience về các kĩ thuật gán nhãn hiện đại trong huấn luyện những mô hình học có giám sát trong học máy. Đây là những kiến thức mình đã tổng hợp trong quá trình đọc các papers, blogs và theo dõi các kênh khoa học liên quan. Nội dung chính của buổi thuyết trình hướng tới ba kĩ thuật chính giúp tận dụng nguồn tài nguyên về dữ liệu chưa được gán nhãn nhằm củng cố tính robust cho mô hình. Bao gồm những phương pháp sau: 1. Semi-supervised Learning: Huấn luyện mô hình kết hợp giữa học có giám sát và không giám sát. 2. Active Learning: Kĩ thuật lựa chọn mẫu thông minh cho dự án học máy. 3. Weak Supervision: Ứng dụng các phương pháp chuyên gia và các mô hình học có giám sát yếu trong gán nhãn tự động. Ví dụ demo trên phân loại nội dung từ Snorkel AI startup. Để thuận tiện cho các bạn thấy được hiệu quả của các phương pháp trong việc cải thiện hiệu suất của mô hình, mình cung cấp thêm những notebooks thực hành tương ứng với các phương pháp khác nhau. Hi vọng rằng những nội dung từ buổi mini-seminar sẽ mang lại thông tin hữu ích và góp phần xây dựng cộng đồng AI Việt Nam vững mạnh hơn. Tài liệu: - Slide: https://docs.google.com/presentation/d/1MYztes2PGiBmzYdcoeBrvLbIxRPorD4nLp-shw1K32c/edit?usp=sharing - Active Learning: https://colab.research.google.com/drive/1m3K9-_u468O9hA_4S-uTbbafiXAPxt86?usp=sharing - Weak Supervision: https://colab.research.google.com/drive/1uwO-hkIohAj0D9hgpw0S8bAS6mLACxD_?usp=sharing - Link video: https://www.youtube.com/watch?v=EB1gCJ6zMz4",,,,,
"Mình đang tìm hiểu về phân loại vải dựa trên ảnh quang phổ. Anh chị nào có biết loại máy quang phổ nào dùng để phân tích cấu trúc, loại vật liệu của vải, nhờ chỉ giúp.
Xin cảm ơn.","Mình đang tìm hiểu về phân loại vải dựa trên ảnh quang phổ. Anh chị nào có biết loại máy quang phổ nào dùng để phân tích cấu trúc, loại vật liệu của vải, nhờ chỉ giúp. Xin cảm ơn.",,,,,
"Hiện tại em đang làm 1 project kiểm tra sản phẩm bị cấm hay ko dựa vào tên của nó. Thì đang gặp 1 số khó khăn với những case như sau.
vd gun là cấm chicken ko cấm.
input là gun chicken chicken chicken chicken chicken chicken => thì nó sẽ detect ra là ko cấm với confident score là 0.97 Nhưng công ty lại muốn những case này thì phải ra là cấm dù chỉ có 1 chữ súng trong nhiều chữ bình thường. Và công ty cho em 1 list words những từ nên tăng ảnh hưởng của nó lên model.
Thì hiện tại em ko biết key word hay phương hướng gì để làm . Mọi người cho em vài solutions hay keyword với :3. Cảm ơn mn",Hiện tại em đang làm 1 project kiểm tra sản phẩm bị cấm hay ko dựa vào tên của nó. Thì đang gặp 1 số khó khăn với những case như sau. vd gun là cấm chicken ko cấm. input là gun chicken chicken chicken chicken chicken chicken => thì nó sẽ detect ra là ko cấm với confident score là 0.97 Nhưng công ty lại muốn những case này thì phải ra là cấm dù chỉ có 1 chữ súng trong nhiều chữ bình thường. Và công ty cho em 1 list words những từ nên tăng ảnh hưởng của nó lên model. Thì hiện tại em ko biết key word hay phương hướng gì để làm . Mọi người cho em vài solutions hay keyword với :3. Cảm ơn mn,,,,,
"YOLO (You Only Look Once) là một trong những mô hình nổi tiếng nhất trong Object Detection (phát hiện đối tượng)
Kể từ khi ra mắt, phiên bản YOLO nào cũng cho thấy sự cải thiện đáng kể (đặc biệt là về mặt tốc độ) so với các mô hình tốt nhất thời điểm đó.
Hãy cùng tìm hiểu một chút về YOLO và cách áp dụng mô hình này cho bài toán Object Detection với thư viện OpenCV trong bài hôm nay.","YOLO (You Only Look Once) là một trong những mô hình nổi tiếng nhất trong Object Detection (phát hiện đối tượng) Kể từ khi ra mắt, phiên bản YOLO nào cũng cho thấy sự cải thiện đáng kể (đặc biệt là về mặt tốc độ) so với các mô hình tốt nhất thời điểm đó. Hãy cùng tìm hiểu một chút về YOLO và cách áp dụng mô hình này cho bài toán Object Detection với thư viện OpenCV trong bài hôm nay.",,,,,
"[AI Sharing]
Do có nhiều bạn mới tham gia cuộc thi cũng như ít có kinh nghiệm làm về AI, nên mình có viết hướng dẫn một số kĩ thuật xử lý dữ liệu cho cuộc thi Data-Centric Competition 2021 (https://datacomp.io/trang-chu) cho các bạn mới tìm hiểu tham khảo.
https://nttuan8.com/huong-dan-cuoc-thi-data-centric-ai-competition-2021/
Chúc các bạn tham gia cuộc thi đạt kết quả tốt.","[AI Sharing] Do có nhiều bạn mới tham gia cuộc thi cũng như ít có kinh nghiệm làm về AI, nên mình có viết hướng dẫn một số kĩ thuật xử lý dữ liệu cho cuộc thi Data-Centric Competition 2021 (https://datacomp.io/trang-chu) cho các bạn mới tìm hiểu tham khảo. https://nttuan8.com/huong-dan-cuoc-thi-data-centric-ai-competition-2021/ Chúc các bạn tham gia cuộc thi đạt kết quả tốt.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 8/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 8/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.",,,,,
"Em chào mọi người ạ, mọi người cho em hỏi em có chạy lệnh pip install kafka-python sau đó em chạy lệnh ""from kafka import KafkaProducer"" thì bị báo lỗi như hình. Mọi người có ai gặp lỗi này không ạ.","Em chào mọi người ạ, mọi người cho em hỏi em có chạy lệnh pip install kafka-python sau đó em chạy lệnh ""from kafka import KafkaProducer"" thì bị báo lỗi như hình. Mọi người có ai gặp lỗi này không ạ.",,,,,
Mọi người cho em hỏi sao forum của machinelearningcoban không còn vào được nữa ạ. Em cảm ơn.,Mọi người cho em hỏi sao forum của machinelearningcoban không còn vào được nữa ạ. Em cảm ơn.,,,,,
"Kính chào các bác. Em đang học về phần xử lý bộ nhớ khi train model để tránh tràn. Cũng không phỉa kiến thức gì quá khó nhưng nhiều bạn chưa biết nên hay bị tràn bộ nhớ khi train.
Nay em mạnh dạn làm bài chia sẻ mong giúp được các bạn newbie.
Mong ad duyệt bài!",Kính chào các bác. Em đang học về phần xử lý bộ nhớ khi train model để tránh tràn. Cũng không phỉa kiến thức gì quá khó nhưng nhiều bạn chưa biết nên hay bị tràn bộ nhớ khi train. Nay em mạnh dạn làm bài chia sẻ mong giúp được các bạn newbie. Mong ad duyệt bài!,,,,,
"Gần đây, Kaggle công bố bản khảo sát về Data Science & Machine Learning với một số thông tin khá thú vị như mức lương trình độ học vấn, thuật toán và các nền tảng phổ biến, ...
Các bạn cùng tham khảo nhé 🙂","Gần đây, Kaggle công bố bản khảo sát về Data Science & Machine Learning với một số thông tin khá thú vị như mức lương trình độ học vấn, thuật toán và các nền tảng phổ biến, ... Các bạn cùng tham khảo nhé",,,,,
Hello mọi người! Mình đang có project cần phát triển một hệ thống Text to Speech dùng model MaryTTS. Mọi người có ai đã từng train thành công chưa ạ? Có thể cho mình hỏi kinh nghiệm với đc ko ạ? Mình xin cảm ơn trước.,Hello mọi người! Mình đang có project cần phát triển một hệ thống Text to Speech dùng model MaryTTS. Mọi người có ai đã từng train thành công chưa ạ? Có thể cho mình hỏi kinh nghiệm với đc ko ạ? Mình xin cảm ơn trước.,,,,,
"Em đang chạy FCENet, bounding box trả về dạng như này ạ. Mọi người cho em hỏi dạng như này tên gọi là gì? Và em muốn convert sang dạng tứ giác thì cách nào hiệu quả nhất ạ? Em định lấy xmin, ymin, xmax, ymax.
Em cảm ơn.","Em đang chạy FCENet, bounding box trả về dạng như này ạ. Mọi người cho em hỏi dạng như này tên gọi là gì? Và em muốn convert sang dạng tứ giác thì cách nào hiệu quả nhất ạ? Em định lấy xmin, ymin, xmax, ymax. Em cảm ơn.",,,,,
"Xin chào mọi người, hiện tại mình đang cần 1 open API về nhận diện động vật, quét hình động vật sẽ lấy được thông số loài vật đó. Mọi người cho mình hỏi có API nào như thế không ạ?","Xin chào mọi người, hiện tại mình đang cần 1 open API về nhận diện động vật, quét hình động vật sẽ lấy được thông số loài vật đó. Mọi người cho mình hỏi có API nào như thế không ạ?",,,,,
"Chào mọi người, em đang có 1 bài toán với input là 2 bản tin, bài báo tiếng Việt. Output sẽ in ra những đoạn, những câu tương đồng về ngữ nghĩa, rồi đánh giá tổng quát độ tương đồng giữa 2 bản tin đó. Mọi người có thể cho em xin lộ trình tìm hiểu & triển khai bài toán với ạ. Em xin cảm ơn","Chào mọi người, em đang có 1 bài toán với input là 2 bản tin, bài báo tiếng Việt. Output sẽ in ra những đoạn, những câu tương đồng về ngữ nghĩa, rồi đánh giá tổng quát độ tương đồng giữa 2 bản tin đó. Mọi người có thể cho em xin lộ trình tìm hiểu & triển khai bài toán với ạ. Em xin cảm ơn",,,,,
"Chào mọi người. Mình chia sẻ sự kiện hằng năm từ The Global Partnership on Artificial Intelligence (https://gpai.ai/): https://gpai.paris/site/program.
Trong đó có sự tham gia của Francis Bach, Yoshua Bengio, ... , và Tổng Thống Pháp Macron.","Chào mọi người. Mình chia sẻ sự kiện hằng năm từ The Global Partnership on Artificial Intelligence (https://gpai.ai/): https://gpai.paris/site/program. Trong đó có sự tham gia của Francis Bach, Yoshua Bengio, ... , và Tổng Thống Pháp Macron.",,,,,
"Cho mình hỏi mình muốn so sánh từ có kết quả gần giống ví dụ: input là ""cn gà"", thì sẽ đoán ra được từ ""con gà"" từ Database. Thì mình xài thuật toán nào trên sagemaker của AWS được ạ?","Cho mình hỏi mình muốn so sánh từ có kết quả gần giống ví dụ: input là ""cn gà"", thì sẽ đoán ra được từ ""con gà"" từ Database. Thì mình xài thuật toán nào trên sagemaker của AWS được ạ?",,,,,
"chào mn
em đang làm đồ án khai phá dữ liệu dùng công cụ ELKI( bất đắc dĩ mới chọn ELKI thôi ạ) mn ai biết dùng công cụ này giúp em với ạ
em cảm ơn mn nhiều
em bất lực quá ạ",chào mn em đang làm đồ án khai phá dữ liệu dùng công cụ ELKI( bất đắc dĩ mới chọn ELKI thôi ạ) mn ai biết dùng công cụ này giúp em với ạ em cảm ơn mn nhiều em bất lực quá ạ,,,,,
"Xin chào mọi người ạ, cho phép em hỏi mọi người có link blogger người Việt về Machine Learning, Deep Learning, Data Science không ạ? Nếu có thì cho em xin với ạ, em cảm ơn.","Xin chào mọi người ạ, cho phép em hỏi mọi người có link blogger người Việt về Machine Learning, Deep Learning, Data Science không ạ? Nếu có thì cho em xin với ạ, em cảm ơn.",,,,,
"Knowledge distillation, thuật toán hữu ích chắt lọc thông tin nhằm cải thiện hiệu suất các models nhỏ.","Knowledge distillation, thuật toán hữu ích chắt lọc thông tin nhằm cải thiện hiệu suất các models nhỏ.",,,,,
"🔻[𝐑𝐞𝐠𝐢𝐬𝐭𝐫𝐚𝐭𝐢𝐨𝐧 𝐎𝐩𝐞𝐧] 𝐃𝐀𝐓𝐀-𝐂𝐄𝐍𝐓𝐑𝐈𝐂 𝐀𝐈 𝐂𝐎𝐌𝐏𝐄𝐓𝐈𝐓𝐈𝐎𝐍 ‼️
👉Link: http://datacomp.io/
Are you ready to challenge your skills in #AI and #MachineLearning? DataComp is a Data-Centric AI competition hosted in #Vietnam for the first time.
Data scientists, Data analysts, Data engineers, IT students, Developers or anyone working in the field of #ArtificialIntelligence and #DataScience is welcome to join!
♦️Find more info here and stay tuned for further updates!","[ ] - ‼ Link: http://datacomp.io/ Are you ready to challenge your skills in and DataComp is a Data-Centric AI competition hosted in for the first time. Data scientists, Data analysts, Data engineers, IT students, Developers or anyone working in the field of and is welcome to join! Find more info here and stay tuned for further updates!",#AI	#MachineLearning?	#Vietnam	#ArtificialIntelligence	#DataScience,,,,
"[Lý thuyết vanilla Transformer]
Chào mọi người, mình có thắc mắc về Transformer của 'Attention is all you need', mong được mọi người giải đáp.
Đã có rất nhiều nguồn giải thích cấu trúc kiến trúc Transformer nhưng có vẻ chưa nhiều nguồn nói chi tiết về cách thông tin truyền trong mạng.
1/ Transformer không recurrent ở phần encoding, nhưng recurrent (theo nghĩa là đầu ra nối trở lại đầu vào) ở phần decoding; để cho ra output ở vị trí t_i thì sublayer encode-decode attention cần thực hiện attention với các hidden vector ở phần encoding cùng với những output trước vị trí t_i nếu là layer dưới cùng, hoặc cùng với những hidden representation của output trước vị trí t_i nếu là layer ở giữa và trên cùng. Bản thân sublayer encode-decode cũng phải thực hiện masked attention. Những điều trên là đúng không ạ?
2/ Sublayer masked multihead attention có nghĩa là hidden representation của output ở vị trí i được sinh ra theo cách là output đó chỉ được phép attend tới các output trước vị trí i. Đúng không ạ?
Em cảm ơn mọi người.","[Lý thuyết vanilla Transformer] Chào mọi người, mình có thắc mắc về Transformer của 'Attention is all you need', mong được mọi người giải đáp. Đã có rất nhiều nguồn giải thích cấu trúc kiến trúc Transformer nhưng có vẻ chưa nhiều nguồn nói chi tiết về cách thông tin truyền trong mạng. 1/ Transformer không recurrent ở phần encoding, nhưng recurrent (theo nghĩa là đầu ra nối trở lại đầu vào) ở phần decoding; để cho ra output ở vị trí t_i thì sublayer encode-decode attention cần thực hiện attention với các hidden vector ở phần encoding cùng với những output trước vị trí t_i nếu là layer dưới cùng, hoặc cùng với những hidden representation của output trước vị trí t_i nếu là layer ở giữa và trên cùng. Bản thân sublayer encode-decode cũng phải thực hiện masked attention. Những điều trên là đúng không ạ? 2/ Sublayer masked multihead attention có nghĩa là hidden representation của output ở vị trí i được sinh ra theo cách là output đó chỉ được phép attend tới các output trước vị trí i. Đúng không ạ? Em cảm ơn mọi người.",,,,,
"Hi mọi người, em mới vừa học xong foundation Machine Learning nhưng vẫn còn hơi rối. Em cảm thấy học xong thì lại quên mất, chưa kết nối được kiến thức. Anh chị có tips nào cho em xin để học tốt ngành học này không ạ?
Em cảm ơn","Hi mọi người, em mới vừa học xong foundation Machine Learning nhưng vẫn còn hơi rối. Em cảm thấy học xong thì lại quên mất, chưa kết nối được kiến thức. Anh chị có tips nào cho em xin để học tốt ngành học này không ạ? Em cảm ơn",,,,,
"Chào mọi người, em mới tìm hiểu về ML. Mn cho e ý tưởng để làm 3 bài này ntn ạ. E đọc sách nhưng k biết cách áp dụng vào bài cụ thể ntn. Bài 2 em làm như dưới thì có đúng ko ạ. Em cảm ơn mn trước","Chào mọi người, em mới tìm hiểu về ML. Mn cho e ý tưởng để làm 3 bài này ntn ạ. E đọc sách nhưng k biết cách áp dụng vào bài cụ thể ntn. Bài 2 em làm như dưới thì có đúng ko ạ. Em cảm ơn mn trước",,,,,
"Chào mọi người, mình đang tìm hiểu về thuật toán nhân dạng danh tính người đeo khẩu trang, mn có tài liệu tham khảo về mảng này có thể chia sẻ cho mình với. Mình cảm ơn","Chào mọi người, mình đang tìm hiểu về thuật toán nhân dạng danh tính người đeo khẩu trang, mn có tài liệu tham khảo về mảng này có thể chia sẻ cho mình với. Mình cảm ơn",,,,,
"Hi anh em, hội nghị GTC21 của NVIDIA diễn ra từ 08-11 tháng 11 năm 2021 là 1 trong những hội nghị lớn nhất về AI cho các developer làm sản phẩm AI. Tại hội nghị này, bên mình có 1 bài trình bày liên quan đến DeepStream do team VNPT và NVIDIA cùng tối ưu trong thời gian qua cho sản phẩm https://smartvision.vnpt.vn/vi bên mình, do anh Thanh Ha Cao trình bày:
https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog?search=A31423&ncid=so-face-570512&fbclid=IwAR1rLJ0cgl-BT8LuJihVIjKjV1QkninTH1QjkzqWSp1y6vga2FzKVvzJzNY&tab.catalogtabfields=1600209910618001TWM3#cid=gtcnov21_so-face_en-sg
Anh em làm sản phẩm cùng đăng ký và đặc biệt cập nhật Keynote của AI Product năm 2021 là gì, cũng như xu hướng sắp tới nhé.
Link sự kiện:
https://www.facebook.com/NVIDIA.AP/photos/a.464775900561063/1473195036385806
https://twitter.com/NVIDIAAP/status/1450695790872211459
https://www.linkedin.com/feed/update/urn:li:activity:6856461472671891457/","Hi anh em, hội nghị GTC21 của NVIDIA diễn ra từ 08-11 tháng 11 năm 2021 là 1 trong những hội nghị lớn nhất về AI cho các developer làm sản phẩm AI. Tại hội nghị này, bên mình có 1 bài trình bày liên quan đến DeepStream do team VNPT và NVIDIA cùng tối ưu trong thời gian qua cho sản phẩm https://smartvision.vnpt.vn/vi bên mình, do anh Thanh Ha Cao trình bày: https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog?search=A31423&ncid=so-face-570512&fbclid=IwAR1rLJ0cgl-BT8LuJihVIjKjV1QkninTH1QjkzqWSp1y6vga2FzKVvzJzNY&tab.catalogtabfields=1600209910618001TWM3#cid=gtcnov21_so-face_en-sg Anh em làm sản phẩm cùng đăng ký và đặc biệt cập nhật Keynote của AI Product năm 2021 là gì, cũng như xu hướng sắp tới nhé. Link sự kiện: https://www.facebook.com/NVIDIA.AP/photos/a.464775900561063/1473195036385806 https://twitter.com/NVIDIAAP/status/1450695790872211459 https://www.linkedin.com/feed/update/urn:li:activity:6856461472671891457/",,,,,
Mời các bạn tham dự cuộc thi phân loại thu nhập do TowarDataScience tổ chức. Đây là cuộc thi rất phù hợp với các bạn sinh viên ưa thử thách và đam mê với lĩnh vực Khoa học Dữ liệu.,Mời các bạn tham dự cuộc thi phân loại thu nhập do TowarDataScience tổ chức. Đây là cuộc thi rất phù hợp với các bạn sinh viên ưa thử thách và đam mê với lĩnh vực Khoa học Dữ liệu.,,,,,
"Hello mọi người, lâu rồi mình mới viết bài chia sẻ về Deep Learning. Lần này là về Deep Learning và Data Privacy với Federated Learning, một lĩnh vực mà mình đang nghiên cứu. Bài viết bằng Tiếng Anh vì mình muốn ý tưởng này tiếp cận được với nhiều người hơn. Tuy nhiên mình cũng viết theo hướng đơn giản dễ hiểu để những ai không có nền tảng kỹ thuật hay học thuật cũng đọc được. Hi vọng bài viết có thể cung cấp thêm kiến thức cho mọi người.","Hello mọi người, lâu rồi mình mới viết bài chia sẻ về Deep Learning. Lần này là về Deep Learning và Data Privacy với Federated Learning, một lĩnh vực mà mình đang nghiên cứu. Bài viết bằng Tiếng Anh vì mình muốn ý tưởng này tiếp cận được với nhiều người hơn. Tuy nhiên mình cũng viết theo hướng đơn giản dễ hiểu để những ai không có nền tảng kỹ thuật hay học thuật cũng đọc được. Hi vọng bài viết có thể cung cấp thêm kiến thức cho mọi người.",,,,,
Mấy anh chị ơi cho em hỏi ai có tài liệu gì về việc Ứng dụng của phân tích SVD để khử nhiễu âm thanh ko ạ. Em tìm trên GG hoài mà không thấy 😭😭,Mấy anh chị ơi cho em hỏi ai có tài liệu gì về việc Ứng dụng của phân tích SVD để khử nhiễu âm thanh ko ạ. Em tìm trên GG hoài mà không thấy,,,,,
"Chào mọi người em mới tìm hiểu nghiên cứu về music recommendation system và em đang đi vào phần collaborative filtering dựa trên mô hình đồ thị , đến phần này em đã tìm được đã tìm được UZ như hình bên dưới , mọi người cho em hỏi tiếp theo muốn tìm được sản phẩm gợi ý cho người dùng và hoàn thành bài toán thì em nên làm những gì nữa ạ, em cám ơn.
nguồn https://portal.ptit.edu.vn/saudaihoc/wp-content/uploads/2020/02/LA_%C4%90%E1%BB%97-Th%E1%BB%8B-Li%C3%AAn.pdf","Chào mọi người em mới tìm hiểu nghiên cứu về music recommendation system và em đang đi vào phần collaborative filtering dựa trên mô hình đồ thị , đến phần này em đã tìm được đã tìm được UZ như hình bên dưới , mọi người cho em hỏi tiếp theo muốn tìm được sản phẩm gợi ý cho người dùng và hoàn thành bài toán thì em nên làm những gì nữa ạ, em cám ơn. nguồn https://portal.ptit.edu.vn/saudaihoc/wp-content/uploads/2020/02/LA_%C4%90%E1%BB%97-Th%E1%BB%8B-Li%C3%AAn.pdf",,,,,
"Object Detection (phát hiện đối tượng) là một trong những công nghệ trong thị giác máy tính được sử dụng phổ biến nhất cho đến nay.
Khi nhắc đến Object Detection dựa trên Deep Learning, khả năng cao là bạn sẽ bắt gặp một trong những “họ” phương pháp như RCNN, SSD, YOLO ...
Bài hôm nay sẽ giới thiệu đến các bạn về phương pháp SSD và cách áp dụng một mô hình thuộc “họ” phương pháp này.","Object Detection (phát hiện đối tượng) là một trong những công nghệ trong thị giác máy tính được sử dụng phổ biến nhất cho đến nay. Khi nhắc đến Object Detection dựa trên Deep Learning, khả năng cao là bạn sẽ bắt gặp một trong những “họ” phương pháp như RCNN, SSD, YOLO ... Bài hôm nay sẽ giới thiệu đến các bạn về phương pháp SSD và cách áp dụng một mô hình thuộc “họ” phương pháp này.",,,,,
"Chào mọi người, sau một thời gian làm việc với bài toán Chatbot mình có tổng hợp một số hiểu biết cơ bản về Chatbot trong bài viết đính kèm bên dưới. Một là để hệ thống lại kiến thức, hai là muốn chia sẻ với những ai đang quan tâm tới bài toán này. Hi vọng nó sẽ giúp bạn đọc có một cái nhìn toàn cảnh về Chatbot và tiếp cận nhanh hơn với bài toán này.
https://viblo.asia/p/tong-quan-ve-chatbot-yMnKMByaZ7P
Chúc cả nhà cuối tuần vui vẻ ạ 😊😊😊","Chào mọi người, sau một thời gian làm việc với bài toán Chatbot mình có tổng hợp một số hiểu biết cơ bản về Chatbot trong bài viết đính kèm bên dưới. Một là để hệ thống lại kiến thức, hai là muốn chia sẻ với những ai đang quan tâm tới bài toán này. Hi vọng nó sẽ giúp bạn đọc có một cái nhìn toàn cảnh về Chatbot và tiếp cận nhanh hơn với bài toán này. https://viblo.asia/p/tong-quan-ve-chatbot-yMnKMByaZ7P Chúc cả nhà cuối tuần vui vẻ ạ",,,,,
"Xin chào các tiền bối ạ, em năm nay sinh viên năm 1 chuyên ngành Trí tuệ nhân tạo. Em đã có nền tảng về Python và cũng có tìm hiểu và học theo khoá học huyền thoại của thầy Andrew Ng và tìm hiểu một số roadmap nhưng hơi mông lung. Các tiền bối cho em xin ít lời khuyên với ạ. Em xin cảm ơn mọi người 🙏","Xin chào các tiền bối ạ, em năm nay sinh viên năm 1 chuyên ngành Trí tuệ nhân tạo. Em đã có nền tảng về Python và cũng có tìm hiểu và học theo khoá học huyền thoại của thầy Andrew Ng và tìm hiểu một số roadmap nhưng hơi mông lung. Các tiền bối cho em xin ít lời khuyên với ạ. Em xin cảm ơn mọi người",,,,,
"Dạo này facebook vaf group có quá nhiều comment spam nhảm nhí.
Các bạn có cao kiến gì hạn chế việc này không?",Dạo này facebook vaf group có quá nhiều comment spam nhảm nhí. Các bạn có cao kiến gì hạn chế việc này không?,,,,,
"Em chào mọi người, em đang muốn sử dụng BERT pretrained model để batch encode cho dataset của em gồm 2500 sentences, mỗi sentences có độ dài dao động trong khoảng 1000-2000 words. Mọi người cho em hỏi nên để batch_size kích thước như thế nào ạ, em để batch_size 8 mà đã hết sạch RAM rồi ạ. Em không biết có nên mua Colab Pro+ để tăng thêm RAM không ạ. Em đang dùng GPU P100 ở chế độ High-RAM 25GB trên colab. Em cảm ơn mọi người ạ.
Chú thích: sentence của em ở đây là protein sequence ạ, mỗi protein sequence gồm 1000-2000 words, và word ở đây thì là letter ạ.","Em chào mọi người, em đang muốn sử dụng BERT pretrained model để batch encode cho dataset của em gồm 2500 sentences, mỗi sentences có độ dài dao động trong khoảng 1000-2000 words. Mọi người cho em hỏi nên để batch_size kích thước như thế nào ạ, em để batch_size 8 mà đã hết sạch RAM rồi ạ. Em không biết có nên mua Colab Pro+ để tăng thêm RAM không ạ. Em đang dùng GPU P100 ở chế độ High-RAM 25GB trên colab. Em cảm ơn mọi người ạ. Chú thích: sentence của em ở đây là protein sequence ạ, mỗi protein sequence gồm 1000-2000 words, và word ở đây thì là letter ạ.",,,,,
"Mình cần giải một số bài toán tối ưu dạng đơn giản như: Tìm x trong một danh sách các đối tượng cho trước, sao f(x) >= giá trị tối thiểu và g(x) là min... Không rõ dạng vấn đề này này thì có thư viện nào của python có thể giải quyết được và sử dụng nó đơn giản không các bạn? Cám ơn.","Mình cần giải một số bài toán tối ưu dạng đơn giản như: Tìm x trong một danh sách các đối tượng cho trước, sao f(x) >= giá trị tối thiểu và g(x) là min... Không rõ dạng vấn đề này này thì có thư viện nào của python có thể giải quyết được và sử dụng nó đơn giản không các bạn? Cám ơn.",,,,,
"Em chào mọi người. Em đng giải bài toán về embedding cho amino acid của trình tự protein (vấn đề này có thể tương tự như trong NLP ở chỗ amino acid là word, protein là sentence, đây là bài toán word embedding). Em đã tìm mô hình pre-trained BERT cho amino acid (https://huggingface.co/Rostlab/prot_bert).
Với mô hình pretrained này, có thể sinh ra các contextual embedding cho amino acid ở trong mỗi trình tự protein. Ở đây, em cần encode cho tâp dữ liệu của mình gồm nhiều trình tự protein (khoảng 2500). Tuy nhiên, kết quả nếu em encode đồng thời toàn bộ cả tập dữ liệu này của mình, thì cần phải allocate tensor có kích thước lớn (2500, 2000, 1024). 2500 là số trình tự protein em có tương ứng với 2500 sentences; 2000 là chiều dài của một trình tự sau khi đã được padding, tương ứng với số amino acid hay số từ trong một sentence; 1024 là vector cột embedding cho mỗi amino acid.
Em gặp lỗi OOM (tràn bộ nhớ GPU) khi mô hình muốn encode thì phải allocate tensor này. Em biết có giải pháp là encoding cho batch của các trình tự, thì em chưa rõ lắm sẽ làm như thế nào, nhưng em có phỏng đoán rằng ta chia tập dữ liệu thành các batch nhỏ của các trình tự rồi từ pretrained tokenizer gọi ra encode_batch_plus cho từng batch trình tự ra được input_ids cho từng batch, sau đó dùng mới pretrained model để lấy ra embedding cho các trình tự trong mỗi batch. Em nghĩ thế đã hợp lý chưa ạ, nếu em hiểu có phần nào sai hoặc chưa đúng thì anh chị và mọi người có thể chỉ giúp em được không ạ. Em xin cảm ơn mọi người ạ.","Em chào mọi người. Em đng giải bài toán về embedding cho amino acid của trình tự protein (vấn đề này có thể tương tự như trong NLP ở chỗ amino acid là word, protein là sentence, đây là bài toán word embedding). Em đã tìm mô hình pre-trained BERT cho amino acid (https://huggingface.co/Rostlab/prot_bert). Với mô hình pretrained này, có thể sinh ra các contextual embedding cho amino acid ở trong mỗi trình tự protein. Ở đây, em cần encode cho tâp dữ liệu của mình gồm nhiều trình tự protein (khoảng 2500). Tuy nhiên, kết quả nếu em encode đồng thời toàn bộ cả tập dữ liệu này của mình, thì cần phải allocate tensor có kích thước lớn (2500, 2000, 1024). 2500 là số trình tự protein em có tương ứng với 2500 sentences; 2000 là chiều dài của một trình tự sau khi đã được padding, tương ứng với số amino acid hay số từ trong một sentence; 1024 là vector cột embedding cho mỗi amino acid. Em gặp lỗi OOM (tràn bộ nhớ GPU) khi mô hình muốn encode thì phải allocate tensor này. Em biết có giải pháp là encoding cho batch của các trình tự, thì em chưa rõ lắm sẽ làm như thế nào, nhưng em có phỏng đoán rằng ta chia tập dữ liệu thành các batch nhỏ của các trình tự rồi từ pretrained tokenizer gọi ra encode_batch_plus cho từng batch trình tự ra được input_ids cho từng batch, sau đó dùng mới pretrained model để lấy ra embedding cho các trình tự trong mỗi batch. Em nghĩ thế đã hợp lý chưa ạ, nếu em hiểu có phần nào sai hoặc chưa đúng thì anh chị và mọi người có thể chỉ giúp em được không ạ. Em xin cảm ơn mọi người ạ.",,,,,
"Chào mọi người, hiện tại em đang xây dựng mô hình để dự đoán tuổi và giới tính. Nhưng trong quá trình train thì acc_train và loss_train thay đổi khá ổn, nhưng em vẫn chưa hiểu acc_val và loss_val vì sau lúc đầu thay đổi tốt nhưng sau đó thì không còn ổn định nữa.
Nhờ mọi người giải thích hộ em là do mô hình chưa phù hợp hay những tham số chưa phù hợp. Em sử dụng pre-train mô hình alexnet.","Chào mọi người, hiện tại em đang xây dựng mô hình để dự đoán tuổi và giới tính. Nhưng trong quá trình train thì acc_train và loss_train thay đổi khá ổn, nhưng em vẫn chưa hiểu acc_val và loss_val vì sau lúc đầu thay đổi tốt nhưng sau đó thì không còn ổn định nữa. Nhờ mọi người giải thích hộ em là do mô hình chưa phù hợp hay những tham số chưa phù hợp. Em sử dụng pre-train mô hình alexnet.",,,,,
"Chào mọi người, hiện tại em đang làm đồ án về nhận dạng đồ bảo hộ của công nhân trong nhà máy ( có mặc không mặc, chủ yếu là nón và áo ) thì không biết có dataset nào có sẳn không ạ, em đang làm cách thủ công tải về từng tấm thấy nó hơi lâu, em cảm ơn !","Chào mọi người, hiện tại em đang làm đồ án về nhận dạng đồ bảo hộ của công nhân trong nhà máy ( có mặc không mặc, chủ yếu là nón và áo ) thì không biết có dataset nào có sẳn không ạ, em đang làm cách thủ công tải về từng tấm thấy nó hơi lâu, em cảm ơn !",,,,,
"Em chào mọi người ạ.
Mọi người cho e hỏi, em có một đoạn dữ liệu khiểu như hình, bây giờ có phương pháp nào có thể phân nó ra thành các đoạn được không ạ ( phần đánh dấu bên dưới là anotation sẵn, bây giờ em muốn có phương pháp nào mà có thể tự động phân đoạn ạ)
Em cảm ơn mọi người nhiều.","Em chào mọi người ạ. Mọi người cho e hỏi, em có một đoạn dữ liệu khiểu như hình, bây giờ có phương pháp nào có thể phân nó ra thành các đoạn được không ạ ( phần đánh dấu bên dưới là anotation sẵn, bây giờ em muốn có phương pháp nào mà có thể tự động phân đoạn ạ) Em cảm ơn mọi người nhiều.",,,,,
"[Học sâu trong y sinh: drug-target affinity (dta), drug-cellline response (dcr)]
[Tl-dr: Sử dụng kiến trúc 'không hợp lý' nhưng kết quả vẫn tốt?]
Mình có đọc một số paper về chủ đề dta, dcr và thấy có một số trong đó dùng kiến trúc không hợp lý nhưng vẫn cho kết quả vượt SOTA, vượt random forest với SVM.
Ví dụ:
+/DeepH-DTA (https://ieeexplore.ieee.org/document/9197589): dùng CNN với dữ liệu dạng tf-idf, dùng CNN lai RNN nhưng (nhiều khả năng) thật ra lại là RNN?
+/DeepGS (https://arxiv.org/abs/2003.13902) (mình chỉ tìm thấy lưu trên arxiv, chưa thấy ở tạp chí khác):Dùng CNN với kernel trượt trên chiều channel/feature?
+/CDRscan (https://www.nature.com/articles/s41598-018-27214-6): Dùng tanh làm activation cho lớp ffnn cuối cùng trong khi tác vụ là hồi quy? (Thật ra mô hình của họ gồm 5 mô hình con, đầu ra mô hình bằng trung bình đầu ra của 5 mô hình con, mô hình tanh chỉ là 1 trong 5 mô hình con đó, 4 mô hình còn lại thì không có activation ở cuối như bình thường) Ghép input hoặc hidden representation của thuốc với cell line trước khi cho vào CNN, tức dùng chung một kernel cho thuốc và cell line?
Vậy kết quả tốt là do mô hình phức tạp hơn, nhiều param hơn, train trên phần cứng tốt hơn bất kể mô hình hay có lời giải thích hợp lý cho những lựa chọn này? Những bài toán ở domain khác có từng sử dụng những kỹ thụaat trên hay không?
Em cảm ơn mọi người.","[Học sâu trong y sinh: drug-target affinity (dta), drug-cellline response (dcr)] [Tl-dr: Sử dụng kiến trúc 'không hợp lý' nhưng kết quả vẫn tốt?] Mình có đọc một số paper về chủ đề dta, dcr và thấy có một số trong đó dùng kiến trúc không hợp lý nhưng vẫn cho kết quả vượt SOTA, vượt random forest với SVM. Ví dụ: +/DeepH-DTA (https://ieeexplore.ieee.org/document/9197589): dùng CNN với dữ liệu dạng tf-idf, dùng CNN lai RNN nhưng (nhiều khả năng) thật ra lại là RNN? +/DeepGS (https://arxiv.org/abs/2003.13902) (mình chỉ tìm thấy lưu trên arxiv, chưa thấy ở tạp chí khác):Dùng CNN với kernel trượt trên chiều channel/feature? +/CDRscan (https://www.nature.com/articles/s41598-018-27214-6): Dùng tanh làm activation cho lớp ffnn cuối cùng trong khi tác vụ là hồi quy? (Thật ra mô hình của họ gồm 5 mô hình con, đầu ra mô hình bằng trung bình đầu ra của 5 mô hình con, mô hình tanh chỉ là 1 trong 5 mô hình con đó, 4 mô hình còn lại thì không có activation ở cuối như bình thường) Ghép input hoặc hidden representation của thuốc với cell line trước khi cho vào CNN, tức dùng chung một kernel cho thuốc và cell line? Vậy kết quả tốt là do mô hình phức tạp hơn, nhiều param hơn, train trên phần cứng tốt hơn bất kể mô hình hay có lời giải thích hợp lý cho những lựa chọn này? Những bài toán ở domain khác có từng sử dụng những kỹ thụaat trên hay không? Em cảm ơn mọi người.",,,,,
"Một hệ thống camera giám sát ở Quế Dương, Trung Quốc chỉ mất 7 phút để xác định được vị trí một phóng viên dựa vào ảnh khuôn mặt của họ. Đây là kết quả của một mạng lưới camera quy mô lớn cùng với công nghệ nhận dạng khuôn mặt (Face Recognition).
Hãy cùng mình tìm hiểu xem công nghệ này là gì, hoạt động như thế nào và thực hành với một ví dụ nho nhỏ ở cuối bài nha.","Một hệ thống camera giám sát ở Quế Dương, Trung Quốc chỉ mất 7 phút để xác định được vị trí một phóng viên dựa vào ảnh khuôn mặt của họ. Đây là kết quả của một mạng lưới camera quy mô lớn cùng với công nghệ nhận dạng khuôn mặt (Face Recognition). Hãy cùng mình tìm hiểu xem công nghệ này là gì, hoạt động như thế nào và thực hành với một ví dụ nho nhỏ ở cuối bài nha.",,,,,
"Em dạo này đang tìm hiểu về Search và Recommender System. Hiện tại em đang đọc cuốn Deep Learning for Search nhưng có vẻ sách không cover nhiều material về những transformers model mới lắm. Không biết trong group có ai build hay làm nhiều về Search với Recommender System có thể giúp point em đến một số material được không ạ
Em cảm ơn mng nhiều !",Em dạo này đang tìm hiểu về Search và Recommender System. Hiện tại em đang đọc cuốn Deep Learning for Search nhưng có vẻ sách không cover nhiều material về những transformers model mới lắm. Không biết trong group có ai build hay làm nhiều về Search với Recommender System có thể giúp point em đến một số material được không ạ Em cảm ơn mng nhiều !,,,,,
"Gần đây mình có chia sẻ một số thông tin về Graph Neural Networks (GNN) cũng như thư viện/codebases như torch_geometric, dlg, và torchdrug. Mình cũng mới nhận được new letter từ trang paperswithcode về xu hướng nghiên cứu về GNN. GNNs đang có những tiến bộ rất nhanh về việc giải quyết các bài toán với dữ liệu phức hợp. Đặc biệt, GNN gần đây được sử dụng để dự đoán cấu trúc phân tử proteins bậc 3 và 4. Việc này có ý nghĩa quan trọng trong việc tìm ra các chất mới có khả năng tương tác sinh học ở mức độ phân tử và tế bào.
Đây là link tới bài https://paperswithcode.com/newsletter/19/","Gần đây mình có chia sẻ một số thông tin về Graph Neural Networks (GNN) cũng như thư viện/codebases như torch_geometric, dlg, và torchdrug. Mình cũng mới nhận được new letter từ trang paperswithcode về xu hướng nghiên cứu về GNN. GNNs đang có những tiến bộ rất nhanh về việc giải quyết các bài toán với dữ liệu phức hợp. Đặc biệt, GNN gần đây được sử dụng để dự đoán cấu trúc phân tử proteins bậc 3 và 4. Việc này có ý nghĩa quan trọng trong việc tìm ra các chất mới có khả năng tương tác sinh học ở mức độ phân tử và tế bào. Đây là link tới bài https://paperswithcode.com/newsletter/19/",,,,,
"Em chào mọi người. Em viết post này với nguyện vọng xin mọi người giúp em định hướng việc học hiểu và xây dựng model sao cho hiệu quả ạ.
Em là sinh viên kinh tế mới tốt nghiệp. Có sẵn một chút vốn liếng đã phân tích dữ liệu khi thi nghiên cứu khoa học ở trường và khả năng tiếp thu toán cấp 3 ổn nên em đã tìm hiểu và muốn đi theo con đường DA. Trong quá trình học, em có tiếp cận Python và tập tành xây dựng các model. Ban đầu, làm theo các dòng code và xây ra được các mô hình phổ biến như Linear regression, Logistics regression khiến em hào hứng vô cùng và oai vô cùng khi tưởng như mình đã bước một chân vào thế giới machine learning đến nơi rồi.
Nhưng càng học thì em lại càng hoang mang và mệt mỏi. Vấn đề chính đó là, các mô hình, thuật toán trong Python quá bao la và rộng lớn. Mỗi khi em tiếp cận một mô hình nào đó, thì em search trên mạng lại có vô số trang nói về nó. Các hàm, phương trình chi chít em cứ đọc hoài nhưng cũng không hiểu ạ (ví dụ như expectation maximization) Kèm theo cái kiến thức chủ đạo lại có cả 1 đống kiến thức bên lề mới toanh khác được chêm vào khi tác giả giải thích thuật toán. Search càng nhiều trang thì mỗi trang lại nói khác đi 1 tí. Đến lúc hiểu rồi thì việc tìm mẫu code để làm khiến em hoang mang tập 2 khi mỗi tác giả họ lại code theo 1 kiểu khác nhau.
Khi học kinh tế ở đại học thì em có được tiếp xúc với kinh tế lượng, xstk nhưng chỉ dừng lại ở việc hiểu cái này áp dụng cho trường hợp nào rồi cứ thế áp dụng luôn chứ không được học sâu, đào sâu để hiểu vấn đề. Chính vì thế nhiều kiến thức học xong dễ dàng rơi vào quên lãng. Vì vậy, hiện nay em cảm thấy việc học một số model và hiểu, nắm được rõ những vấn đề liên quan đến xác suất, thống kê trở thành trở ngại đối với em khi đọc tài liệu về machine learning.
Em hi vọng nếu mọi người đọc đến đây thì có thể cho em xin định hướng học được không ạ? Dưới đây là 1 số câu hỏi của em
- Kiến thức bổ trợ nào về toán cần tập trung khi DA làm machine learning?
- Sách nào nên đọc? Course nào nên học?
- Khi anh chị tiếp xúc với 1 bài toán, anh chị vạch ra hướng đi để giải quyết nó như thế nào? Khi anh chị tìm hiểu và thấy nhiều cách viết code quá thì anh chị sẽ làm gì?
Em vô cùng biết ơn nếu anh chị giúp đỡ em định hướng tốt hơn ạ. Chúc anh chị một tuần làm việc nhiều niềm vui và ý nghĩa.","Em chào mọi người. Em viết post này với nguyện vọng xin mọi người giúp em định hướng việc học hiểu và xây dựng model sao cho hiệu quả ạ. Em là sinh viên kinh tế mới tốt nghiệp. Có sẵn một chút vốn liếng đã phân tích dữ liệu khi thi nghiên cứu khoa học ở trường và khả năng tiếp thu toán cấp 3 ổn nên em đã tìm hiểu và muốn đi theo con đường DA. Trong quá trình học, em có tiếp cận Python và tập tành xây dựng các model. Ban đầu, làm theo các dòng code và xây ra được các mô hình phổ biến như Linear regression, Logistics regression khiến em hào hứng vô cùng và oai vô cùng khi tưởng như mình đã bước một chân vào thế giới machine learning đến nơi rồi. Nhưng càng học thì em lại càng hoang mang và mệt mỏi. Vấn đề chính đó là, các mô hình, thuật toán trong Python quá bao la và rộng lớn. Mỗi khi em tiếp cận một mô hình nào đó, thì em search trên mạng lại có vô số trang nói về nó. Các hàm, phương trình chi chít em cứ đọc hoài nhưng cũng không hiểu ạ (ví dụ như expectation maximization) Kèm theo cái kiến thức chủ đạo lại có cả 1 đống kiến thức bên lề mới toanh khác được chêm vào khi tác giả giải thích thuật toán. Search càng nhiều trang thì mỗi trang lại nói khác đi 1 tí. Đến lúc hiểu rồi thì việc tìm mẫu code để làm khiến em hoang mang tập 2 khi mỗi tác giả họ lại code theo 1 kiểu khác nhau. Khi học kinh tế ở đại học thì em có được tiếp xúc với kinh tế lượng, xstk nhưng chỉ dừng lại ở việc hiểu cái này áp dụng cho trường hợp nào rồi cứ thế áp dụng luôn chứ không được học sâu, đào sâu để hiểu vấn đề. Chính vì thế nhiều kiến thức học xong dễ dàng rơi vào quên lãng. Vì vậy, hiện nay em cảm thấy việc học một số model và hiểu, nắm được rõ những vấn đề liên quan đến xác suất, thống kê trở thành trở ngại đối với em khi đọc tài liệu về machine learning. Em hi vọng nếu mọi người đọc đến đây thì có thể cho em xin định hướng học được không ạ? Dưới đây là 1 số câu hỏi của em - Kiến thức bổ trợ nào về toán cần tập trung khi DA làm machine learning? - Sách nào nên đọc? Course nào nên học? - Khi anh chị tiếp xúc với 1 bài toán, anh chị vạch ra hướng đi để giải quyết nó như thế nào? Khi anh chị tìm hiểu và thấy nhiều cách viết code quá thì anh chị sẽ làm gì? Em vô cùng biết ơn nếu anh chị giúp đỡ em định hướng tốt hơn ạ. Chúc anh chị một tuần làm việc nhiều niềm vui và ý nghĩa.",,,,,
"Em chào anh chị và các bạn ạ! Hiện tại em đang làm chuyên đề tốt nghiệp về thuật toán phân cụm K-means bằng Python ạ, bộ dữ liệu của e gồm 2000 khách hàng vay vốn của 1 ngân hàng, em đã phân cụm chọn K và phân cụm 1 2 3... cho từng khách hàng rồi ạ. Tuy nhiên sau khi em nộp thì thầy có yêu cầu em phân từng khách hàng ra gồm 9 loại từ AAA đến C ( AAA, BB,...C là xếp hạng tín dụng) nhưng em không biết phải làm sao, anh chị nào biết có thể tư vấn giúp em được không ạ? Em cảm ơn ạ!😞","Em chào anh chị và các bạn ạ! Hiện tại em đang làm chuyên đề tốt nghiệp về thuật toán phân cụm K-means bằng Python ạ, bộ dữ liệu của e gồm 2000 khách hàng vay vốn của 1 ngân hàng, em đã phân cụm chọn K và phân cụm 1 2 3... cho từng khách hàng rồi ạ. Tuy nhiên sau khi em nộp thì thầy có yêu cầu em phân từng khách hàng ra gồm 9 loại từ AAA đến C ( AAA, BB,...C là xếp hạng tín dụng) nhưng em không biết phải làm sao, anh chị nào biết có thể tư vấn giúp em được không ạ? Em cảm ơn ạ!",,,,,
"Dear all,
I’m looking for a talented PhD candidate to work with me on theoretical Artificial Intelligence. I work mostly at the intersection of theoretical reinforcement learning, multiagent systems, and game theory. In particular, I’m mainly interested in the following topics: multi-agent learning with strategic agents, fairness and truthfulness in multi-agent learning, and learning with structured data.
If you are interested in any of these topics, don’t hesitate to contact me. 
Details: 
Deadline for applications: January 31, 2022.
Application decisions: Decisions will be announced in March 2022.
Eligibility: Scholarships are open to Home, EU, and international applicants.

A few words about Warwick and its CS department: 
The Computer Science Department at Warwick is ranked 1st in the UK for scientific output and 2nd overall in the latest UK Research Excellence Framework. The PhD scholarship will give you the opportunity to join a leading Computer Science department, and pursue academic excellence in your chosen area of Computer Science: Artificial Intelligence, Applied Computing, Data Science, Human Centred Computing, Security, Systems, or Theory. In addition, Warwick a university is constantly ranked in top 70-80 of the world in the major rankings.

You can find more details about the application here:
https://warwick.ac.uk/fac/sci/dcs/research/doctoralstudies/fundingadvice/
You can also check my departmental web page for more of my lab/research interest:
https://warwick.ac.uk/fac/sci/dcs/people/long_tran-thanh/","Dear all, I’m looking for a talented PhD candidate to work with me on theoretical Artificial Intelligence. I work mostly at the intersection of theoretical reinforcement learning, multiagent systems, and game theory. In particular, I’m mainly interested in the following topics: multi-agent learning with strategic agents, fairness and truthfulness in multi-agent learning, and learning with structured data. If you are interested in any of these topics, don’t hesitate to contact me. Details: Deadline for applications: January 31, 2022. Application decisions: Decisions will be announced in March 2022. Eligibility: Scholarships are open to Home, EU, and international applicants. A few words about Warwick and its CS department: The Computer Science Department at Warwick is ranked 1st in the UK for scientific output and 2nd overall in the latest UK Research Excellence Framework. The PhD scholarship will give you the opportunity to join a leading Computer Science department, and pursue academic excellence in your chosen area of Computer Science: Artificial Intelligence, Applied Computing, Data Science, Human Centred Computing, Security, Systems, or Theory. In addition, Warwick a university is constantly ranked in top 70-80 of the world in the major rankings. You can find more details about the application here: https://warwick.ac.uk/fac/sci/dcs/research/doctoralstudies/fundingadvice/ You can also check my departmental web page for more of my lab/research interest: https://warwick.ac.uk/fac/sci/dcs/people/long_tran-thanh/",,,,,
"Chào mọi người, em có vài chỗ chưa hiểu rõ về Transfer learning, mong mọi người chỉ dạy!

Ví dụ em đang làm với pretrain weight VGG16 (base network).
Model của em có dạng: VGG16 (bỏ các layer fully connected ở cuối) + kiến trúc riêng của mình
Flow: sau khi truyền pretrain weight vào base network, nhập input vào, qua được base network sẽ cho output là các feature tốt, rồi mới đưa vô kiến trúc riêng.
1) Em thấy trên mạng, khi dùng lại base network người ta bỏ đi lớp Fully connected để làm gì vậy? (Em nghĩ là họ chỉ muốn dừng lại ở layer conv cuối nhằm lấy được higher level feature tốt)
2) Nếu em bỏ randomly vài node thì pre-weight vẫn load được nhưng theo em 100% feature ở cuối base network sẽ ra khác mong đợi?
3) Có người nói: các layer conv2D ở cuối VGG16 mới trích xuất các feature high level. Vậy nếu chỉ chừa vài layer cuối của VGG16 lại làm base network thôi thì sẽ vẫn tốt? (theo em, quan điểm này sai, vì feature phải được rút trích theo từng layer mới đến được high level feature)
4) Vậy, việc thay đổi cấu trúc trong base network + load pretrain weight có khả thi không ?
5) Thông thường ,khi nào mình mới freeze phần pretrain-weight và chỉ cập nhật weight ở khúc sau? Và khi nào mình mới load pretrain-weight để nhằm mục đích khởi tạo + train lại toàn bộ model?

Cảm ơn mọi người đã đọc!
#transferlearning","Chào mọi người, em có vài chỗ chưa hiểu rõ về Transfer learning, mong mọi người chỉ dạy! Ví dụ em đang làm với pretrain weight VGG16 (base network). Model của em có dạng: VGG16 (bỏ các layer fully connected ở cuối) + kiến trúc riêng của mình Flow: sau khi truyền pretrain weight vào base network, nhập input vào, qua được base network sẽ cho output là các feature tốt, rồi mới đưa vô kiến trúc riêng. 1) Em thấy trên mạng, khi dùng lại base network người ta bỏ đi lớp Fully connected để làm gì vậy? (Em nghĩ là họ chỉ muốn dừng lại ở layer conv cuối nhằm lấy được higher level feature tốt) 2) Nếu em bỏ randomly vài node thì pre-weight vẫn load được nhưng theo em 100% feature ở cuối base network sẽ ra khác mong đợi? 3) Có người nói: các layer conv2D ở cuối VGG16 mới trích xuất các feature high level. Vậy nếu chỉ chừa vài layer cuối của VGG16 lại làm base network thôi thì sẽ vẫn tốt? (theo em, quan điểm này sai, vì feature phải được rút trích theo từng layer mới đến được high level feature) 4) Vậy, việc thay đổi cấu trúc trong base network + load pretrain weight có khả thi không ? 5) Thông thường ,khi nào mình mới freeze phần pretrain-weight và chỉ cập nhật weight ở khúc sau? Và khi nào mình mới load pretrain-weight để nhằm mục đích khởi tạo + train lại toàn bộ model? Cảm ơn mọi người đã đọc!",#transferlearning,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 10/2020 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 10/2020 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"Chào mọi người, em đang thực hiện 1 đồ án tốt nghiệp về detect chim trong bộ data là các video quay được trên các cánh đồng. Vấn đề em gặp phải là object quá nhỏ chỉ khoảng 3-6 pixels. Em có sử dụng YOLOv4 kết quả detect được khá tốt các objects chính, nhưng còn False Negative khá cao. Em thử kết hợp MobileNet để classify lại thì object chính bị phân loại sai khá nhiều dẫn đến F1-score không còn cao như ban đầu nữa. Mọi người có biện pháp nào khác phục được vấn đề này không ạ? Em xin cảm ơn.","Chào mọi người, em đang thực hiện 1 đồ án tốt nghiệp về detect chim trong bộ data là các video quay được trên các cánh đồng. Vấn đề em gặp phải là object quá nhỏ chỉ khoảng 3-6 pixels. Em có sử dụng YOLOv4 kết quả detect được khá tốt các objects chính, nhưng còn False Negative khá cao. Em thử kết hợp MobileNet để classify lại thì object chính bị phân loại sai khá nhiều dẫn đến F1-score không còn cao như ban đầu nữa. Mọi người có biện pháp nào khác phục được vấn đề này không ạ? Em xin cảm ơn.",,,,,
"Em chào mọi người. Em có 1 thắc mắc về việc sinh các điểm dữ liệu thông qua Phân phối chuẩn nhiều chiều , ma trận hiệp phương sai nửa xác định dương tự chọn, điểm kỳ vọng tự chọn. Em muốn hỏi là nguyên lý làm cách nào mà chương trình có thể sinh được dữ liệu thông qua 3 yếu tố đấy. Em có go definition của 1 hàm numpy.random.multivariate_normal(mean, covariace matrix, N) để tìm hiểu nhưng không tìm thấy implements thực sự mà chỉ biết được là nó trả về 1 list các điểm có dimention giống mean. Mong mọi người giải đáp giúp em. Em cám ơn.","Em chào mọi người. Em có 1 thắc mắc về việc sinh các điểm dữ liệu thông qua Phân phối chuẩn nhiều chiều , ma trận hiệp phương sai nửa xác định dương tự chọn, điểm kỳ vọng tự chọn. Em muốn hỏi là nguyên lý làm cách nào mà chương trình có thể sinh được dữ liệu thông qua 3 yếu tố đấy. Em có go definition của 1 hàm numpy.random.multivariate_normal(mean, covariace matrix, N) để tìm hiểu nhưng không tìm thấy implements thực sự mà chỉ biết được là nó trả về 1 list các điểm có dimention giống mean. Mong mọi người giải đáp giúp em. Em cám ơn.",,,,,
"Xin chào các bạn, cho mình hỏi, mình sử dụng sklearn train data, khi train mình có sử dụng hàm StandardScaler().fit_transform(x) để chuẩn hóa dữ liệu (dữ liệu 70k row). Vây khi mình cần predict() 1 dòng dữ liệu, thì có cần phải sử dụng StandardScaler().fit_transform(x) để chuẩn hóa nó trước khi predict(0 ko?","Xin chào các bạn, cho mình hỏi, mình sử dụng sklearn train data, khi train mình có sử dụng hàm StandardScaler().fit_transform(x) để chuẩn hóa dữ liệu (dữ liệu 70k row). Vây khi mình cần predict() 1 dòng dữ liệu, thì có cần phải sử dụng StandardScaler().fit_transform(x) để chuẩn hóa nó trước khi predict(0 ko?",,,,,
"Em xin làm phiền mọi người 1 chút ạ. Em đang tìm hiểu về 1 con camera an ninh dựa trên việc phân tích hành động rồi từ đó đưa ra dự đoán là hành động đó nó có khả nghi không. Hiện tại thì em đã có thể nhận diện và vẽ những hành động đó dưới dạng 2d, nhưng mà em vẫn chưa biết làm sao để máy tính có thể phân tích và đưa ra kết luận là hành động đó có nguy hiểm không. Vì thế nên e lên đây để xin mọi người những ý kiến, gợi ý cũng như là hướng dẫn làm sao để 𝐦𝐚́𝐲 𝐭𝐢́𝐧𝐡 𝐜𝐨́ 𝐭𝐡𝐞̂̉ 𝐩𝐡𝐚̂𝐧 𝐭𝐢́𝐜𝐡 𝐯𝐚̀ đ𝐮̛𝐚 𝐫𝐚 𝐝𝐮̛̣ đ𝐨𝐚́𝐧 𝐯𝐞̂̀ 𝐦𝐮̛́𝐜 đ𝐨̣̂ 𝐧𝐠𝐮𝐲 𝐡𝐢𝐞̂̉𝐦 𝐜𝐮̉𝐚 𝐡𝐚̀𝐧𝐡 đ𝐨̣̂𝐧𝐠 ạ.
Em xin cảm ơn ạ.","Em xin làm phiền mọi người 1 chút ạ. Em đang tìm hiểu về 1 con camera an ninh dựa trên việc phân tích hành động rồi từ đó đưa ra dự đoán là hành động đó nó có khả nghi không. Hiện tại thì em đã có thể nhận diện và vẽ những hành động đó dưới dạng 2d, nhưng mà em vẫn chưa biết làm sao để máy tính có thể phân tích và đưa ra kết luận là hành động đó có nguy hiểm không. Vì thế nên e lên đây để xin mọi người những ý kiến, gợi ý cũng như là hướng dẫn làm sao để ́ ́ ́ ̂̉ ̂ ́ ̀ đ̛ ̛̣ đ́ ̂̀ ̛́ đ̣̂ ̂̉ ̉ ̀ đ̣̂ ạ. Em xin cảm ơn ạ.",,,,,
"Em chào mọi người, hiện tại em đang muốn học thêm khóa học để nâng cao kiến thức về NLP và computer vision, mọi người có thể cho em xin một vài gợi ý được không ạ. Khóa học nào có certificate càng tốt ạ :D","Em chào mọi người, hiện tại em đang muốn học thêm khóa học để nâng cao kiến thức về NLP và computer vision, mọi người có thể cho em xin một vài gợi ý được không ạ. Khóa học nào có certificate càng tốt ạ :D",,,,,
"em xin chào mọi người, hiện tại bọn em là sinh viên và đang có hướng làm bài toán về nhận dạng khuôn mặt + điểm danh trên camera. hiện tại bước chọn camera bọn em đang gặp khó khăn vì kinh nghiệm về camera sao cho phù hợp còn thiếu. vì vậy em mong  mọi người ai đã có kinh nghiệm hoặc từng làm bài toán liên quan có thể gợi ý cho bọn em các loại camera có thể dùng được không ạ
yêu cầu với camera: có thể kết nối và implement code được trên bộ xử lí của camera, có thể truyền data về server để ứng dụng thêm trên môi trường web.
mong ad duyệt bài, cảm ơn mọi người rất nhiều ạ!","em xin chào mọi người, hiện tại bọn em là sinh viên và đang có hướng làm bài toán về nhận dạng khuôn mặt + điểm danh trên camera. hiện tại bước chọn camera bọn em đang gặp khó khăn vì kinh nghiệm về camera sao cho phù hợp còn thiếu. vì vậy em mong mọi người ai đã có kinh nghiệm hoặc từng làm bài toán liên quan có thể gợi ý cho bọn em các loại camera có thể dùng được không ạ yêu cầu với camera: có thể kết nối và implement code được trên bộ xử lí của camera, có thể truyền data về server để ứng dụng thêm trên môi trường web. mong ad duyệt bài, cảm ơn mọi người rất nhiều ạ!",,,,,
"Xin chào mọi người, e có thắc mắc là nếu e định hướng chuyên sâu về data science thì có cần học sâu vào các mảng của A, ML, DL không ạ. sâu ở đây là có thể built đc model DL from scratch luôn ạ. Hay là chỉ cần tìm hiểu về kiến trúc và biết cách sử dụng các pre-train model thôi ạ. Thank mn đã đọc.","Xin chào mọi người, e có thắc mắc là nếu e định hướng chuyên sâu về data science thì có cần học sâu vào các mảng của A, ML, DL không ạ. sâu ở đây là có thể built đc model DL from scratch luôn ạ. Hay là chỉ cần tìm hiểu về kiến trúc và biết cách sử dụng các pre-train model thôi ạ. Thank mn đã đọc.",,,,,
"Chào mọi người,
Em đang dùng mô hình sinh ảnh Glow model và em có xem code sử dụng pytorch này, có đoạn code này em chưa hiểu người ta muốn làm gì ạ. Ảnh ở đây người ta sử dụng là ở tập Celeba dataset kích cỡ (3,64,64). n_bits = 5 và n_bins = 2 mũ 5.
Em cảm ơn mọi người ạ.","Chào mọi người, Em đang dùng mô hình sinh ảnh Glow model và em có xem code sử dụng pytorch này, có đoạn code này em chưa hiểu người ta muốn làm gì ạ. Ảnh ở đây người ta sử dụng là ở tập Celeba dataset kích cỡ (3,64,64). n_bits = 5 và n_bins = 2 mũ 5. Em cảm ơn mọi người ạ.",,,,,
"Mình có một thú vui khá vô bổ là ngồi tự code lại các thuật toán. Tương đối mất thời gian hình dung cấu trúc dữ liệu và giải thuật rồi gỡ rối. Mình làm bảo vệ nên thời gian cũng nhiều.
Bù lại, sau khi ra kết quả cũng thấy hay hay, hiểu ra một số điều mà không thể hiểu nếu chỉ đọc sách hay chạy lại code của cao thủ khác.
Mình mất đúng một ngày để code lại thuật toán K-Means clustering (không tham khảo code trên mạng nhé).
Sau đó vẽ đường bao quanh các cluster sau khi phân chia thấy một đặc điểm các đường bao mỗi cluster không giao nhau.
Mai nghịch tiếp Matplotlib animation để hoạt hình từng bước chạy một xem thế nào.
Mọi thuật toán phức tạp khi visualize trong không gian 2-3 chiều trở nên rất đẹp. Mình là fan của Youtube channel 3Blue1Brown.
Animation xong sẽ viết bài hướng dẫn nhé","Mình có một thú vui khá vô bổ là ngồi tự code lại các thuật toán. Tương đối mất thời gian hình dung cấu trúc dữ liệu và giải thuật rồi gỡ rối. Mình làm bảo vệ nên thời gian cũng nhiều. Bù lại, sau khi ra kết quả cũng thấy hay hay, hiểu ra một số điều mà không thể hiểu nếu chỉ đọc sách hay chạy lại code của cao thủ khác. Mình mất đúng một ngày để code lại thuật toán K-Means clustering (không tham khảo code trên mạng nhé). Sau đó vẽ đường bao quanh các cluster sau khi phân chia thấy một đặc điểm các đường bao mỗi cluster không giao nhau. Mai nghịch tiếp Matplotlib animation để hoạt hình từng bước chạy một xem thế nào. Mọi thuật toán phức tạp khi visualize trong không gian 2-3 chiều trở nên rất đẹp. Mình là fan của Youtube channel 3Blue1Brown. Animation xong sẽ viết bài hướng dẫn nhé",,,,,
"Em chào m.n, m.n cho em hỏi máy em cấu hình thấp yếu tải sql hiện ra lỗi ntn ( kể cả 2008, 2012 ) thì làm ntn để khắc phục ạ? E xin cảm ơn","Em chào m.n, m.n cho em hỏi máy em cấu hình thấp yếu tải sql hiện ra lỗi ntn ( kể cả 2008, 2012 ) thì làm ntn để khắc phục ạ? E xin cảm ơn",,,,,
"[Video Meeting Data Question]
Mình xin chia sẻ tới các bạn buổi meeting Data Question được tổ chức nhằm hỗ trợ các bạn giải quyết các vấn đề liên quan tới mô hình, dữ kiệu và các kĩ năng nghề nghiệp cần thiết trong ngành AI và Machine Learning. Các nội dung chính bao gồm:
- Những kiến thức và kĩ năng cần trang bị để theo học ngành Data Science đối với người muốn chuyển ngành?
- Knowledge Domain có thực sự cần thiết? Làm thế nào để trang bị kiến thức về Business đối với sinh viên khối IT?
- Kinh nghiệm về lựa chọn Data Augmentation và thực nghiệm mô hình?
- Đâu là phương pháp handle missing data phù hợp?
- Các tools gán nhãn hiệu quả cho bài toán NER annotation.
- Nâng cao hiệu suất trích lọc nội dung cho bài toán OCR.
- Kinh nghiệm về con đường hướng tới AI Engineer.
TowardDataScience xin cảm ơn sự tham gia của khách mời Nguyễn Việt Anh và các bạn để có được một sự kiện thành công.
Các bạn có thể theo dõi video buổi meeting bên dưới:
https://www.youtube.com/watch?v=SNoOwtfjbQo
Nếu bạn thấy nội dung video hữu ích, vui lòng chia sẻ video tới bạn bè và đừng quên cho mình một like video.
-------------------------------------
P/S: Buổi meeting tiếp theo sẽ được tổ chức vào thứ 4 ngày 3/11/2021 từ 19:30-20:30 PM về chủ đề ""Metrics Selection in Machine Learning"". Mời các bạn tham dự và đặt câu hỏi tại link:
https://meet.google.com/yjs-xudp-vpb","[Video Meeting Data Question] Mình xin chia sẻ tới các bạn buổi meeting Data Question được tổ chức nhằm hỗ trợ các bạn giải quyết các vấn đề liên quan tới mô hình, dữ kiệu và các kĩ năng nghề nghiệp cần thiết trong ngành AI và Machine Learning. Các nội dung chính bao gồm: - Những kiến thức và kĩ năng cần trang bị để theo học ngành Data Science đối với người muốn chuyển ngành? - Knowledge Domain có thực sự cần thiết? Làm thế nào để trang bị kiến thức về Business đối với sinh viên khối IT? - Kinh nghiệm về lựa chọn Data Augmentation và thực nghiệm mô hình? - Đâu là phương pháp handle missing data phù hợp? - Các tools gán nhãn hiệu quả cho bài toán NER annotation. - Nâng cao hiệu suất trích lọc nội dung cho bài toán OCR. - Kinh nghiệm về con đường hướng tới AI Engineer. TowardDataScience xin cảm ơn sự tham gia của khách mời Nguyễn Việt Anh và các bạn để có được một sự kiện thành công. Các bạn có thể theo dõi video buổi meeting bên dưới: https://www.youtube.com/watch?v=SNoOwtfjbQo Nếu bạn thấy nội dung video hữu ích, vui lòng chia sẻ video tới bạn bè và đừng quên cho mình một like video. ------------------------------------- P/S: Buổi meeting tiếp theo sẽ được tổ chức vào thứ 4 ngày 3/11/2021 từ 19:30-20:30 PM về chủ đề ""Metrics Selection in Machine Learning"". Mời các bạn tham dự và đặt câu hỏi tại link: https://meet.google.com/yjs-xudp-vpb",,,,,
"Hiện tại mình đang theo học thạc sĩ ngành ML theo hướng nghiên cứu NLP với đề tài là Fake news detection. Mình được giáo sư ra yêu cầu nghiên cứu về một chủ đề về Fake news detection để publish paper. Mình nghĩ rằng để publish paper thì cần phải nghĩ ra một cái gì đó mới, nhưng thực sự mình đọc rất nhiều paper thì thấy hầu như giới khoa học đã làm hết rồi nên không biết phải làm gì. Không biết anh/chị/bạn bè nào có kinh nghiệm về mảng này có thể cho mình lời khuyên ạ.
Mình thật sự cảm ơn","Hiện tại mình đang theo học thạc sĩ ngành ML theo hướng nghiên cứu NLP với đề tài là Fake news detection. Mình được giáo sư ra yêu cầu nghiên cứu về một chủ đề về Fake news detection để publish paper. Mình nghĩ rằng để publish paper thì cần phải nghĩ ra một cái gì đó mới, nhưng thực sự mình đọc rất nhiều paper thì thấy hầu như giới khoa học đã làm hết rồi nên không biết phải làm gì. Không biết anh/chị/bạn bè nào có kinh nghiệm về mảng này có thể cho mình lời khuyên ạ. Mình thật sự cảm ơn",,,,,
Xin chia sẻ với các bạn một repo về SQL do mình tổng hợp từ quá trình làm việc.,Xin chia sẻ với các bạn một repo về SQL do mình tổng hợp từ quá trình làm việc.,,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 10/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 10/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.",,,,,
"Chào mọi người, em hiện tại đang làm dự án cho người mù và em muốn có chức năng cảnh báo va chạm, mọi người có thể cho em xin ý kiến làm sao để làm không ạ?","Chào mọi người, em hiện tại đang làm dự án cho người mù và em muốn có chức năng cảnh báo va chạm, mọi người có thể cho em xin ý kiến làm sao để làm không ạ?",,,,,
"Cuốn kinh điển mà bác Tiệp khuyến khích các bạn đọc hiện đã sắp ra bản mới. Bản xem trước đã được công bố tại https://probml.github.io/pml-book/book1.html
Em đã xem (lướt) bản cũ và thấy nó cũng dễ hiểu nhưng khá kĩ về toán, ngôn ngữ ML hơi cổ. Bác nào xem bản mới xong cho ae cái review nữa càng tốt. Thanks các bác","Cuốn kinh điển mà bác Tiệp khuyến khích các bạn đọc hiện đã sắp ra bản mới. Bản xem trước đã được công bố tại https://probml.github.io/pml-book/book1.html Em đã xem (lướt) bản cũ và thấy nó cũng dễ hiểu nhưng khá kĩ về toán, ngôn ngữ ML hơi cổ. Bác nào xem bản mới xong cho ae cái review nữa càng tốt. Thanks các bác",,,,,
"Có bạn gần đây hỏi bản màu của cuốn ""Machine Learning cơ bản"", mình không nhớ bạn hỏi ở đâu nên đăng lên đây cho mọi người cùng download nếu cần.
https://github.com/tiepvupsu/ebookMLCB/blob/master/book_ML_color.pdf
Đây là bản trước khi ra sách giấy với nhiều từ khoá vẫn để tiếng Anh và văn phong chưa được chỉnh. Sau đó bên biên tập sách của NXB yêu cầu sửa lại rất nhiều văn vẻ và từ khoá sang tiếng Việt.
Không ngờ bản màu chưa qua biên tập (từ NXB) lại được đón nhận nhiều hơn.","Có bạn gần đây hỏi bản màu của cuốn ""Machine Learning cơ bản"", mình không nhớ bạn hỏi ở đâu nên đăng lên đây cho mọi người cùng download nếu cần. https://github.com/tiepvupsu/ebookMLCB/blob/master/book_ML_color.pdf Đây là bản trước khi ra sách giấy với nhiều từ khoá vẫn để tiếng Anh và văn phong chưa được chỉnh. Sau đó bên biên tập sách của NXB yêu cầu sửa lại rất nhiều văn vẻ và từ khoá sang tiếng Việt. Không ngờ bản màu chưa qua biên tập (từ NXB) lại được đón nhận nhiều hơn.",,,,,
"Em chào mọi người,
Em đang đọc một số nghiên cứu về Phương pháp Gradient với mô hình Inexact Oracle của Nesterov và Devolder. Trong đó, khi tác giả đưa ra đồ thị về sai số giữa giá trị hàm f(yk) và giá trị min y* ( như trong hình vẽ) em không biết tác giả đã tính giá trị đó nhờ công thức nào hay kinh nghiệm để tính từ đâu ra? Bởi vì y* thì mình không biết. Mong ace nào từng nghiên cứu qua có thể giải thích giúp em với ạ.
Em xin cám ơn nhiều.","Em chào mọi người, Em đang đọc một số nghiên cứu về Phương pháp Gradient với mô hình Inexact Oracle của Nesterov và Devolder. Trong đó, khi tác giả đưa ra đồ thị về sai số giữa giá trị hàm f(yk) và giá trị min y* ( như trong hình vẽ) em không biết tác giả đã tính giá trị đó nhờ công thức nào hay kinh nghiệm để tính từ đâu ra? Bởi vì y* thì mình không biết. Mong ace nào từng nghiên cứu qua có thể giải thích giúp em với ạ. Em xin cám ơn nhiều.",,,,,
Một cách biểu diễn quá trình đào tạo mạng neural nhân tạo cho mọi người,Một cách biểu diễn quá trình đào tạo mạng neural nhân tạo cho mọi người,,,,,
"Chào mọi người, em đang tìm hiểu về time series và có vài thắc mắc sau, nhờ mọi người giải đáp giúp em.
Trong 2 chuỗi time series, làm sao mình biết chuỗi nào thay đổi sẽ kéo theo thay đổi của chuỗi còn lại? Em có thử dùng Granger causality test nhưng mà có vẻ kết quả nó ra không hợp lý trên dataset của em (kiểu kết quả nó vô lý và mình biết vô lý dựa theo kinh nghiệm), và em có đọc tài liệu thì cái Granger causality test này cũng không chắc thể hiện được mối quan hệ nhân quả thực sự giữa 2 chuỗi. Vậy có cách nào khác đáng tin hơn để biết chuỗi nào thay đổi sẽ kéo theo thay đổi của chuỗi còn lại và thay đổi như thế nào (ví dụ chuỗi 2 giảm thì chuỗi 1 tăng hoặc chuỗi 1 tăng thì chuỗi 2 cũng tăng) không ạ? 
 Có trường hợp nào mà trong khoảng thời gian (t_1; t_2) chuỗi này không ảnh hưởng đến chuỗi kia (chuỗi này thay đổi không kéo theo sự thay đổi của chuỗi kia) mà đến khoảng thời gian (t_1’; t_2’) thì mới bắt đầu ảnh hưởng không ạ? Và làm sao mình kiểm tra được điều này?
Ví dụ bây giờ mình có nhiều hơn 2 chuỗi,  bằng cách nào đó, mình biết chuỗi s_1 thay đổi kéo theo s_2 thay đổi, s_2  kéo theo s_3,… , s_(n-1) kéo theo s_n, vậy mình có thể kết luận chuỗi s_1 thay đổi sẽ kéo theo s_n thay đổi được không?
Ví dụ trong hình này, ở giai đoạn sau 2009 một xíu, các chuỗi này hầu như đều đi xuống, vậy có cách nào mình xác định được chuỗi nào sẽ thay đổi đầu tiên (bị đi xuống đầu tiên) và thứ tự các chuỗi sau đó cũng bị đi xuống không ạ? 
Liên quan đến câu 4, khi các chuỗi này không đồng loạt đi xuống cùng lúc, mà sẽ chênh nhau một khoảng thời gian, vậy có cách nào xác định được khoảng thời gian chênh nhau này giữa các chuỗi không ạ?
Nhờ mọi người giải đáp giúp em, em cảm ơn nhiều ạ.","Chào mọi người, em đang tìm hiểu về time series và có vài thắc mắc sau, nhờ mọi người giải đáp giúp em. Trong 2 chuỗi time series, làm sao mình biết chuỗi nào thay đổi sẽ kéo theo thay đổi của chuỗi còn lại? Em có thử dùng Granger causality test nhưng mà có vẻ kết quả nó ra không hợp lý trên dataset của em (kiểu kết quả nó vô lý và mình biết vô lý dựa theo kinh nghiệm), và em có đọc tài liệu thì cái Granger causality test này cũng không chắc thể hiện được mối quan hệ nhân quả thực sự giữa 2 chuỗi. Vậy có cách nào khác đáng tin hơn để biết chuỗi nào thay đổi sẽ kéo theo thay đổi của chuỗi còn lại và thay đổi như thế nào (ví dụ chuỗi 2 giảm thì chuỗi 1 tăng hoặc chuỗi 1 tăng thì chuỗi 2 cũng tăng) không ạ? Có trường hợp nào mà trong khoảng thời gian (t_1; t_2) chuỗi này không ảnh hưởng đến chuỗi kia (chuỗi này thay đổi không kéo theo sự thay đổi của chuỗi kia) mà đến khoảng thời gian (t_1’; t_2’) thì mới bắt đầu ảnh hưởng không ạ? Và làm sao mình kiểm tra được điều này? Ví dụ bây giờ mình có nhiều hơn 2 chuỗi, bằng cách nào đó, mình biết chuỗi s_1 thay đổi kéo theo s_2 thay đổi, s_2 kéo theo s_3,… , s_(n-1) kéo theo s_n, vậy mình có thể kết luận chuỗi s_1 thay đổi sẽ kéo theo s_n thay đổi được không? Ví dụ trong hình này, ở giai đoạn sau 2009 một xíu, các chuỗi này hầu như đều đi xuống, vậy có cách nào mình xác định được chuỗi nào sẽ thay đổi đầu tiên (bị đi xuống đầu tiên) và thứ tự các chuỗi sau đó cũng bị đi xuống không ạ? Liên quan đến câu 4, khi các chuỗi này không đồng loạt đi xuống cùng lúc, mà sẽ chênh nhau một khoảng thời gian, vậy có cách nào xác định được khoảng thời gian chênh nhau này giữa các chuỗi không ạ? Nhờ mọi người giải đáp giúp em, em cảm ơn nhiều ạ.",,,,,
Mình đang muốn áp dụng walk forward validation cho decision tree. Mỗi một train/test split sẽ cho ra một decision tree. Hiện tại mình chưa biết cách tổng hợp các decision tree dể cho ra quyết định tốt nhất và nhanh nhất. Các bạn/anh/chị có hướng giải quyết hoặc thuật toán nào thì đề xuất giúp mình với ạ!,Mình đang muốn áp dụng walk forward validation cho decision tree. Mỗi một train/test split sẽ cho ra một decision tree. Hiện tại mình chưa biết cách tổng hợp các decision tree dể cho ra quyết định tốt nhất và nhanh nhất. Các bạn/anh/chị có hướng giải quyết hoặc thuật toán nào thì đề xuất giúp mình với ạ!,,,,,
Các ace cho mình hỏi cách để colab ko bị dừng lại nếu ko tương tác trong 90p ạ. Cám ơn mn.,Các ace cho mình hỏi cách để colab ko bị dừng lại nếu ko tương tác trong 90p ạ. Cám ơn mn.,,,,,
"Chào mọi người,
Mình có bài toán như sau nhờ mọi người tư vấn giúp nên dùng mô hình nào cho phù hợp.
6 chấm tròn xanh có tọa độ (x1,y1) ...(x6,y6)
Các điểm được đánh số P1...P6 ( như hình vẽ)
Số lượng: 1000 mẫu
vị trí tương đối giữa các chấm tròn thay đổi không nhiều
Bài toán:
1. Input: tọa độ 6 điểm bất kì; Output: phân loại vào 6 group (P1..P6)
2. Input: tọa độ 4 điểm, Output: phân loại vào 4 group từ đó tìm ra vị trí 2 điểm còn thiếu
3. Input: tọa độ 8 điểm, Output: phân loại vào 6 group (P1..P6), có group sẽ có nhiều hơn 2 điểm
bạn nào có giải pháp hay, mình xin phép cảm ơn và hậu tạ.
Edit1:
Cái mình băn khoăn khi dùng Kmeans là vị trí tương đối giữa các điểm thay đổi ý nhưng cả cụm 6 điểm trong mặt phẳng thì thay đổi nhiều
Chính vì thế mình định tính trung bình cộng của các điểm, mục đích là để bỏ đi offset giữa các cụm 6 điểm.
Tuy nhiên, cách này fail khi đầu vào không đủ, ( ví dụ thiếu điểm 1 và 4) sẽ làm trung bình cộng giữa các điểm bị mất.","Chào mọi người, Mình có bài toán như sau nhờ mọi người tư vấn giúp nên dùng mô hình nào cho phù hợp. 6 chấm tròn xanh có tọa độ (x1,y1) ...(x6,y6) Các điểm được đánh số P1...P6 ( như hình vẽ) Số lượng: 1000 mẫu vị trí tương đối giữa các chấm tròn thay đổi không nhiều Bài toán: 1. Input: tọa độ 6 điểm bất kì; Output: phân loại vào 6 group (P1..P6) 2. Input: tọa độ 4 điểm, Output: phân loại vào 4 group từ đó tìm ra vị trí 2 điểm còn thiếu 3. Input: tọa độ 8 điểm, Output: phân loại vào 6 group (P1..P6), có group sẽ có nhiều hơn 2 điểm bạn nào có giải pháp hay, mình xin phép cảm ơn và hậu tạ. Edit1: Cái mình băn khoăn khi dùng Kmeans là vị trí tương đối giữa các điểm thay đổi ý nhưng cả cụm 6 điểm trong mặt phẳng thì thay đổi nhiều Chính vì thế mình định tính trung bình cộng của các điểm, mục đích là để bỏ đi offset giữa các cụm 6 điểm. Tuy nhiên, cách này fail khi đầu vào không đủ, ( ví dụ thiếu điểm 1 và 4) sẽ làm trung bình cộng giữa các điểm bị mất.",,,,,
"Mọi người thông não hộ mình cái vấn đề này với.
Mình mô tả project môn học face recognition mà mình đang làm chút xíu
Mình detect faces, lấy landmarks, tính distance giữa các landmark và áp dụng K-nn classifier (ORC), Dùng train_test_split để tách lấy train và test
sau khi fit mình lấy matching_score = clf.predict_proba(X_test)
mình tính genius score và impostor score để vẽ biểu đồ, tìm D-prime, cũng như tìm ra EER
mọi thứ ok hết rồi, và trong project có yêu cầu 1 vấn đề như hình
Thực sự đọc trên mạng rất mong lung và khó hiểu, đại khái cái này là sao vậy mọi người ơi... ngay cả resource cung cấp chỉ ghi đúng definition của phần này... mình ko hiểu và cũng ko biết phải áp dụng vào project mình đang làm kiểu gì nửa hjx.
Mọi người có thể cho mình lời khuyên được không.
Cảm ơn mọi người nhiều","Mọi người thông não hộ mình cái vấn đề này với. Mình mô tả project môn học face recognition mà mình đang làm chút xíu Mình detect faces, lấy landmarks, tính distance giữa các landmark và áp dụng K-nn classifier (ORC), Dùng train_test_split để tách lấy train và test sau khi fit mình lấy matching_score = clf.predict_proba(X_test) mình tính genius score và impostor score để vẽ biểu đồ, tìm D-prime, cũng như tìm ra EER mọi thứ ok hết rồi, và trong project có yêu cầu 1 vấn đề như hình Thực sự đọc trên mạng rất mong lung và khó hiểu, đại khái cái này là sao vậy mọi người ơi... ngay cả resource cung cấp chỉ ghi đúng definition của phần này... mình ko hiểu và cũng ko biết phải áp dụng vào project mình đang làm kiểu gì nửa hjx. Mọi người có thể cho mình lời khuyên được không. Cảm ơn mọi người nhiều",,,,,
em đã tải 1 bộ dataset chứa ảnh x-quang covid và non-covid ở phổi. Em cũng load được dữ liệu và chuẩn hóa dữ liệu. Nhưng em có vấn đề ở chỗ mô hình huấn luyện. Em tìm thấy trên mạng về keras senquential. phần tạo các lớp ( layer) em vẫn chưa hiểu được. em đã thử tìm trên trang của keras về nó. Mong mọi người giúp em hiểu dc nó và làm sao e có thể sử dụng nó để phân loại covid và noncovid. Cảm ơn mọi người,em đã tải 1 bộ dataset chứa ảnh x-quang covid và non-covid ở phổi. Em cũng load được dữ liệu và chuẩn hóa dữ liệu. Nhưng em có vấn đề ở chỗ mô hình huấn luyện. Em tìm thấy trên mạng về keras senquential. phần tạo các lớp ( layer) em vẫn chưa hiểu được. em đã thử tìm trên trang của keras về nó. Mong mọi người giúp em hiểu dc nó và làm sao e có thể sử dụng nó để phân loại covid và noncovid. Cảm ơn mọi người,,,,,
"Một thư viện hay cho bài toán OCR giúp chuyển công thức toán thành dạng text (từ ảnh công thức trong file PDF/LaTex).
https://github.com/lukas-blecher/LaTeX-OCR",Một thư viện hay cho bài toán OCR giúp chuyển công thức toán thành dạng text (từ ảnh công thức trong file PDF/LaTex). https://github.com/lukas-blecher/LaTeX-OCR,,,,,
"[Wav2vec 2.0 fairseq pretrained model]:
Chào mọi người. Em đang cần pretrained model của wav2vec 2.0 bằng Tensorflow hay Pytorch đều đc ạ, nhưng tầm dưới 100M params (tương đương base model) và đc train trên nhiều ngôn ngữ (có tiếng Việt, Đức, Ả Rập thì tốt quá).
Fairseq thì có XLSR large model đc train ~50 ngôn ngữ nhưng tận 300M params nên máy lab e nạp ko nổi (8 GPU 2080 Ti).
Mọi người ai có thì cho em xin với ạ. Em củm ơn rất nhìu ạ.
https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md","[Wav2vec 2.0 fairseq pretrained model]: Chào mọi người. Em đang cần pretrained model của wav2vec 2.0 bằng Tensorflow hay Pytorch đều đc ạ, nhưng tầm dưới 100M params (tương đương base model) và đc train trên nhiều ngôn ngữ (có tiếng Việt, Đức, Ả Rập thì tốt quá). Fairseq thì có XLSR large model đc train ~50 ngôn ngữ nhưng tận 300M params nên máy lab e nạp ko nổi (8 GPU 2080 Ti). Mọi người ai có thì cho em xin với ạ. Em củm ơn rất nhìu ạ. https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md",,,,,
"Em chào mọi người ạ, Mọi người có thể cho em xin mô tả về công việc của một Machine Learning Engineering và một Sample Project của một mô hình Machine Learning với người đi làm với ạ! Em cảm ơn ạ","Em chào mọi người ạ, Mọi người có thể cho em xin mô tả về công việc của một Machine Learning Engineering và một Sample Project của một mô hình Machine Learning với người đi làm với ạ! Em cảm ơn ạ",,,,,
"[AI SHARING]
Chia sẻ với các bạn cách chọn lọc paper phù hợp giúp đọc nhanh hiểu sâu hơn kèm các tool, tips giúp ích cho việc đọc paper.","[AI SHARING] Chia sẻ với các bạn cách chọn lọc paper phù hợp giúp đọc nhanh hiểu sâu hơn kèm các tool, tips giúp ích cho việc đọc paper.",,,,,
"Chào mọi người, mình hiện đang trainning một model, với lượng data 128Gb, nhưng thường bị crash do quá RAM (là là 120GB). Không biết cách khắc phục như thế nào ạ? Rất cám ơn mọi người đã giúp đỡ","Chào mọi người, mình hiện đang trainning một model, với lượng data 128Gb, nhưng thường bị crash do quá RAM (là là 120GB). Không biết cách khắc phục như thế nào ạ? Rất cám ơn mọi người đã giúp đỡ",,,,,
"Kính chào các bác, em mới học về phần này nên mạnh dạn làm video chia sẻ cho các bạn mới học.
Mong ad duyệt bài!","Kính chào các bác, em mới học về phần này nên mạnh dạn làm video chia sẻ cho các bạn mới học. Mong ad duyệt bài!",,,,,
"Hello ae, team mình có viết pipeline cho bài toán tracking, các bạn tham khảo và đóng góp ý kiến giúp mình với ạ. Mình mới tập viết nên mong mọi người góp ý nhiều. Cảm ơn mọi người","Hello ae, team mình có viết pipeline cho bài toán tracking, các bạn tham khảo và đóng góp ý kiến giúp mình với ạ. Mình mới tập viết nên mong mọi người góp ý nhiều. Cảm ơn mọi người",,,,,
"Hi mọi người, em là newbie đang cần hướng xử lý cho một bài toán là: từ một bức ảnh đầu vào có thể vẽ lại được một bức ảnh tương tự. Mục đích là sau này từ một đoạn text em có thể render ra được bức ảnh có nội dung gần giống vs text. Cảm ơn mọi người đã xem ạ","Hi mọi người, em là newbie đang cần hướng xử lý cho một bài toán là: từ một bức ảnh đầu vào có thể vẽ lại được một bức ảnh tương tự. Mục đích là sau này từ một đoạn text em có thể render ra được bức ảnh có nội dung gần giống vs text. Cảm ơn mọi người đã xem ạ",,,,,
"[Data-centric AI Competition]
Cuộc thi lần này được tập trung vào việc xử lý dữ liệu, mô hình sẽ được giữ cố định cho tất cả các đội chơi.
Bài toán của cuộc thi là nhận diện vật thể (object detection) với 3 lớp là mặt đeo khẩu trang, mặt không đeo khẩu trang và mặt đeo khẩu trang không đúng cách.
Các bạn chơi sẽ được cấp 1000 ảnh training, 100 ảnh public test, 1 pretrained model và code để train. Sau đó các bạn xử lý, augment và có thể nộp lại tối đa là 3000 ảnh để ban tổ chức chấm điểm.
Chi tiết về cuộc thi ở đây: https://datacomp.io/trang-chu
Video giới thiệu về cuộc thi ở đây: https://www.youtube.com/watch?v=E8CG06a9sQI
Group chính thức của cuộc thi: https://www.facebook.com/groups/966726683795290
Ps: Giải thưởng thì như trong hình, còn chờ gì nữa các bạn ơiiiii","[Data-centric AI Competition] Cuộc thi lần này được tập trung vào việc xử lý dữ liệu, mô hình sẽ được giữ cố định cho tất cả các đội chơi. Bài toán của cuộc thi là nhận diện vật thể (object detection) với 3 lớp là mặt đeo khẩu trang, mặt không đeo khẩu trang và mặt đeo khẩu trang không đúng cách. Các bạn chơi sẽ được cấp 1000 ảnh training, 100 ảnh public test, 1 pretrained model và code để train. Sau đó các bạn xử lý, augment và có thể nộp lại tối đa là 3000 ảnh để ban tổ chức chấm điểm. Chi tiết về cuộc thi ở đây: https://datacomp.io/trang-chu Video giới thiệu về cuộc thi ở đây: https://www.youtube.com/watch?v=E8CG06a9sQI Group chính thức của cuộc thi: https://www.facebook.com/groups/966726683795290 Ps: Giải thưởng thì như trong hình, còn chờ gì nữa các bạn ơiiiii",,,,,
"Em muốn hỏi chút về Data Camp. Giờ đang có deal 129 Euro cho Premium Plan, liệu có giá nào phải chăng hơn không ạ?
Đương nhiên là để học thì em hok tiếc bỏ ra 129 Euro để học, nhưng sinh viên thì cứ càng phải chăng hơn một chút thì càng tốt.
Nên có anh chị nào có phương pháp nào rẻ hơn, hay cách nào rẻ hơn thì có thể chia sẻ cho em với ạ!
Nếu không thì em cũng sẵn sàng mua 129 Euro cho 1 năm thôi! 😅","Em muốn hỏi chút về Data Camp. Giờ đang có deal 129 Euro cho Premium Plan, liệu có giá nào phải chăng hơn không ạ? Đương nhiên là để học thì em hok tiếc bỏ ra 129 Euro để học, nhưng sinh viên thì cứ càng phải chăng hơn một chút thì càng tốt. Nên có anh chị nào có phương pháp nào rẻ hơn, hay cách nào rẻ hơn thì có thể chia sẻ cho em với ạ! Nếu không thì em cũng sẵn sàng mua 129 Euro cho 1 năm thôi!",,,,,
"https://github.com/mrharicot/monodepth
cho mình hỏi làm thế nào để sử dụng model này trong java. Mình đang làm app android cần sử dụng nó.",https://github.com/mrharicot/monodepth cho mình hỏi làm thế nào để sử dụng model này trong java. Mình đang làm app android cần sử dụng nó.,,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới mạnh khỏe và bình an.",,,,,
"VinAI NLP workshop 2021: www.vinai.io/NLPworkshop2021
Mình xin được mời mọi người tham dự VinAI NLP workshop 2021 (online workshop), vào sáng thứ 6 tuần sau (9:00 AM – 12:15 PM, 29/10/2021).
Chương trình workshop có bao gồm một số nghiên cứu liên quan đến xử lý ngôn ngữ tiếng Việt. Hi vọng được mọi người tham gia và cùng thảo luận.
Đăng ký miễn phí để tham gia workshop tại: https://forms.gle/AuXe5JVXr3PbYWQe8","VinAI NLP workshop 2021: www.vinai.io/NLPworkshop2021 Mình xin được mời mọi người tham dự VinAI NLP workshop 2021 (online workshop), vào sáng thứ 6 tuần sau (9:00 AM – 12:15 PM, 29/10/2021). Chương trình workshop có bao gồm một số nghiên cứu liên quan đến xử lý ngôn ngữ tiếng Việt. Hi vọng được mọi người tham gia và cùng thảo luận. Đăng ký miễn phí để tham gia workshop tại: https://forms.gle/AuXe5JVXr3PbYWQe8",,,,,
"Chào mọi người, hiện tại bên em đang muốn mua thiết bị phần cứng để nghiên cứu bài toán Pose Estimation cho 1 chuỗi cửa hàng. Hiện tại có căn cứ nào để tính được nên mua phần cứng như thế nào (GPU, CPU, RAM, ...) để đảm bảo tốc độ xử lý khi nghiên cứu bài toán không mọi người, cảm ơn mọi người rất nhiều :D","Chào mọi người, hiện tại bên em đang muốn mua thiết bị phần cứng để nghiên cứu bài toán Pose Estimation cho 1 chuỗi cửa hàng. Hiện tại có căn cứ nào để tính được nên mua phần cứng như thế nào (GPU, CPU, RAM, ...) để đảm bảo tốc độ xử lý khi nghiên cứu bài toán không mọi người, cảm ơn mọi người rất nhiều :D",,,,,
"Em được giao công việc là nghiên cứu ứng dụng Deep Learning để bóc tách được địa chỉ tại Việt Nam:
Ví dụ: Số nhà AA, Thôn BB, Xã CC, Huyện DD, Tỉnh EE
Hoặc, Sn AA, đường bb, qCC, tpDD
---
Mục đích là bóc tách được thông tin địa chỉ, và đánh giá độ chính xác.
Em đang lạc lối giữa biển tài liệu ...
Các bác có tài liệu hoặc lộ trình cụ thể, hoặc lớp học ONLINE or OFFILNE chất lượng k ạ? Em xin tư vấn của các bác. Em cần học gấp trong khoảng 3 tháng tới ạ.
Em có thể lập trình python cơ bản, tiếng anh đọc hiểu.
--
Mong các cao nhân mở lối ạ :((","Em được giao công việc là nghiên cứu ứng dụng Deep Learning để bóc tách được địa chỉ tại Việt Nam: Ví dụ: Số nhà AA, Thôn BB, Xã CC, Huyện DD, Tỉnh EE Hoặc, Sn AA, đường bb, qCC, tpDD --- Mục đích là bóc tách được thông tin địa chỉ, và đánh giá độ chính xác. Em đang lạc lối giữa biển tài liệu ... Các bác có tài liệu hoặc lộ trình cụ thể, hoặc lớp học ONLINE or OFFILNE chất lượng k ạ? Em xin tư vấn của các bác. Em cần học gấp trong khoảng 3 tháng tới ạ. Em có thể lập trình python cơ bản, tiếng anh đọc hiểu. -- Mong các cao nhân mở lối ạ :((",,,,,
"Chào các bạn, mình đang tìm hiểu về kỹ thuật Hierarchical Temporal Memory để nhận dạng bất thường trong dữ liệu. Bạn nào đã từng nghiên cứu về cái này vui lòng hướng dẫn hay chỉ mình nơi để tìm hiểu kỹ hơn.
Xin cảm ơn","Chào các bạn, mình đang tìm hiểu về kỹ thuật Hierarchical Temporal Memory để nhận dạng bất thường trong dữ liệu. Bạn nào đã từng nghiên cứu về cái này vui lòng hướng dẫn hay chỉ mình nơi để tìm hiểu kỹ hơn. Xin cảm ơn",,,,,
"Hello mọi người, mọi người tư vấn và hướng dẫn giúp mình với nha.
Project face_recognition của mình có dataset hình của từng người.
Hiện tại mình đã add landmarks cho hầu hết các hình và hàm trả về X và y, X(array list của tất cả các hình, y là location của hình)
Mình dùng classifier của sklearn.neighbors.KNeighborsClassifier
Bây giờ mình đang bí ở chỗ ko biết apply thế nào cho cả template và query.
Và sau khi apply xong sẽ làm thế nào để so sánh giữa cả 2 phần template và query để xem nó có match hay không.
À còn nửa, mình đọc sách theo thực tế thì K-NN thì dùng các feature như màu da, khoảng cách để tính toán thì làm sao mình biết sẽ dùng gì và làm thế nào để apply ạ
Mình đã thử phương pháp dùng library: face_recognition và xuất thành công ảnh có tên của người trong ảnh, nhưng phương pháp này ko mang lại hiệu quả nghiên cứu nên mình chuyển sang dùng cái này nhưng đang bị bí tại đây. Mong mọi người từng làm qua có thể cho mình lời khuyên và giúp mình giải đáp thắc mắc.
Cảm ơn nhiều ạ","Hello mọi người, mọi người tư vấn và hướng dẫn giúp mình với nha. Project face_recognition của mình có dataset hình của từng người. Hiện tại mình đã add landmarks cho hầu hết các hình và hàm trả về X và y, X(array list của tất cả các hình, y là location của hình) Mình dùng classifier của sklearn.neighbors.KNeighborsClassifier Bây giờ mình đang bí ở chỗ ko biết apply thế nào cho cả template và query. Và sau khi apply xong sẽ làm thế nào để so sánh giữa cả 2 phần template và query để xem nó có match hay không. À còn nửa, mình đọc sách theo thực tế thì K-NN thì dùng các feature như màu da, khoảng cách để tính toán thì làm sao mình biết sẽ dùng gì và làm thế nào để apply ạ Mình đã thử phương pháp dùng library: face_recognition và xuất thành công ảnh có tên của người trong ảnh, nhưng phương pháp này ko mang lại hiệu quả nghiên cứu nên mình chuyển sang dùng cái này nhưng đang bị bí tại đây. Mong mọi người từng làm qua có thể cho mình lời khuyên và giúp mình giải đáp thắc mắc. Cảm ơn nhiều ạ",,,,,
"Xin chào mọi người. Hiện tại mình đang nghiên cứu bài toán eKYC cho công ty, có trích xuất OCR và nhận diện CMT/ Căn cước giả mạo. Mọi người có thể cho mình xin keyword cho bài toán này được không ạ?","Xin chào mọi người. Hiện tại mình đang nghiên cứu bài toán eKYC cho công ty, có trích xuất OCR và nhận diện CMT/ Căn cước giả mạo. Mọi người có thể cho mình xin keyword cho bài toán này được không ạ?",,,,,
"Bạn nào hướng dẫn mình bài này dc ko ạ.
Mình chỉ làm dc 1 input 1 output. Nhưng đề bài yêu cầu 1 input 2 output (và chỉ dc sử dụng 1 hàm để tính ra cả chiều cao với tuổi).",Bạn nào hướng dẫn mình bài này dc ko ạ. Mình chỉ làm dc 1 input 1 output. Nhưng đề bài yêu cầu 1 input 2 output (và chỉ dc sử dụng 1 hàm để tính ra cả chiều cao với tuổi).,,,,,
"Mọi người cho em hỏi ạ:
Em mới hoc ML, em đang muốn tìm hệ số K và T của phương trình sau khi biết các điểm (x,y) thì sẽ xử lý bài toán như thế nào ạ
Tks mọi người","Mọi người cho em hỏi ạ: Em mới hoc ML, em đang muốn tìm hệ số K và T của phương trình sau khi biết các điểm (x,y) thì sẽ xử lý bài toán như thế nào ạ Tks mọi người",,,,,
"Anh chị có ai đã tìm hiểu về ""diffusion"" so với knn như ảnh dưới rồi có thể giải thích giùm em ý tưởng thực hiện của nó được không ạ. Em cảm ơn
https://arxiv.org/pdf/1811.10907.pdf","Anh chị có ai đã tìm hiểu về ""diffusion"" so với knn như ảnh dưới rồi có thể giải thích giùm em ý tưởng thực hiện của nó được không ạ. Em cảm ơn https://arxiv.org/pdf/1811.10907.pdf",,,,,
"Mọi người có thể tư vấn cho mình đang làm project face_recognition
Mình nhận dc dataset có nhiều subfolder, mỗi cái là 43 tấm hình của một người độ phân giải cao và nhiều góc mặt khác nhau
....
Project này thực sự mở và mình chưa từng code kiểu dùng opencv hay đại loại như vậy bao giờ nên rất là khó hiểu
Tìm trên mạng thì nó cứ webcam thui à. Còn mình thử sửa webcam thành đọc file ảnh nhưng ví dụ trên mạnh sử dụng face_recognition library mình truyền vào báo lỗi tè le... nghiên cứu cả tuần nay hầu như bế tắt
Ban đầu kế hoạch của mình là :
Trong đống dataset đó chọn 1 vài hình để test còn lại sẽ lấy train, dùng KNN để classifier
Hiện tại thì trôi dạt bến bờ nào luôn rồi bởi vì kế hoạch là một chuyện mà đụng vào code lại ko dc như ý ...
Mọi ngừoi có ai từng làm hoặc có source nào hưu ích chia sẽ mình với nha
Mình biết project này research là chính nhưng nếu code ko chạy thì làm sao nâng cấp lên đây hjx hjx","Mọi người có thể tư vấn cho mình đang làm project face_recognition Mình nhận dc dataset có nhiều subfolder, mỗi cái là 43 tấm hình của một người độ phân giải cao và nhiều góc mặt khác nhau .... Project này thực sự mở và mình chưa từng code kiểu dùng opencv hay đại loại như vậy bao giờ nên rất là khó hiểu Tìm trên mạng thì nó cứ webcam thui à. Còn mình thử sửa webcam thành đọc file ảnh nhưng ví dụ trên mạnh sử dụng face_recognition library mình truyền vào báo lỗi tè le... nghiên cứu cả tuần nay hầu như bế tắt Ban đầu kế hoạch của mình là : Trong đống dataset đó chọn 1 vài hình để test còn lại sẽ lấy train, dùng KNN để classifier Hiện tại thì trôi dạt bến bờ nào luôn rồi bởi vì kế hoạch là một chuyện mà đụng vào code lại ko dc như ý ... Mọi ngừoi có ai từng làm hoặc có source nào hưu ích chia sẽ mình với nha Mình biết project này research là chính nhưng nếu code ko chạy thì làm sao nâng cấp lên đây hjx hjx",,,,,
"Em chào mọi người ạ, hiện em đang tìm hiểu về Domain Adaptation/Domain Generalization từ những bước đầu tiên (định nghĩa, lí thuyết, ứng dụng,...). Mn ai đã từng nghiên cứu về phần này có thể cho em xin tài liệu hoặc nơi có thể tìm kiếm tài liệu đc k ạ? Vì e tìm trên internet về nó thì kết quả ra khá mơ hồ. E cảm ơn mn ạ!","Em chào mọi người ạ, hiện em đang tìm hiểu về Domain Adaptation/Domain Generalization từ những bước đầu tiên (định nghĩa, lí thuyết, ứng dụng,...). Mn ai đã từng nghiên cứu về phần này có thể cho em xin tài liệu hoặc nơi có thể tìm kiếm tài liệu đc k ạ? Vì e tìm trên internet về nó thì kết quả ra khá mơ hồ. E cảm ơn mn ạ!",,,,,
All backbones in one package,All backbones in one package,,,,,
,nan,,,,,
"Xin chào mọi người, không biết trong đây có ai có bộ dữ liệu VSLP 2016 hay 2018 cho NER không ạ ? Nếu có bạn có thể chia sẻ cho mình không ? Mình cảm ơn ạ.","Xin chào mọi người, không biết trong đây có ai có bộ dữ liệu VSLP 2016 hay 2018 cho NER không ạ ? Nếu có bạn có thể chia sẻ cho mình không ? Mình cảm ơn ạ.",,,,,
"Do được một số bạn, anh chị trong gruop khuyến khích chia sẻ, mình xin đại diện cho team Train4Ever mình xin được tóm tắt solution đứng vị trí thứ 6 trong cuộc thi RSNA-MICCAI Brain Tumor Radiogenomic Classification. Mặc dù không phủ nhận yếu tố may mắn trong các cuộc thi shake up lớn như này, tuy nhiên bọn mình cũng đã chuẩn bị trước và cố gắng tìm ra giải pháp cân bằng nhất giữa validation score và public score, thực tế 2 submission được lựa chọn là 2 submission có kết quả private tốt nhất. Hi vọng có thể học hỏi từ những góp ý của các bạn.
Bài toán: Phân loại xem khối u não có chứa một đoạn gene gọi là MGMT promoter methylation hay không, data sử dụng là ảnh cộng hưởng từ MRI. Mục tiêu là có thể đưa ra chuẩn đoán mà không cần thực hiện giải phẫu (invasive diagnose).
Giải pháp của nhóm gồm 2 stage training và infer. Các model đều nhận vào input là ảnh 2D (bài toán này có thể tiếp cận theo hướng 3D).
Stage 1 train model segmentation trên bộ dữ liệu của track Segmentation khối u (track Segmentation được tổ chức song song của BTC, track trên Kaggle là classification). Model stage 1 đưa ra mask đánh dấu vị trí khối u. Điều này là quan trọng để giúp model focus vào những vị trí quan trọng (khối u) để giảm bias vào nhưng feature khác đối với bài toán ít dữ liệu như vậy.
Stage 2 train sequence model LSTM với input là 1 chuỗi các ảnh 3 kênh (1 kênh ảnh gốc và 2 kênh còn lại là 2 loại mask do model segmentation dự đoán ra). Chuỗi ảnh input tuần tự theo vị trí khi scan não bộ. Model sử dụng backbone CNN extract các embedding sau đó đưa các time step vào Bidirectional LSTM rồi đến lớp classification.
Chi tiết solution và kết quả có thể xem trong:
Github: https://github.com/gallegi/T4E_MICCAI_BrainTumor
Kaggle discussion: https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/discussion/280402
Nhật Trường Bùi Khánh Vũ Duy Tuyen Dam","Do được một số bạn, anh chị trong gruop khuyến khích chia sẻ, mình xin đại diện cho team Train4Ever mình xin được tóm tắt solution đứng vị trí thứ 6 trong cuộc thi RSNA-MICCAI Brain Tumor Radiogenomic Classification. Mặc dù không phủ nhận yếu tố may mắn trong các cuộc thi shake up lớn như này, tuy nhiên bọn mình cũng đã chuẩn bị trước và cố gắng tìm ra giải pháp cân bằng nhất giữa validation score và public score, thực tế 2 submission được lựa chọn là 2 submission có kết quả private tốt nhất. Hi vọng có thể học hỏi từ những góp ý của các bạn. Bài toán: Phân loại xem khối u não có chứa một đoạn gene gọi là MGMT promoter methylation hay không, data sử dụng là ảnh cộng hưởng từ MRI. Mục tiêu là có thể đưa ra chuẩn đoán mà không cần thực hiện giải phẫu (invasive diagnose). Giải pháp của nhóm gồm 2 stage training và infer. Các model đều nhận vào input là ảnh 2D (bài toán này có thể tiếp cận theo hướng 3D). Stage 1 train model segmentation trên bộ dữ liệu của track Segmentation khối u (track Segmentation được tổ chức song song của BTC, track trên Kaggle là classification). Model stage 1 đưa ra mask đánh dấu vị trí khối u. Điều này là quan trọng để giúp model focus vào những vị trí quan trọng (khối u) để giảm bias vào nhưng feature khác đối với bài toán ít dữ liệu như vậy. Stage 2 train sequence model LSTM với input là 1 chuỗi các ảnh 3 kênh (1 kênh ảnh gốc và 2 kênh còn lại là 2 loại mask do model segmentation dự đoán ra). Chuỗi ảnh input tuần tự theo vị trí khi scan não bộ. Model sử dụng backbone CNN extract các embedding sau đó đưa các time step vào Bidirectional LSTM rồi đến lớp classification. Chi tiết solution và kết quả có thể xem trong: Github: https://github.com/gallegi/T4E_MICCAI_BrainTumor Kaggle discussion: https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/discussion/280402 Nhật Trường Bùi Khánh Vũ Duy Tuyen Dam",,,,,
"Huấn luyện models với low numberical Precision giúp không những duy trì được hiệu năng của mô hình mà còn giúp GIẢM:
a) kích cỡ mô hình
b) dung lượng bộ nhớ VRAM trên GPU hay TPU
c) năng lượng tiêu thụ
Chi tiết xem trong đường dẫn bên dưới
✨ Low Numerical Precision in PyTorch ✨
Most DL models are single-precision floats by default.
Lower numerical precision - while reasonably maintaining accuracy - reduces:
a) model size
b) memory required
c) power consumed",Huấn luyện models với low numberical Precision giúp không những duy trì được hiệu năng của mô hình mà còn giúp GIẢM: a) kích cỡ mô hình b) dung lượng bộ nhớ VRAM trên GPU hay TPU c) năng lượng tiêu thụ Chi tiết xem trong đường dẫn bên dưới Low Numerical Precision in PyTorch Most DL models are single-precision floats by default. Lower numerical precision - while reasonably maintaining accuracy - reduces: a) model size b) memory required c) power consumed,,,,,
"cho em hỏi với ạ, em có lưu con model của em về dang checkpoint nhưng h em muốn móc nó ra để xài thì em dùng hàm gì ạ :'( tutorial trên mạng toàn saveweight với loadweight nhưng em ko xài đc ạ","cho em hỏi với ạ, em có lưu con model của em về dang checkpoint nhưng h em muốn móc nó ra để xài thì em dùng hàm gì ạ :'( tutorial trên mạng toàn saveweight với loadweight nhưng em ko xài đc ạ",,,,,
"Cho em hỏi về False Positive ạ. Em đang làm về template matching. Nếu khi thực hiện matching với kết quả 0<IOU<Threshold thì đó là FP phải không ạ. Nhưng trường hợp nó matching một wrong object, tức IOU = 0 thì có phải cũng tính là 1 FP không ạ? Nếu đúng thế thì Precision của nó chính bằng phần trăm successful matching với một threshold cho trước đúng không ạ?","Cho em hỏi về False Positive ạ. Em đang làm về template matching. Nếu khi thực hiện matching với kết quả 0<IOU<Threshold thì đó là FP phải không ạ. Nhưng trường hợp nó matching một wrong object, tức IOU = 0 thì có phải cũng tính là 1 FP không ạ? Nếu đúng thế thì Precision của nó chính bằng phần trăm successful matching với một threshold cho trước đúng không ạ?",,,,,
Chúc mừng ngày phụ nữ Việt Nam 20/10,Chúc mừng ngày phụ nữ Việt Nam 20/10,,,,,
"Chào mọi người. Em đang có một đề tài ML về hành vi bất thường của người trong nhà, từ đó xác định tai nạn. Cho em hỏi là có ai có tập data, có thể cho em tham khảo được không ạ, hoặc em có thể kiếm ở đâu? Em cảm ơn ạ","Chào mọi người. Em đang có một đề tài ML về hành vi bất thường của người trong nhà, từ đó xác định tai nạn. Cho em hỏi là có ai có tập data, có thể cho em tham khảo được không ạ, hoặc em có thể kiếm ở đâu? Em cảm ơn ạ",,,,,
"Chào mọi người,
Em đang làm đề tài về khai phá dữ liệu trong thương mại điện tử. Em đang cần tìm tập dữ liệu bao gồm thông tin hành vi khách hàng (click xem sản phẩm, thêm giỏ hàng, mua,...) và đánh giá của họ về sản phẩm.
Hiên tại thì em chỉ tìm được tập dữ liệu riêng về 2 loại thông tin trên. Mọi người có biết tập dữ liệu nào có cả 2 thông tin trên thì cho em xin với ạ! Em có tìm trên nhiều nguồn nhưng chưa thấy.
Em xin cảm ơn!","Chào mọi người, Em đang làm đề tài về khai phá dữ liệu trong thương mại điện tử. Em đang cần tìm tập dữ liệu bao gồm thông tin hành vi khách hàng (click xem sản phẩm, thêm giỏ hàng, mua,...) và đánh giá của họ về sản phẩm. Hiên tại thì em chỉ tìm được tập dữ liệu riêng về 2 loại thông tin trên. Mọi người có biết tập dữ liệu nào có cả 2 thông tin trên thì cho em xin với ạ! Em có tìm trên nhiều nguồn nhưng chưa thấy. Em xin cảm ơn!",,,,,
"Anh chị cho em hỏi vấn đề này với ạ. Em đang làm bài tập Face Regconite, em dùng network GoogleNet, sao khi detect face, em cắt hình để làm input cho model googlenet, xong em output ra class (tên người) và chèn chữ {tên người} vào rectangle, nhưng nếu là người lạ chưa được train thì module ra output sai ạ. Cho em hỏi có cách nào khi bắt ảnh người mà chưa được train thì model cho ra class ""unknown"" không ạ, em xài Dense - softmax.
Em cảm ơn ạ.","Anh chị cho em hỏi vấn đề này với ạ. Em đang làm bài tập Face Regconite, em dùng network GoogleNet, sao khi detect face, em cắt hình để làm input cho model googlenet, xong em output ra class (tên người) và chèn chữ {tên người} vào rectangle, nhưng nếu là người lạ chưa được train thì module ra output sai ạ. Cho em hỏi có cách nào khi bắt ảnh người mà chưa được train thì model cho ra class ""unknown"" không ạ, em xài Dense - softmax. Em cảm ơn ạ.",,,,,
,nan,,,,,
"Mn có ai đã từng tìm hiểu về query expansion trong bài toán information retrieval cụ thể là ""alpha weighted query expansion"" . thì có thể tóm lược cách thực hiện giúp e với đk ko ạ. E đọc vào paper nhưng không hiểu lắm ạ . em cảm ơn mn","Mn có ai đã từng tìm hiểu về query expansion trong bài toán information retrieval cụ thể là ""alpha weighted query expansion"" . thì có thể tóm lược cách thực hiện giúp e với đk ko ạ. E đọc vào paper nhưng không hiểu lắm ạ . em cảm ơn mn",,,,,
"Em chào mọi người ạ !
Em mới được tiếp cận với Machine Learning.
Và em đang học bản mềm của cuốn Machine Learning của tác giả Vũ Hữu Tiệp ạ.
Thì em có thấy chương đầu của có nói về các kiến thức toán cần thiết.
Em cũng hơi thắc mắc ạ. Vì em nghĩ làm được bài chưa chắc đã hiểu vấn đề đó là gì.
Vì giả sử nếu để làm bài để thi thì em nên mạng học đi học lại cái thuật toán người ta hay làm rồi làm theo là được ạ. Vậy mọi người cho em hỏi là học toán như nào là đúng để có thể tiếp cận được với Machine Learning ạ? Em cảm ơn !",Em chào mọi người ạ ! Em mới được tiếp cận với Machine Learning. Và em đang học bản mềm của cuốn Machine Learning của tác giả Vũ Hữu Tiệp ạ. Thì em có thấy chương đầu của có nói về các kiến thức toán cần thiết. Em cũng hơi thắc mắc ạ. Vì em nghĩ làm được bài chưa chắc đã hiểu vấn đề đó là gì. Vì giả sử nếu để làm bài để thi thì em nên mạng học đi học lại cái thuật toán người ta hay làm rồi làm theo là được ạ. Vậy mọi người cho em hỏi là học toán như nào là đúng để có thể tiếp cận được với Machine Learning ạ? Em cảm ơn !,,,,,
"Chúc mừng 2 đội Việt Nam đạt thứ hạng cao (hạng mục nhận tiền thưởng) trong cuộc thi ""RSNA-MICCAI Brain Tumor Radiogenomic Classification"". Bài thi này là môt minh chứng cho hiện tượng Overfitting nặng. Đây là lần thứ 2 mình chứng kiến hiện tượng này sau cuộc thi phân loại Casava diseases. Các bạn có thể xem hinh bên dưới, bạn Minh Phan đã leo 595 bậc từ public leaderboard so với private leaderboard. Tương tự nhóm Train4Ever cũng leo 429 bậc. Người về nhất trong mục private leaderboard còn leo >1000 bậc. Tiết lộ thêm, mình cũng thử bài này, kết quả trên validation set của mình đạt ~0.69, nên thấy có vẻ không ăn thua nên từ bỏ, không submit kết quả! Mình nghĩ sẽ có nhiều người rất bất ngờ khi nhận giải ở cuộc thi này","Chúc mừng 2 đội Việt Nam đạt thứ hạng cao (hạng mục nhận tiền thưởng) trong cuộc thi ""RSNA-MICCAI Brain Tumor Radiogenomic Classification"". Bài thi này là môt minh chứng cho hiện tượng Overfitting nặng. Đây là lần thứ 2 mình chứng kiến hiện tượng này sau cuộc thi phân loại Casava diseases. Các bạn có thể xem hinh bên dưới, bạn Minh Phan đã leo 595 bậc từ public leaderboard so với private leaderboard. Tương tự nhóm Train4Ever cũng leo 429 bậc. Người về nhất trong mục private leaderboard còn leo >1000 bậc. Tiết lộ thêm, mình cũng thử bài này, kết quả trên validation set của mình đạt ~0.69, nên thấy có vẻ không ăn thua nên từ bỏ, không submit kết quả! Mình nghĩ sẽ có nhiều người rất bất ngờ khi nhận giải ở cuộc thi này",,,,,
"Mình chưa biết gì về machine learning. Hiện tại mình đang dùng api google vision nhận diện text từ hình ảnh chụp và so sánh text này với DB giống nhau thì trả về kết quả.
Bây giờ mình muốn áp dụng machine learning vào thì tìm hiểu và học như nào ạ. Mình có xin account của Aws từ công ty.",Mình chưa biết gì về machine learning. Hiện tại mình đang dùng api google vision nhận diện text từ hình ảnh chụp và so sánh text này với DB giống nhau thì trả về kết quả. Bây giờ mình muốn áp dụng machine learning vào thì tìm hiểu và học như nào ạ. Mình có xin account của Aws từ công ty.,,,,,
"Chào mọi người ạ, mình có 2 thắc mắc này muốn nhờ giải đáp:
1/ Dùng kiến trúc CNN nhận đầu vào là vector tf-idf?
Thực ra không hẳn là tf-idf. 2 paper 'peptide frequency' và 'deeph-dta' thiết kế một biểu diễn cho dữ liệu chuỗi amino acid, gọi là peptide frequency of word frequency, nhưng nó cũng tương đồng với tf-idf thôi. Và 'peptide frequency' đề xuất dùng kiến trúc 1D-CNN để học một embedding cho đối tượng protein mà chuỗi amino acid đó biểu diễn, còn 'deeph-dta' thì dùng DenseNet. Nhưng mình là tại sao lại dùng kiến trúc CNN cho dữ liệu kiểu tf-idf, vì biểu diễn tf-idf đã làm mất đi quan hệ lân cận của các từ rồi?
2/ Dùng kiến trúc ConvLSTM cho đầu vào là chuỗi vector?
'deeph-dta' xử lý dữ liệu chuỗi SMILES (1 kiểu chuỗi biểu diễn cho phân tử hoá học) theo kiểu: cho chuỗi đi qua lớp embedding trước khi vào các lớp ConvLSTM. Điều mình không hiểu là: sau khi qua lớp embedding, mỗi chuỗi ký tự sẽ tương đương với 1 chuỗi vector (vai trò như một feature vector cho một đối tượng từ của chuỗi); còn ConvLSTM, nếu là phiên bản ít chiều nhất, ConvLSTM1D thì đầu vào của nó có hình dạng (batch, length, row, channel). Vậy nếu cho chuỗi embedding vào ConvLSTM thì hoặc phải coi length=1 (thế thì dùng kiến trúc lai RNN là vô nghĩa) hoặc phải coi channel=1 (không hợp lý khi lớp embedding trước đó đã embed mỗi từ thành 1 vector nhiều chiều) hoặc phải coi row=1 (cũng không hợp lý, giống như dùng CNN cho ảnh luôn cao 1 pixel hay rộng 1 pixel vậy).
Link 'peptide frequency': https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7147459/
'deeph-dta':https://ieeexplore.ieee.org/document/9197589
Mình cảm ơn mọi người ạ.","Chào mọi người ạ, mình có 2 thắc mắc này muốn nhờ giải đáp: 1/ Dùng kiến trúc CNN nhận đầu vào là vector tf-idf? Thực ra không hẳn là tf-idf. 2 paper 'peptide frequency' và 'deeph-dta' thiết kế một biểu diễn cho dữ liệu chuỗi amino acid, gọi là peptide frequency of word frequency, nhưng nó cũng tương đồng với tf-idf thôi. Và 'peptide frequency' đề xuất dùng kiến trúc 1D-CNN để học một embedding cho đối tượng protein mà chuỗi amino acid đó biểu diễn, còn 'deeph-dta' thì dùng DenseNet. Nhưng mình là tại sao lại dùng kiến trúc CNN cho dữ liệu kiểu tf-idf, vì biểu diễn tf-idf đã làm mất đi quan hệ lân cận của các từ rồi? 2/ Dùng kiến trúc ConvLSTM cho đầu vào là chuỗi vector? 'deeph-dta' xử lý dữ liệu chuỗi SMILES (1 kiểu chuỗi biểu diễn cho phân tử hoá học) theo kiểu: cho chuỗi đi qua lớp embedding trước khi vào các lớp ConvLSTM. Điều mình không hiểu là: sau khi qua lớp embedding, mỗi chuỗi ký tự sẽ tương đương với 1 chuỗi vector (vai trò như một feature vector cho một đối tượng từ của chuỗi); còn ConvLSTM, nếu là phiên bản ít chiều nhất, ConvLSTM1D thì đầu vào của nó có hình dạng (batch, length, row, channel). Vậy nếu cho chuỗi embedding vào ConvLSTM thì hoặc phải coi length=1 (thế thì dùng kiến trúc lai RNN là vô nghĩa) hoặc phải coi channel=1 (không hợp lý khi lớp embedding trước đó đã embed mỗi từ thành 1 vector nhiều chiều) hoặc phải coi row=1 (cũng không hợp lý, giống như dùng CNN cho ảnh luôn cao 1 pixel hay rộng 1 pixel vậy). Link 'peptide frequency': https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7147459/ 'deeph-dta':https://ieeexplore.ieee.org/document/9197589 Mình cảm ơn mọi người ạ.",,,,,
,nan,,,,,
"[AI Share – Distribution is all you need]
Nếu như trong Deep learning nói chung, hay NLP, CV nói riêng thì “Attention is all you need” . Vậy trong toán học thì sao ? Câu trả lời là “Distribution is all You Need”. Thật ra câu trả lời chỉ nói lên các phân phối là một phần rất quan trọng và không thể thiếu trong toán, chứ để học tốt toán còn cần học nhiều thứ nữa 😀. “Distribution is all You Need” là một hướng dẫn các phân phối xác suất cơ bản và hay được dùng nhiều nhất cho những người nghiên cứu về Deep Learning.
Ảnh đầu tiên là overview về các phân phối và mối quan hệ giữa các phân phối với nhau, các ảnh sau là ảnh minh họa từng phân phối).","[AI Share – Distribution is all you need] Nếu như trong Deep learning nói chung, hay NLP, CV nói riêng thì “Attention is all you need” . Vậy trong toán học thì sao ? Câu trả lời là “Distribution is all You Need”. Thật ra câu trả lời chỉ nói lên các phân phối là một phần rất quan trọng và không thể thiếu trong toán, chứ để học tốt toán còn cần học nhiều thứ nữa . “Distribution is all You Need” là một hướng dẫn các phân phối xác suất cơ bản và hay được dùng nhiều nhất cho những người nghiên cứu về Deep Learning. Ảnh đầu tiên là overview về các phân phối và mối quan hệ giữa các phân phối với nhau, các ảnh sau là ảnh minh họa từng phân phối).",,,,,
"Em chào các anh ạ, em hiện đang làm 1 bài tập về code nhận diện biển báo giao thông, em cũng tham khảo trên mạng và copy chắp vá đủ kiểu và hôm trước code chạy ngon các kiểu nhưng giờ sau 3 tuần em vào lại thì bỗng có lỗi như này em tìm cả ngày nay trên google mà không có cách giải quyết, mà hạn bài tập tới nơi rồi ạ,
các anh các chú xem giúp em với ạ, model train thì chạy ook rồi nay em làm file test thì tự nhiên bị lỗi thế này
file ở đây ạ, các anh giúp em với ạ em cảm ơn nhiều ạ
Dạ em xin lỗi không trình bày rõ ràng, em là dẫn học bên oto đang làm nckh không phải dân dev nên kiến thức về cái này khá ngu ngơ.
em cảm ơn các anh đã góp ý ạ
https://drive.google.com/file/d/1RxNm5bK5RgEJuw2OMLJV8zwO0HTGjEQ4/view?usp=sharing","Em chào các anh ạ, em hiện đang làm 1 bài tập về code nhận diện biển báo giao thông, em cũng tham khảo trên mạng và copy chắp vá đủ kiểu và hôm trước code chạy ngon các kiểu nhưng giờ sau 3 tuần em vào lại thì bỗng có lỗi như này em tìm cả ngày nay trên google mà không có cách giải quyết, mà hạn bài tập tới nơi rồi ạ, các anh các chú xem giúp em với ạ, model train thì chạy ook rồi nay em làm file test thì tự nhiên bị lỗi thế này file ở đây ạ, các anh giúp em với ạ em cảm ơn nhiều ạ Dạ em xin lỗi không trình bày rõ ràng, em là dẫn học bên oto đang làm nckh không phải dân dev nên kiến thức về cái này khá ngu ngơ. em cảm ơn các anh đã góp ý ạ https://drive.google.com/file/d/1RxNm5bK5RgEJuw2OMLJV8zwO0HTGjEQ4/view?usp=sharing",,,,,
"CHINH PHỤC ĐỈNH CAO DỮ LIỆU CÙNG DATA-CENTRIC AI COMPETITION   

Với tổng giá trị giải thưởng lên đến 500,000,000 VND - Data-Centric AI Competition là cuộc thi đầu tiên tại Việt Nam tập trung vào việc xử lý dữ liệu và lấy dữ liệu làm trung tâm. 

👉Đăng ký ngay: https://datacomp.io/ 
⏰ Thời gian đăng ký và nộp dữ liệu: 15/10/2021 - 22/12/2021
 Một cuộc thi hoàn toàn mới mẻ, mang tính đột phát và khác biệt so với tất cả các cuộc thi model-centric trước đây, hứa hẹn sẽ là sân chơi đem đến những trải nghiệm chưa từng có dành cho các thí sinh đam mê ngành Dữ liệu
Lịch trình cuộc thi: 
⏰ Thời gian công bố dữ liệu & mã nguồn: 22/10/2021
⏰ Chấm điểm: 23/12/2021 - 27/12/2021
⏰ Trao giải: 29/12/2021
DATACOMP - JOIN TO TAKE AN AMBITION TO THE TOP
---------
🌐Website: http://datacomp.io/ 
🎯Group cuộc thi: https://bit.ly/datacomp-group 
📩Email: support.ailab@fsoft.com.vn 
#DataCentricAICompetition #Datacomp2021 #FPTSoftware #FSOFTAILab #iRender","CHINH PHỤC ĐỈNH CAO DỮ LIỆU CÙNG DATA-CENTRIC AI COMPETITION Với tổng giá trị giải thưởng lên đến 500,000,000 VND - Data-Centric AI Competition là cuộc thi đầu tiên tại Việt Nam tập trung vào việc xử lý dữ liệu và lấy dữ liệu làm trung tâm. Đăng ký ngay: https://datacomp.io/ ⏰ Thời gian đăng ký và nộp dữ liệu: 15/10/2021 - 22/12/2021 Một cuộc thi hoàn toàn mới mẻ, mang tính đột phát và khác biệt so với tất cả các cuộc thi model-centric trước đây, hứa hẹn sẽ là sân chơi đem đến những trải nghiệm chưa từng có dành cho các thí sinh đam mê ngành Dữ liệu Lịch trình cuộc thi: ⏰ Thời gian công bố dữ liệu & mã nguồn: 22/10/2021 ⏰ Chấm điểm: 23/12/2021 - 27/12/2021 ⏰ Trao giải: 29/12/2021 DATACOMP - JOIN TO TAKE AN AMBITION TO THE TOP --------- Website: http://datacomp.io/ Group cuộc thi: https://bit.ly/datacomp-group Email: support.ailab@fsoft.com.vn",#DataCentricAICompetition	#Datacomp2021	#FPTSoftware	#FSOFTAILab	#iRender,,,,
"Các bạn cho mình hỏi, có người nó học phải học theo lộ trình ML => AI => Deep Learning. Và phải tốn cỡ 100 triệu cho 1 sever. Nghe hoang mang quá. Các bạn ai có kinh nghiệm chỉ mình với, Mình đội ơn","Các bạn cho mình hỏi, có người nó học phải học theo lộ trình ML => AI => Deep Learning. Và phải tốn cỡ 100 triệu cho 1 sever. Nghe hoang mang quá. Các bạn ai có kinh nghiệm chỉ mình với, Mình đội ơn",,,,,
Em chào các anh trong group. Em mới bắt đầu với ML ạ. Em có đọc trong sách của anh Vũ Hữu Tiệp có phần Gradient của trace(AX) như mục 2.4.6 trong hình và thấy tổng chỗ 2.15 hình như có lỗi nhỏ theo em khai triển ra thì nó như ct bên phải. Không biết do em biến đổi sai hay do sách bị đánh nhầm ạ. Mong các anh xem giúp em.,Em chào các anh trong group. Em mới bắt đầu với ML ạ. Em có đọc trong sách của anh Vũ Hữu Tiệp có phần Gradient của trace(AX) như mục 2.4.6 trong hình và thấy tổng chỗ 2.15 hình như có lỗi nhỏ theo em khai triển ra thì nó như ct bên phải. Không biết do em biến đổi sai hay do sách bị đánh nhầm ạ. Mong các anh xem giúp em.,,,,,
"Chào mọi người, em có một thắc mắc như sau mong mọi người giúp đỡ.
Hiện tại em đang muốn train một mô hình phân loại với loss function được define như hình bên dưới. Mục tiêu là bên cạnh việc minimize Cross-Entropy function, em còn muốn kiểm soát sự thay đổi trọng số của mô hình. Cụ thể là không muốn trọng số được update tại epoch hiện tại thay đổi quá nhiều so với trọng số tại epoch trước đó.
Em đang mắc kẹt ở chổ define L2-norm của ""w"" và ""w_t"" (chổ khoanh đỏ) trong Tensorflow. Hi vọng được mọi người trong group giúp đỡ. Em xin cảm ơn ạ.","Chào mọi người, em có một thắc mắc như sau mong mọi người giúp đỡ. Hiện tại em đang muốn train một mô hình phân loại với loss function được define như hình bên dưới. Mục tiêu là bên cạnh việc minimize Cross-Entropy function, em còn muốn kiểm soát sự thay đổi trọng số của mô hình. Cụ thể là không muốn trọng số được update tại epoch hiện tại thay đổi quá nhiều so với trọng số tại epoch trước đó. Em đang mắc kẹt ở chổ define L2-norm của ""w"" và ""w_t"" (chổ khoanh đỏ) trong Tensorflow. Hi vọng được mọi người trong group giúp đỡ. Em xin cảm ơn ạ.",,,,,
"[Xác định điểm/Locating Seed Placements/Segmentation]
Chào các bạn,
mình đang làm về đề tài tìm điểm lên mầm cây dựa vào dữ liệu ảnh. Mình hi vọng là nhận được góp ý của các bạn. Mình dùng hình minh hoạ dưới và trình bày vấn đề qua các đặc điểm dưới đây.
1. Hình trong dữ liệu là hình được chụp từ trên xuống (Hình [1] chỉ là minh hoạ về việc cây mầm, hình chụp từ một phía)
2. Mục tiêu: tính các điểm mầm cây
3. Vấn đề đang gặp phải (hình [2]): ngọn lá và phần gốc không được phân biệt vào một số trường hợp. Mô hình máy học hiện tại dự đoán điểm ""ngọn lá"" cũng là một điểm mầm cây.
Các bạn đã từng làm đề tài tương tự có thể cho mình một vài từ khoá được không ạ? Các bạn có gợi ý cho mình ý tưởng làm thế nào để phân biệt tốt hơn phần ngọn lá và phần gốc (chỗ thật sự lên mầm cây)?
Mình cảm ơn các bạn rất nhiều! ☘","[Xác định điểm/Locating Seed Placements/Segmentation] Chào các bạn, mình đang làm về đề tài tìm điểm lên mầm cây dựa vào dữ liệu ảnh. Mình hi vọng là nhận được góp ý của các bạn. Mình dùng hình minh hoạ dưới và trình bày vấn đề qua các đặc điểm dưới đây. 1. Hình trong dữ liệu là hình được chụp từ trên xuống (Hình [1] chỉ là minh hoạ về việc cây mầm, hình chụp từ một phía) 2. Mục tiêu: tính các điểm mầm cây 3. Vấn đề đang gặp phải (hình [2]): ngọn lá và phần gốc không được phân biệt vào một số trường hợp. Mô hình máy học hiện tại dự đoán điểm ""ngọn lá"" cũng là một điểm mầm cây. Các bạn đã từng làm đề tài tương tự có thể cho mình một vài từ khoá được không ạ? Các bạn có gợi ý cho mình ý tưởng làm thế nào để phân biệt tốt hơn phần ngọn lá và phần gốc (chỗ thật sự lên mầm cây)? Mình cảm ơn các bạn rất nhiều!",,,,,
,nan,,,,,
"[AI News - State of AI Report 2021]
Chúng ta đã thấy AI ngày càng trở nên quan trọng để tạo ra những đột phá mới trong mọi thứ. “State of AI Report” năm nay đã là năm thứ 4, được thực hiện bởi Nathan Benaich và Ian Hogarth, phân tích những diễn biến đáng chú ý nhất trong AI, đặc biệt là những đột phá trong NLP, CV và Sinh học trong một năm qua.
Báo cáo năm nay đặc biệt xem xét sự nổi bật của Transformer từ lĩnh vực NLP, CV, thậm chí cả dự đoán cấu trúc protein trong Sinh học. Báo cáo cũng làm sáng tỏ một thời điểm quan trọng trong lĩnh vực sinh học, khi mà các phương pháp tiếp cận AI đầu tiên đã đưa sinh học đi như vũ bão, có tiềm năng thay đổi trong khám phá thuốc và chăm sóc sức khỏe.","[AI News - State of AI Report 2021] Chúng ta đã thấy AI ngày càng trở nên quan trọng để tạo ra những đột phá mới trong mọi thứ. “State of AI Report” năm nay đã là năm thứ 4, được thực hiện bởi Nathan Benaich và Ian Hogarth, phân tích những diễn biến đáng chú ý nhất trong AI, đặc biệt là những đột phá trong NLP, CV và Sinh học trong một năm qua. Báo cáo năm nay đặc biệt xem xét sự nổi bật của Transformer từ lĩnh vực NLP, CV, thậm chí cả dự đoán cấu trúc protein trong Sinh học. Báo cáo cũng làm sáng tỏ một thời điểm quan trọng trong lĩnh vực sinh học, khi mà các phương pháp tiếp cận AI đầu tiên đã đưa sinh học đi như vũ bão, có tiềm năng thay đổi trong khám phá thuốc và chăm sóc sức khỏe.",,,,,
Mọi người cho em hỏi có cách nào triển khai thuật toán Levenberg-Marquardt (LM) để training NN model bằng pytorch không ạ ? Em thấy torch không có hỗ trợ optimizer này. Em xin cảm ơn ạ,Mọi người cho em hỏi có cách nào triển khai thuật toán Levenberg-Marquardt (LM) để training NN model bằng pytorch không ạ ? Em thấy torch không có hỗ trợ optimizer này. Em xin cảm ơn ạ,,,,,
"Xin chào mọi người, em là newbie tìm hiểu về ML, hiện tại đang học toán ML trong cuốn ML cơ bản phần Gradient Decent cho hàm nhiều biến thì em thấy có 2 vấn đề:
* Tại sao trong hàm myGD lại có grad ? ý nghĩa của nó là gì ạ ?
* Em đã copy lại code trong sách nhưng kết quả hoàn toàn khác so với trong sách và không đúng.
Mong mọi người có thể giúp em hiểu rõ hơn về các vấn đề này, em xin cảm ơn ạ !","Xin chào mọi người, em là newbie tìm hiểu về ML, hiện tại đang học toán ML trong cuốn ML cơ bản phần Gradient Decent cho hàm nhiều biến thì em thấy có 2 vấn đề: * Tại sao trong hàm myGD lại có grad ? ý nghĩa của nó là gì ạ ? * Em đã copy lại code trong sách nhưng kết quả hoàn toàn khác so với trong sách và không đúng. Mong mọi người có thể giúp em hiểu rõ hơn về các vấn đề này, em xin cảm ơn ạ !",,,,,
"mn giúp e với ạ, có cách nào có thể sử dụng model resnet101 trên keras 2.1.2 không ạ. vì version này em import nhưng không được. nó sẽ báo lỗi module 'keras.applications' has no attribute ResNet101 . hoặc có cách nào có thể sử dụng nó vs weight imagenet mà không cần phải cài lại version không ạ. em cảm ơn mn","mn giúp e với ạ, có cách nào có thể sử dụng model resnet101 trên keras 2.1.2 không ạ. vì version này em import nhưng không được. nó sẽ báo lỗi module 'keras.applications' has no attribute ResNet101 . hoặc có cách nào có thể sử dụng nó vs weight imagenet mà không cần phải cài lại version không ạ. em cảm ơn mn",,,,,
Chào các ac trong nhóm.Em đang làm bài về Nhận diện biển báo giao thông sử dụng mạng Neural Convolution CNN.Em chạy 2 file Main và Test.File Main em chạy thành công và lưu dạng h5.còn file Test em bị lỗi dòng 93 mong được ac trong nhóm giúp đỡ ạ,Chào các ac trong nhóm.Em đang làm bài về Nhận diện biển báo giao thông sử dụng mạng Neural Convolution CNN.Em chạy 2 file Main và Test.File Main em chạy thành công và lưu dạng h5.còn file Test em bị lỗi dòng 93 mong được ac trong nhóm giúp đỡ ạ,,,,,
"Giải trí, thư giãn 1 chút với Kaggle và Kagglers. Dữ liệu thu thập vào ngày 4/10/2021 về top-100, top-1000 kagglers và phân bố các top kagglers theo Quốc gia (Việt Nam có 6/100 và 21/1000), nếu theo công ty thì Zalo cũng góp vui với 2 người. Thú vị hơn nữa là so với năm 2016, thì số người Việt tham gia Kaggle gia tăng đáng kể và có thành tích tốt. Tuy nhiên, nếu so với cùng thời gian kể trên, ta còn thua xa Nhật Bản! Hi vọng, trong thời gian tới các DA/DE/DS ở Việt Nam sẽ có tham gia tích cực hơn và có thành tích cao hơn nữa. Dataset các bạn có thể xem ở đây https://www.kaggle.com/hdsk38/comp-top-1000-data
Credit: https://www.kaggle.com/hdsk38/top-1000-users-in-2021-by-country-and-more","Giải trí, thư giãn 1 chút với Kaggle và Kagglers. Dữ liệu thu thập vào ngày 4/10/2021 về top-100, top-1000 kagglers và phân bố các top kagglers theo Quốc gia (Việt Nam có 6/100 và 21/1000), nếu theo công ty thì Zalo cũng góp vui với 2 người. Thú vị hơn nữa là so với năm 2016, thì số người Việt tham gia Kaggle gia tăng đáng kể và có thành tích tốt. Tuy nhiên, nếu so với cùng thời gian kể trên, ta còn thua xa Nhật Bản! Hi vọng, trong thời gian tới các DA/DE/DS ở Việt Nam sẽ có tham gia tích cực hơn và có thành tích cao hơn nữa. Dataset các bạn có thể xem ở đây https://www.kaggle.com/hdsk38/comp-top-1000-data Credit: https://www.kaggle.com/hdsk38/top-1000-users-in-2021-by-country-and-more",,,,,
"xin chào mn, hiện tại em có 1 câu hỏi cơ bản mà em không hiểu nên muốn hỏi mn ạ. Như mn có thể thấy ở hình dưới khi em apply convolution transpose cho tensor ở side (batch, height, width, channels) với padding = same cho conv2dTranspose trong keras thì height và width của image k đổi ạ. Em muốn hỏi là làm sao để padding tương tự như vậy cho torch tensor để khi forward qua layer conv2dTranspose của torch thì height và width của tensor vẫn không đổi ạ","xin chào mn, hiện tại em có 1 câu hỏi cơ bản mà em không hiểu nên muốn hỏi mn ạ. Như mn có thể thấy ở hình dưới khi em apply convolution transpose cho tensor ở side (batch, height, width, channels) với padding = same cho conv2dTranspose trong keras thì height và width của image k đổi ạ. Em muốn hỏi là làm sao để padding tương tự như vậy cho torch tensor để khi forward qua layer conv2dTranspose của torch thì height và width của tensor vẫn không đổi ạ",,,,,
Mọi người ai có tài liệu gì về build mô hình cho xG metrics trong trận bóng đá ko? Mình chỉ web scrape đc stat đã có sẵn như dưới đây. K biết xG được xây dựng và predict như thế nào,Mọi người ai có tài liệu gì về build mô hình cho xG metrics trong trận bóng đá ko? Mình chỉ web scrape đc stat đã có sẵn như dưới đây. K biết xG được xây dựng và predict như thế nào,,,,,
"Xin chào mọi người. Hiện tại em đang tìm hiểu về bài toán Automatic License Plate Recognition (ALPR) cụ thể là mô hình WPOD-NET ở bài báo License Plate Detection and Recognition in Unconstrained Scenarios(https://bit.ly/2X2Q6Fw). Em gặp khó khăn và chưa hiểu lắm về hàm loss của tác giả đưa ra ạ.
Theo em hiểu là hàm loss của tác giả đưa ra bao gồm 2 phần:
- Phần thứ nhất là để tính toán độ sai lệch giữa 4 điểm góc của biển số thực tế và dự đoán thì tác giả đưa ra transformation(hàm T) để chuyển đổi 4 điểm q_i về 4 điểm mới có thể so sánh được với 4 điểm thực tế đã được labeled và để phù hợp với độ phân giải đầu ra của mạng thì 4 điểm thực tế cũng được re-scaled lại qua normalization function(hàm A).
- Phần thứ hai thì em hiểu hàm này giải quyết vấn đề liên quan đến xác suất có/không có đối tượng.
Em chỉ hiểu ý tưởng chính của từng hàm nhưng chưa hiểu kỹ ý nghĩa biểu thức toán và tại sao tác giả có thể đưa ra các biểu thức hàm như vậy. Anh chị nào biết có thể giải thích giúp em hoặc cho em một vài keywords và tài liệu để em tìm hiểu thêm được không ạ. Em cũng chỉ mới bập bẹ bước chân vào mảng này mong được mọi người giúp đỡ ạ :'(
Em cảm ơn.",Xin chào mọi người. Hiện tại em đang tìm hiểu về bài toán Automatic License Plate Recognition (ALPR) cụ thể là mô hình WPOD-NET ở bài báo License Plate Detection and Recognition in Unconstrained Scenarios(https://bit.ly/2X2Q6Fw). Em gặp khó khăn và chưa hiểu lắm về hàm loss của tác giả đưa ra ạ. Theo em hiểu là hàm loss của tác giả đưa ra bao gồm 2 phần: - Phần thứ nhất là để tính toán độ sai lệch giữa 4 điểm góc của biển số thực tế và dự đoán thì tác giả đưa ra transformation(hàm T) để chuyển đổi 4 điểm q_i về 4 điểm mới có thể so sánh được với 4 điểm thực tế đã được labeled và để phù hợp với độ phân giải đầu ra của mạng thì 4 điểm thực tế cũng được re-scaled lại qua normalization function(hàm A). - Phần thứ hai thì em hiểu hàm này giải quyết vấn đề liên quan đến xác suất có/không có đối tượng. Em chỉ hiểu ý tưởng chính của từng hàm nhưng chưa hiểu kỹ ý nghĩa biểu thức toán và tại sao tác giả có thể đưa ra các biểu thức hàm như vậy. Anh chị nào biết có thể giải thích giúp em hoặc cho em một vài keywords và tài liệu để em tìm hiểu thêm được không ạ. Em cũng chỉ mới bập bẹ bước chân vào mảng này mong được mọi người giúp đỡ ạ :'( Em cảm ơn.,,,,,
e chào mn. em là newbie và đang tìm hiểu về truy xuất hình ảnh. mn cho em hỏi ai đã từng implement RMAC theo bài báo này https://arxiv.org/pdf/1511.05879.pdf và ra kết quả độ chính xác gần giống với table 1 trong paper tại L=3 chưa ạ. hoặc có code nào imlement cho ra kết quả gần như vậy ko ạ. vì e dùng các code source sẵn test đều cho ra kết quả thấp hơn rất nhiều (49% so với công bố 66%) . bacbone sử dụng là vgg16 ạ. em cảm ơn ạ.,e chào mn. em là newbie và đang tìm hiểu về truy xuất hình ảnh. mn cho em hỏi ai đã từng implement RMAC theo bài báo này https://arxiv.org/pdf/1511.05879.pdf và ra kết quả độ chính xác gần giống với table 1 trong paper tại L=3 chưa ạ. hoặc có code nào imlement cho ra kết quả gần như vậy ko ạ. vì e dùng các code source sẵn test đều cho ra kết quả thấp hơn rất nhiều (49% so với công bố 66%) . bacbone sử dụng là vgg16 ạ. em cảm ơn ạ.,,,,,
"[Matrix multiplication viewpoint]
Nếu bạn đã học qua toán cao cấp rồi thì chắc bạn không lạ gì phép nhân hai ma trận với quy tắc: ""hàng nhân cột"". Tuy nhiên cách nhân ma trận này chỉ để tính toán thôi, phép nhân ma trận còn có nhiều cách nhìn khác nhau và có các ứng dụng khác nhau. Ví dụ: nhân theo hàng, theo cột, hay cả hàng và cột. Biết và hiểu các cách nhân ma trận khác nhau giúp mọi người hiểu bản chất vấn đề hơn.
1. Nhân dạng cột: Nó liên quan hàng loạt các tính chất về sau của ma trận như độc lập tuyến tính, rank,...
2. Nhân dạng hàng: Chính là biến đổi về dạng Gaussian elimination.
3. Dạng hàng nhân cột: Như khi khai triển Singular Value Decomposition (SVD) và tính ma trận xấp xỉ.","[Matrix multiplication viewpoint] Nếu bạn đã học qua toán cao cấp rồi thì chắc bạn không lạ gì phép nhân hai ma trận với quy tắc: ""hàng nhân cột"". Tuy nhiên cách nhân ma trận này chỉ để tính toán thôi, phép nhân ma trận còn có nhiều cách nhìn khác nhau và có các ứng dụng khác nhau. Ví dụ: nhân theo hàng, theo cột, hay cả hàng và cột. Biết và hiểu các cách nhân ma trận khác nhau giúp mọi người hiểu bản chất vấn đề hơn. 1. Nhân dạng cột: Nó liên quan hàng loạt các tính chất về sau của ma trận như độc lập tuyến tính, rank,... 2. Nhân dạng hàng: Chính là biến đổi về dạng Gaussian elimination. 3. Dạng hàng nhân cột: Như khi khai triển Singular Value Decomposition (SVD) và tính ma trận xấp xỉ.",,,,,
"Xin chào mọi người. Em có 1 đoạn code đơn giản như thế này ( trên ảnh ) , em chạy test trên 2 máy
- máy số 1 : máy ryzen 2600x ( 6 core 12 thread, xung 3.4ghz ) , hệ điều hành ubuntu
-máy số 2 : máy xeon dual 2678v3 ( 24 core 48 thread , xung 2.4ghz . hệ điều hành window 10
Cả 2 máy đều cài thư viện qua pip và pytorch bản chỉ cpu
. Đoạn code trên rất đơn giản ko làm gì phức tạp nhưng khi chạy trên xe máy xeon thì ngốn tận 33% cpu ( 33% cpu của all core tức ngốn tận 8 core cpu ) , trong khi chạy máy ryzen chỉ 1,2% cpu ( gần như không tốn gì ).
Khi em debug thì thấy dòng code img = img.float() là đoạn code chạy ngốn cpu ở con xeon kia.
Ngoài đoạn code test ở trên thì em còn compare giữa time inference của 1 model cực nhỏ, và cũng tình trạng trên ( máy ryzen ngồn cpu cực ít còn xeon cực cao , cpu ngốn gấp vài chục lần so với ryzen )
Nên em xin hỏi mọi người có phải là pytorch ko tối ưu trên máy dual xeon hay bug hay gì ạ... ? và em xin cách khắc phục ạ ?
Em xin cảm ơn .","Xin chào mọi người. Em có 1 đoạn code đơn giản như thế này ( trên ảnh ) , em chạy test trên 2 máy - máy số 1 : máy ryzen 2600x ( 6 core 12 thread, xung 3.4ghz ) , hệ điều hành ubuntu -máy số 2 : máy xeon dual 2678v3 ( 24 core 48 thread , xung 2.4ghz . hệ điều hành window 10 Cả 2 máy đều cài thư viện qua pip và pytorch bản chỉ cpu . Đoạn code trên rất đơn giản ko làm gì phức tạp nhưng khi chạy trên xe máy xeon thì ngốn tận 33% cpu ( 33% cpu của all core tức ngốn tận 8 core cpu ) , trong khi chạy máy ryzen chỉ 1,2% cpu ( gần như không tốn gì ). Khi em debug thì thấy dòng code img = img.float() là đoạn code chạy ngốn cpu ở con xeon kia. Ngoài đoạn code test ở trên thì em còn compare giữa time inference của 1 model cực nhỏ, và cũng tình trạng trên ( máy ryzen ngồn cpu cực ít còn xeon cực cao , cpu ngốn gấp vài chục lần so với ryzen ) Nên em xin hỏi mọi người có phải là pytorch ko tối ưu trên máy dual xeon hay bug hay gì ạ... ? và em xin cách khắc phục ạ ? Em xin cảm ơn .",,,,,
"Chúng ta thường train models với 32-bit optimization, hay mixed 16-bit với 32-bit optimization, hay còn gọi là Half (mixed) precision (với các GPUs dòng 20XX trở lên). Nếu dùng Pytorch-lightning, mình biết có thể set để train chỉ với 16-bit mà thôi. Gần đây có một số nghiên cứu tập trung vào train models với chỉ 8-bit (quantization) optimization mà không làm giảm prediction performance của các models. Bên cạnh đó, lợi ích của việc train models với giúp tiết kiệm tài nguyên tính toán và có thể triển khai các models trên các thiết bị di động. Đây là bài báo về chủ đề này có tên ""8-BIT OPTIMIZERS VIA BLOCK-WISE QUANTIZATION"" tại đây https://arxiv.org/pdf/2110.02861.pdf. Họ cũng chia sẽ source code với thư viện có tên bitsandbytes tại đây https://github.com/facebookresearch/bitsandbytes","Chúng ta thường train models với 32-bit optimization, hay mixed 16-bit với 32-bit optimization, hay còn gọi là Half (mixed) precision (với các GPUs dòng 20XX trở lên). Nếu dùng Pytorch-lightning, mình biết có thể set để train chỉ với 16-bit mà thôi. Gần đây có một số nghiên cứu tập trung vào train models với chỉ 8-bit (quantization) optimization mà không làm giảm prediction performance của các models. Bên cạnh đó, lợi ích của việc train models với giúp tiết kiệm tài nguyên tính toán và có thể triển khai các models trên các thiết bị di động. Đây là bài báo về chủ đề này có tên ""8-BIT OPTIMIZERS VIA BLOCK-WISE QUANTIZATION"" tại đây https://arxiv.org/pdf/2110.02861.pdf. Họ cũng chia sẽ source code với thư viện có tên bitsandbytes tại đây https://github.com/facebookresearch/bitsandbytes",,,,,
"Trong formum, có anh chị nào có khả năng viết code mạnh để training (lưu ý là code mới, chứ không phải dạng bài toán chỉ chạy thư viện rồi ra kết quả training) không? Hiện mình đang phát triển nhiều thuật toán mới về machine learning (thông qua các kỹ thuật sâu của giải tích tối ưu), nhưng mình lại không code được do Mình làm toán cơ bản, và không có thời gian tìm hiểu lập trình. Mình hy vọng các anh chị em có khả năng code mạnh mẽ tham gia hợp tác nghiên cứu, publish paper in high level trong lĩnh vực deep learning.","Trong formum, có anh chị nào có khả năng viết code mạnh để training (lưu ý là code mới, chứ không phải dạng bài toán chỉ chạy thư viện rồi ra kết quả training) không? Hiện mình đang phát triển nhiều thuật toán mới về machine learning (thông qua các kỹ thuật sâu của giải tích tối ưu), nhưng mình lại không code được do Mình làm toán cơ bản, và không có thời gian tìm hiểu lập trình. Mình hy vọng các anh chị em có khả năng code mạnh mẽ tham gia hợp tác nghiên cứu, publish paper in high level trong lĩnh vực deep learning.",,,,,
"Chào mọi ng, e mới bắt đầu làm quen python và machine learning, đang thực hành vidu trên blog của anh Tiệp nhưng đến khúc này đang bị vướng, chưa giải quyết được, mong được mọi ng giúp đỡ ạ
Em cảm ơn nhiều ạ","Chào mọi ng, e mới bắt đầu làm quen python và machine learning, đang thực hành vidu trên blog của anh Tiệp nhưng đến khúc này đang bị vướng, chưa giải quyết được, mong được mọi ng giúp đỡ ạ Em cảm ơn nhiều ạ",,,,,
"mn cho em hỏi tí với ạ
Em muốn sampling cái phân khối dirichlet từ hình bên dưới. Em có sample từ gamma của numpy với size là (1000, 6) cho die (xúc xắc). Sample được Vk rồi thì tính theta_k như bên dưới có đúng ko ạ vì chỗ chia cho tổng đó phải là chia cho tổng 1000 số trong phân khối chứ nhỉ ?
Em cảm ơn ạ","mn cho em hỏi tí với ạ Em muốn sampling cái phân khối dirichlet từ hình bên dưới. Em có sample từ gamma của numpy với size là (1000, 6) cho die (xúc xắc). Sample được Vk rồi thì tính theta_k như bên dưới có đúng ko ạ vì chỗ chia cho tổng đó phải là chia cho tổng 1000 số trong phân khối chứ nhỉ ? Em cảm ơn ạ",,,,,
"Em chào mọi người ạ.
Hiện tại em đang muốn tạo mask (như hình bên dưới ạ). Mục đích để tạo ra training data cho Unet model ạ. Ảnh bên trái là hình ảnh gốc và bên phải là mask của nó ạ , nhưng em chưa biết tạo mask của hình ảnh như nào. Anh chị nào đã làm gán dán nhãn hình ảnh hay tạo mask cho hình ảnh kiểu giống hình bên dưới có thể cho sẻ cho em 1 số kinh nghiệm hoặc công cụ để làm điều đó không ạ?
Em cảm ơn mọi người ạ.","Em chào mọi người ạ. Hiện tại em đang muốn tạo mask (như hình bên dưới ạ). Mục đích để tạo ra training data cho Unet model ạ. Ảnh bên trái là hình ảnh gốc và bên phải là mask của nó ạ , nhưng em chưa biết tạo mask của hình ảnh như nào. Anh chị nào đã làm gán dán nhãn hình ảnh hay tạo mask cho hình ảnh kiểu giống hình bên dưới có thể cho sẻ cho em 1 số kinh nghiệm hoặc công cụ để làm điều đó không ạ? Em cảm ơn mọi người ạ.",,,,,
"Vietnamese sentence embedding using PhoBERT & Sentence Transformers

Xin chào cả nhà,
Em xin được chia sẻ với mọi người một sản phẩm của nhóm em làm về sentence embeddings, theo phương pháp của paper này [1], sử dụng pre-trained model PhoBERT [2].
Phương pháp sentence embeddings này có tính chất: (i) có thể dùng cosine distance để tính độ tương đồng về ngữ nghĩa của các câu; áp dụng để phân nhóm câu theo ngữ nghĩa (semantic clustering), hệ thống truy vấn thông tin theo ngữ nghĩa (information retrieval), ... (ii) có thể sử dụng như pre-trained trong transfer learning.

Source code và chạy thử trên colab tại [3], [4].

Chúc cả nhà một ngày tươi đẹp :D

[1]: https://www.aclweb.org/anthology/D19-1410.pdf
[2]: https://github.com/VinAIResearch/PhoBERT
[3]: https://github.com/Datami555/sentence-transformers
[4]: https://colab.research.google.com/drive/1IHIYxV7COaMr6GrfWWb-I4TwT706scYi?usp=sharing

https://www.youtube.com/watch?v=7d--8leiZgA
 — với Trua Nguyen.","Vietnamese sentence embedding using PhoBERT & Sentence Transformers Xin chào cả nhà, Em xin được chia sẻ với mọi người một sản phẩm của nhóm em làm về sentence embeddings, theo phương pháp của paper này [1], sử dụng pre-trained model PhoBERT [2]. Phương pháp sentence embeddings này có tính chất: (i) có thể dùng cosine distance để tính độ tương đồng về ngữ nghĩa của các câu; áp dụng để phân nhóm câu theo ngữ nghĩa (semantic clustering), hệ thống truy vấn thông tin theo ngữ nghĩa (information retrieval), ... (ii) có thể sử dụng như pre-trained trong transfer learning. Source code và chạy thử trên colab tại [3], [4]. Chúc cả nhà một ngày tươi đẹp :D [1]: https://www.aclweb.org/anthology/D19-1410.pdf [2]: https://github.com/VinAIResearch/PhoBERT [3]: https://github.com/Datami555/sentence-transformers [4]: https://colab.research.google.com/drive/1IHIYxV7COaMr6GrfWWb-I4TwT706scYi?usp=sharing https://www.youtube.com/watch?v=7d--8leiZgA — với Trua Nguyen.",,,,,
"Hello mọi người,
Có bác nào đã làm với tóm tắt đa văn bản chưa ạ, kiểu như: có 5 bài viết khác nhau cùng một chủ đề. Minh sẽ tạo ra 1 đoạn văn bản tổng hợp ý của 5 văn bản trên ạ. E có đọc được tài liệu dưới đây
Thank ace","Hello mọi người, Có bác nào đã làm với tóm tắt đa văn bản chưa ạ, kiểu như: có 5 bài viết khác nhau cùng một chủ đề. Minh sẽ tạo ra 1 đoạn văn bản tổng hợp ý của 5 văn bản trên ạ. E có đọc được tài liệu dưới đây Thank ace",,,,,
"Em chào mọi người, em mới đăng ký colab pro 1 tháng 10$ thử, em thấy một số điểm như sau mà em muốn hỏi mọi người chút ạ, và anh chị em nào dùng colab pro+ cũng cho em hỏi thêm ạ, nếu đáng tiền thì em cũng muốn bỏ tiền ra thử nghiệm cho mô hình em đang quan tâm ạ.
1. Em thấy rằng hình như colab pro không cho phép chạy cùng lúc 2 phiên sử dụng GPU, không rõ là colab pro+ có cho phép điều đó không ạ?
2. Em đang train với batch_size = 256, và 50 epochs, 5 fold cross validation thì đã mất khoảng hơn 6 tiếng, mà em muốn có thể nhanh hơn (vì em cần khoảng đến 15 lần chạy như vậy) thì không rõ colab pro+ có thực sự đáng hơn không, hiện tại phiên mà em train ở phía trên trên colab pro em đang connect tới dùng GPU P100, mà em đọc được là colab pro+ dùng V100 thậm chí A100 không rõ đúng hay không và tốc độ có nhanh hơn hẳn không?
3. Em cũng gặp trục trặc với chuyện disconnect phiên colab, em đã thử dùng JavaScript để chạy đoạn code trên console để liên tục bấm vào vị trí nút connect. Em có thử nghiệm, tắt cửa sổ đó đi, hoặc tắt trình duyệt đi, mở ra thì nó vẫn đang train (như vậy có tính là chạy trong nền không?), nhưng tối qua khi gập máy đi ngủ thì có vẻ nó đã dừng lại, thời gian em ước lượng từ khi bắt đầu train đến khi dừng lại khoảng hơn 6 tiếng. Em không rõ là colab pro+ có chế độ chạy trong nền thì có cần thiết hay không nếu như em tắt trình duyệt hay tắt máy đi, liệu nó vẫn tiếp tục chạy?
Em cảm ơn mọi người đã đọc, đây là 3 câu hỏi khá dài. Em mới làm nên nhiều bỡ ngỡ, cũng đã tra cứu, giờ em muốn tham khảo ý kiến mọi người ạ. Em cảm ơn mọi người nhiều ạ.","Em chào mọi người, em mới đăng ký colab pro 1 tháng 10$ thử, em thấy một số điểm như sau mà em muốn hỏi mọi người chút ạ, và anh chị em nào dùng colab pro+ cũng cho em hỏi thêm ạ, nếu đáng tiền thì em cũng muốn bỏ tiền ra thử nghiệm cho mô hình em đang quan tâm ạ. 1. Em thấy rằng hình như colab pro không cho phép chạy cùng lúc 2 phiên sử dụng GPU, không rõ là colab pro+ có cho phép điều đó không ạ? 2. Em đang train với batch_size = 256, và 50 epochs, 5 fold cross validation thì đã mất khoảng hơn 6 tiếng, mà em muốn có thể nhanh hơn (vì em cần khoảng đến 15 lần chạy như vậy) thì không rõ colab pro+ có thực sự đáng hơn không, hiện tại phiên mà em train ở phía trên trên colab pro em đang connect tới dùng GPU P100, mà em đọc được là colab pro+ dùng V100 thậm chí A100 không rõ đúng hay không và tốc độ có nhanh hơn hẳn không? 3. Em cũng gặp trục trặc với chuyện disconnect phiên colab, em đã thử dùng JavaScript để chạy đoạn code trên console để liên tục bấm vào vị trí nút connect. Em có thử nghiệm, tắt cửa sổ đó đi, hoặc tắt trình duyệt đi, mở ra thì nó vẫn đang train (như vậy có tính là chạy trong nền không?), nhưng tối qua khi gập máy đi ngủ thì có vẻ nó đã dừng lại, thời gian em ước lượng từ khi bắt đầu train đến khi dừng lại khoảng hơn 6 tiếng. Em không rõ là colab pro+ có chế độ chạy trong nền thì có cần thiết hay không nếu như em tắt trình duyệt hay tắt máy đi, liệu nó vẫn tiếp tục chạy? Em cảm ơn mọi người đã đọc, đây là 3 câu hỏi khá dài. Em mới làm nên nhiều bỡ ngỡ, cũng đã tra cứu, giờ em muốn tham khảo ý kiến mọi người ạ. Em cảm ơn mọi người nhiều ạ.",,,,,
"Kính chào cả nhà. Em đang học đến phần gán nhãn dữ liệu nên mạnh dạn làm bài chia sẻ. Hi vọng bài viết sẽ giúp được những bước cơ bản nhất cho các bạn mới học.
Mong admin duyệt bài!",Kính chào cả nhà. Em đang học đến phần gán nhãn dữ liệu nên mạnh dạn làm bài chia sẻ. Hi vọng bài viết sẽ giúp được những bước cơ bản nhất cho các bạn mới học. Mong admin duyệt bài!,,,,,
"#pytorch, Mọi người cho em hỏi là tại sao khi em dùng torch.nn.Sequential để tạo custom layers thì khi mình forward nó lại lỗi ạ : do hiện khi em tạo lại block code tương tự bên ngoài vẫn dùng attribute forward được ạ",Mọi người cho em hỏi là tại sao khi em dùng torch.nn.Sequential để tạo custom layers thì khi mình forward nó lại lỗi ạ : do hiện khi em tạo lại block code tương tự bên ngoài vẫn dùng attribute forward được ạ,"#pytorch,",,,,
"#imagemorphing
Em chào mọi người ạ, em đang phải tự nghiên cứu và làm việc với image morphing algorithms nên thấy khá mông lung. Có anh chị và các bạn làm về phẩn này không ạ, cho e xin contact để em học tập với ạ.
Em cảm ơn ạ ^^","Em chào mọi người ạ, em đang phải tự nghiên cứu và làm việc với image morphing algorithms nên thấy khá mông lung. Có anh chị và các bạn làm về phẩn này không ạ, cho e xin contact để em học tập với ạ. Em cảm ơn ạ ^^",#imagemorphing,,,,
"[Hỏi về Topic Modelling]
Mọi người cho mình hỏi là nên sử dụng LDA model bình thường hay là LDA Mallet, vì mình đọc nhiều tutorial đều nói là LDA Mallet cho hiệu quả tốt hơn.
Bài toán của mình là Content-based Recommendation và thư viện sử dụng là Gensim","[Hỏi về Topic Modelling] Mọi người cho mình hỏi là nên sử dụng LDA model bình thường hay là LDA Mallet, vì mình đọc nhiều tutorial đều nói là LDA Mallet cho hiệu quả tốt hơn. Bài toán của mình là Content-based Recommendation và thư viện sử dụng là Gensim",,,,,
"Hỏi về features extraction trong CNN
Mọi người cho em hỏi chút, trong CNN, lớp classification layer cuối cùng, mình có cần phải định nghĩa rõ ràng cho các class để các filter có thể được cập nhật, hay lớp classification chỉ nhằm mục đích tạo task cho CNN, còn phần filter sẽ được cập nhật dựa theo dữ liệu đầu vào?
ở trong notebook này
https://www.kaggle.com/eryash15/handwritten-signature-feature-extraction/notebook
em thấy đầu vào định nghĩa cho dữ liệu chữ chỉ gồm 2 nhãn 1 và 0, cho toàn bộ chữ ký giả và thật, nhưng theo em hiểu thì nhãn 0 sẽ ứng với chữ ký thật, 1 là giả, thì toàn bộ các nhãn cho chữ ký của user 1 sẽ là 0 và tất cả user còn lại + chữ ký giả sẽ là 1
Em cảm ơn","Hỏi về features extraction trong CNN Mọi người cho em hỏi chút, trong CNN, lớp classification layer cuối cùng, mình có cần phải định nghĩa rõ ràng cho các class để các filter có thể được cập nhật, hay lớp classification chỉ nhằm mục đích tạo task cho CNN, còn phần filter sẽ được cập nhật dựa theo dữ liệu đầu vào? ở trong notebook này https://www.kaggle.com/eryash15/handwritten-signature-feature-extraction/notebook em thấy đầu vào định nghĩa cho dữ liệu chữ chỉ gồm 2 nhãn 1 và 0, cho toàn bộ chữ ký giả và thật, nhưng theo em hiểu thì nhãn 0 sẽ ứng với chữ ký thật, 1 là giả, thì toàn bộ các nhãn cho chữ ký của user 1 sẽ là 0 và tất cả user còn lại + chữ ký giả sẽ là 1 Em cảm ơn",,,,,
"[ Góc xin trợ giúp ] Em xin chào anh chị và các bạn ạ
Hiện tại em đang nhập môn làm đề tài liên quan tới Machine learning ""Sử dụng camera vision check lỗi ở màn hình điện thoại( scratch- lỗi xước , alien - lỗi dị vật , Bubble - lỗi bóng khí bề mặt ) "" tuy nhiên hiện tại tài liệu data ảnh ở 3 lỗi trên em tìm hiểu vẫn không có nhiều dữ liệu ảnh. Mong a chị nào có :
1.Data ảnh những lỗi trên, thì cho em xin tham khảo để em train ảnh với ;((((
2 Cộng với hướng check từng lỗi với ạ . Hướng em đưa ra còn mông lung quá .
Em đang suy nghĩ hướng này :
Chụp ảnh sample không có dị vật/ bọt khí/ xước tiền xử lý ảnh gốc
Các nội dung tiền xử lý bao gồm:
- Chuyển đổi ảnh thành ảnh xám( ảnh chụp bởi camera Basler là ảnh xám)
- Tiến hành lọc nhiễu các đặc tính của hình ảnh sử dụng bộ lọc Median( lọc muối tiêu), Gaussian( lọc các nhiễu nhiều hình dạng)
- Đăng ký ngoại lệ các tọa độ lỗ chân không, vết xước cố định bằng thuật toán tìm biên Canny
Bước 2: Đăng ký ngoại lệ cho các đặc tính hình ảnh sẵn có.
Bước 3: Chụp hình ảnh live, tiền xử lý ảnh…
Sau khi chụp ảnh live việc đầu tiên ta cần là phải tiền xử lý ảnh live tương tự như ảnh gốc:
- Chuyển đổi ảnh thành ảnh xám
- Tiến hành lọc nhiễu, lọc biên các đặc tính của hình ảnh sử dụng bộ lọc Median
- Đăng ký ngoại lệ các tọa độ lỗ chân không, vết xước cố định bằng thuật toán tìm biên Candy
Sau đó ta cần, xác nhận lại vùng ROI( read of image) của ảnh live do trong quá trình chụp thì ảnh bị xoay một góc rất nhỏ, góc lệc này có thể làm sai lệch các giá trị tính toán.
Bước 4: Xử lý ảnh dùng giải thuật Template matching và Subtract Background.
Bước 5: Đưa ra các điểm nghi ngờ dị vật/bọt khí/xước
So sánh các tọa độ của các đặc tính thu được trên ảnh live và ảnh gốc để tìm ra các tọa độ đặc tính khác nhau.
Các tọa độ thu được chính là các nghi ngờ dị vật/bọt khí/xước
Bước 6: Phát cảnh báo( Alarm) từ thiết bị - Confirm
Mong a chị giúp đỡ em. Em xin cảm ơn","[ Góc xin trợ giúp ] Em xin chào anh chị và các bạn ạ Hiện tại em đang nhập môn làm đề tài liên quan tới Machine learning ""Sử dụng camera vision check lỗi ở màn hình điện thoại( scratch- lỗi xước , alien - lỗi dị vật , Bubble - lỗi bóng khí bề mặt ) "" tuy nhiên hiện tại tài liệu data ảnh ở 3 lỗi trên em tìm hiểu vẫn không có nhiều dữ liệu ảnh. Mong a chị nào có : 1.Data ảnh những lỗi trên, thì cho em xin tham khảo để em train ảnh với ;(((( 2 Cộng với hướng check từng lỗi với ạ . Hướng em đưa ra còn mông lung quá . Em đang suy nghĩ hướng này : Chụp ảnh sample không có dị vật/ bọt khí/ xước tiền xử lý ảnh gốc Các nội dung tiền xử lý bao gồm: - Chuyển đổi ảnh thành ảnh xám( ảnh chụp bởi camera Basler là ảnh xám) - Tiến hành lọc nhiễu các đặc tính của hình ảnh sử dụng bộ lọc Median( lọc muối tiêu), Gaussian( lọc các nhiễu nhiều hình dạng) - Đăng ký ngoại lệ các tọa độ lỗ chân không, vết xước cố định bằng thuật toán tìm biên Canny Bước 2: Đăng ký ngoại lệ cho các đặc tính hình ảnh sẵn có. Bước 3: Chụp hình ảnh live, tiền xử lý ảnh… Sau khi chụp ảnh live việc đầu tiên ta cần là phải tiền xử lý ảnh live tương tự như ảnh gốc: - Chuyển đổi ảnh thành ảnh xám - Tiến hành lọc nhiễu, lọc biên các đặc tính của hình ảnh sử dụng bộ lọc Median - Đăng ký ngoại lệ các tọa độ lỗ chân không, vết xước cố định bằng thuật toán tìm biên Candy Sau đó ta cần, xác nhận lại vùng ROI( read of image) của ảnh live do trong quá trình chụp thì ảnh bị xoay một góc rất nhỏ, góc lệc này có thể làm sai lệch các giá trị tính toán. Bước 4: Xử lý ảnh dùng giải thuật Template matching và Subtract Background. Bước 5: Đưa ra các điểm nghi ngờ dị vật/bọt khí/xước So sánh các tọa độ của các đặc tính thu được trên ảnh live và ảnh gốc để tìm ra các tọa độ đặc tính khác nhau. Các tọa độ thu được chính là các nghi ngờ dị vật/bọt khí/xước Bước 6: Phát cảnh báo( Alarm) từ thiết bị - Confirm Mong a chị giúp đỡ em. Em xin cảm ơn",,,,,
"Vẫn là ResNet50 nhưng với kĩ thuật training và các thiết lập Hyperparameters khác nhau (như hàm loss, thay vì dùng SGD thì dùng LAMB, kết hợp với Repeat Augmentation + BCE,...) mà không cần thêm data hay distillation learning đã giúp model này đạt accuracy lên tới 80.4% trên tập dữ liệu ImageNet. Bài báo xem tại đây:","Vẫn là ResNet50 nhưng với kĩ thuật training và các thiết lập Hyperparameters khác nhau (như hàm loss, thay vì dùng SGD thì dùng LAMB, kết hợp với Repeat Augmentation + BCE,...) mà không cần thêm data hay distillation learning đã giúp model này đạt accuracy lên tới 80.4% trên tập dữ liệu ImageNet. Bài báo xem tại đây:",,,,,
"Chào mọi người. Em đang là đang là một Software Engineer với một background về Computer Science vững và understand sơ về component của một data systems và machine learning. Em muốn học thêm về Data Science concepts (A/B testing, make business data-driven decision,..) và kiểu work process thì nên học ở đâu ạ.
Em search course trên mạng thì nhiều lúc nó bị loãng với Python với R các thứ. Không biết mọi người có ai học course nào có component về cái này tốt có thể giới thiệu không ạ. Cảm ơn mọi người nhiều ạ !","Chào mọi người. Em đang là đang là một Software Engineer với một background về Computer Science vững và understand sơ về component của một data systems và machine learning. Em muốn học thêm về Data Science concepts (A/B testing, make business data-driven decision,..) và kiểu work process thì nên học ở đâu ạ. Em search course trên mạng thì nhiều lúc nó bị loãng với Python với R các thứ. Không biết mọi người có ai học course nào có component về cái này tốt có thể giới thiệu không ạ. Cảm ơn mọi người nhiều ạ !",,,,,
"[Hỏi về Linear Regression] #discution #ML #LR
Em đang tìm hiểu Linear Regression thì có 2 vấn đề khi em test code :
Bài toán: Dự đoán thu nhập bình quân đầu người canada theo năm dùng Linear Regression.
Dataset: dữ liệu từ năm 1970 -> 2016 với thu nhập bình quân tương ứng.
1. Ảnh 1
Khi model đã fit thì em dự báo ngược năm 1970 hoặc năm sau đó thì ra giá trị âm.
Theo cách hiểu của em thì với model đã được train thì năm 1970 phải ra giá trị dương có thể khác giá trị gốc chứ sao lại âm được?
2. Ảnh 2
Biểu đồ biểu diễn có cái 1e7 em không hiểu sao lại vậy, và tại sao nó không ra đường thẳng và các điểm ở gần xung quanh thay vì đi ngang như bên dưới hình.
Em kì vọng nó giống ảnh 3 ạ để mình có thể đối chiếu model với dữ liệu (ảnh này ở bộ dữ liệu khác - giá nhà theo diện tích).
Em để file code ở đây ạ
https://drive.google.com/drive/folders/1SysK4Le8Xg_Y0rnrEbONfLlqvnsVVvks?usp=sharing","[Hỏi về Linear Regression] Em đang tìm hiểu Linear Regression thì có 2 vấn đề khi em test code : Bài toán: Dự đoán thu nhập bình quân đầu người canada theo năm dùng Linear Regression. Dataset: dữ liệu từ năm 1970 -> 2016 với thu nhập bình quân tương ứng. 1. Ảnh 1 Khi model đã fit thì em dự báo ngược năm 1970 hoặc năm sau đó thì ra giá trị âm. Theo cách hiểu của em thì với model đã được train thì năm 1970 phải ra giá trị dương có thể khác giá trị gốc chứ sao lại âm được? 2. Ảnh 2 Biểu đồ biểu diễn có cái 1e7 em không hiểu sao lại vậy, và tại sao nó không ra đường thẳng và các điểm ở gần xung quanh thay vì đi ngang như bên dưới hình. Em kì vọng nó giống ảnh 3 ạ để mình có thể đối chiếu model với dữ liệu (ảnh này ở bộ dữ liệu khác - giá nhà theo diện tích). Em để file code ở đây ạ https://drive.google.com/drive/folders/1SysK4Le8Xg_Y0rnrEbONfLlqvnsVVvks?usp=sharing",#discution	#ML	#LR,,,,
"Chào mọi người!!
Mình hiện đang là sinh viên năm 3 ngành Khoa học Máy tính. Mình có background tương đối về toán (đại số tuyến tính, giải tích và xác suất thống kê). Thời gian qua mình học về toán và machine learning cơ bản và tập tành tìm hiểu về các model cũng như ứng dụng để tìm hướng đi cụ thể. Mình nhận thấy bản thân thích đi theo hướng xử lý ảnh (image processing và computer vision) và đang bắt đầu tìm hiểu về nền tảng và các kiến thức căn bản của xử lý ảnh (image processing). Mình khá chú trọng vào việc học các kiến thức nền tảng khi tiếp cận một vấn đề nào đó. Mình đang đọc cuốn DIP của Rafael C. Gonzalez and Richard E. Woods và kết hợp tìm hiểu một số nguồn online, như khóa fundamentals trên Coursera... Điều mà mình thấy được sau một khoảng thời gian tìm hiểu là image processing là một chủ đề khó và sẽ mất nhiều thời gian, bên cạnh đó, ở thời điểm hiện tại mình gặp phải nhiều khó khăn trong việc đọc các kiến thức về frequency domain trong xử lý ảnh, có nhiều thuật ngữ mà mình chưa nghe qua. Và cá nhân mình cảm thấy những nguồn tài liệu mà mình đang tiếp cận cũng không phù hợp cho beginner như mình lắm, nên mình khá phân vân không biết có nên tiếp tục đào sâu vào các kiến thức nền tảng hay không, như về signal processing chẳng hạn. Anh/chị/bạn nào đi trước có kinh nghiệm trong chủ đề này có thể cho mình lời khuyên với ạ, nếu có thể thì cho mình một số gợi ý về các nguồn tài liệu mà mình có thể tham khảo.
Mình xin chân thành cảm ơn!","Chào mọi người!! Mình hiện đang là sinh viên năm 3 ngành Khoa học Máy tính. Mình có background tương đối về toán (đại số tuyến tính, giải tích và xác suất thống kê). Thời gian qua mình học về toán và machine learning cơ bản và tập tành tìm hiểu về các model cũng như ứng dụng để tìm hướng đi cụ thể. Mình nhận thấy bản thân thích đi theo hướng xử lý ảnh (image processing và computer vision) và đang bắt đầu tìm hiểu về nền tảng và các kiến thức căn bản của xử lý ảnh (image processing). Mình khá chú trọng vào việc học các kiến thức nền tảng khi tiếp cận một vấn đề nào đó. Mình đang đọc cuốn DIP của Rafael C. Gonzalez and Richard E. Woods và kết hợp tìm hiểu một số nguồn online, như khóa fundamentals trên Coursera... Điều mà mình thấy được sau một khoảng thời gian tìm hiểu là image processing là một chủ đề khó và sẽ mất nhiều thời gian, bên cạnh đó, ở thời điểm hiện tại mình gặp phải nhiều khó khăn trong việc đọc các kiến thức về frequency domain trong xử lý ảnh, có nhiều thuật ngữ mà mình chưa nghe qua. Và cá nhân mình cảm thấy những nguồn tài liệu mà mình đang tiếp cận cũng không phù hợp cho beginner như mình lắm, nên mình khá phân vân không biết có nên tiếp tục đào sâu vào các kiến thức nền tảng hay không, như về signal processing chẳng hạn. Anh/chị/bạn nào đi trước có kinh nghiệm trong chủ đề này có thể cho mình lời khuyên với ạ, nếu có thể thì cho mình một số gợi ý về các nguồn tài liệu mà mình có thể tham khảo. Mình xin chân thành cảm ơn!",,,,,
"Dự án dịch JupyterLab sang tiếng việt.
Chào các bạn, trong group chắc hẳn các bạn đã quen với việc sử dụng JupyterLab. Giao diện hiện tại bằng tiếng anh không gây khó khăn gì trong việc sử dụng nhưng để mở rộng đối tượng người dùng, nhất là cho việc sử dụng JupyterLab trong trường học, hiện tại JupyterLab đã hỗ trợ các ngôn ngữ khác nhau thông qua hệ thống dịch dựa trên cộng đồng tại trang https://crowdin.com/project/jupyterlab.
Hiện tại bản dịch tiếng Việt đã được khởi động nhưng mới chỉ được 1% do rất thiếu người dịch. Vì vậy mình muốn chia sẻ post này để mong các bạn cùng chung tay tạo ra 1 phiên bản JupyterLab Việt hóa.
Để tham gia dự án, các bạn cần đăng nhập vào crowdin (qua tài khoản google, github...) và đăng kí tham gia dự án JupyterLab, sau khi dc xét duyện thì bạn có thể bắt đầu dịch, chi tiết như trong các ảnh phía dưới.
Gói ngôn ngữ tiếng việt cho JupyterLab đã có thể được cài đặt bằng pip:
pip install jupyterlab-language-pack-vi-VN
Hi vọng tiếng Việt sẽ nhanh chóng là ngôn ngữ tiếp theo được dịch hoàn chỉnh trong JupyterLab.","Dự án dịch JupyterLab sang tiếng việt. Chào các bạn, trong group chắc hẳn các bạn đã quen với việc sử dụng JupyterLab. Giao diện hiện tại bằng tiếng anh không gây khó khăn gì trong việc sử dụng nhưng để mở rộng đối tượng người dùng, nhất là cho việc sử dụng JupyterLab trong trường học, hiện tại JupyterLab đã hỗ trợ các ngôn ngữ khác nhau thông qua hệ thống dịch dựa trên cộng đồng tại trang https://crowdin.com/project/jupyterlab. Hiện tại bản dịch tiếng Việt đã được khởi động nhưng mới chỉ được 1% do rất thiếu người dịch. Vì vậy mình muốn chia sẻ post này để mong các bạn cùng chung tay tạo ra 1 phiên bản JupyterLab Việt hóa. Để tham gia dự án, các bạn cần đăng nhập vào crowdin (qua tài khoản google, github...) và đăng kí tham gia dự án JupyterLab, sau khi dc xét duyện thì bạn có thể bắt đầu dịch, chi tiết như trong các ảnh phía dưới. Gói ngôn ngữ tiếng việt cho JupyterLab đã có thể được cài đặt bằng pip: pip install jupyterlab-language-pack-vi-VN Hi vọng tiếng Việt sẽ nhanh chóng là ngôn ngữ tiếp theo được dịch hoàn chỉnh trong JupyterLab.",,,,,
"Chào m.n
E đang tìm hiểu về Machine learning phần Feauture selection, thì e thấy có rất ít tài liệu, sách hay bài giảng chi tiết về phần này. Ae có ai có tài liệu về phần này không cho em xin thông tin tham khảo với, nếu có ebook thì càng tốt ạ. E xin cảm ơn.
*Ảnh là 1 cuốn e tìm hiểu về phần này nhưng không có ebook ạ.","Chào m.n E đang tìm hiểu về Machine learning phần Feauture selection, thì e thấy có rất ít tài liệu, sách hay bài giảng chi tiết về phần này. Ae có ai có tài liệu về phần này không cho em xin thông tin tham khảo với, nếu có ebook thì càng tốt ạ. E xin cảm ơn. *Ảnh là 1 cuốn e tìm hiểu về phần này nhưng không có ebook ạ.",,,,,
"Thông báo thêm quyền tải về cho độc giả đã mua ebook """"Thực hành Học Máy với Scikit-Learn, Keras & TensorFlow - Tập 1"".
https://handson-ml.mlbvn.org/
Sau khi cân nhắc các điều khoản hợp đồng với O'Reilly, nhóm dịch đã quyết định cho phép độc giả tải bản pdf về đọc từ thiết bị cá nhân thay vì đọc trên trình duyệt như trước đây. Việc này hy vọng giúp độc giả có trải nghiệm đọc sách tốt hơn.
Để đảm bảo hơn, nhóm đã thêm một số biện pháp giúp truy xuất nguồn gốc phát tán nếu có sự chia sẻ tràn lan. Hy vọng các bạn tôn trọng bản quyền để chúng ta có cơ hội tiếp tục làm việc với O'Reilly trong các dự án sau.","Thông báo thêm quyền tải về cho độc giả đã mua ebook """"Thực hành Học Máy với Scikit-Learn, Keras & TensorFlow - Tập 1"". https://handson-ml.mlbvn.org/ Sau khi cân nhắc các điều khoản hợp đồng với O'Reilly, nhóm dịch đã quyết định cho phép độc giả tải bản pdf về đọc từ thiết bị cá nhân thay vì đọc trên trình duyệt như trước đây. Việc này hy vọng giúp độc giả có trải nghiệm đọc sách tốt hơn. Để đảm bảo hơn, nhóm đã thêm một số biện pháp giúp truy xuất nguồn gốc phát tán nếu có sự chia sẻ tràn lan. Hy vọng các bạn tôn trọng bản quyền để chúng ta có cơ hội tiếp tục làm việc với O'Reilly trong các dự án sau.",,,,,
"Chào mọi người ạ!
Mọi người trong nhóm đã ai thử triển khai một mô hình CV (Fast R-CNN, Faster R-CNN, Mask R-CNN) trên C# (.Net framework) cho bài toán detection, Segmentation,... chưa ạ!
Có thể cho em xin tài liệu, git tham khảo được không?
Hiện em có tìm và làm thử đc mô hình theo Git này (""https://github.com/halanch599/EmguCVDemoV4.x.x.git"") nhưng khi làm thử trên . Net framework 4.6-4.8 thì ko được hỗ trợ emgu.CV.runtime.
Thanks m.n!","Chào mọi người ạ! Mọi người trong nhóm đã ai thử triển khai một mô hình CV (Fast R-CNN, Faster R-CNN, Mask R-CNN) trên C# (.Net framework) cho bài toán detection, Segmentation,... chưa ạ! Có thể cho em xin tài liệu, git tham khảo được không? Hiện em có tìm và làm thử đc mô hình theo Git này (""https://github.com/halanch599/EmguCVDemoV4.x.x.git"") nhưng khi làm thử trên . Net framework 4.6-4.8 thì ko được hỗ trợ emgu.CV.runtime. Thanks m.n!",,,,,
"Chào các ace trong group. Mọi người cho e hỏi có bộ data về từ đồng nghĩa trong tiếng việt không ạ. Ví dụ: ăn cơm -> xơi cơm, bố trí -> sắp xếp
Cảm ơn ace nhiều","Chào các ace trong group. Mọi người cho e hỏi có bộ data về từ đồng nghĩa trong tiếng việt không ạ. Ví dụ: ăn cơm -> xơi cơm, bố trí -> sắp xếp Cảm ơn ace nhiều",,,,,
"Chào mọi người, hiện mình là web developer level middle. Mình cảm thấy khá đam mê với computer vision nên đã tìm hiểu khá nhiều. Giờ mình đã pass tensorflow certificate, mọi người cho mình xin lời khuyên có nên chuyển sang Machine learning luôn k ạ. Liệu nhập cuộc muộn có bị đánh giá thấp hơn những bạn trẻ đi từ đầu ko ạ.
Xin lắng nghe ý kiến từ mọi người.","Chào mọi người, hiện mình là web developer level middle. Mình cảm thấy khá đam mê với computer vision nên đã tìm hiểu khá nhiều. Giờ mình đã pass tensorflow certificate, mọi người cho mình xin lời khuyên có nên chuyển sang Machine learning luôn k ạ. Liệu nhập cuộc muộn có bị đánh giá thấp hơn những bạn trẻ đi từ đầu ko ạ. Xin lắng nghe ý kiến từ mọi người.",,,,,
"Cuối tuần ...nhạt tý. Các bạn có thấy ML giống với Hóa học không? Tính suy luận, suy diễn không cao như Vật lý. Với Hóa học hay ML bạn hoặc là lấy công thức đã được công bố rồi chế thành phẩm có ích như món ăn, phân bón, thuốc độc; hoặc là bạn ngồi phòng thí nghiệm trộn chất nọ, thay tỷ lệ kia, thêm chút xúc tác lạ, ...và rồi, nếu may mắn thì BÙM, bạn có một công trình để đời.
Có đúng vậy ko nhỉ? Hay là do mình dốt môn hóa (dù là trò cưng của cô).","Cuối tuần ...nhạt tý. Các bạn có thấy ML giống với Hóa học không? Tính suy luận, suy diễn không cao như Vật lý. Với Hóa học hay ML bạn hoặc là lấy công thức đã được công bố rồi chế thành phẩm có ích như món ăn, phân bón, thuốc độc; hoặc là bạn ngồi phòng thí nghiệm trộn chất nọ, thay tỷ lệ kia, thêm chút xúc tác lạ, ...và rồi, nếu may mắn thì BÙM, bạn có một công trình để đời. Có đúng vậy ko nhỉ? Hay là do mình dốt môn hóa (dù là trò cưng của cô).",,,,,
"Xin trợ giúp về DBSCAN cluster.
Mình đang cần dùng Dbscan cluster để lọc dữ liệu trùng lặp. Cần dựa vào biến toạ độ (lat/lon) và thời gian (timestamp) để phân cụm. Do mình mới nghiên cứu về cluster nên chưa biết cách phân cụm với dữ liệu nhiều biến. Hiện tại mình đang làm như sau:
- Bước 1: phân cụm với biến toạ độ (lat/lon)
- Bước 2: dựa trên từng cụm đã gom ở B1, tiếp tục phân cụm với dữ liệu biến thời gian
Tức là mình đang làm 2 lần cluster liên tục.
Mình xin nhờ trợ giúp 2 vấn đề sau:
1- Làm thế nào để phân cụm trong cùng 1 lần với cả bộ dữ liệu? Các biến có tính chất khác nhau thì phải làm sao? Vì hệ số epsilon sẽ khác nhau. Vd hiện tại mình chọn epsilon bằng 2miles với B1 và 30 phút với B2. Nếu gộp chung lại thì chọn epsilon ntn? Liệu có cần transform data về một dạng chung gì đó (một vài ví dụ mình đọc thấy dùng StandardScaler). Ở đây mình cần thiết phải dùng cả 2 biến lat/lon và time để phân cụm. Mình chỉ là chưa biết dùng cách nào để làm trong 1 lần mà k cần chia 2 bước như trên.
2- Trong tương lai mình có thể cần add thêm biến tên đường (dữ liệu string) vào để tiếp tục phân cụm, vậy có phải lại phức tạp hơn nhiều không? Và có phải dữ liệu string này cần biến đổi thành dạng số để chạy phân cụm?
Cảm ơn các bạn nhiều.","Xin trợ giúp về DBSCAN cluster. Mình đang cần dùng Dbscan cluster để lọc dữ liệu trùng lặp. Cần dựa vào biến toạ độ (lat/lon) và thời gian (timestamp) để phân cụm. Do mình mới nghiên cứu về cluster nên chưa biết cách phân cụm với dữ liệu nhiều biến. Hiện tại mình đang làm như sau: - Bước 1: phân cụm với biến toạ độ (lat/lon) - Bước 2: dựa trên từng cụm đã gom ở B1, tiếp tục phân cụm với dữ liệu biến thời gian Tức là mình đang làm 2 lần cluster liên tục. Mình xin nhờ trợ giúp 2 vấn đề sau: 1- Làm thế nào để phân cụm trong cùng 1 lần với cả bộ dữ liệu? Các biến có tính chất khác nhau thì phải làm sao? Vì hệ số epsilon sẽ khác nhau. Vd hiện tại mình chọn epsilon bằng 2miles với B1 và 30 phút với B2. Nếu gộp chung lại thì chọn epsilon ntn? Liệu có cần transform data về một dạng chung gì đó (một vài ví dụ mình đọc thấy dùng StandardScaler). Ở đây mình cần thiết phải dùng cả 2 biến lat/lon và time để phân cụm. Mình chỉ là chưa biết dùng cách nào để làm trong 1 lần mà k cần chia 2 bước như trên. 2- Trong tương lai mình có thể cần add thêm biến tên đường (dữ liệu string) vào để tiếp tục phân cụm, vậy có phải lại phức tạp hơn nhiều không? Và có phải dữ liệu string này cần biến đổi thành dạng số để chạy phân cụm? Cảm ơn các bạn nhiều.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 4/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 4/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"Em chào mọi người, em mới dùng thử dịch vụ bên iRender, em thuê 1 con GPU RTX-3090, VRAM 24Gb, CPU Xeon-2245, RAM 64Gb . Khi em create image, em có chọn image type là pre-install software và chọn là conda vì em nghĩ là em có thể sử dụng conda để tạo môi trường mới thỏa mãn tensorflow-gpu với câu lệnh conda create --name <tên môi trường mình đặt> tensorflow-gpu, thì nó tự động install giúp em một loạt các package trong đó có cudatoolkit ver 10.1, cudnn ver 7.6.5, tensorflow-base và tensorflow-gpu ver 2.4. Quá trình trên setup mất khoảng 7'. Em muốn hỏi mọi người một chút ạ
Quá trình setup môi trường tensorflow-gpu trên tầm đó có phải là lâu không ạ? Nếu có thể cải thiện thì em nên chọn pre-install cho image type có nên là conda không ạ? Vì em thấy rằng có các software khác như họ cho cả CUDA 11.0, cuDNN 8.0.5, hoặc tensorflow 2.4? Em nghĩ là mình chọn conda thì mình có thể dễ dàng install những thằng chưa có như tensorflow, CUDA, cuDNN, không biết rằng chọn pre-install khác thì thuận lợi hơn không ạ?
Em cảm ơn mọi người nhiều ạ!","Em chào mọi người, em mới dùng thử dịch vụ bên iRender, em thuê 1 con GPU RTX-3090, VRAM 24Gb, CPU Xeon-2245, RAM 64Gb . Khi em create image, em có chọn image type là pre-install software và chọn là conda vì em nghĩ là em có thể sử dụng conda để tạo môi trường mới thỏa mãn tensorflow-gpu với câu lệnh conda create --name <tên môi trường mình đặt> tensorflow-gpu, thì nó tự động install giúp em một loạt các package trong đó có cudatoolkit ver 10.1, cudnn ver 7.6.5, tensorflow-base và tensorflow-gpu ver 2.4. Quá trình trên setup mất khoảng 7'. Em muốn hỏi mọi người một chút ạ Quá trình setup môi trường tensorflow-gpu trên tầm đó có phải là lâu không ạ? Nếu có thể cải thiện thì em nên chọn pre-install cho image type có nên là conda không ạ? Vì em thấy rằng có các software khác như họ cho cả CUDA 11.0, cuDNN 8.0.5, hoặc tensorflow 2.4? Em nghĩ là mình chọn conda thì mình có thể dễ dàng install những thằng chưa có như tensorflow, CUDA, cuDNN, không biết rằng chọn pre-install khác thì thuận lợi hơn không ạ? Em cảm ơn mọi người nhiều ạ!",,,,,
From three little pigs to ...,From three little pigs to ...,,,,,
"Hi cả nhà. Mình xin chia sẻ problem solutions của Stanford's CS229 (2019, 2020), bao gồm cả bt code và bt viết (latex). Phiên bản solutions này phù hợp với phiên bản gần đây nhất được chia sẻ trên youtube của khóa này (Summer 2019). Mình đã cố gắng viết tương đối chi tiết, hi vọng nó sẽ có ích cho các bạn tự học.","Hi cả nhà. Mình xin chia sẻ problem solutions của Stanford's CS229 (2019, 2020), bao gồm cả bt code và bt viết (latex). Phiên bản solutions này phù hợp với phiên bản gần đây nhất được chia sẻ trên youtube của khóa này (Summer 2019). Mình đã cố gắng viết tương đối chi tiết, hi vọng nó sẽ có ích cho các bạn tự học.",,,,,
"Dear anh em,
Mình tham diễn đàn khá lâu, hôm nay mạn phép xin anh em 1 phút:
Mình đến từ Trung tâm Sáng Tạo, VNPT: https://icenter.ai/en (120 members, team core là đội Kĩ Sư Tài Năng K52 BKHN), mình về nước sau 8 năm ở Nhật và đang giữ vị trí Product Leads một số sản phẩm AI (Computer Vision, NLP, Speech, Bigdata):
https://ekyc.vnpt.vn/en
https://smartvision.vnpt.vn/en
https://smartvoice.vnpt.vn
https://smartbot.vnpt.vn/en
https://vnface.vnpt.vn/en
https://smartrpa.vnpt.vn/en
https://vnsocial.vnpt.vn
Mỗi sản phẩm hiện tại đều trên dưới vài triệu requests AI xử lý mỗi ngày, đa số đều chạy docker trên nền K8S, cung cấp dưới dạng SAAS hoặc Onpremise. Số lượng khách hàng lớn và tiếp tục tăng dần.
Tuy nhiên để chinh phục thị trường trong nước và hướng ra quốc tế, sản phẩm đòi hỏi phải tối ưu cải thiện liên tục. Hiện tại bên mình cũng đang hoàn thiện hệ thống MLOps (https://ml-ops.org/). Và đặc biệt đang thai nghén một sản phẩm AI kết hợp tất cả các lĩnh vực đang có.
Rất mong các anh em có cùng chí hướng làm AI Product join cùng để cùng nhau chinh phục các thử thách. Bên mình đang open rất nhiều vị trí AI Researcher, AI Engineer, Data Engineer, Software Engineer, Devops, BA, PM, Tester, UI/UX... và đặc biệt mong muốn đồng hành cùng các bạn vẫn đang là sinh viên các trường đại học qua chương trình Fresher.
Anh em có thể apply tại https://icenter.ai/en nhé.
==============
Mình chia sẻ thêm 1 chút về những câu chuyện thú vị từ phía Khách hàng trong khi làm sản phẩm AI:
""Anh nhìn rõ thế này, nghe rõ thế này mà AI của chú không xử lý chính xác được là như nào? AI phải hơn người chứ. AI này đã có Deep Learning chưa, hay vẫn chỉ Machine Learning? Trong vòng 1-2 ngày, bổ sung thêm tính năng này cho AI được không?""
""Em có thể cung cấp 1 hệ thống onpremise mà anh tự tay training model hay không? Bên anh có GPU không ạ? Không có em à. Bên anh có team đánh nhãn không ạ? Đánh nhãn là gì vậy em?”
""Độ chính xác là 99.9%, tức là 10000 mẫu sai mất 10 mẫu rồi, vậy ai sẽ trả tiền cho anh với 10 mẫu này?""
Khi làm sản phẩm, nhiều khi 99% công sức tập trung vào phần non-AI để 1% AI có thể phát huy sức mạnh. Vì vậy, để có một sản phẩm AI thành công cần rất nhiều yếu tố. Chúng ta luôn phải ở trong một tư thế vừa lắng nghe, vừa học hỏi, vừa cải thiện, vừa chinh phục khách hàng từng ngày. Nói chung rất vất vả, không hề dễ dàng một chút nào.
Trong quá trình tuyển dụng, nếu có vấn đề gì thì anh em PM mình theo địa chỉ Email lethaihung89@vnpt.vn nhé. Trên tựa đề của email anh em ghi rõ ""[Career]..."" để mình không bỏ sót bất cứ email nào.
Cám ơn anh em đã đọc và chia sẻ.","Dear anh em, Mình tham diễn đàn khá lâu, hôm nay mạn phép xin anh em 1 phút: Mình đến từ Trung tâm Sáng Tạo, VNPT: https://icenter.ai/en (120 members, team core là đội Kĩ Sư Tài Năng K52 BKHN), mình về nước sau 8 năm ở Nhật và đang giữ vị trí Product Leads một số sản phẩm AI (Computer Vision, NLP, Speech, Bigdata): https://ekyc.vnpt.vn/en https://smartvision.vnpt.vn/en https://smartvoice.vnpt.vn https://smartbot.vnpt.vn/en https://vnface.vnpt.vn/en https://smartrpa.vnpt.vn/en https://vnsocial.vnpt.vn Mỗi sản phẩm hiện tại đều trên dưới vài triệu requests AI xử lý mỗi ngày, đa số đều chạy docker trên nền K8S, cung cấp dưới dạng SAAS hoặc Onpremise. Số lượng khách hàng lớn và tiếp tục tăng dần. Tuy nhiên để chinh phục thị trường trong nước và hướng ra quốc tế, sản phẩm đòi hỏi phải tối ưu cải thiện liên tục. Hiện tại bên mình cũng đang hoàn thiện hệ thống MLOps (https://ml-ops.org/). Và đặc biệt đang thai nghén một sản phẩm AI kết hợp tất cả các lĩnh vực đang có. Rất mong các anh em có cùng chí hướng làm AI Product join cùng để cùng nhau chinh phục các thử thách. Bên mình đang open rất nhiều vị trí AI Researcher, AI Engineer, Data Engineer, Software Engineer, Devops, BA, PM, Tester, UI/UX... và đặc biệt mong muốn đồng hành cùng các bạn vẫn đang là sinh viên các trường đại học qua chương trình Fresher. Anh em có thể apply tại https://icenter.ai/en nhé. ============== Mình chia sẻ thêm 1 chút về những câu chuyện thú vị từ phía Khách hàng trong khi làm sản phẩm AI: ""Anh nhìn rõ thế này, nghe rõ thế này mà AI của chú không xử lý chính xác được là như nào? AI phải hơn người chứ. AI này đã có Deep Learning chưa, hay vẫn chỉ Machine Learning? Trong vòng 1-2 ngày, bổ sung thêm tính năng này cho AI được không?"" ""Em có thể cung cấp 1 hệ thống onpremise mà anh tự tay training model hay không? Bên anh có GPU không ạ? Không có em à. Bên anh có team đánh nhãn không ạ? Đánh nhãn là gì vậy em?” ""Độ chính xác là 99.9%, tức là 10000 mẫu sai mất 10 mẫu rồi, vậy ai sẽ trả tiền cho anh với 10 mẫu này?"" Khi làm sản phẩm, nhiều khi 99% công sức tập trung vào phần non-AI để 1% AI có thể phát huy sức mạnh. Vì vậy, để có một sản phẩm AI thành công cần rất nhiều yếu tố. Chúng ta luôn phải ở trong một tư thế vừa lắng nghe, vừa học hỏi, vừa cải thiện, vừa chinh phục khách hàng từng ngày. Nói chung rất vất vả, không hề dễ dàng một chút nào. Trong quá trình tuyển dụng, nếu có vấn đề gì thì anh em PM mình theo địa chỉ Email lethaihung89@vnpt.vn nhé. Trên tựa đề của email anh em ghi rõ ""[Career]..."" để mình không bỏ sót bất cứ email nào. Cám ơn anh em đã đọc và chia sẻ.",,,,,
"Mọi người cho em hỏi, em có capture màn hình với opencv (capture cửa sổ zoom meeting) nhưng hình ảnh capture không có real time như trên cửa sổ. Có cách nào xử lý không ạ. Với lại có phương pháp nào để phân biệt 1 ảnh tĩnh và 1 ảnh động không ạ.
Em cám ơn mọi người","Mọi người cho em hỏi, em có capture màn hình với opencv (capture cửa sổ zoom meeting) nhưng hình ảnh capture không có real time như trên cửa sổ. Có cách nào xử lý không ạ. Với lại có phương pháp nào để phân biệt 1 ảnh tĩnh và 1 ảnh động không ạ. Em cám ơn mọi người",,,,,
"Dạ em xin chào anh chị. Nay em có một vấn đề đang bị bí, mong anh chị chỉ dạy em.
Em đang nghiên cứu về Hidden Markov Model và thuật toán Baum Welch. Thuật toán thì hiện tại em đã code xong rồi, và em đã thử các cách implement khác nhau, để tránh bị lỗi code. Nhưng em đang gặp vấn đề với việc hội tụ và kết quả của thuật toán. Cụ thể là khi em khởi tạo các ma trận init probability (PI), transition probability (A) và emission probability ( B ) khác nhau thì sau 100 lần lặp thuật toán Baum Welch của em đều đưa ra 3 bộ PI, A, B khác nhau. Kể cả em đã thử cho chạy vô hạn và chỉ dừng khi norm 2 của PI, A, B với bộ PI, A, B cũ có sai số dưới 1e-6 nhưng có vẻ khởi tạo khác nhau cũng đưa ra các kết quả của PI, A, B đều khác nhau.
Em xin hỏi là có cách nào biết bộ PI, A, B khởi tạo hiện tại là tốt nhất không và tiêu chí để hội tụ cho thuật toán Baum Welch là gì hay chỉ cần chạy đúng 100 lần là được.
Em xin cảm ơn anh chị trước ạ.","Dạ em xin chào anh chị. Nay em có một vấn đề đang bị bí, mong anh chị chỉ dạy em. Em đang nghiên cứu về Hidden Markov Model và thuật toán Baum Welch. Thuật toán thì hiện tại em đã code xong rồi, và em đã thử các cách implement khác nhau, để tránh bị lỗi code. Nhưng em đang gặp vấn đề với việc hội tụ và kết quả của thuật toán. Cụ thể là khi em khởi tạo các ma trận init probability (PI), transition probability (A) và emission probability ( B ) khác nhau thì sau 100 lần lặp thuật toán Baum Welch của em đều đưa ra 3 bộ PI, A, B khác nhau. Kể cả em đã thử cho chạy vô hạn và chỉ dừng khi norm 2 của PI, A, B với bộ PI, A, B cũ có sai số dưới 1e-6 nhưng có vẻ khởi tạo khác nhau cũng đưa ra các kết quả của PI, A, B đều khác nhau. Em xin hỏi là có cách nào biết bộ PI, A, B khởi tạo hiện tại là tốt nhất không và tiêu chí để hội tụ cho thuật toán Baum Welch là gì hay chỉ cần chạy đúng 100 lần là được. Em xin cảm ơn anh chị trước ạ.",,,,,
"Em chào các ac.
Lần trước em có đăng bài và được các ac hướng dẫn có thể cài tool của anh Viet Anh Tran
https://github.com/coinForRich/coin-for-rich?fbclid=IwAR3fWKHQHJOovIHljjSqILS14IKKLI_ti3mdag4ZLHpyjqsgwT8T8fN2J0U#quickstart
Em đã thực hiện, chạy ra được biểu đồ nến như hình 1. Nhưng kết quả hiện em cần là bảng biểu thị thời gian thực, high, low... (hình 2) mà e ko biết làm thế nào để có thể thu được kết quả như vậy. Mong các ac có thể xem qua và hỗ trợ giúp em được không ạ.
Em xin cảm ơn rất nhiều!!!","Em chào các ac. Lần trước em có đăng bài và được các ac hướng dẫn có thể cài tool của anh Viet Anh Tran https://github.com/coinForRich/coin-for-rich?fbclid=IwAR3fWKHQHJOovIHljjSqILS14IKKLI_ti3mdag4ZLHpyjqsgwT8T8fN2J0U#quickstart Em đã thực hiện, chạy ra được biểu đồ nến như hình 1. Nhưng kết quả hiện em cần là bảng biểu thị thời gian thực, high, low... (hình 2) mà e ko biết làm thế nào để có thể thu được kết quả như vậy. Mong các ac có thể xem qua và hỗ trợ giúp em được không ạ. Em xin cảm ơn rất nhiều!!!",,,,,
"Em chào mọi người, em có một thắc mắc nhỏ.
Mọi người cho em hỏi bản dịch cuốn sách của Ian Goodfellow của dự án https://dlbookvn.gitlab.io/deeplearning/ còn available hay public không ạ, và nếu available thì muốn truy cập vào thì em có thể làm như thế nào ạ, em cảm ơn mọi người.","Em chào mọi người, em có một thắc mắc nhỏ. Mọi người cho em hỏi bản dịch cuốn sách của Ian Goodfellow của dự án https://dlbookvn.gitlab.io/deeplearning/ còn available hay public không ạ, và nếu available thì muốn truy cập vào thì em có thể làm như thế nào ạ, em cảm ơn mọi người.",,,,,
"Chào mọi người! Tôi tên là Sergey đang làm việc quản lý Search Platform ở TIKI. Đây là 1 bài viết chia sẻ phương pháp A/B testing của Tiki Search. Bài này sẽ giới thiệu các bạn với những phương pháp để đánh giá kết quả A/B test và những metric như sensitivity và false positive rate để chọn 1 phương pháp tốt nhất với domain riêng của các bạn.
Mời mọi người đọc bài và giao lưu feedback với. :)",Chào mọi người! Tôi tên là Sergey đang làm việc quản lý Search Platform ở TIKI. Đây là 1 bài viết chia sẻ phương pháp A/B testing của Tiki Search. Bài này sẽ giới thiệu các bạn với những phương pháp để đánh giá kết quả A/B test và những metric như sensitivity và false positive rate để chọn 1 phương pháp tốt nhất với domain riêng của các bạn. Mời mọi người đọc bài và giao lưu feedback với. :),,,,,
"Dạ em chào các Anh/ Chị
Em đang làm về hệ thống tư vấn khoá học dựa trên skill thiếu. Em có một vài thắc mắc mong nhận được sự chỉ dạy từ anh/ Chị
Đầu tiên: Em có tập các kỹ năng được đánh theo trọng số (trọng số thể hiện mức độ quan trọng) vd, Python:5, C#:3, R:1 (thì python quan trọng nhất). em tính sum_weight (tổng trọng số ứng với các skill trong khoá học). Num_Tech_Remain là các skill còn lại mà khoá học cung cấp.
Em sẽ lựa những khoá học chứa những skill thiếu (một khoá học có thể cung cấp 1 hoặc nhiều skill thiếu). Để chọn 1 khoá học tư vấn, em dựa trên các tiêu chí như: ưu tiên duyệt skill quan trọng, khoá học nào cung cấp nhiều skill, độ rating khoá học cao, số người học khoá học đó nhiều (nhưng em gặp trường hợp rating cao nhưng số người học lại thấp), người học có thể chọn học các course trong một thời gian nào đó (vd 3 tháng) hoặc nằm trong khoảng chi phí có thể bỏ ra (5 triệu).
Em đang phân vân không biết mình nên đề ra công thức nào để có sự chọn lựa course tư vấn phù hợp. Em rất mong nhận được sự chỉ dạy từ anh/ chị.","Dạ em chào các Anh/ Chị Em đang làm về hệ thống tư vấn khoá học dựa trên skill thiếu. Em có một vài thắc mắc mong nhận được sự chỉ dạy từ anh/ Chị Đầu tiên: Em có tập các kỹ năng được đánh theo trọng số (trọng số thể hiện mức độ quan trọng) vd, Python:5, C#:3, R:1 (thì python quan trọng nhất). em tính sum_weight (tổng trọng số ứng với các skill trong khoá học). Num_Tech_Remain là các skill còn lại mà khoá học cung cấp. Em sẽ lựa những khoá học chứa những skill thiếu (một khoá học có thể cung cấp 1 hoặc nhiều skill thiếu). Để chọn 1 khoá học tư vấn, em dựa trên các tiêu chí như: ưu tiên duyệt skill quan trọng, khoá học nào cung cấp nhiều skill, độ rating khoá học cao, số người học khoá học đó nhiều (nhưng em gặp trường hợp rating cao nhưng số người học lại thấp), người học có thể chọn học các course trong một thời gian nào đó (vd 3 tháng) hoặc nằm trong khoảng chi phí có thể bỏ ra (5 triệu). Em đang phân vân không biết mình nên đề ra công thức nào để có sự chọn lựa course tư vấn phù hợp. Em rất mong nhận được sự chỉ dạy từ anh/ chị.",,,,,
"Có thư viện nào train dataset cho hồi quy phi tuyến vậy mn?
Em cảm ơn",Có thư viện nào train dataset cho hồi quy phi tuyến vậy mn? Em cảm ơn,,,,,
"#NLP
Mình không phải dân AI hay NLP nhưng hiện tại, mình đang cần làm một bộ lọc từ ngữ thô tục trong Tiếng Việt, theo hướng sẽ loại bỏ comment đó nếu nó có ý nghĩa thô tục ví dụ ""Mày bị điên à"" là câu cần loại bỏ trong khi ""Tôi thích cái này phát điên"" thì giữ lại, ai có giải pháp hay hướng đi gì thì cho mình biết với ạ, mình cảm ơn mọi người.","Mình không phải dân AI hay NLP nhưng hiện tại, mình đang cần làm một bộ lọc từ ngữ thô tục trong Tiếng Việt, theo hướng sẽ loại bỏ comment đó nếu nó có ý nghĩa thô tục ví dụ ""Mày bị điên à"" là câu cần loại bỏ trong khi ""Tôi thích cái này phát điên"" thì giữ lại, ai có giải pháp hay hướng đi gì thì cho mình biết với ạ, mình cảm ơn mọi người.",#NLP,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 2/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 2/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"anh chị cho e hỏi ngu này với ạ?
Do lần đầu e dùng đến train model nên e ko rõ mong anh chị giúp đỡ ạ.
Tình hình là e đang làm 1 ứng dụng hoặc 1 web dự đoán giá cổ phiếu. E có lấy được dữ liệu cổ phiếu của nhiều công ty khác nhau, mỗi cty là 1 file csv. Vậy mỗi một công ty e phải đọc từng file csv, và cũng phải gọi hàm train cho từng file riêng luôn hay sao ạ?
Và nếu khi đã train xong thì làm cách nào để mình sử dụng cho ứng dụng mình ạ. E cảm ơn ạ.","anh chị cho e hỏi ngu này với ạ? Do lần đầu e dùng đến train model nên e ko rõ mong anh chị giúp đỡ ạ. Tình hình là e đang làm 1 ứng dụng hoặc 1 web dự đoán giá cổ phiếu. E có lấy được dữ liệu cổ phiếu của nhiều công ty khác nhau, mỗi cty là 1 file csv. Vậy mỗi một công ty e phải đọc từng file csv, và cũng phải gọi hàm train cho từng file riêng luôn hay sao ạ? Và nếu khi đã train xong thì làm cách nào để mình sử dụng cho ứng dụng mình ạ. E cảm ơn ạ.",,,,,
"Share công cụ - ứng dụng thu thập dữ liệu cryptocurrencies - dành cho các bạn muốn phân tích dữ liệu thời gian thực/dữ liệu time series: https://github.com/coinForRich/coin-for-rich (cái tên là mình và Vuong Hoai Nam hứng lên nghĩ ra lúc đầu và giờ không đổi được 😂)
Các tính năng nổi bật:
☑️ Fully containerized nhờ Docker Compose
☑️ Kéo dữ liệu thời gian thực từ websockets và dữ liệu lịch sử từ REST APIs của 3 sàn: Binance, Bitfinex và Bittrex
☑️ Database cho dữ liệu kéo về được auto dựng schema ngay từ đầu và sẵn sàng để sử dụng
☑️ Ứng dụng có các đầu API nối ra từ Postgresql, Redis, web (websocket + REST API), giúp bạn có thể dễ dàng xây dựng ứng dụng phân tích dữ liệu của riêng bạn
☑️ Biểu đồ nến hiển thị giá thời gian thực theo từng phút hoặc từng khoảng thời gian được định sẵn
☑️ Đồng thời cũng nhờ Docker Compose, bạn có thể tuỳ chỉnh ứng dụng theo ý mình và chạy lại ngay trên database đã được kéo","Share công cụ - ứng dụng thu thập dữ liệu cryptocurrencies - dành cho các bạn muốn phân tích dữ liệu thời gian thực/dữ liệu time series: https://github.com/coinForRich/coin-for-rich (cái tên là mình và Vuong Hoai Nam hứng lên nghĩ ra lúc đầu và giờ không đổi được ) Các tính năng nổi bật: Fully containerized nhờ Docker Compose Kéo dữ liệu thời gian thực từ websockets và dữ liệu lịch sử từ REST APIs của 3 sàn: Binance, Bitfinex và Bittrex Database cho dữ liệu kéo về được auto dựng schema ngay từ đầu và sẵn sàng để sử dụng Ứng dụng có các đầu API nối ra từ Postgresql, Redis, web (websocket + REST API), giúp bạn có thể dễ dàng xây dựng ứng dụng phân tích dữ liệu của riêng bạn Biểu đồ nến hiển thị giá thời gian thực theo từng phút hoặc từng khoảng thời gian được định sẵn Đồng thời cũng nhờ Docker Compose, bạn có thể tuỳ chỉnh ứng dụng theo ý mình và chạy lại ngay trên database đã được kéo",,,,,
"Xin chào tất cả mọi người. Mình có 1 câu hỏi liên quan đến DS role.
Introduce background của mình 1 chút:
- Mình có kinh nghiệm làm về ML được 3-5 năm.
- Làm chủ yếu về NLP.
Gần đây mình muốn đi sâu hơn về thế giới ML để hiểu bản chất của data và lấy insight từ data.
Mình nghĩ hướng về lâu dài nếu mình đi về DS sẽ phù hợp. Nên mình muốn hỏi trên thực tế, các bạn DS làm việc ở công ty thì có đụng nhiều về:
- Có áp dụng Statistic / Math để hiểu về data hay không.
- Công việc thường nhật làm là gì?
- DS có đụng nhiều về ML và dùng ML để mining insight hay k.
Mình rất mù mờ về công việc thực tế của DS role, rất mong các bạn có thể chia sẻ về góc nhìn cho vấn đề này.
Mình xin cảm ơn các bạn đã dành thời gian đọc bài này. Thanks","Xin chào tất cả mọi người. Mình có 1 câu hỏi liên quan đến DS role. Introduce background của mình 1 chút: - Mình có kinh nghiệm làm về ML được 3-5 năm. - Làm chủ yếu về NLP. Gần đây mình muốn đi sâu hơn về thế giới ML để hiểu bản chất của data và lấy insight từ data. Mình nghĩ hướng về lâu dài nếu mình đi về DS sẽ phù hợp. Nên mình muốn hỏi trên thực tế, các bạn DS làm việc ở công ty thì có đụng nhiều về: - Có áp dụng Statistic / Math để hiểu về data hay không. - Công việc thường nhật làm là gì? - DS có đụng nhiều về ML và dùng ML để mining insight hay k. Mình rất mù mờ về công việc thực tế của DS role, rất mong các bạn có thể chia sẻ về góc nhìn cho vấn đề này. Mình xin cảm ơn các bạn đã dành thời gian đọc bài này. Thanks",,,,,
"Chào mọi người!
Mọi người trong nhóm đã ai thử chuyển thành công model weight của pytorch (.pth) qua model của TF (.pb) chưa ạ. Có thể cho em xin git hướng dẫn hoặc notebook tham khảo được không ạ!
Em đã thử covert bằng https://github.com/kalaspuffar/onnx-convert-example và https://quq99.github.io/blog/2019-08/onnx-convert-trained-pytorch-model-to-tensorflow-model/ nhưng không đc ạ!
Thanks m,n!
Model e train với pre detectron2 : https://github.com/facebookresearch","Chào mọi người! Mọi người trong nhóm đã ai thử chuyển thành công model weight của pytorch (.pth) qua model của TF (.pb) chưa ạ. Có thể cho em xin git hướng dẫn hoặc notebook tham khảo được không ạ! Em đã thử covert bằng https://github.com/kalaspuffar/onnx-convert-example và https://quq99.github.io/blog/2019-08/onnx-convert-trained-pytorch-model-to-tensorflow-model/ nhưng không đc ạ! Thanks m,n! Model e train với pre detectron2 : https://github.com/facebookresearch",,,,,
Free event to learn more about data-centric AI tomorrow (with Alex Ratner & Andrew Ng),Free event to learn more about data-centric AI tomorrow (with Alex Ratner & Andrew Ng),,,,,
"Chào mọi người
Em hiện đang nghiên cứu về đề tài Social Recommendation sử dụng Graph Neural Network. Anh chị nào có từng nghiên cứu về đề tài này có thể cho em xin nguồn tài liệu tham khảo với ạ. Em cảm ơn",Chào mọi người Em hiện đang nghiên cứu về đề tài Social Recommendation sử dụng Graph Neural Network. Anh chị nào có từng nghiên cứu về đề tài này có thể cho em xin nguồn tài liệu tham khảo với ạ. Em cảm ơn,,,,,
"Trong bài báo này (https://arxiv.org/pdf/2004.03844.pdf) họ chỉ ra rằng chúng ta có thể bỏ bớt một số layers của model bert mà kết quả vẫn còn chấp nhận được. Ưu điểm là hạn chế được thời gian train trên các GPU.
Cho mình hỏi: Nếu muốn drop bớt một số layers của bert như trong bài báo thì phải làm như thế nào trong code phần model?
Mình cần chạy thử việc train có đúng là nhanh hơn và dung lượng save checkpoint của bert sẽ giảm không?
Xin cảm ơn.",Trong bài báo này (https://arxiv.org/pdf/2004.03844.pdf) họ chỉ ra rằng chúng ta có thể bỏ bớt một số layers của model bert mà kết quả vẫn còn chấp nhận được. Ưu điểm là hạn chế được thời gian train trên các GPU. Cho mình hỏi: Nếu muốn drop bớt một số layers của bert như trong bài báo thì phải làm như thế nào trong code phần model? Mình cần chạy thử việc train có đúng là nhanh hơn và dung lượng save checkpoint của bert sẽ giảm không? Xin cảm ơn.,,,,,
"Em chào mọi người, em mới học về NLP.
Hiện tại em đang có một vấn đề như sau: làm sao để kiểm tra một từ tiếng việt đang viết đúng chính tả ạ?
Các anh chị có thể giúp em đưa ra ý tưởng, hoặc các tài liệu liên quan đến nó được không ạ. Em cũng đã tìm kiếm rồi mà chưa ra.
Em xin cảm ơn ạ!","Em chào mọi người, em mới học về NLP. Hiện tại em đang có một vấn đề như sau: làm sao để kiểm tra một từ tiếng việt đang viết đúng chính tả ạ? Các anh chị có thể giúp em đưa ra ý tưởng, hoặc các tài liệu liên quan đến nó được không ạ. Em cũng đã tìm kiếm rồi mà chưa ra. Em xin cảm ơn ạ!",,,,,
"Xin chào các bác. Em đang học đến phần Unet và sử dụng thư viện để làm một số bài toán nên mạnh dạn làm video chia sẻ mong giúp được các bạn mới học ạ.
Mong admin duyệt bài!",Xin chào các bác. Em đang học đến phần Unet và sử dụng thư viện để làm một số bài toán nên mạnh dạn làm video chia sẻ mong giúp được các bạn mới học ạ. Mong admin duyệt bài!,,,,,
Nhờ sự giúp đỡ nhiệt tình của anh em trong nhóm team mình đã hoàn thành dự án đọc thông tin chứng minh thư. cám ơn anh em rất nhiều nhé:,Nhờ sự giúp đỡ nhiệt tình của anh em trong nhóm team mình đã hoàn thành dự án đọc thông tin chứng minh thư. cám ơn anh em rất nhiều nhé:,,,,,
dạ do là em đang học môn Big data tại trường em chạy mô hình dự báo bệnh tim bằng Azure ML studio. Mng cho em hỏi là ra điểm Skewness và Kurtosis ntn là bị lệch dữ liệu r fk ạ? E nên Transform hay Normalize data hay là gì ở bước tiếp theo để xử lí dữ liệu ạ? Do e mới sử dụng lần đầu hong biết j mong anh/ chị giúp đỡ ạ em cảm ơn nhiều ạ 😭💗,dạ do là em đang học môn Big data tại trường em chạy mô hình dự báo bệnh tim bằng Azure ML studio. Mng cho em hỏi là ra điểm Skewness và Kurtosis ntn là bị lệch dữ liệu r fk ạ? E nên Transform hay Normalize data hay là gì ở bước tiếp theo để xử lí dữ liệu ạ? Do e mới sử dụng lần đầu hong biết j mong anh/ chị giúp đỡ ạ em cảm ơn nhiều ạ,,,,,
"Xin chào mọi người!
Em là sinh viên ngành oto hiện đang làm nghiên cứu về phần ứng dụng Ai để lập trình phần mền tự lái cho oto.
Vì tự tìm hiểu nên khá là loay hoay phần code, này em gặp lỗi như này mong anh chị chỉ giúp em với ạ.
em có tìm hiểu thì trên mạng người ta bảo cài về tensorflow 1.14 và em làm theo chay được nhưng đến phần load gpu thì nó lại kêu ko có cudu 10.0.
anh chị cho em hỏi em phải làm gì để sửa code này em dân ngoài ngành kiến thức hơi hạn hẹp.
code này là về self driving của udacity.
anh chị nào đã làm dự án kiểu này rồi cho em xin file code với cho em kết bạn hỏi 1 số điều ạ, em cảm ơn mọi người
from tensorflow.contrib.layers import flatten","Xin chào mọi người! Em là sinh viên ngành oto hiện đang làm nghiên cứu về phần ứng dụng Ai để lập trình phần mền tự lái cho oto. Vì tự tìm hiểu nên khá là loay hoay phần code, này em gặp lỗi như này mong anh chị chỉ giúp em với ạ. em có tìm hiểu thì trên mạng người ta bảo cài về tensorflow 1.14 và em làm theo chay được nhưng đến phần load gpu thì nó lại kêu ko có cudu 10.0. anh chị cho em hỏi em phải làm gì để sửa code này em dân ngoài ngành kiến thức hơi hạn hẹp. code này là về self driving của udacity. anh chị nào đã làm dự án kiểu này rồi cho em xin file code với cho em kết bạn hỏi 1 số điều ạ, em cảm ơn mọi người from tensorflow.contrib.layers import flatten",,,,,
"em đang dùng ubuntu20.04. Hiện tại em đang chạy 1 source code dùng python3.6 và opencv ver 4.1.1.26, nhưng em không thể install version này. em đang đã thử dùng pip và curl nhưng đều k đc. em có câu hỏi :
1. Có phải do version pip e đang dùng nó k hỗ trợ install opencv2 ver 4.1.1.26 không. Nếu đúng thì cài pip ver nào ( em k tìm đc trên gg).
2. mE mới học và sử dụng trên ubuntu, em có install nhiều version của 1 thư viện thì, mỗi khi e chạy lệnh python3.6, 3.7 hay 3.8 thì nó sẽ link đi đâu, link với thư viện ntn ?","em đang dùng ubuntu20.04. Hiện tại em đang chạy 1 source code dùng python3.6 và opencv ver 4.1.1.26, nhưng em không thể install version này. em đang đã thử dùng pip và curl nhưng đều k đc. em có câu hỏi : 1. Có phải do version pip e đang dùng nó k hỗ trợ install opencv2 ver 4.1.1.26 không. Nếu đúng thì cài pip ver nào ( em k tìm đc trên gg). 2. mE mới học và sử dụng trên ubuntu, em có install nhiều version của 1 thư viện thì, mỗi khi e chạy lệnh python3.6, 3.7 hay 3.8 thì nó sẽ link đi đâu, link với thư viện ntn ?",,,,,
"Mình rất mong sự hỗ trợ của các bạn.
Mình có một đồ thị với rất nhiều điểm. Nhưng mình chỉ muốn tìm phương trình của hàm xấp xỉ với bậc nhỏ như 3, bậc 5, hay bậc 7,... Có một thư viện Python hay một chương trình ứng dụng nào khác cũng được để có thể đưa ra kết quả của hàm gần đúng này không? Cảm ơn các bạn.","Mình rất mong sự hỗ trợ của các bạn. Mình có một đồ thị với rất nhiều điểm. Nhưng mình chỉ muốn tìm phương trình của hàm xấp xỉ với bậc nhỏ như 3, bậc 5, hay bậc 7,... Có một thư viện Python hay một chương trình ứng dụng nào khác cũng được để có thể đưa ra kết quả của hàm gần đúng này không? Cảm ơn các bạn.",,,,,
"Chào mọi người, em đang xây dựng 1 model dựa trên mô hình PointNet :https://arxiv.org/pdf/1612.00593.pdf . Nhưng ở đoạn, concatenate local feature và global feature như hình dưới đây. Em không thể concatenate (n,64) và (none, 1024) được. Xin hỏi mọi người, có cách nào để concatenate chúng với nhau không, hay em đã hiểu sai vấn đề? Rất cám ơn mọi người","Chào mọi người, em đang xây dựng 1 model dựa trên mô hình PointNet :https://arxiv.org/pdf/1612.00593.pdf . Nhưng ở đoạn, concatenate local feature và global feature như hình dưới đây. Em không thể concatenate (n,64) và (none, 1024) được. Xin hỏi mọi người, có cách nào để concatenate chúng với nhau không, hay em đã hiểu sai vấn đề? Rất cám ơn mọi người",,,,,
"Em có câu hỏi này cần mọi người hỗ trợ ạ. Hiện em đang có một tập dữ liệu rất lớn, là ảnh của các nhân viên ở cty, em đã training xong và có file kết quả model. Nhưng vì nhân sự của cty luôn thay đổi nên mọi người tư vấn giúp e xem có technique và keyword nào để khi dữ liệu thay đổi (tăng lên hoặc giảm đi ) thì mk không phải training lại cả model mà vẫn nhận diện được data mới hoặc k nhận diện data đã bỏ đi được không ạ?Em cảm ơn ạ.","Em có câu hỏi này cần mọi người hỗ trợ ạ. Hiện em đang có một tập dữ liệu rất lớn, là ảnh của các nhân viên ở cty, em đã training xong và có file kết quả model. Nhưng vì nhân sự của cty luôn thay đổi nên mọi người tư vấn giúp e xem có technique và keyword nào để khi dữ liệu thay đổi (tăng lên hoặc giảm đi ) thì mk không phải training lại cả model mà vẫn nhận diện được data mới hoặc k nhận diện data đã bỏ đi được không ạ?Em cảm ơn ạ.",,,,,
"[Newbie]
Em chào các anh chị. E vừa bước và việc tìm hiểu Machine learning để dự báo tiền điện tử.
Em có thấy bài của a Viet Anh Tran share về công cụ thu thập dữ liệu cryptocurriencies. Em có đọc và tìm hiểu, cũng đã cài đặt Docker. Nhưng do kiến thức hạn hẹp thiếu sót nên e không hiểu lắm, từ khi bắt đầu ở bước dưới hình. Em không biết anh chị nào đã thực hiện có thể hỗ trợ giúp em được không ạ!
Mong các anh chị nói dễ dễ hiểu giúp em ạ, e còn mới nen e chưa biết nhiều, em xin cảm ơn nhiều ạ!
Cảm ơn admin đã duyệt bài!
https://github.com/coinForRich/coin-for-rich#inside-the-app","[Newbie] Em chào các anh chị. E vừa bước và việc tìm hiểu Machine learning để dự báo tiền điện tử. Em có thấy bài của a Viet Anh Tran share về công cụ thu thập dữ liệu cryptocurriencies. Em có đọc và tìm hiểu, cũng đã cài đặt Docker. Nhưng do kiến thức hạn hẹp thiếu sót nên e không hiểu lắm, từ khi bắt đầu ở bước dưới hình. Em không biết anh chị nào đã thực hiện có thể hỗ trợ giúp em được không ạ! Mong các anh chị nói dễ dễ hiểu giúp em ạ, e còn mới nen e chưa biết nhiều, em xin cảm ơn nhiều ạ! Cảm ơn admin đã duyệt bài! https://github.com/coinForRich/coin-for-rich#inside-the-app",,,,,
"Cheatsheets là cách ghi nhớ những gì cơ bản và quan trọng để tra cứu. Nay mình tình cơ thấy có người tổng hợp được một loạt các Cheatsheets, nên mình chia sẻ lại tại đây để mọi người tham khảo https://bit.ly/3m5aRcf. Nó bao gồm rất nhiều nội dung, từ Linux; tới Git; tới 1 số ngôn ngữ lập trình (R, Python, Scala,...); cho tới các thư viện cơ bản như Numpy, Pandas,... cho khoa học dữ liệu; toán xác suất thống kê; Docker & Kurbenetes; AI như NLP; và quay lại với Excel;...
Hi vọng nó hữu ích với mọi người","Cheatsheets là cách ghi nhớ những gì cơ bản và quan trọng để tra cứu. Nay mình tình cơ thấy có người tổng hợp được một loạt các Cheatsheets, nên mình chia sẻ lại tại đây để mọi người tham khảo https://bit.ly/3m5aRcf. Nó bao gồm rất nhiều nội dung, từ Linux; tới Git; tới 1 số ngôn ngữ lập trình (R, Python, Scala,...); cho tới các thư viện cơ bản như Numpy, Pandas,... cho khoa học dữ liệu; toán xác suất thống kê; Docker & Kurbenetes; AI như NLP; và quay lại với Excel;... Hi vọng nó hữu ích với mọi người",,,,,
"Chào mọi người, em là newbie ạ. Mọi người cho em hỏi những thuật toán như linear , logistic, softmax regression… có tác dụng ntn ạ? Vì em thấy mình chỉ làm việc với mạng neural và em cũng chưa thấy ứng dụng của các thuật toán ấy. Mong mọi người chỉ bảo ạ!","Chào mọi người, em là newbie ạ. Mọi người cho em hỏi những thuật toán như linear , logistic, softmax regression… có tác dụng ntn ạ? Vì em thấy mình chỉ làm việc với mạng neural và em cũng chưa thấy ứng dụng của các thuật toán ấy. Mong mọi người chỉ bảo ạ!",,,,,
"Chào mọi người, em đang viết bài báo cáo về một số thuật toán học máy: support vector mạchine, logistic regession, và random forest. Em muốn tìm một số tài liệu (sách) để có thể dùng tham khảo cho bài báo cáo của em.
Hy vọng mọi người có thể giúp giùm em.","Chào mọi người, em đang viết bài báo cáo về một số thuật toán học máy: support vector mạchine, logistic regession, và random forest. Em muốn tìm một số tài liệu (sách) để có thể dùng tham khảo cho bài báo cáo của em. Hy vọng mọi người có thể giúp giùm em.",,,,,
"Chào mọi người!
Em đang train1 model mask rcnn với detectron2 bằng colab.
Phần train của em đang bị lỗi này và em đã thử một số cách trên gg, stack, nhưng vẫn không khắc phục được!
m.n có thể cho em xin phương án khắc phục được không ạ!
Thanks m.n!
Colab: https://drive.google.com/file/d/1OB30sShdrIYQ4kDuE-S7moAUEdhvT6gF/view?usp=sharing","Chào mọi người! Em đang train1 model mask rcnn với detectron2 bằng colab. Phần train của em đang bị lỗi này và em đã thử một số cách trên gg, stack, nhưng vẫn không khắc phục được! m.n có thể cho em xin phương án khắc phục được không ạ! Thanks m.n! Colab: https://drive.google.com/file/d/1OB30sShdrIYQ4kDuE-S7moAUEdhvT6gF/view?usp=sharing",,,,,
"Xin chào mọi người!
Em có một thắc mắc là tại sao người ta lại nói 3D UNet nó too complex khi so sánh với 2D UNET trong khi No. parameters của 3D UNET là 16.21 (x10^3) và 2D UNet là 31.38 (x10^3). Em không biết liệu có phải là khi mọi người mentioned complex ở đây là time complexity (FLOPs)?
Em cảm ơn Admin đã duyệt bài ạ.",Xin chào mọi người! Em có một thắc mắc là tại sao người ta lại nói 3D UNet nó too complex khi so sánh với 2D UNET trong khi No. parameters của 3D UNET là 16.21 (x10^3) và 2D UNet là 31.38 (x10^3). Em không biết liệu có phải là khi mọi người mentioned complex ở đây là time complexity (FLOPs)? Em cảm ơn Admin đã duyệt bài ạ.,,,,,
"Nhi đang dựng một ML cho assessor property cho một tiểu bang của Mỹ ( chỉ là dự án cá nhân, Nhi tải dataset trên data.gov về)
Nhưng lúc tạo histogram cho mỗi numerical attribute thì có một số biểu đồ chỉ xuất hiện 1 cột. Ví dụ như “SALE PRICE” và “LAND VAL” mặc dù trong excel hiện giá trị từ 0-1,000,000 nhưng lúc làm biểu đồ cũng hiện 1 cột duy nhất.
Có ai có biết lí do tại sao không ạ?
Thanks.
P/s: do mình học ở nước ngoài, nên có mấy từ Nhi ko biết tiếng Việt.","Nhi đang dựng một ML cho assessor property cho một tiểu bang của Mỹ ( chỉ là dự án cá nhân, Nhi tải dataset trên data.gov về) Nhưng lúc tạo histogram cho mỗi numerical attribute thì có một số biểu đồ chỉ xuất hiện 1 cột. Ví dụ như “SALE PRICE” và “LAND VAL” mặc dù trong excel hiện giá trị từ 0-1,000,000 nhưng lúc làm biểu đồ cũng hiện 1 cột duy nhất. Có ai có biết lí do tại sao không ạ? Thanks. P/s: do mình học ở nước ngoài, nên có mấy từ Nhi ko biết tiếng Việt.",,,,,
"Em muốn hỏi cách xử lý dữ liệu trước khi đưa vào model. Em có bài toán về CV. Một folder có dạng như bên dưới, bên trong các folder 111 và 112 là hình ảnh và label được chia theo tên của folder chứa nó, ví dụ hình trong folder '18/112/abc.jpg' thì có label là 18 và 112(18 là tuổi, 112 là giới tính). Em đang xử dụng thư viện OS của python để xử lý nhưng thấy rất cồng kềnh, có công cụ nào khác hiệu quả hơn không ạ?","Em muốn hỏi cách xử lý dữ liệu trước khi đưa vào model. Em có bài toán về CV. Một folder có dạng như bên dưới, bên trong các folder 111 và 112 là hình ảnh và label được chia theo tên của folder chứa nó, ví dụ hình trong folder '18/112/abc.jpg' thì có label là 18 và 112(18 là tuổi, 112 là giới tính). Em đang xử dụng thư viện OS của python để xử lý nhưng thấy rất cồng kềnh, có công cụ nào khác hiệu quả hơn không ạ?",,,,,
"[Góc newbie]
Chào mn. Mình có một thắc mắc nhỏ. Mình đang làm dự án về NLP. Mình đang cần 1 ML model dự đoán số người share 1 mẩu tin tức nào đó trên mạng xã hội. Bài toán có X là 1 mẩu tin (mình đã chuyển thành bag-of-word), Y là số người chia sẻ mẩu tin đó (Y là số tự nhiên lớn hơn 0)
Mình đã có dataset rồi nhưng đang không biết sử dụng model nào để có thể train được bài toán này. Không biết mn có thể gợi ý giúp mình được k ạ
Mình cảm ơn mn","[Góc newbie] Chào mn. Mình có một thắc mắc nhỏ. Mình đang làm dự án về NLP. Mình đang cần 1 ML model dự đoán số người share 1 mẩu tin tức nào đó trên mạng xã hội. Bài toán có X là 1 mẩu tin (mình đã chuyển thành bag-of-word), Y là số người chia sẻ mẩu tin đó (Y là số tự nhiên lớn hơn 0) Mình đã có dataset rồi nhưng đang không biết sử dụng model nào để có thể train được bài toán này. Không biết mn có thể gợi ý giúp mình được k ạ Mình cảm ơn mn",,,,,
"""Primer: Searching for Efficient Transformers for Language Modeling"" là bài báo mới nhất của Google Brain (link tại đây: https://arxiv.org/pdf/2109.08668.pdf; source code tại đây: https://github.com/google-research/google-research/tree/master/primer).
Một số cải tiến của Primer (một biến thể cái tiến của kiến trúc Transformer) so với các kết quả trước đây:
1/ Train nhanh 3-4 lần so với Transformer như T5 và GPT-3XL cho tác vụ xử lí ngôn ngữ tự nhiên;
2/ Sử dụng hàm activation có tên squared ReLU
3/ Bổ sung kiến truc depthwise convs phía sau Q, K, V projection heads trong self-attention","""Primer: Searching for Efficient Transformers for Language Modeling"" là bài báo mới nhất của Google Brain (link tại đây: https://arxiv.org/pdf/2109.08668.pdf; source code tại đây: https://github.com/google-research/google-research/tree/master/primer). Một số cải tiến của Primer (một biến thể cái tiến của kiến trúc Transformer) so với các kết quả trước đây: 1/ Train nhanh 3-4 lần so với Transformer như T5 và GPT-3XL cho tác vụ xử lí ngôn ngữ tự nhiên; 2/ Sử dụng hàm activation có tên squared ReLU 3/ Bổ sung kiến truc depthwise convs phía sau Q, K, V projection heads trong self-attention",,,,,
"Một cách dẫn dắt Cross-Entropy Loss dễ hiểu, đặc biệt dành cho dân ""lười"" ngó lại toán, mà vẫn dễ hiểu.
Nếu các bạn đọc về phần Cross-Entropy (CE) trong các tài liệu ML, sẽ gặp rất nhiều các dẫn dắt từ Entropy trong Lý thuyết thông tin (và thậm chí từ Vật lý), với công thức mà những người lâu không đụng lại toán sẽ rất KHÓ HÌNH DUNG, tức là tại sao người ta lại ""móc"" đâu ra cái thứ CE đau đầu này.
Mình bắt gặp cách Dẫn Dắt trong khóa Pytorch, thấy nó Dễ Tiếp Nhận, chia sẻ với các bạn. Xin hãy lướt thứ tự từng Photo theo thứ tự để hiểu.
PS: đây là cảm nhận riêng mình khi học ML, nên có thể không đúng với các bạn. Ngoài ra mình trình bày ko được chuẩn tắc cho lắm, các bạn bỏ qua.","Một cách dẫn dắt Cross-Entropy Loss dễ hiểu, đặc biệt dành cho dân ""lười"" ngó lại toán, mà vẫn dễ hiểu. Nếu các bạn đọc về phần Cross-Entropy (CE) trong các tài liệu ML, sẽ gặp rất nhiều các dẫn dắt từ Entropy trong Lý thuyết thông tin (và thậm chí từ Vật lý), với công thức mà những người lâu không đụng lại toán sẽ rất KHÓ HÌNH DUNG, tức là tại sao người ta lại ""móc"" đâu ra cái thứ CE đau đầu này. Mình bắt gặp cách Dẫn Dắt trong khóa Pytorch, thấy nó Dễ Tiếp Nhận, chia sẻ với các bạn. Xin hãy lướt thứ tự từng Photo theo thứ tự để hiểu. PS: đây là cảm nhận riêng mình khi học ML, nên có thể không đúng với các bạn. Ngoài ra mình trình bày ko được chuẩn tắc cho lắm, các bạn bỏ qua.",,,,,
Hiện mình đang làm speech recognition trên collab. Mình đang làm đến chuyển file wav sang csv. Đây là code open source mình lấy từ github nhưng lúc mình import file wav và convert ra thì chỉ ra toàn là dãy số mà không có từ. Có thể chỉ giúp mình là mình sai ở đâu không. Mình cám ơn!,Hiện mình đang làm speech recognition trên collab. Mình đang làm đến chuyển file wav sang csv. Đây là code open source mình lấy từ github nhưng lúc mình import file wav và convert ra thì chỉ ra toàn là dãy số mà không có từ. Có thể chỉ giúp mình là mình sai ở đâu không. Mình cám ơn!,,,,,
"Để giữ gìn sự trong sáng cho tiếng việt mình xin cắt bớt một số khúc không được trong sáng và chỉ giữ lại ý chính. Cám ơn mọi người.
Có ai đã từng tiếp xúc với People Analytics chưa ạ? Cụ thể là mình sẽ có những hướng đi thế nào?
Và cám ơn chia sẻ của bạn Hoàng bên dưới ạ.",Để giữ gìn sự trong sáng cho tiếng việt mình xin cắt bớt một số khúc không được trong sáng và chỉ giữ lại ý chính. Cám ơn mọi người. Có ai đã từng tiếp xúc với People Analytics chưa ạ? Cụ thể là mình sẽ có những hướng đi thế nào? Và cám ơn chia sẻ của bạn Hoàng bên dưới ạ.,,,,,
a/c cho e hỏi cách crawl dữ liệu cổ phiếu Việt Nam với ạ. E cảm ơn ạ!,a/c cho e hỏi cách crawl dữ liệu cổ phiếu Việt Nam với ạ. E cảm ơn ạ!,,,,,
"Chào mọi người, em đang tìm hiểu về framework tensorflow vì thầy hướng dẫn của em gợi ý nên học và sử dụng cho academic, hiện tại em đang làm khóa luận đại học nên có quan tâm về sử dụng framework này cho nghiên cứu. Trước giờ em vẫn build, train và evaluate model với keras. Em có lên mạng tìm hiểu thì cũng nắm được một số ưu điểm của tensorflow so với keras về thời gian chạy, xây dựng những model với performance cao, dataset lớn, nhưng nhược điểm là khó implement, debug. Hiện tại, cái em đang muốn làm là implement lại một architecture từ 1 paper viết lại dưới dạng tensorflow, sau đó train và evaluate trên dataset cũng từ paper xem có thể reproduce được performance metrics gần giống paper không. Mục đích cuối cùng là làm sao đó, em có thể modify lại code cho architecture, thêm new data cho dataset cũ, hoặc tunning các hyperparameters ở các module trong model để làm sao cho perfomance metrics cải thiện, time và space resource được rút ngắn. Em có hai câu hỏi:
Mọi người có thể cho em ý kiến về vấn đề mà em trình bày cách tiếp cận để xây dựng model tốt hơn hoặc bằng mà em cải thiện từ paper được không ạ?
Mọi người có biết sử dụng trick để chuyển code từ keras sang thuần tensorflow không ạ.
Em cảm ơn mọi người nhiều ạ","Chào mọi người, em đang tìm hiểu về framework tensorflow vì thầy hướng dẫn của em gợi ý nên học và sử dụng cho academic, hiện tại em đang làm khóa luận đại học nên có quan tâm về sử dụng framework này cho nghiên cứu. Trước giờ em vẫn build, train và evaluate model với keras. Em có lên mạng tìm hiểu thì cũng nắm được một số ưu điểm của tensorflow so với keras về thời gian chạy, xây dựng những model với performance cao, dataset lớn, nhưng nhược điểm là khó implement, debug. Hiện tại, cái em đang muốn làm là implement lại một architecture từ 1 paper viết lại dưới dạng tensorflow, sau đó train và evaluate trên dataset cũng từ paper xem có thể reproduce được performance metrics gần giống paper không. Mục đích cuối cùng là làm sao đó, em có thể modify lại code cho architecture, thêm new data cho dataset cũ, hoặc tunning các hyperparameters ở các module trong model để làm sao cho perfomance metrics cải thiện, time và space resource được rút ngắn. Em có hai câu hỏi: Mọi người có thể cho em ý kiến về vấn đề mà em trình bày cách tiếp cận để xây dựng model tốt hơn hoặc bằng mà em cải thiện từ paper được không ạ? Mọi người có biết sử dụng trick để chuyển code từ keras sang thuần tensorflow không ạ. Em cảm ơn mọi người nhiều ạ",,,,,
"Data leakage trong machine learning
Chào mọi người, em đang làm 1 project về dữ liệu dạng bảng. Việc phân chia tập dữ liệu của e như sau: lấy 450k dữ liệu cho train_val set (thực hiện CV 5 fold), 150k cho local test set. Và hiện tại e đang phân vân 2 strategies:
Dùng quantile transform để đưa train-val về Gaussian distribution, sau đó trong quá trình kfold, thực hiện min-max scaler để chuẩn hóa dữ liệu, bỏ outlier
Trong quá trình kfold, thực hiện cả quantile transform và min-max scaler, tất nhiên là fit trên tập train và transform trên tập validation. 
Với [1], data leakage xảy ra khi thực hiện transform trước khi chia fold. Với [2], sẽ không có rủi ro do data leakage gây ra. Nhưng  khi so sánh trên cả CV và local test set, loss của [2] thấp hơn [1], vậy em sai ở đâu ạ, và theo a/c thì cách nào tốt hơn","Data leakage trong machine learning Chào mọi người, em đang làm 1 project về dữ liệu dạng bảng. Việc phân chia tập dữ liệu của e như sau: lấy 450k dữ liệu cho train_val set (thực hiện CV 5 fold), 150k cho local test set. Và hiện tại e đang phân vân 2 strategies: Dùng quantile transform để đưa train-val về Gaussian distribution, sau đó trong quá trình kfold, thực hiện min-max scaler để chuẩn hóa dữ liệu, bỏ outlier Trong quá trình kfold, thực hiện cả quantile transform và min-max scaler, tất nhiên là fit trên tập train và transform trên tập validation. Với [1], data leakage xảy ra khi thực hiện transform trước khi chia fold. Với [2], sẽ không có rủi ro do data leakage gây ra. Nhưng khi so sánh trên cả CV và local test set, loss của [2] thấp hơn [1], vậy em sai ở đâu ạ, và theo a/c thì cách nào tốt hơn",,,,,
"Xin chào các bạn mình là Nguyễn Hưng Quang Khải, sinh viên năm 4 khoa Toán Ứng Dụng trường ĐH Quốc Tế - ĐHQG TPHCM.

Hôm nay trong bài viết medium mình đưa đến các bạn, chủ đề mình viết là về Natural Language Processing. Trong bài viết này :
Mình đã cập nhật đầy đủ các bước cơ bản để xử lý text data
Fitting các mô hình ML 
Sử dụng Ensemble Model để kết hợp sức mạnh các ML models
Mình đã ứng dụng các kĩ thuật trên để sử dụng vào project phân loại review khen/chê của nhà hàng 

Bài viết này mình đã cố gắng viết dễ hiểu, chi tiết, và những người beginners trong lĩnh vực NLP đều có thể hiểu được 

Hi vọng bài viết này có ích cho các bạn, và đừng quên share/like và clap hands trên medium để ủng hộ mình nữa nha !!!!

https://medium.com/@liangnguyen612/sentiment-analysis-in-python-81-accuracy-ab5d694b7ef8","Xin chào các bạn mình là Nguyễn Hưng Quang Khải, sinh viên năm 4 khoa Toán Ứng Dụng trường ĐH Quốc Tế - ĐHQG TPHCM. Hôm nay trong bài viết medium mình đưa đến các bạn, chủ đề mình viết là về Natural Language Processing. Trong bài viết này : Mình đã cập nhật đầy đủ các bước cơ bản để xử lý text data Fitting các mô hình ML Sử dụng Ensemble Model để kết hợp sức mạnh các ML models Mình đã ứng dụng các kĩ thuật trên để sử dụng vào project phân loại review khen/chê của nhà hàng Bài viết này mình đã cố gắng viết dễ hiểu, chi tiết, và những người beginners trong lĩnh vực NLP đều có thể hiểu được Hi vọng bài viết này có ích cho các bạn, và đừng quên share/like và clap hands trên medium để ủng hộ mình nữa nha !!!! https://medium.com/@liangnguyen612/sentiment-analysis-in-python-81-accuracy-ab5d694b7ef8",,,,,
#book,,#book,,,,
"Nhân dịp đang tìm hiểu vấn đề này, em mạnh dạn gửi tới anh em newbie một bài toán mới về CBIR - Content-based Image Retrieval.
Chú ý: Đây là ở dạng cơ bản nhất, để anh em newbie hiểu vấn đề. Để dùng thực tế còn phải nghiên cứu nhiều nữa. Mong giúp được các bạn newbie!
Mong ad duyệt bài!","Nhân dịp đang tìm hiểu vấn đề này, em mạnh dạn gửi tới anh em newbie một bài toán mới về CBIR - Content-based Image Retrieval. Chú ý: Đây là ở dạng cơ bản nhất, để anh em newbie hiểu vấn đề. Để dùng thực tế còn phải nghiên cứu nhiều nữa. Mong giúp được các bạn newbie! Mong ad duyệt bài!",,,,,
"Chào mn, em là sv mới bập bẽ học về AI trên trường, em mới học đc 2 buổi nhưng phải dky BTL. Em định làm về cái dự báo thời tiết hoặc dự báo cái gì đó thì a/c trong nhóm có ai có tài liệu hữu ích không cho em tham khảo với ah","Chào mn, em là sv mới bập bẽ học về AI trên trường, em mới học đc 2 buổi nhưng phải dky BTL. Em định làm về cái dự báo thời tiết hoặc dự báo cái gì đó thì a/c trong nhóm có ai có tài liệu hữu ích không cho em tham khảo với ah",,,,,
"Deep learning for computer vision with python, Adrian Rosebrock
https://drive.google.com/uc?export=download&id=1H33SjUAYMzfwD8PM3UR1pHiCij4pB2KZ","Deep learning for computer vision with python, Adrian Rosebrock https://drive.google.com/uc?export=download&id=1H33SjUAYMzfwD8PM3UR1pHiCij4pB2KZ",,,,,
Minh xin chia sẻ lại phần thuyết trình và trình bày của các đội thi TOP 3 cuộc thi AICOVIDVN 115M Challenge.,Minh xin chia sẻ lại phần thuyết trình và trình bày của các đội thi TOP 3 cuộc thi AICOVIDVN 115M Challenge.,,,,,
"Mọi người cho em hỏi, dataset như nào thì thích hợp để train model tacotron2 ạ, em có theo 1 bài trên viblo tải file mp3 truyện ma nhưng output khá fail ạ. tks","Mọi người cho em hỏi, dataset như nào thì thích hợp để train model tacotron2 ạ, em có theo 1 bài trên viblo tải file mp3 truyện ma nhưng output khá fail ạ. tks",,,,,
"Thời gian gần đây mình dành chút thời gian để phát triển thư viện TabML cho dữ liệu dạng bảng. Chủ yếu là trau đồi thêm kiến thức lập trình cũng như cập nhật các thư viện ML liên quan.
https://github.com/tiepvupsu/tabml
Thư viện TabML này có một số chức năng nổi bật:
1. Có một hệ thống quản lý đặc trưng riêng với mỗi đặc trưng được quản lý trong một class. Sự phụ thuộc giữa các đặc trưng được lưu trong một file config dạng protobuf. Việc này giúp việc quản lý dễ dàng hơn.
2. Mỗi experiment được quản lý bởi một file config dưới dạng một protobuf. Người dùng chỉ cần chỉ định loại mô hình, nhãn, đặc trưng. Config, log huấn luyện và mô hình được tự động lưu lại để sử dụng về sau.
3. Hỗ trợ phân tích mô hình trên nhiều hạng mục của dữ liệu với nhiều metrics. Hỗ trợ hiển thị feature importance ngay trong log.
4. Tích hợp với MLflow để quản lý từng thí nghiệm.
5. Sau khi huấn luyện mô hình xong, người dùng chỉ cần đưa dữ liệu thô chưa qua xử lý vào. Pipeline sẽ tự động sinh đặc trưng tương ứng để mô hình dự đoán.
6. ...
Thư viện này trước đây mình và một nhóm kỹ sư ở VNPT phát triển nội bộ cho một cuộc thi trên Kaggle và dành thứ hạng cao. Hiện tại mình muốn phát triển thêm và rất mong có thêm nhiều người dùng cùng góp ý.
Có một sự kiện thúc đẩy mình tách riêng bộ quản lý đặc trưng (FeatureManager) ra khỏi pipeline huấn luyện. Mình tham gia một dự án có hai nhóm data science và data engineer làm việc gần như độc lập. Nhóm DS làm mọi thứ trên notebook và pandas có kết quả ok. Nhóm này tạo ra khoảng hơn 100 đặc trưng phụ thuộc chồng chéo và cập nhật liên tục. Nhóm DE **nhìn** code đó và chuyển sang pyspark chạy với dữ liệu lớn. Kết quả là hai mô hinh chạy khác hẳn nhau mặc dù *mắt thường* không phát hiển ra điểm khác biệt. Mình đã nghĩ ngay đến việc phải tách từng feature ra và tính toán theo hai cách pandas và pyspark rồi tạo unit test so sánh kết quả sau từng bước. Sau đó thì phần FeatureManger trong thư viện này ra đời.","Thời gian gần đây mình dành chút thời gian để phát triển thư viện TabML cho dữ liệu dạng bảng. Chủ yếu là trau đồi thêm kiến thức lập trình cũng như cập nhật các thư viện ML liên quan. https://github.com/tiepvupsu/tabml Thư viện TabML này có một số chức năng nổi bật: 1. Có một hệ thống quản lý đặc trưng riêng với mỗi đặc trưng được quản lý trong một class. Sự phụ thuộc giữa các đặc trưng được lưu trong một file config dạng protobuf. Việc này giúp việc quản lý dễ dàng hơn. 2. Mỗi experiment được quản lý bởi một file config dưới dạng một protobuf. Người dùng chỉ cần chỉ định loại mô hình, nhãn, đặc trưng. Config, log huấn luyện và mô hình được tự động lưu lại để sử dụng về sau. 3. Hỗ trợ phân tích mô hình trên nhiều hạng mục của dữ liệu với nhiều metrics. Hỗ trợ hiển thị feature importance ngay trong log. 4. Tích hợp với MLflow để quản lý từng thí nghiệm. 5. Sau khi huấn luyện mô hình xong, người dùng chỉ cần đưa dữ liệu thô chưa qua xử lý vào. Pipeline sẽ tự động sinh đặc trưng tương ứng để mô hình dự đoán. 6. ... Thư viện này trước đây mình và một nhóm kỹ sư ở VNPT phát triển nội bộ cho một cuộc thi trên Kaggle và dành thứ hạng cao. Hiện tại mình muốn phát triển thêm và rất mong có thêm nhiều người dùng cùng góp ý. Có một sự kiện thúc đẩy mình tách riêng bộ quản lý đặc trưng (FeatureManager) ra khỏi pipeline huấn luyện. Mình tham gia một dự án có hai nhóm data science và data engineer làm việc gần như độc lập. Nhóm DS làm mọi thứ trên notebook và pandas có kết quả ok. Nhóm này tạo ra khoảng hơn 100 đặc trưng phụ thuộc chồng chéo và cập nhật liên tục. Nhóm DE **nhìn** code đó và chuyển sang pyspark chạy với dữ liệu lớn. Kết quả là hai mô hinh chạy khác hẳn nhau mặc dù *mắt thường* không phát hiển ra điểm khác biệt. Mình đã nghĩ ngay đến việc phải tách từng feature ra và tính toán theo hai cách pandas và pyspark rồi tạo unit test so sánh kết quả sau từng bước. Sau đó thì phần FeatureManger trong thư viện này ra đời.",,,,,
"Anh chị em vui lòng chỉ phương hướng bài toán optimization này giúp mình với.
Giả sử có N người đang cần được phân phát quà, và có V mức quà khá nhau với giá trị từ thấp đến cao (kể cả giá trị bằng 0, tức là không phát quà). Mỗi người chỉ được nhận 1 món quà hoặc ko có quà. Và mình có dữ liệu khảo sát về mức độ niềm vui dưới dạng matrix P[n][v] (0<=p<=1) của mỗi người thứ n đối với từng món quà v.
Làm sao để formulate bài toán để maximize tổng niềm vui, trong khi giới hạn tổng giá trị quà? Nếu được nói cho mình cách formulate dưới dạng matrix nhé. Hoặc cho mình một số keywords để tìm hiểu dạng bài toán này.
Cảm ơn cả nhà.","Anh chị em vui lòng chỉ phương hướng bài toán optimization này giúp mình với. Giả sử có N người đang cần được phân phát quà, và có V mức quà khá nhau với giá trị từ thấp đến cao (kể cả giá trị bằng 0, tức là không phát quà). Mỗi người chỉ được nhận 1 món quà hoặc ko có quà. Và mình có dữ liệu khảo sát về mức độ niềm vui dưới dạng matrix P[n][v] (0<=p<=1) của mỗi người thứ n đối với từng món quà v. Làm sao để formulate bài toán để maximize tổng niềm vui, trong khi giới hạn tổng giá trị quà? Nếu được nói cho mình cách formulate dưới dạng matrix nhé. Hoặc cho mình một số keywords để tìm hiểu dạng bài toán này. Cảm ơn cả nhà.",,,,,
"Câu hỏi về việc xây dựng đặc trưng cho dữ liệu dạng bảng.
Làm việc với dữ liệu dạng bảng thì phần tạo đặc trưng thường quan trọng hơn phần xây dựng mô hình. Tuy nhiên xây dựng đặc trưng thường có rất nhiều bước nhỏ lẻ tỉ mỉ:
1. Mỗi đặc trưng được xử lý (clean, impute) một cách khác nhau.
2. Nhiều đặc trưng phụ thuộc nhau, ví dụ từ dữ liệu tuổi ở dạng numerical có thể chuyển về dạng categorical dạng ""trẻ em"", ""thanh niên"", ""trung niên"", ""người già"".
3. Cách xây dựng đặc trưng có thể thay đổi với các tham số khác nhau. Ví dụ các boundary để xác định nhóm tuổi.
4. Dữ liệu lớn và không muốn tính toán lại toàn bộ đặc trưng mà chỉ muốn cập nhật một số. Khi cập nhật (sửa, xóa) một đặc trưng thì cũng phải cập nhật các đặc trưng khác phụ thuộc vào nó.
5. Team lớn, mỗi thành viên có thể tập trung vào một nhóm đặc trưng khác nhau và làm đồng thời. Làm thể nào để hạn chế code conflict.
...
Thường thì mình thấy các project mẫu làm các bước này một cách tự do gồm nhiều bước xử lý pandas tràn lan dẫn đến khó quản lý. Mình không nghĩ đây là lựa chọn tốt trong production.
Các bạn thường dùng tool nào để quản lý những tác vị này?","Câu hỏi về việc xây dựng đặc trưng cho dữ liệu dạng bảng. Làm việc với dữ liệu dạng bảng thì phần tạo đặc trưng thường quan trọng hơn phần xây dựng mô hình. Tuy nhiên xây dựng đặc trưng thường có rất nhiều bước nhỏ lẻ tỉ mỉ: 1. Mỗi đặc trưng được xử lý (clean, impute) một cách khác nhau. 2. Nhiều đặc trưng phụ thuộc nhau, ví dụ từ dữ liệu tuổi ở dạng numerical có thể chuyển về dạng categorical dạng ""trẻ em"", ""thanh niên"", ""trung niên"", ""người già"". 3. Cách xây dựng đặc trưng có thể thay đổi với các tham số khác nhau. Ví dụ các boundary để xác định nhóm tuổi. 4. Dữ liệu lớn và không muốn tính toán lại toàn bộ đặc trưng mà chỉ muốn cập nhật một số. Khi cập nhật (sửa, xóa) một đặc trưng thì cũng phải cập nhật các đặc trưng khác phụ thuộc vào nó. 5. Team lớn, mỗi thành viên có thể tập trung vào một nhóm đặc trưng khác nhau và làm đồng thời. Làm thể nào để hạn chế code conflict. ... Thường thì mình thấy các project mẫu làm các bước này một cách tự do gồm nhiều bước xử lý pandas tràn lan dẫn đến khó quản lý. Mình không nghĩ đây là lựa chọn tốt trong production. Các bạn thường dùng tool nào để quản lý những tác vị này?",,,,,
vnquant package version 0.0.3,vnquant package version 0.0.3,,,,,
"Chào các bạn!
Xin chia sẻ với các masters Catboost một trải nghiệm không biết nên gọi tên như thế nào. Nếu ai đó đã gặp trường hợp tương tự thì mách mình cách giải quyết với nhé. Xin cảm ơn trước rất nhiều.
Số là mình có dùng Catboost cho bài toán phân loại nhị phân. Đầu vào có khoảng 11k điểm dữ liệu với 31 fields: 29 float + 2 categorical. Mình train model với toàn bộ tham số để ở mặc định, chỉ cài đặt random_state và chỉ định index của categorical features. Kết quả không cao lắm nhưng chấp nhận được. Mình có thử trên máy với CPU i7 và i5, thấy kết quả dự đoán probability giống nhau, chỉ khác nhau cỡ 10^-16, nghĩ chắc do tính toán gì đó. Tới khi chạy trên CPU Xeon, hoặc AMD thì tá hỏa kết quả sai lệch rất nhiều, không biết chuyện gì đã xảy ra, mặc dù môi trường cài đặt y chang nhau. Tức là không tái lập kết quả được. Mình có tham khảo github của Catboost thì được gợi ý hãy disable cái AVX2 branch và build lại CatBooost từ source code C++.
Cái này nếu làm trong cùng đội nhóm thì không sao, nhưng nếu triển khai cho khách hàng và họ nâng cấp phần cứng, thay đổi hệ điều hành thì chắc là có chuyện. Vậy nên mình không đi theo cách này mà tìm cách thay đổi các hyper-parameters. Tới đây thì kết quả có thể hòm hòm tái lập được bằng cách tăng chiều sâu của cây (depth) từ mặc định 6 lên 8 (hoặc lớn hơn) và giảm tỉ số colsample_bylevel/rms xuống 0.5 (và thấp hơn) (tương đương việc không lấy nhiều hơn nửa số cột mỗi lần build tree tiếp theo). Cách thứ nhất thì bị overfit trên tập train, cách thứ 2 thì không.
Thật sự là cũng chưa đoán được chuyện gì đã xảy ra với các con chip và tập lệnh hướng dẫn (AVX) của nó. Hoặc chuyện gì đó đã xảy ra với dữ liệu đầu vào. Nhưng chắc không quy chụp cho dữ liệu được, vì dữ liệu có ảnh hưởng xấu thì cũng xấu như nhau với các con chip chứ?","Chào các bạn! Xin chia sẻ với các masters Catboost một trải nghiệm không biết nên gọi tên như thế nào. Nếu ai đó đã gặp trường hợp tương tự thì mách mình cách giải quyết với nhé. Xin cảm ơn trước rất nhiều. Số là mình có dùng Catboost cho bài toán phân loại nhị phân. Đầu vào có khoảng 11k điểm dữ liệu với 31 fields: 29 float + 2 categorical. Mình train model với toàn bộ tham số để ở mặc định, chỉ cài đặt random_state và chỉ định index của categorical features. Kết quả không cao lắm nhưng chấp nhận được. Mình có thử trên máy với CPU i7 và i5, thấy kết quả dự đoán probability giống nhau, chỉ khác nhau cỡ 10^-16, nghĩ chắc do tính toán gì đó. Tới khi chạy trên CPU Xeon, hoặc AMD thì tá hỏa kết quả sai lệch rất nhiều, không biết chuyện gì đã xảy ra, mặc dù môi trường cài đặt y chang nhau. Tức là không tái lập kết quả được. Mình có tham khảo github của Catboost thì được gợi ý hãy disable cái AVX2 branch và build lại CatBooost từ source code C++. Cái này nếu làm trong cùng đội nhóm thì không sao, nhưng nếu triển khai cho khách hàng và họ nâng cấp phần cứng, thay đổi hệ điều hành thì chắc là có chuyện. Vậy nên mình không đi theo cách này mà tìm cách thay đổi các hyper-parameters. Tới đây thì kết quả có thể hòm hòm tái lập được bằng cách tăng chiều sâu của cây (depth) từ mặc định 6 lên 8 (hoặc lớn hơn) và giảm tỉ số colsample_bylevel/rms xuống 0.5 (và thấp hơn) (tương đương việc không lấy nhiều hơn nửa số cột mỗi lần build tree tiếp theo). Cách thứ nhất thì bị overfit trên tập train, cách thứ 2 thì không. Thật sự là cũng chưa đoán được chuyện gì đã xảy ra với các con chip và tập lệnh hướng dẫn (AVX) của nó. Hoặc chuyện gì đó đã xảy ra với dữ liệu đầu vào. Nhưng chắc không quy chụp cho dữ liệu được, vì dữ liệu có ảnh hưởng xấu thì cũng xấu như nhau với các con chip chứ?",,,,,
"Hi mọi người,
Em được supervisor yêu cầu học Tensorflow và Unet để đọc hiểu code, reproduce và làm một số bài toán đơn giản ạ nhưng em chưa biết bắt đầu từ đâu ạ. Em có biết một chút về Machine learning ạ. Mọi người có thể recommend cho em một số courses để học cái này không ạ?","Hi mọi người, Em được supervisor yêu cầu học Tensorflow và Unet để đọc hiểu code, reproduce và làm một số bài toán đơn giản ạ nhưng em chưa biết bắt đầu từ đâu ạ. Em có biết một chút về Machine learning ạ. Mọi người có thể recommend cho em một số courses để học cái này không ạ?",,,,,
"Chào mọi người
Hiện tại mình là du học sinh ở Úc, dự định 1 là đi làm 2 là đăng ký học master of AI
Mình muốn hỏi theo kinh nghiệm các bác ở đây thì AI có thể tự học được ko?
Vì mình rất muốn học AI mà nếu đi làm và đi học trong trường cùng 1 lúc chỉ sợ học ko đến nơi mà tốn tiền","Chào mọi người Hiện tại mình là du học sinh ở Úc, dự định 1 là đi làm 2 là đăng ký học master of AI Mình muốn hỏi theo kinh nghiệm các bác ở đây thì AI có thể tự học được ko? Vì mình rất muốn học AI mà nếu đi làm và đi học trong trường cùng 1 lúc chỉ sợ học ko đến nơi mà tốn tiền",,,,,
"On-device ML đang là xu thế. Người dùng sẽ có nhiều quyền riêng tư hơn khi dữ liệu của họ không chuyển qua chuyển lại tới cloud. Bù lại, họ sẽ gánh chi phí tính toán cho các mô hình ML bằng cách mua các thiết bị hỗ trợ tốt các tính toán liên quan tới ML.","On-device ML đang là xu thế. Người dùng sẽ có nhiều quyền riêng tư hơn khi dữ liệu của họ không chuyển qua chuyển lại tới cloud. Bù lại, họ sẽ gánh chi phí tính toán cho các mô hình ML bằng cách mua các thiết bị hỗ trợ tốt các tính toán liên quan tới ML.",,,,,
"Xin chào mọi người, em muốn làm 1 project về Machine learning là ""phân loại tình trạng của bệnh nhân mắc covid-19, sau đó đưa ra giải pháp chữa bênh hợp lí"". Em cũng mới bắt đầu học ML nên không rõ ràng lắm, mọi người cho em hỏi là có thể tìm được dataset về covid ở đâu vậy ạ. Em cảm ơn nhiều 🥰🥰","Xin chào mọi người, em muốn làm 1 project về Machine learning là ""phân loại tình trạng của bệnh nhân mắc covid-19, sau đó đưa ra giải pháp chữa bênh hợp lí"". Em cũng mới bắt đầu học ML nên không rõ ràng lắm, mọi người cho em hỏi là có thể tìm được dataset về covid ở đâu vậy ạ. Em cảm ơn nhiều",,,,,
"[AI Share – Machine Learning Cheat Sheet]
Một cheat sheet về Machine Learning chỉ dài 5 trang tập trung vào các thuật toán phổ biến nhất. Cheat sheet tóm tắt các thuật toán Machine Learning, các ưu điểm và các trường hợp sử dụng. Cheat sheet này được lấy cảm hứng từ cuốn sách và các bài viết hay nhất của MachineLearningMastery và HackingNote.","[AI Share – Machine Learning Cheat Sheet] Một cheat sheet về Machine Learning chỉ dài 5 trang tập trung vào các thuật toán phổ biến nhất. Cheat sheet tóm tắt các thuật toán Machine Learning, các ưu điểm và các trường hợp sử dụng. Cheat sheet này được lấy cảm hứng từ cuốn sách và các bài viết hay nhất của MachineLearningMastery và HackingNote.",,,,,
"Chào anh em, mình đang tìm hiểu về Object Tracking cho điện thoại Android.
Nhưng mình thấy chỉ có sử dụng được trên máy tính nhúng hoặc máy tính.
Bạn nào biết ví dụ nào mà có thể dùng Object Tracking trên điện thoại Android thì cho mình xin để học hỏi.
Cảm ơn,","Chào anh em, mình đang tìm hiểu về Object Tracking cho điện thoại Android. Nhưng mình thấy chỉ có sử dụng được trên máy tính nhúng hoặc máy tính. Bạn nào biết ví dụ nào mà có thể dùng Object Tracking trên điện thoại Android thì cho mình xin để học hỏi. Cảm ơn,",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 3/2021 vào trong comment của post này.
Hoặc https://forum.machinelearningcoban.com/c/jobs-events
Chúc các bạn tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 3/2021 vào trong comment của post này. Hoặc https://forum.machinelearningcoban.com/c/jobs-events Chúc các bạn tháng mới vui vẻ.",,,,,
"Chào các anh chị, em đang làm một task liên quan đến việc thu thập tất cả các địa điểm công cộng tại Việt Nam, sau đó nhóm chúng lại thành các mục (bệnh viện, trường học, nhà hàng,...).
Em đang tính sử dụng dữ liệu bản đồ từ OpenStreetMap để làm việc này. Anh chị cho em hỏi có cách nào giải quyết vấn đề này tốt hơn không ạ?","Chào các anh chị, em đang làm một task liên quan đến việc thu thập tất cả các địa điểm công cộng tại Việt Nam, sau đó nhóm chúng lại thành các mục (bệnh viện, trường học, nhà hàng,...). Em đang tính sử dụng dữ liệu bản đồ từ OpenStreetMap để làm việc này. Anh chị cho em hỏi có cách nào giải quyết vấn đề này tốt hơn không ạ?",,,,,
"Hi mọi người, đợt nghỉ lễ 2/9 này mình có thử làm 1 project cá nhân nho nhỏ mà khác với những domain mình làm hằng ngày, là thử train 1 model speech separation. Nhận đầu vào là 1 mixed speech của 2 người, output model trả ra là 2 separated speech cua 2 người đó.
Ban đầu search google thì mình thấy có rất nhiều những phương pháp khác nhau, và đa số là khá phức tạp để có thể quickly implement và có được feeling về bài toán. Có paper thì yêu cầu tập data đến vài trăm GB, có paper thì idea chưa đủ chi tiết để mình hiện thực, có những paper mặc dù đã có implementation trên github nhưng nhiều người vẫn không thể train ra được kết quả tốt.
Sau 4 ngày, mình có thử đọc 1 số paper và chọn ra tập data đơn giản nhất, những idea đơn giản nhất, sơ khai nhất để thử implement và train thử. Qua 1 qua trinh experiment thì cũng train được 1 model có kết quả cũng tạm chấp nhận được.
Mình muốn share link github bên dưới cho mấy bạn nào muốn thử train nhé. Cho mình 1 star nếu mấy bạn cảm thấy hữu ích.
Thời gian training model tầm 2-3 HOURS với 1 normal GPU","Hi mọi người, đợt nghỉ lễ 2/9 này mình có thử làm 1 project cá nhân nho nhỏ mà khác với những domain mình làm hằng ngày, là thử train 1 model speech separation. Nhận đầu vào là 1 mixed speech của 2 người, output model trả ra là 2 separated speech cua 2 người đó. Ban đầu search google thì mình thấy có rất nhiều những phương pháp khác nhau, và đa số là khá phức tạp để có thể quickly implement và có được feeling về bài toán. Có paper thì yêu cầu tập data đến vài trăm GB, có paper thì idea chưa đủ chi tiết để mình hiện thực, có những paper mặc dù đã có implementation trên github nhưng nhiều người vẫn không thể train ra được kết quả tốt. Sau 4 ngày, mình có thử đọc 1 số paper và chọn ra tập data đơn giản nhất, những idea đơn giản nhất, sơ khai nhất để thử implement và train thử. Qua 1 qua trinh experiment thì cũng train được 1 model có kết quả cũng tạm chấp nhận được. Mình muốn share link github bên dưới cho mấy bạn nào muốn thử train nhé. Cho mình 1 star nếu mấy bạn cảm thấy hữu ích. Thời gian training model tầm 2-3 HOURS với 1 normal GPU",,,,,
"Chào các anh, chị, em trong grp
Mình đang làm 1 ứng dụng liên quan đến dự đoán hiển thị thông số ca bệnh trong dịch covid 19 tại Việt Nam theo tỉnh.
Mình tìm trên mạng mà không có nguồn nào có bộ data theo ngày cả (trang của bộ y tế chỉ có thể lấy số liệu 1 ngày hiện tại).
Anh, chị, em nào có biết nguồn nào thì cho mình xin thông tin với.","Chào các anh, chị, em trong grp Mình đang làm 1 ứng dụng liên quan đến dự đoán hiển thị thông số ca bệnh trong dịch covid 19 tại Việt Nam theo tỉnh. Mình tìm trên mạng mà không có nguồn nào có bộ data theo ngày cả (trang của bộ y tế chỉ có thể lấy số liệu 1 ngày hiện tại). Anh, chị, em nào có biết nguồn nào thì cho mình xin thông tin với.",,,,,
"Các bác cho em hỏi sao em load thư viện mnist mà lần nào n cũng ghi là không tìm được nhỉ , trong khi em tải 4 file của mnist trên trang chủ của họ rồi giải nén và đem vào thư mục mnist kia rồi . em thử đi thử lại mà không được","Các bác cho em hỏi sao em load thư viện mnist mà lần nào n cũng ghi là không tìm được nhỉ , trong khi em tải 4 file của mnist trên trang chủ của họ rồi giải nén và đem vào thư mục mnist kia rồi . em thử đi thử lại mà không được",,,,,
Em mới học ML trên blog của anh Tiệp. Python không load đươc dữ liệu MNIST và cứ báo lỗi như dưới. mọi người cho em hỏi cách khắc phục với ạ. em loay hoay cả ngày hôm qua mà vẫn chưa fix đc. dưới đây có cả ảnh thư muc em lưu data ạ,Em mới học ML trên blog của anh Tiệp. Python không load đươc dữ liệu MNIST và cứ báo lỗi như dưới. mọi người cho em hỏi cách khắc phục với ạ. em loay hoay cả ngày hôm qua mà vẫn chưa fix đc. dưới đây có cả ảnh thư muc em lưu data ạ,,,,,
Mọi người giải thích cho mình máy dòng đỏ ở dưới vs. Help me😁,Mọi người giải thích cho mình máy dòng đỏ ở dưới vs. Help me,,,,,
"[AI Share]
Top 50 Machine Learning Interview Questions & Answers (2021)
Mọi người lưu lại để ôn nhé <3.
Link pdf: https://drive.google.com/file/d/1-U8JC2KtAD-ryHO8BuaaZ9VRT9qOE6E_/view?usp=sharing
Nguồn: https://www.guru99.com/machine-learning-interview-questions.html",[AI Share] Top 50 Machine Learning Interview Questions & Answers (2021) Mọi người lưu lại để ôn nhé <3. Link pdf: https://drive.google.com/file/d/1-U8JC2KtAD-ryHO8BuaaZ9VRT9qOE6E_/view?usp=sharing Nguồn: https://www.guru99.com/machine-learning-interview-questions.html,,,,,
Link hay cho bạn học AI và Robot,Link hay cho bạn học AI và Robot,,,,,
"Khi Newbie cũng tập tành XGBoost. Mong giúp được các bạn mới học một chút cơ bản nhất về món này.
Nghỉ 2/9 mà lại dính Dịch COVID nên tranh thủ gửi đến anh em video về XGBooot. Thuật toán thần thánh rất hay dành chiến thắng trong các cuộc thi về AI, gần đây nhất là Cuộc thi chẩn đoán tiếng ho mùa COVID.
Mong admin duyệt bài!","Khi Newbie cũng tập tành XGBoost. Mong giúp được các bạn mới học một chút cơ bản nhất về món này. Nghỉ 2/9 mà lại dính Dịch COVID nên tranh thủ gửi đến anh em video về XGBooot. Thuật toán thần thánh rất hay dành chiến thắng trong các cuộc thi về AI, gần đây nhất là Cuộc thi chẩn đoán tiếng ho mùa COVID. Mong admin duyệt bài!",,,,,
"Cần giúp đỡ để lưu lại giọng nói của ông em
Ông em giờ yếu lắm rồi, chắc chỉ còn sống được vài năm nữa thôi là tốt lắm rồi. Em không biết có cách nào dùng DL để có thể chuyển đổi giọng nói của ông em thành trợ lý ảo được không nhỉ mọi người? Hầu hết các projectem làm đều về CV nên em không biết gì về NLP với audio ạ.
Nếu mọi người k giúp được làm ơn không thả emoji haha ạ. Em cảm ơn mọi người nhiều","Cần giúp đỡ để lưu lại giọng nói của ông em Ông em giờ yếu lắm rồi, chắc chỉ còn sống được vài năm nữa thôi là tốt lắm rồi. Em không biết có cách nào dùng DL để có thể chuyển đổi giọng nói của ông em thành trợ lý ảo được không nhỉ mọi người? Hầu hết các projectem làm đều về CV nên em không biết gì về NLP với audio ạ. Nếu mọi người k giúp được làm ơn không thả emoji haha ạ. Em cảm ơn mọi người nhiều",,,,,
"Chào mọi người!
Em đang có một bài toán
Dự đoán ra một khoảng giá trị từ A -> B (ví dụ 80->130, 99-100...), các khoảng này có biên độ không cố định.
Các label sẽ là các cụm dữ liệu có các khảng bằng nhau (ví dụ như 0-100, 101-200, 201-300) Các cụm này cố định, đã có từ trước, (chỉ có 3 cụm)
Vì thế nên một giá trị X sẽ có thể nhận nhiều Y (Theo ví dụ thì mẫu dữ liệu cho ra KQ 80-130 sẽ nằm trong 2 khoảng là 0-100 và 101-200)
Bây giờ có cách nào để đánh giá độ tin cậy của model không ạ. Ví dụ như nằm trên khoảng một là 20, nằm khoảng 2 là 30 thì khoảng 2 sẽ có độ tin cậy cao hơn.
Em cảm ơn mọi người.","Chào mọi người! Em đang có một bài toán Dự đoán ra một khoảng giá trị từ A -> B (ví dụ 80->130, 99-100...), các khoảng này có biên độ không cố định. Các label sẽ là các cụm dữ liệu có các khảng bằng nhau (ví dụ như 0-100, 101-200, 201-300) Các cụm này cố định, đã có từ trước, (chỉ có 3 cụm) Vì thế nên một giá trị X sẽ có thể nhận nhiều Y (Theo ví dụ thì mẫu dữ liệu cho ra KQ 80-130 sẽ nằm trong 2 khoảng là 0-100 và 101-200) Bây giờ có cách nào để đánh giá độ tin cậy của model không ạ. Ví dụ như nằm trên khoảng một là 20, nằm khoảng 2 là 30 thì khoảng 2 sẽ có độ tin cậy cao hơn. Em cảm ơn mọi người.",,,,,
FYI!,FYI!,,,,,
"Dear anh chị,
Em mới tìm hiểu về. Ml với phương pháp random-forest, decision tree. Có một số thông số như Max_Depth,Max_Features, Min_samples_split, Random_state em ko rõ. Nhờ anh chị giải thích giúp em với ạ. Nếu kèo ví dụ thì tốt ạ.
Em xin cảm ơn","Dear anh chị, Em mới tìm hiểu về. Ml với phương pháp random-forest, decision tree. Có một số thông số như Max_Depth,Max_Features, Min_samples_split, Random_state em ko rõ. Nhờ anh chị giải thích giúp em với ạ. Nếu kèo ví dụ thì tốt ạ. Em xin cảm ơn",,,,,
"Em chào mọi người ạ
Em có học về CNN thì hiểu tích chập là trượt một kernel được khởi tạo ngẫu nhiên trên ảnh input đầu vào. Nhưng em đọc 1 bài báo thì họ giải thích 1 phép tích chập 2D bao gồm 2 bước như hình mà em không hiểu sampling với Grid R trong ảnh là như thế nào cho đúng. Rất mong mọi người giúp đỡ ạ.
Em cảm ơn mọi người",Em chào mọi người ạ Em có học về CNN thì hiểu tích chập là trượt một kernel được khởi tạo ngẫu nhiên trên ảnh input đầu vào. Nhưng em đọc 1 bài báo thì họ giải thích 1 phép tích chập 2D bao gồm 2 bước như hình mà em không hiểu sampling với Grid R trong ảnh là như thế nào cho đúng. Rất mong mọi người giúp đỡ ạ. Em cảm ơn mọi người,,,,,
"Em chào anh/chị.
Hiện tại em đang muốn mô phỏng lại kết quả thí nghiệm của một paper về Federated Learning kết hợp với một chút Meta Learning. Thầy em có yêu cầu phải ước lượng được cấu hình máy dùng trong training (RAM, disk, GPU, CPU) mà em thì chưa làm việc này bao giờ :<. Mọi người có kinh nghiệm về lĩnh vực này có thể cho em lời khuyên về việc mình sẽ estimate như thế nào được không ạ?
Em cảm ơn mọi người nhiều ạ.
Link paper: https://arxiv.org/abs/1802.07876
Hình đính kèm là kịch thước dữ liệu được dùng trong paper.","Em chào anh/chị. Hiện tại em đang muốn mô phỏng lại kết quả thí nghiệm của một paper về Federated Learning kết hợp với một chút Meta Learning. Thầy em có yêu cầu phải ước lượng được cấu hình máy dùng trong training (RAM, disk, GPU, CPU) mà em thì chưa làm việc này bao giờ :<. Mọi người có kinh nghiệm về lĩnh vực này có thể cho em lời khuyên về việc mình sẽ estimate như thế nào được không ạ? Em cảm ơn mọi người nhiều ạ. Link paper: https://arxiv.org/abs/1802.07876 Hình đính kèm là kịch thước dữ liệu được dùng trong paper.",,,,,
"Hi, chào mọi người, mình có mong muốn build một model để đánh giá chất lượng một đoạn tín hiệu PPG đưa vào dựa trên data MIMIC (mỗi sample là một chuỗi tín hiệu timeseries gồm nhiều chu kì có cả tín hiệu tốt và xấu). Thì cho mình hỏi có cách nào theo hướng unsuperivised learning cho bài toán này không nhỉ! Mình cảm ơn nhiều ạ!","Hi, chào mọi người, mình có mong muốn build một model để đánh giá chất lượng một đoạn tín hiệu PPG đưa vào dựa trên data MIMIC (mỗi sample là một chuỗi tín hiệu timeseries gồm nhiều chu kì có cả tín hiệu tốt và xấu). Thì cho mình hỏi có cách nào theo hướng unsuperivised learning cho bài toán này không nhỉ! Mình cảm ơn nhiều ạ!",,,,,
"#bayesian_neural_network
Chào mọi người
Em đang tìm hiểu về Bayesian neural networks, đây là phần keras implementation (https://keras.io/examples/keras_recipes/bayesian_neural_networks/) nhưng đang thắc mắc chỗ prior và posterior (ở mục 2 : Experiment 2: Bayesian neural network (BNN) ) như trên hình.
Em không hiểu 2 tham số kernel_size và bias_size ở đây là gì và từ đâu ra. Phần gọi prior và posterior ở hàm create_bnn_model em cũng không hiểu.
Mong mọi người giải thích. Em cảm ơn.","Chào mọi người Em đang tìm hiểu về Bayesian neural networks, đây là phần keras implementation (https://keras.io/examples/keras_recipes/bayesian_neural_networks/) nhưng đang thắc mắc chỗ prior và posterior (ở mục 2 : Experiment 2: Bayesian neural network (BNN) ) như trên hình. Em không hiểu 2 tham số kernel_size và bias_size ở đây là gì và từ đâu ra. Phần gọi prior và posterior ở hàm create_bnn_model em cũng không hiểu. Mong mọi người giải thích. Em cảm ơn.",#bayesian_neural_network,,,,
"xin phép các a/c/e, mình hiện tại đang học về machine learing và có 1 số thắc mắc
hiện tại các công ty của việt nam làm về AI họ lấy datasource dành cho người việt ở đâu ạ ? và giá cả ra sao ạ ?
Em cảm ơn mọi người đã dành thời gian đọc cái câu hỏi hơi ngu ngốc này :<","xin phép các a/c/e, mình hiện tại đang học về machine learing và có 1 số thắc mắc hiện tại các công ty của việt nam làm về AI họ lấy datasource dành cho người việt ở đâu ạ ? và giá cả ra sao ạ ? Em cảm ơn mọi người đã dành thời gian đọc cái câu hỏi hơi ngu ngốc này :<",,,,,
Hiện mình đang làm về speech recognition sử dụng deepspeech để train. Mình đã train được những file mẫu giờ mình đang tìm hiểu cách để đưa data lên để train. Cho mình hỏi là làm thế nào để generate ra các file train.csv dev.csv và test.csv từ data mình thu âm từ file wav để train vậy. Mình làm trên colab.,Hiện mình đang làm về speech recognition sử dụng deepspeech để train. Mình đã train được những file mẫu giờ mình đang tìm hiểu cách để đưa data lên để train. Cho mình hỏi là làm thế nào để generate ra các file train.csv dev.csv và test.csv từ data mình thu âm từ file wav để train vậy. Mình làm trên colab.,,,,,
"Em chào mọi người em là newbie ạ, em đang dùng detecto để làm thử bài toàn obj detect. khi em trainning xong chạy test luôn thì không báo lỗi gì cả những khi load model đã lưu lên để chạy thì lại lỗi như hình dưới ai biết chỉ em với","Em chào mọi người em là newbie ạ, em đang dùng detecto để làm thử bài toàn obj detect. khi em trainning xong chạy test luôn thì không báo lỗi gì cả những khi load model đã lưu lên để chạy thì lại lỗi như hình dưới ai biết chỉ em với",,,,,
"[AI học Boxing]
Trong những trò chơi đối kháng 1 vs 1 như boxing thì các vận động viên cần di chuyển linh hoạt và có các chiến thuật khác nhau trong trận đấu. Trong nghiên cứu này, tác giả sử dụng Reinforcement Learning để giả lập trận đấu của 2 vận động viên bằng cách cho máy học các kĩ năng cơ bản, sau đó các chiến thuật nâng cao.","[AI học Boxing] Trong những trò chơi đối kháng 1 vs 1 như boxing thì các vận động viên cần di chuyển linh hoạt và có các chiến thuật khác nhau trong trận đấu. Trong nghiên cứu này, tác giả sử dụng Reinforcement Learning để giả lập trận đấu của 2 vận động viên bằng cách cho máy học các kĩ năng cơ bản, sau đó các chiến thuật nâng cao.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 7/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 7/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"Chào mọi người,
Mình đang tìm hiểu về Bert và PhoBert, không biết có ai đã run code về PhoBert trên Windows rồi có thể share cho mình tham khảo được không ạ.
Thanks mọi người.","Chào mọi người, Mình đang tìm hiểu về Bert và PhoBert, không biết có ai đã run code về PhoBert trên Windows rồi có thể share cho mình tham khảo được không ạ. Thanks mọi người.",,,,,
"Hi mọi người,
Em có 1 bộ data tên người ( tên thật theo chứng minh thư),
Em muốn làm 1 model chỉ dựa trên bộ dữ liệu trên để phân loại 1 tập mới như sau:
Ví dụ: có 1 tên mới model sẽ phân loại nó là tên thật hoặc tên fake.
Em có tìm hiểu đây là model kiểu one class classification, bài toán của em là dùng dữ liệu đầu vào là text và chỉ có label cho 1 class( tên thật)? Không biết có đúng k? Ngoài ra e có thấy mấy phương pháp như GAN hay autoencoder thì có áp dụng gì được trường hợp này không?
Không biết là bài toán này có khả thi không, và đã có anh nào tìm hiểu về vấn đề tương tự chưa cho e 1 số nguồn tham khảo với ạ.
Ps: ( em có thể thêm 1 số biến khác như dân tộc gì, quê quán )
Cám ơn admin accept post.","Hi mọi người, Em có 1 bộ data tên người ( tên thật theo chứng minh thư), Em muốn làm 1 model chỉ dựa trên bộ dữ liệu trên để phân loại 1 tập mới như sau: Ví dụ: có 1 tên mới model sẽ phân loại nó là tên thật hoặc tên fake. Em có tìm hiểu đây là model kiểu one class classification, bài toán của em là dùng dữ liệu đầu vào là text và chỉ có label cho 1 class( tên thật)? Không biết có đúng k? Ngoài ra e có thấy mấy phương pháp như GAN hay autoencoder thì có áp dụng gì được trường hợp này không? Không biết là bài toán này có khả thi không, và đã có anh nào tìm hiểu về vấn đề tương tự chưa cho e 1 số nguồn tham khảo với ạ. Ps: ( em có thể thêm 1 số biến khác như dân tộc gì, quê quán ) Cám ơn admin accept post.",,,,,
"Chào mọi người,
Minh có viết một bài blog hướng dẫn về cách xây dựng một hệ thống học máy sử dụng các công cụ mã nguồn mở như Label Studio, Cometml, DVC, Kubeflow, KFServing, Docker, Kubernetes, Pytorch, AWS,… theo hướng thực hành step-by-step. Hiện tại mình đã viết được 6 bài, các bài tiếp theo sẽ được cập nhật sau.
Hi vọng sẽ giúp những bạn nào đang tìm hiểu cách xây dựng một hệ thống học máy một chút nào đó.
Blog: https://thanhhau097.github.io/mlpipeline/
Github repo: https://github.com/thanhhau097/mlpipeline
Đừng quên nhấn Star cho GitHub repo nếu cảm thấy có ích nhé ! :D","Chào mọi người, Minh có viết một bài blog hướng dẫn về cách xây dựng một hệ thống học máy sử dụng các công cụ mã nguồn mở như Label Studio, Cometml, DVC, Kubeflow, KFServing, Docker, Kubernetes, Pytorch, AWS,… theo hướng thực hành step-by-step. Hiện tại mình đã viết được 6 bài, các bài tiếp theo sẽ được cập nhật sau. Hi vọng sẽ giúp những bạn nào đang tìm hiểu cách xây dựng một hệ thống học máy một chút nào đó. Blog: https://thanhhau097.github.io/mlpipeline/ Github repo: https://github.com/thanhhau097/mlpipeline Đừng quên nhấn Star cho GitHub repo nếu cảm thấy có ích nhé ! :D",,,,,
"Em chào các anh/chị/ bạn trong group ạ.
Em có một chút câu hỏi về chương trình đào tạo kĩ sư AI của Vingroup, do chương trình này khá mới (theo em biết thì mới tổ chức năm 2020 nên mới có 1 hoặc 2 đợt các học viên).
Em có được thư giới thiệu của một researcher trong Vin nên muốn tìm hiểu vì thấy chương trình khá hấp dẫn để trải nghiệm và học hỏi, không biết có anh/chị/bạn nào trong group đã từng trải nghiệm có thể comment cho em hỏi một số vấn đề được không ạ? (Các vòng tuyển chọn, nghĩa vụ sau khi được đào tạo, chương trình đào tạo,…)
Em cám ơn ạ.","Em chào các anh/chị/ bạn trong group ạ. Em có một chút câu hỏi về chương trình đào tạo kĩ sư AI của Vingroup, do chương trình này khá mới (theo em biết thì mới tổ chức năm 2020 nên mới có 1 hoặc 2 đợt các học viên). Em có được thư giới thiệu của một researcher trong Vin nên muốn tìm hiểu vì thấy chương trình khá hấp dẫn để trải nghiệm và học hỏi, không biết có anh/chị/bạn nào trong group đã từng trải nghiệm có thể comment cho em hỏi một số vấn đề được không ạ? (Các vòng tuyển chọn, nghĩa vụ sau khi được đào tạo, chương trình đào tạo,…) Em cám ơn ạ.",,,,,
Em chào mọi người em là newbie ạ. Hiện nay em đang học về Linear Regression trong đó có phần train dữ liệu bằng Gradient Descent. Do data làm bài Lab quá lớn dẫn đến tìm các trọng số khá lâu. Làm trên Colab 5 đến 6h thì bị disconnect. Mọi người còn biết phương pháp nào chạy online như Google Colab mà treo được lâu không ạ. Xin chân thành cảm ơn ạ,Em chào mọi người em là newbie ạ. Hiện nay em đang học về Linear Regression trong đó có phần train dữ liệu bằng Gradient Descent. Do data làm bài Lab quá lớn dẫn đến tìm các trọng số khá lâu. Làm trên Colab 5 đến 6h thì bị disconnect. Mọi người còn biết phương pháp nào chạy online như Google Colab mà treo được lâu không ạ. Xin chân thành cảm ơn ạ,,,,,
"Mọi người cho em hỏi là em mới bắt đầu với ML, em đang thử train một model giống MNIST có 15k examples, tỉ lệ train:val:test là 60:30:10 (Chinese MNIST - https://www.kaggle.com/fedesoriano/chinese-mnist-digit-recognizer).
Em đang gặp vấn đề là em đã thử (mò) nhiều model khác nhau, từ có ít layer đến nhiều layer, từ ít node (20) đến nhiều node (500), và em cũng đã thử dùng regularization nhưng đều gặp tình trạng là cuối cùng training loss rất thấp, nhưng validation loss lại bị tiệm cận ở một mức nào đó (có trường hợp còn tăng lại). Dữ liệu của em đã được scale về [0, 1].
Cho em hỏi đây có phải là overfitting không, và mình nên xử lý như thế nào ạ? (trừ sử dụng các network phức tạp hơn như CNN vì em chưa học đến).
Với các thông số trong hình dưới đây thì cuối cùng accuracy của (train, validate, test) = (0.99, 0.76, 0.758).","Mọi người cho em hỏi là em mới bắt đầu với ML, em đang thử train một model giống MNIST có 15k examples, tỉ lệ train:val:test là 60:30:10 (Chinese MNIST - https://www.kaggle.com/fedesoriano/chinese-mnist-digit-recognizer). Em đang gặp vấn đề là em đã thử (mò) nhiều model khác nhau, từ có ít layer đến nhiều layer, từ ít node (20) đến nhiều node (500), và em cũng đã thử dùng regularization nhưng đều gặp tình trạng là cuối cùng training loss rất thấp, nhưng validation loss lại bị tiệm cận ở một mức nào đó (có trường hợp còn tăng lại). Dữ liệu của em đã được scale về [0, 1]. Cho em hỏi đây có phải là overfitting không, và mình nên xử lý như thế nào ạ? (trừ sử dụng các network phức tạp hơn như CNN vì em chưa học đến). Với các thông số trong hình dưới đây thì cuối cùng accuracy của (train, validate, test) = (0.99, 0.76, 0.758).",,,,,
https://www.facebook.com/1602089060038379/posts/3037091456538125/,https://www.facebook.com/1602089060038379/posts/3037091456538125/,,,,,
"Hợp tác làm nhận diện khuôn mặt đeo khẩu trang.
Hiện tại bên mình đang có nhu cầu giải quyết bài toán face recognition nhưng đeo khẩu trang.
Ảnh chụp mặt chính diện, điều kiện ánh sáng trong phòng.
Cả nhà ai có kinh nghiệm trong mảng này muốn hợp tác không ạ.","Hợp tác làm nhận diện khuôn mặt đeo khẩu trang. Hiện tại bên mình đang có nhu cầu giải quyết bài toán face recognition nhưng đeo khẩu trang. Ảnh chụp mặt chính diện, điều kiện ánh sáng trong phòng. Cả nhà ai có kinh nghiệm trong mảng này muốn hợp tác không ạ.",,,,,
"Cuốn ""Thực hành Học máy với Scikit-learn, Keras & TensorFlow"" mặc dù không miễn phí, những bài tập ở dạng notebook hoàn toàn mở cho cộng đồng. Các bạn quan tâm có thể tham khảo tại
https://github.com/mlbvn/handson-ml2-vn
Nhóm cũng đã dịch các notebok này ra tiếng Việt. ""Star"" github repo nếu bạn thấy hữu ích.
===
Những notebook này hoàn toàn có thể chạy được trên môi trường Google colab hoặc Kaggle kernel, các bạn có thể chạy trực tiếp trên trình duyệt mà không cần tải về hay cài đặt thư viện liên quan.","Cuốn ""Thực hành Học máy với Scikit-learn, Keras & TensorFlow"" mặc dù không miễn phí, những bài tập ở dạng notebook hoàn toàn mở cho cộng đồng. Các bạn quan tâm có thể tham khảo tại https://github.com/mlbvn/handson-ml2-vn Nhóm cũng đã dịch các notebok này ra tiếng Việt. ""Star"" github repo nếu bạn thấy hữu ích. === Những notebook này hoàn toàn có thể chạy được trên môi trường Google colab hoặc Kaggle kernel, các bạn có thể chạy trực tiếp trên trình duyệt mà không cần tải về hay cài đặt thư viện liên quan.",,,,,
"Mọi người ơi cho em hỏi về logic python khi mà loop sao cho không bị Index error với
Tại sao trong 2 loop này nó bị out of index được nhỉ khi mà đến element cuối cùng của loop
Nếu index bắt đầu = 0 , thì maximum value của index nó chỉ là giá trị của ctr_x * ctr_y thôi chứ","Mọi người ơi cho em hỏi về logic python khi mà loop sao cho không bị Index error với Tại sao trong 2 loop này nó bị out of index được nhỉ khi mà đến element cuối cùng của loop Nếu index bắt đầu = 0 , thì maximum value của index nó chỉ là giá trị của ctr_x * ctr_y thôi chứ",,,,,
"JUPYTER-NOTEBOOK
Chào các bác ạ,
Hiện tại thì em có gặp trường hợp thế này, UI notebook của em rất khác so với thông thường. Có bác nào đã từng gặp UI nào như này chưa ạ? Em hiện tịa đang dùng Ubuntu 20.04, và Jupyter-notebook này là mặc định trong máy của em, em chưa cài nhưng vẫn có. Bây giờ em đang tính xóa để cài lại xem sao chứ UI như này thiếu rất nhiều tính năng ạ.
Em cảm ơn các bác","JUPYTER-NOTEBOOK Chào các bác ạ, Hiện tại thì em có gặp trường hợp thế này, UI notebook của em rất khác so với thông thường. Có bác nào đã từng gặp UI nào như này chưa ạ? Em hiện tịa đang dùng Ubuntu 20.04, và Jupyter-notebook này là mặc định trong máy của em, em chưa cài nhưng vẫn có. Bây giờ em đang tính xóa để cài lại xem sao chứ UI như này thiếu rất nhiều tính năng ạ. Em cảm ơn các bác",,,,,
"Chào mn, cho mình hỏi trong python có sẵn thư viện giải quyết Singular Value Thresholding (SVT) ko ạ?","Chào mn, cho mình hỏi trong python có sẵn thư viện giải quyết Singular Value Thresholding (SVT) ko ạ?",,,,,
"Chào các anh em, không biết có anh em nào có bộ dataset về ảnh selfie không ạ? Hay đã từng crawl data image từ facebook, instagram,... có thể hướng dẫn cho em hoặc share em bộ các anh từng crawl trên các nền tàng được không ạ?","Chào các anh em, không biết có anh em nào có bộ dataset về ảnh selfie không ạ? Hay đã từng crawl data image từ facebook, instagram,... có thể hướng dẫn cho em hoặc share em bộ các anh từng crawl trên các nền tàng được không ạ?",,,,,
"𝐋𝐢𝐯𝐞𝐬𝐭𝐫𝐞𝐚𝐦 𝐒𝐮̛̣ 𝐤𝐢𝐞̣̂𝐧 𝐍𝐠𝐚̀𝐲 𝐡𝐨̣̂𝐢 𝐂𝐨̂𝐧𝐠 𝐧𝐠𝐡𝐞̣̂ 𝐀𝐈 𝐭𝐫𝐨𝐧𝐠 𝐤𝐡𝐮𝐨̂𝐧 𝐤𝐡𝐨̂̉ 𝐈𝐉𝐂𝐀𝐈 𝟐𝟎𝟐𝟏 – hội nghị quốc tế lớn nhất thế giới về trí tuệ nhân tạo có lịch sử đã 52 năm (từ 1969) diễn ra hôm nay, 25/08/2021.
Cùng lắng nghe những thông tin và kiến thức cập nhật nhất về ứng dụng thực tiễn và các tác động xã hội của AI từ các doanh nhân và nhà khoa học đến từ những tập đoàn công nghệ hàng đầu thế giới như IBM, Sony, Fujitsu, Baidu, Sea, JD,…
Và chứng kiến trí tuệ Việt được tôn vinh tại Ngày hội với lễ trao giải ACM SIGAI Industry Award 2021 cho sản phẩm DrAidTM cho Chẩn đoán hình ảnh của Công ty VinBrain (thuộc Tập đoàn Vingroup) – giải thưởng danh giá trong lĩnh vực trí tuệ nhân tạo được trao hàng năm cho duy nhất một sản phẩm ứng dụng trí tuệ nhân tạo xuất sắc nhất ♥️","̛̣ ̣̂ ̀ ̣̂ ̂ ̣̂ ̂ ̂̉ – hội nghị quốc tế lớn nhất thế giới về trí tuệ nhân tạo có lịch sử đã 52 năm (từ 1969) diễn ra hôm nay, 25/08/2021. Cùng lắng nghe những thông tin và kiến thức cập nhật nhất về ứng dụng thực tiễn và các tác động xã hội của AI từ các doanh nhân và nhà khoa học đến từ những tập đoàn công nghệ hàng đầu thế giới như IBM, Sony, Fujitsu, Baidu, Sea, JD,… Và chứng kiến trí tuệ Việt được tôn vinh tại Ngày hội với lễ trao giải ACM SIGAI Industry Award 2021 cho sản phẩm DrAidTM cho Chẩn đoán hình ảnh của Công ty VinBrain (thuộc Tập đoàn Vingroup) – giải thưởng danh giá trong lĩnh vực trí tuệ nhân tạo được trao hàng năm cho duy nhất một sản phẩm ứng dụng trí tuệ nhân tạo xuất sắc nhất",,,,,
"xin phép Admin
Mình xin chia sẻ tới mọi người dự án ""MÁY LẤY MẪU COVID"" do ASMTechG nghiên cứu và phát triển, hi vọng mọi người có ý tưởng hay hãy thực hiện vì cộng đồng ạ!
https://fb.watch/7B_daTgFgT/","xin phép Admin Mình xin chia sẻ tới mọi người dự án ""MÁY LẤY MẪU COVID"" do ASMTechG nghiên cứu và phát triển, hi vọng mọi người có ý tưởng hay hãy thực hiện vì cộng đồng ạ! https://fb.watch/7B_daTgFgT/",,,,,
"Xin chào mọi người.
Em vừa có ý tưởng cho một đề tài tốt nghiệp tại trường.
Đề tài: xây dựng một hệ thống hỗ trợ học tập, giúp học sinh ở trường trung học phổ thông làm bài tập.
Một chatbox có tích hợp AI được sử dụng để gợi ý câu trả lời cho học sinh nếu học sinh không biết hoặc trả lời sai.
Em hy vọng mọi người có thể cho em một số trang web với các tính năng như trên để tìm hiểu thêm. Cũng như các bài báo hoặc kỹ thuật để hiện thực topic.
Rất cảm ơn mọi người.
Chúc mọi người nhiều sức khỏe.","Xin chào mọi người. Em vừa có ý tưởng cho một đề tài tốt nghiệp tại trường. Đề tài: xây dựng một hệ thống hỗ trợ học tập, giúp học sinh ở trường trung học phổ thông làm bài tập. Một chatbox có tích hợp AI được sử dụng để gợi ý câu trả lời cho học sinh nếu học sinh không biết hoặc trả lời sai. Em hy vọng mọi người có thể cho em một số trang web với các tính năng như trên để tìm hiểu thêm. Cũng như các bài báo hoặc kỹ thuật để hiện thực topic. Rất cảm ơn mọi người. Chúc mọi người nhiều sức khỏe.",,,,,
"Em chào anh, chị và các bạn. Em đang gặp 1 bài toán muốn nhờ anh chị ai có kinh nghiệm thì chia sẻ giúp em phương án:

Hiện tại em đang có 1 file weight nặng 256mb (em train dùng yolov4), em muốn deploy lên device để chạy realtime. Mà file này nặng quá nên e muốn hỏi các anh chị và các bạn đã ai có kinh nghiệm xử lý vấn đề này chưa ạ? 

Em cảm ơn mọi người rất nhiều ạ!","Em chào anh, chị và các bạn. Em đang gặp 1 bài toán muốn nhờ anh chị ai có kinh nghiệm thì chia sẻ giúp em phương án: Hiện tại em đang có 1 file weight nặng 256mb (em train dùng yolov4), em muốn deploy lên device để chạy realtime. Mà file này nặng quá nên e muốn hỏi các anh chị và các bạn đã ai có kinh nghiệm xử lý vấn đề này chưa ạ? Em cảm ơn mọi người rất nhiều ạ!",,,,,
"Chào các bác! Nằm vùng trong group đã lâu nên em xin đóng góp một bài blog nhỏ về chủ để ""Kiểm thử hệ thống Machine Learning"". Các bác thấy hay thì tích cực like và share để ủng hộ em với ạ :D.
Chúc các bác một tuần làm việc hiệu quả!","Chào các bác! Nằm vùng trong group đã lâu nên em xin đóng góp một bài blog nhỏ về chủ để ""Kiểm thử hệ thống Machine Learning"". Các bác thấy hay thì tích cực like và share để ủng hộ em với ạ :D. Chúc các bác một tuần làm việc hiệu quả!",,,,,
"Mọi người cho em hỏi về kết quả sau khi training thì biểu diễn như thế nào với ạ. cụ thể là bài Bài 13: Softmax Regression của thầy Tiệp(link ở dưới) thì em thấy đã ra ma trận weights thì sau đấy em không biết biểu diễn như thế nào ạ
https://machinelearningcoban.com/2017/02/17/softmax/#-vi-du-voi-python",Mọi người cho em hỏi về kết quả sau khi training thì biểu diễn như thế nào với ạ. cụ thể là bài Bài 13: Softmax Regression của thầy Tiệp(link ở dưới) thì em thấy đã ra ma trận weights thì sau đấy em không biết biểu diễn như thế nào ạ https://machinelearningcoban.com/2017/02/17/softmax/#-vi-du-voi-python,,,,,
"Chào cả nhà,
Mình đang đo size gương mặt của 1 người (ceo) trong ảnh và size chữ ký của ceo, trong báo cáo tài chính.
Hiện mình đang làm bằng tay, mình có gần 100,000 reports nên không biết khi nào mình mới làm xong mà thời gian dự án sắp kết thúc.
Có cách nào để code để giải quyết cho nhanh không ạ?","Chào cả nhà, Mình đang đo size gương mặt của 1 người (ceo) trong ảnh và size chữ ký của ceo, trong báo cáo tài chính. Hiện mình đang làm bằng tay, mình có gần 100,000 reports nên không biết khi nào mình mới làm xong mà thời gian dự án sắp kết thúc. Có cách nào để code để giải quyết cho nhanh không ạ?",,,,,
"Có cách nào để làm tự động thay vì làm tay, khi mình muốn đo tỷ lệ dài/rộng khuôn mặt trong ảnh và size chữ ký không ạ?","Có cách nào để làm tự động thay vì làm tay, khi mình muốn đo tỷ lệ dài/rộng khuôn mặt trong ảnh và size chữ ký không ạ?",,,,,
"FACEBOOK FELLOWSHIP PROGRAM
SUPPORTING PHD STUDENTS ENGAGED IN INNOVATIVE RESEARCH
The Facebook Fellowship is a global program designed to encourage and support promising doctoral students who are engaged in innovative and relevant research in areas related to computer science and engineering at an accredited university.
The program is open to students in any year of their PhD study. We also encourage people of diverse backgrounds and experiences to apply, especially those from traditionally under-represented minority groups. Applications are evaluated based on the strength of the student’s research statement, publication record, and recommendation letters.
Winners of the Fellowship are entitled to receive two years of paid tuition and fees, a $42,000 annual stipend to cover living and conference travel costs, a paid visit to Facebook headquarters for the annual Fellowship Summit, and various opportunities to engage with Facebook researchers.
APPLY NOW: https://research.fb.com/fellowship/","FACEBOOK FELLOWSHIP PROGRAM SUPPORTING PHD STUDENTS ENGAGED IN INNOVATIVE RESEARCH The Facebook Fellowship is a global program designed to encourage and support promising doctoral students who are engaged in innovative and relevant research in areas related to computer science and engineering at an accredited university. The program is open to students in any year of their PhD study. We also encourage people of diverse backgrounds and experiences to apply, especially those from traditionally under-represented minority groups. Applications are evaluated based on the strength of the student’s research statement, publication record, and recommendation letters. Winners of the Fellowship are entitled to receive two years of paid tuition and fees, a $42,000 annual stipend to cover living and conference travel costs, a paid visit to Facebook headquarters for the annual Fellowship Summit, and various opportunities to engage with Facebook researchers. APPLY NOW: https://research.fb.com/fellowship/",,,,,
"[Palm Detection]
Chào mn, em nghiên cứu làm dự án môn học về Palm Detection, nhận diện lòng bàn tay, tích hợp ML & DSP nhưng bị fail, kết quả không nhận diện tốt, nhờ mấy anh có kinh nghiệm xem code, chỉ hướng giải quyết giúp em với ạ
Lỗi kết quả: cùng tay thì nhận diện cùng người, nhma so với ground truth thì sai người, dù em đổi model các thử, và thử training theo nhiều kiểu, thì kết quả sai vẫn giống nhau 🙁
TLTK: em làm theo cái mindset của bài ni mà kết quả không tốt
https://viblo.asia/p/xay-dung-he-thong-kiem-soat-nhan-dang-khuon-mat-voi-opencv-dlib-va-deep-learning-4P8566ma5Y3
Em gửi kèm mã nguồn dự án, nếu có anh/thầy kinh nghiệm có thời gian có thể xem qua và chỉ lỗi vấn đề giúp em vs ạ. Em cảm ơn nhiều
https://colab.research.google.com/drive/1-7Ca3_LxpQOkdGhaezrznf9wwjM68nuV
https://drive.google.com/drive/folders/13t6aeG3WFmMGjDinnkgDf4BFsu00SdKs
!!bên dưới là ảnh kết quả sau khi thử train ạ","[Palm Detection] Chào mn, em nghiên cứu làm dự án môn học về Palm Detection, nhận diện lòng bàn tay, tích hợp ML & DSP nhưng bị fail, kết quả không nhận diện tốt, nhờ mấy anh có kinh nghiệm xem code, chỉ hướng giải quyết giúp em với ạ Lỗi kết quả: cùng tay thì nhận diện cùng người, nhma so với ground truth thì sai người, dù em đổi model các thử, và thử training theo nhiều kiểu, thì kết quả sai vẫn giống nhau TLTK: em làm theo cái mindset của bài ni mà kết quả không tốt https://viblo.asia/p/xay-dung-he-thong-kiem-soat-nhan-dang-khuon-mat-voi-opencv-dlib-va-deep-learning-4P8566ma5Y3 Em gửi kèm mã nguồn dự án, nếu có anh/thầy kinh nghiệm có thời gian có thể xem qua và chỉ lỗi vấn đề giúp em vs ạ. Em cảm ơn nhiều https://colab.research.google.com/drive/1-7Ca3_LxpQOkdGhaezrznf9wwjM68nuV https://drive.google.com/drive/folders/13t6aeG3WFmMGjDinnkgDf4BFsu00SdKs !!bên dưới là ảnh kết quả sau khi thử train ạ",,,,,
"[ VIDEO THUẬT TOÁN MPP TRONG PHÂN BỐ DANH MỤC ĐẦU TƯ]
Hello các bạn, mình tên là Nguyễn Hưng Quang Khải, sinh viên K18, khoa Toán Ứng Dụng trường ĐH Quốc Tế TPHCM. Trong bài viết kì trước, mình đã có giới thiệu với các bạn về blog của mình trên medium. Hôm nay mình xin đăng video thuyết trình của nhóm nghiên cứu của mình, ứng dụng toán trong thị trường tài chính. Đây là bài thuyết trình của nhóm mình tại chung kết kì thi Olympic Kinh Tế Lượng Toàn Quốc 2021. Đề tài này, được sự hướng dẫn trực tiếp của Tiến Sĩ Tạ Quốc Bảo
Mục đích của video này, là mình muốn cho mọi người thấy, ứng dụng của toán vào thị trường tài chính là ra sao. Từ đó tạo cảm hứng để cho các bạn có thể đào sâu nghiên cứu mảng Financial Mathematics
Đề tài của tụi mình giải quyết bài toán tối ưu hoá danh mục đầu tư. Nói cho dễ hiểu, là giả sử các bạn có 1 tỷ , thì mình nên lấy bao nhiều tiền trong 1 tỷ đó để đầu tư vào vàng, vào chứng khoán, hay vào BĐS. Chúng ta nên phân bố tài sản đầu tư đó làm sao để xác suất lời trong tương lai là cao nhất có thể?
Thuật toán của tụi mình hay ở chỗ, khi đầu tư vào thị trường Việt Nam, thì khả năng sinh lời rất cao, mô hình thì rất dễ dùng. Chỉ trong vòng 2-3 phút là có thể dùng ngay, và đầu tư luôn vào ngày mai
Nội dung của video bao gồm:
Phần thuyết trình : Khải (từ phút 0:0 đến phút 14:24) , Vũ ( từ phút 14:25 đến 26:00)
Phần trả lời câu hỏi và tranh luận với BGK : từ phút 26:01 đến phút 46:00 ( phần này khá gây cấn)
Phần kết qủa của nhóm: 46:00 đến hết
Người chủ toạ của kì thi là tiến sĩ Võ Trí Thành ( Viện trưởng Viện Nghiên cứu chiến lược thương hiệu và cạnh tranh,nguyên cố vấn kinh tế của thủ tướng Nguyễn Tấn Dũng )
Nhóm nghiên cứu sinh đề tài bao gồm:
- Huỳnh Tấn Vũ
- Hồ Hữu Bình
- Nguyễn Ngọc Phụng
- Nguyễn Hưng Quang Khải
- Trần Hoàng Phi
Dưới sự hướng dẫn trực tiếp đến từ Tiến Sĩ Tạ Quốc Bảo
Hi vọng video này sẽ tạo nguồn cảm hứng gì đó cho các bạn, giúp đẩy mạnh hơn mảng nghiên cứu Financial Mathematics tại Việt Nam.","[ VIDEO THUẬT TOÁN MPP TRONG PHÂN BỐ DANH MỤC ĐẦU TƯ] Hello các bạn, mình tên là Nguyễn Hưng Quang Khải, sinh viên K18, khoa Toán Ứng Dụng trường ĐH Quốc Tế TPHCM. Trong bài viết kì trước, mình đã có giới thiệu với các bạn về blog của mình trên medium. Hôm nay mình xin đăng video thuyết trình của nhóm nghiên cứu của mình, ứng dụng toán trong thị trường tài chính. Đây là bài thuyết trình của nhóm mình tại chung kết kì thi Olympic Kinh Tế Lượng Toàn Quốc 2021. Đề tài này, được sự hướng dẫn trực tiếp của Tiến Sĩ Tạ Quốc Bảo Mục đích của video này, là mình muốn cho mọi người thấy, ứng dụng của toán vào thị trường tài chính là ra sao. Từ đó tạo cảm hứng để cho các bạn có thể đào sâu nghiên cứu mảng Financial Mathematics Đề tài của tụi mình giải quyết bài toán tối ưu hoá danh mục đầu tư. Nói cho dễ hiểu, là giả sử các bạn có 1 tỷ , thì mình nên lấy bao nhiều tiền trong 1 tỷ đó để đầu tư vào vàng, vào chứng khoán, hay vào BĐS. Chúng ta nên phân bố tài sản đầu tư đó làm sao để xác suất lời trong tương lai là cao nhất có thể? Thuật toán của tụi mình hay ở chỗ, khi đầu tư vào thị trường Việt Nam, thì khả năng sinh lời rất cao, mô hình thì rất dễ dùng. Chỉ trong vòng 2-3 phút là có thể dùng ngay, và đầu tư luôn vào ngày mai Nội dung của video bao gồm: Phần thuyết trình : Khải (từ phút 0:0 đến phút 14:24) , Vũ ( từ phút 14:25 đến 26:00) Phần trả lời câu hỏi và tranh luận với BGK : từ phút 26:01 đến phút 46:00 ( phần này khá gây cấn) Phần kết qủa của nhóm: 46:00 đến hết Người chủ toạ của kì thi là tiến sĩ Võ Trí Thành ( Viện trưởng Viện Nghiên cứu chiến lược thương hiệu và cạnh tranh,nguyên cố vấn kinh tế của thủ tướng Nguyễn Tấn Dũng ) Nhóm nghiên cứu sinh đề tài bao gồm: - Huỳnh Tấn Vũ - Hồ Hữu Bình - Nguyễn Ngọc Phụng - Nguyễn Hưng Quang Khải - Trần Hoàng Phi Dưới sự hướng dẫn trực tiếp đến từ Tiến Sĩ Tạ Quốc Bảo Hi vọng video này sẽ tạo nguồn cảm hứng gì đó cho các bạn, giúp đẩy mạnh hơn mảng nghiên cứu Financial Mathematics tại Việt Nam.",,,,,
"[Phân loại ảnh CT về brain tumor]
Em chào cả nhà, em là newbie mới tìm hiểu về mảng Deep learning một thời gian, hiện em đang làm project về phân loại ảnh CT về brain tumor.
Có một số vấn đề em chưa có kinh nghiệm, nên em mạnh dạn viết lên đây rất mong có cơ hội trao đổi với các anh chị ạ.
Em có một tập dữ liệu khoảng 30 ảnh CT là tumor và 70 ảnh non tumor. Em đã thử rất nhiều cách khác nhau để phân loại tập dữ liệu này.
—> Cách 1: dùng gói lệnh Pyradiomics để chiết xuất các features từ hình ảnh, rồi dùng các thuật toán Machine learning để phân loại ảnh. Em cũng kết hợp với các cách để cân bằng tập dữ liệu (SMOTE, Random oversampling) nhưng kết quả ko đc khả quan lắm.
—> Cách 2: Xây dựng mô hình 3D-CNN kết hợp với với một số phương pháp data augmentation và cân bằng dữ liệu. Kết quả là do tập dữ liệu còn quá nhỏ nên dẫn đến overfitting.
Em đang muốn thử dùng thêm transfer learning, nhưng vấn đề là để áp dụng được các pretrained model thì em phải đưa dữ liệu về size ảnh mà các model này đã đc train. Em đang có một số thắc mắc là:
1. Phương pháp nào là tốt nhất để xử lý ảnh CT và đưa về cùng size ảnh input của các pretrain model mà giữ được nhiều nhất thông tin của ảnh.
2. Đối với tập dữ liệu nhỏ như vậy, theo kinh nghiệm của anh chị thì nên dùng pretrained model nào thì phù hợp ạ.
3. Em đang nghĩ đến việc dùng phương pháp Variational Autoencoder (2D và 3D-CNN) để phân loại ảnh, em mới đọc tài liệu về phương pháp này nhưng em cũng chưa rõ lắm, có phải là mình sẽ áp dụng các machine learning models để phân loại samples được sinh ra trong latent space không ạ, hay là mình sẽ dùng output của layer (tạo ra \mu và \Sigma trong hình bên dưới) như input của một vài fully connected layer mới để phân loại ảnh. (Gần giống như phương pháp fine tuning trong transfer learning). Các anh chị có kinh nghiệm về mảng này cho em xin thêm tư vấn với ạ. Em cám ơn nhiều <3","[Phân loại ảnh CT về brain tumor] Em chào cả nhà, em là newbie mới tìm hiểu về mảng Deep learning một thời gian, hiện em đang làm project về phân loại ảnh CT về brain tumor. Có một số vấn đề em chưa có kinh nghiệm, nên em mạnh dạn viết lên đây rất mong có cơ hội trao đổi với các anh chị ạ. Em có một tập dữ liệu khoảng 30 ảnh CT là tumor và 70 ảnh non tumor. Em đã thử rất nhiều cách khác nhau để phân loại tập dữ liệu này. —> Cách 1: dùng gói lệnh Pyradiomics để chiết xuất các features từ hình ảnh, rồi dùng các thuật toán Machine learning để phân loại ảnh. Em cũng kết hợp với các cách để cân bằng tập dữ liệu (SMOTE, Random oversampling) nhưng kết quả ko đc khả quan lắm. —> Cách 2: Xây dựng mô hình 3D-CNN kết hợp với với một số phương pháp data augmentation và cân bằng dữ liệu. Kết quả là do tập dữ liệu còn quá nhỏ nên dẫn đến overfitting. Em đang muốn thử dùng thêm transfer learning, nhưng vấn đề là để áp dụng được các pretrained model thì em phải đưa dữ liệu về size ảnh mà các model này đã đc train. Em đang có một số thắc mắc là: 1. Phương pháp nào là tốt nhất để xử lý ảnh CT và đưa về cùng size ảnh input của các pretrain model mà giữ được nhiều nhất thông tin của ảnh. 2. Đối với tập dữ liệu nhỏ như vậy, theo kinh nghiệm của anh chị thì nên dùng pretrained model nào thì phù hợp ạ. 3. Em đang nghĩ đến việc dùng phương pháp Variational Autoencoder (2D và 3D-CNN) để phân loại ảnh, em mới đọc tài liệu về phương pháp này nhưng em cũng chưa rõ lắm, có phải là mình sẽ áp dụng các machine learning models để phân loại samples được sinh ra trong latent space không ạ, hay là mình sẽ dùng output của layer (tạo ra \mu và \Sigma trong hình bên dưới) như input của một vài fully connected layer mới để phân loại ảnh. (Gần giống như phương pháp fine tuning trong transfer learning). Các anh chị có kinh nghiệm về mảng này cho em xin thêm tư vấn với ạ. Em cám ơn nhiều <3",,,,,
"Nhóm vừa upload bản đọc thử ở đây https://drive.google.com/file/d/1y-jjYf_KNPS2DNqjqUmcZjhwUTmEqBKQ/view
Tối nay giờ VN các bạn order sớm sẽ nhận được ebook qua email nhé.",Nhóm vừa upload bản đọc thử ở đây https://drive.google.com/file/d/1y-jjYf_KNPS2DNqjqUmcZjhwUTmEqBKQ/view Tối nay giờ VN các bạn order sớm sẽ nhận được ebook qua email nhé.,,,,,
"Chính thức phát hành ebook (pdf) ""Thực hành Học máy với Scikit-Learn, Keas & TensorFlow"" (Hands-On Machine Learning with Scikit-Learn and TensorFlow).
https://handson-ml.mlbvn.org/
Cảm ơn các bạn trong nhóm dịch đã làm việc chăm chỉ từ đầu năm để ra bản dịch Tập 1 này. Cảm ơn FUNiX đã hỗ trợ nhiệt tình làm việc với các nhà xuất bản để chúng tôi hoàn thiện cuốn sách này.
Tập hai dự kiến ra mắt vào đầu năm 2022.
===
Các bạn có câu hỏi thì gửi về handson-ml@mlbvn.org. Mình không phụ trách trả lời câu hỏi của cuốn này nhé, mình không trả lời tin nhắn các bạn được đâu.","Chính thức phát hành ebook (pdf) ""Thực hành Học máy với Scikit-Learn, Keas & TensorFlow"" (Hands-On Machine Learning with Scikit-Learn and TensorFlow). https://handson-ml.mlbvn.org/ Cảm ơn các bạn trong nhóm dịch đã làm việc chăm chỉ từ đầu năm để ra bản dịch Tập 1 này. Cảm ơn FUNiX đã hỗ trợ nhiệt tình làm việc với các nhà xuất bản để chúng tôi hoàn thiện cuốn sách này. Tập hai dự kiến ra mắt vào đầu năm 2022. === Các bạn có câu hỏi thì gửi về handson-ml@mlbvn.org. Mình không phụ trách trả lời câu hỏi của cuốn này nhé, mình không trả lời tin nhắn các bạn được đâu.",,,,,
,nan,,,,,
"Chào mọi người, 
Em đang học ngành sinh học, khóa luận tốt nghiệp của em là về xây dựng một mô hình học sâu để dự đoán liệu hai protein có tương tác với nhau không dựa vào đầu vào là các trình tự chuỗi amino acid (cấu trúc bậc 1 của protein). Em có đọc và tham khảo một số bài báo về chủ đề này trong đó em thấy có bài báo https://academic.oup.com/bioinformatics/article/35/14/i305/5529260 đưa ra mô hình có tên là PIPR là state of the art năm 2019 và https://www.biorxiv.org/content/10.1101/2021.01.22.427866v1 mới đây năm 2021 đưa ra mô hình có tên là D-SCRIPT có vẻ nhỉnh hơn mô hình PIPR khi sử dụng training trên dữ liệu kích thước nhỏ hơn và có khả năng generalize với các tập dữ liệu khác (mở rộng khả năng dự đoán cho các sinh vật khác). Em đang gặp một số vấn đề muốn được trả lời
Mô hình của em có nên follow mấy bài báo đã xây dựng mô hình state of the art không và nếu follow thì là follow một mô hình trong 1 bài báo chăng? 
Sau khi em đã follow thì điểm gì sẽ tạo nên chất riêng trong khóa luận của em vì em đã follow nó, thực sự thì em sẽ cần làm gì với kiến trúc em đã lựa chọn, có lẽ em nên tunning các module trong kiến trúc chăng, hay curate lại dữ liệu hay sử dụng các regularization cho mô hình nhằm đạt được các performance metrics thậm chí còn tốt hơn mô hình state of the art trong bài báo gốc?
Liệu có kiến trúc hay loại mô hình nào là rất thích hợp cho tiếp cận bài toán này ở thời điểm hiện tại, hôm nọ em có đọc được sử dụng mô hình GNN thì rất tiện vì các protein bản chất chính là các đỉnh trong một đồ thị tương tác, đồ thị trao đổi chất, con đường tín hiệu, trao đổi chất? Liệu có hướng tiếp cận nào mà mọi người nghĩ là phù hợp hơn không ạ?
Em cảm ơn mọi người ạ.","Chào mọi người, Em đang học ngành sinh học, khóa luận tốt nghiệp của em là về xây dựng một mô hình học sâu để dự đoán liệu hai protein có tương tác với nhau không dựa vào đầu vào là các trình tự chuỗi amino acid (cấu trúc bậc 1 của protein). Em có đọc và tham khảo một số bài báo về chủ đề này trong đó em thấy có bài báo https://academic.oup.com/bioinformatics/article/35/14/i305/5529260 đưa ra mô hình có tên là PIPR là state of the art năm 2019 và https://www.biorxiv.org/content/10.1101/2021.01.22.427866v1 mới đây năm 2021 đưa ra mô hình có tên là D-SCRIPT có vẻ nhỉnh hơn mô hình PIPR khi sử dụng training trên dữ liệu kích thước nhỏ hơn và có khả năng generalize với các tập dữ liệu khác (mở rộng khả năng dự đoán cho các sinh vật khác). Em đang gặp một số vấn đề muốn được trả lời Mô hình của em có nên follow mấy bài báo đã xây dựng mô hình state of the art không và nếu follow thì là follow một mô hình trong 1 bài báo chăng? Sau khi em đã follow thì điểm gì sẽ tạo nên chất riêng trong khóa luận của em vì em đã follow nó, thực sự thì em sẽ cần làm gì với kiến trúc em đã lựa chọn, có lẽ em nên tunning các module trong kiến trúc chăng, hay curate lại dữ liệu hay sử dụng các regularization cho mô hình nhằm đạt được các performance metrics thậm chí còn tốt hơn mô hình state of the art trong bài báo gốc? Liệu có kiến trúc hay loại mô hình nào là rất thích hợp cho tiếp cận bài toán này ở thời điểm hiện tại, hôm nọ em có đọc được sử dụng mô hình GNN thì rất tiện vì các protein bản chất chính là các đỉnh trong một đồ thị tương tác, đồ thị trao đổi chất, con đường tín hiệu, trao đổi chất? Liệu có hướng tiếp cận nào mà mọi người nghĩ là phù hợp hơn không ạ? Em cảm ơn mọi người ạ.",,,,,
"Sau khi giới thiệu CoPilot bởi GitHub và Codex bởi OpenAI, tới lượt Google Brain công bố bài báo mới nhất về chủ đề tương tự tại đây https://arxiv.org/abs/2108.07732. Kết quả chính là họ sử dụng các models ngôn ngữ lớn để ""viết"" các chương trình máy tính, chương trình thực thi, giải các bào toán, hội thoại với người để tinh chỉnh code. Mô hình họ báo cáo có thể giải chính xác 60% các chương trình máy tính và 81% các bài toán như hình ví dụ bên dưới","Sau khi giới thiệu CoPilot bởi GitHub và Codex bởi OpenAI, tới lượt Google Brain công bố bài báo mới nhất về chủ đề tương tự tại đây https://arxiv.org/abs/2108.07732. Kết quả chính là họ sử dụng các models ngôn ngữ lớn để ""viết"" các chương trình máy tính, chương trình thực thi, giải các bào toán, hội thoại với người để tinh chỉnh code. Mô hình họ báo cáo có thể giải chính xác 60% các chương trình máy tính và 81% các bài toán như hình ví dụ bên dưới",,,,,
"Mình vừa chiến thắng cuộc thi SIIM-FISABIO-RSNA COVID-19 Detection của Kaggle.
https://www.kaggle.com/c/siim-covid19-detection/leaderboard
Cuộc thi này là sự kết hợp của 2 task classification và detection.
Source code của mình dùng để chiến thắng cuộc thi này: https://github.com/dungnb1333/SIIM-COVID19-Detection
Mời mọi người tham khảo.",Mình vừa chiến thắng cuộc thi SIIM-FISABIO-RSNA COVID-19 Detection của Kaggle. https://www.kaggle.com/c/siim-covid19-detection/leaderboard Cuộc thi này là sự kết hợp của 2 task classification và detection. Source code của mình dùng để chiến thắng cuộc thi này: https://github.com/dungnb1333/SIIM-COVID19-Detection Mời mọi người tham khảo.,,,,,
"Khi train Yolo trên colab. Làm thế nào để ra được biểu đồ Loss - mAP thế này các bác nhỉ.
E đã thêm lệnh -map như ở dưới rồi mà vẫn không tìm thấy file ảnh đâu:
! ./darknet detector train obj.data yolov3-tiny_obj.cfg yolov3-tiny.conv.15 -map
---------
Solved: Bật Opencv=1 lên sẽ xuất hiện file chart.png",Khi train Yolo trên colab. Làm thế nào để ra được biểu đồ Loss - mAP thế này các bác nhỉ. E đã thêm lệnh -map như ở dưới rồi mà vẫn không tìm thấy file ảnh đâu: ! ./darknet detector train obj.data yolov3-tiny_obj.cfg yolov3-tiny.conv.15 -map --------- Solved: Bật Opencv=1 lên sẽ xuất hiện file chart.png,,,,,
"Chào mọi người
E đang làm 1 dự án cá nhân nhỏ để tìm hiểu về NLP( tiếng anh ). Ý tưởng của e là giải quyết vấn đề tìm tên phim dựa trên những mô tả của user.
Giải pháp em đang vạch ra là tạo ra 1 bộ lọc gồm nhiều yếu tố khác nhau nhưng quan trọng nhất vẫn là người dùng viết 1 đoạn description mô tả về phim cần tìm. Input là dạng text và thường dao động từ 5-15 câu. Để truy vấn ra tên phim cần tìm thì model sẽ so sánh giữa đoạn description nhập vào và dataset gồm các đoạn plot của wikipedia mô tả rất chi tiết về cốt truyện của 3000 phim ( dao động từ 60-70 câu ) để tìm ra điểm text similarity cao nhất sau đó đề xuất các phim phù hợp với miêu tả nhất. 1 số techniques e có search ra là TF-IDF, word2vec sau đó dùng consine similarity để tính điểm.
Nhưng vấn đề là sự chênh lệch giữa độ dài giữa input và data, các chi tiết của input cũng nằm rải rác trong đoạn plot, nhân vật trong input cũng đứng ngôi thứ 3 thay vì trong plot là có tên riêng rõ ràng,...
Cho e hỏi là có những phương pháp gì để giải quyết các bài toán trên ( có thể dùng text summarize lên data plot để rút ngắn plot được không ạ )
#NLP","Chào mọi người E đang làm 1 dự án cá nhân nhỏ để tìm hiểu về NLP( tiếng anh ). Ý tưởng của e là giải quyết vấn đề tìm tên phim dựa trên những mô tả của user. Giải pháp em đang vạch ra là tạo ra 1 bộ lọc gồm nhiều yếu tố khác nhau nhưng quan trọng nhất vẫn là người dùng viết 1 đoạn description mô tả về phim cần tìm. Input là dạng text và thường dao động từ 5-15 câu. Để truy vấn ra tên phim cần tìm thì model sẽ so sánh giữa đoạn description nhập vào và dataset gồm các đoạn plot của wikipedia mô tả rất chi tiết về cốt truyện của 3000 phim ( dao động từ 60-70 câu ) để tìm ra điểm text similarity cao nhất sau đó đề xuất các phim phù hợp với miêu tả nhất. 1 số techniques e có search ra là TF-IDF, word2vec sau đó dùng consine similarity để tính điểm. Nhưng vấn đề là sự chênh lệch giữa độ dài giữa input và data, các chi tiết của input cũng nằm rải rác trong đoạn plot, nhân vật trong input cũng đứng ngôi thứ 3 thay vì trong plot là có tên riêng rõ ràng,... Cho e hỏi là có những phương pháp gì để giải quyết các bài toán trên ( có thể dùng text summarize lên data plot để rút ngắn plot được không ạ )",#NLP,,,,
"Dạo các khóa trên udemy với coursera thì thấy cái này trông ""ngon lành nhất"". Có bác nào học rồi cho mình hỏi Certificate của 5 course này có đáng đồng tiền bát gạo ko ạ. Thanks ae","Dạo các khóa trên udemy với coursera thì thấy cái này trông ""ngon lành nhất"". Có bác nào học rồi cho mình hỏi Certificate của 5 course này có đáng đồng tiền bát gạo ko ạ. Thanks ae",,,,,
"Em chào mọi người ạ
Em đang tìm hiểu về mạng neural Bayes , đọc slide đến đoạn dấu tích phân này mà e không hiểu lắm. Mọi người cho e hỏi ý nghĩa của công thức có dấu tích phân trong hình được không ạ
Em cảm ơn moij người nhiều","Em chào mọi người ạ Em đang tìm hiểu về mạng neural Bayes , đọc slide đến đoạn dấu tích phân này mà e không hiểu lắm. Mọi người cho e hỏi ý nghĩa của công thức có dấu tích phân trong hình được không ạ Em cảm ơn moij người nhiều",,,,,
"Em đang build 1 model translate Eng-Vi và em dùng open-NMT không biết mọi người ai đã từng làm về translate rồi cho em hỏi 1 chút về preprocess , encoder và decoder tiếng việt mọi người thường làm thế nào ?","Em đang build 1 model translate Eng-Vi và em dùng open-NMT không biết mọi người ai đã từng làm về translate rồi cho em hỏi 1 chút về preprocess , encoder và decoder tiếng việt mọi người thường làm thế nào ?",,,,,
"DeepMind ra mắt PonderNet- thuât toán AI mới cho phép suy nghĩ kỹ trước khi trả lời, thông tin chi tiết tại paper dưới","DeepMind ra mắt PonderNet- thuât toán AI mới cho phép suy nghĩ kỹ trước khi trả lời, thông tin chi tiết tại paper dưới",,,,,
"Tháng trước mình có đăng post này thông báo trong tháng 7 sẽ có ebook nhưng rất tiếc nhóm dịch không làm kịp xong thủ tục.
Công việc biên tập đã hoàn chỉnh, hiện giờ chỉ đang chờ giấy phép xuất bản và phát hành. Hy vọng mọi thủ tục xong trong tháng 8.
Tình hình shipper khan hiếm nên giấy phép xuất bản sẽ về muộn một chút. Rất may là nhóm đã chọn làm ebook ngay từ đầu, nếu làm sách giấy giờ có lẽ rất khó chuyển phát.","Tháng trước mình có đăng post này thông báo trong tháng 7 sẽ có ebook nhưng rất tiếc nhóm dịch không làm kịp xong thủ tục. Công việc biên tập đã hoàn chỉnh, hiện giờ chỉ đang chờ giấy phép xuất bản và phát hành. Hy vọng mọi thủ tục xong trong tháng 8. Tình hình shipper khan hiếm nên giấy phép xuất bản sẽ về muộn một chút. Rất may là nhóm đã chọn làm ebook ngay từ đầu, nếu làm sách giấy giờ có lẽ rất khó chuyển phát.",,,,,
"https://play.google.com/store/apps/details?id=com.app_cnx.image2sketch
Chia sẻ với mọi người dự án triển khai model GAN lên thiết bị di động chạy offline trên thiết bị di động không cầm server. Model GAN này có nhiệm vụ chuyển đổi từ anh thường sang dạng sketch style rất mong được sự góp ý của mọi người về dự án.",https://play.google.com/store/apps/details?id=com.app_cnx.image2sketch Chia sẻ với mọi người dự án triển khai model GAN lên thiết bị di động chạy offline trên thiết bị di động không cầm server. Model GAN này có nhiệm vụ chuyển đổi từ anh thường sang dạng sketch style rất mong được sự góp ý của mọi người về dự án.,,,,,
"Hỏi về ngôn ngữ R.
.
Ở đây có một đoạn code R, đang query dữ liệu trong một file excel bên ngoài.
Mình ko hiểu cú pháp ở trong phần gạch dưới, nhưng cũng ko biết phải search từ khóa gì để tham khảo.
Mọi người ai biết có thể giải thích đoạn code giúp mình hoặc cho mình TỪ KHÓA để search với.
.
Thanks :D
(Hình 1: code, Hình 2: file excel 192_CO1007.xlsx)","Hỏi về ngôn ngữ R. . Ở đây có một đoạn code R, đang query dữ liệu trong một file excel bên ngoài. Mình ko hiểu cú pháp ở trong phần gạch dưới, nhưng cũng ko biết phải search từ khóa gì để tham khảo. Mọi người ai biết có thể giải thích đoạn code giúp mình hoặc cho mình TỪ KHÓA để search với. . Thanks :D (Hình 1: code, Hình 2: file excel 192_CO1007.xlsx)",,,,,
"Giới thiệu với các bạn phần tiếp theo của cuốn ""Machine Learning với dữ liệu dạng bảng"":
https://machinelearningcoban.com/tabml_book/ch_recommendation_system/introduction.html
Mình viết lại phần ""Hệ thống gợi ý"" của ""Machine Learning cơ bản"" với những góc nhìn thực tế hơn và ít nặng về toán. Mình cũng bổ sung phần ""Factorization Machine"" khá thú vị khi các dữ liệu bên lề về người dùng và sản phẩm có thể được tận dụng. Vấn đề ""khởi đầu lạnh"" (cold-start problem) cũng được đề cập cùng với một vài kỹ thuật khắc phục.
Chúc mọi người an toàn trong mùa dịch!","Giới thiệu với các bạn phần tiếp theo của cuốn ""Machine Learning với dữ liệu dạng bảng"": https://machinelearningcoban.com/tabml_book/ch_recommendation_system/introduction.html Mình viết lại phần ""Hệ thống gợi ý"" của ""Machine Learning cơ bản"" với những góc nhìn thực tế hơn và ít nặng về toán. Mình cũng bổ sung phần ""Factorization Machine"" khá thú vị khi các dữ liệu bên lề về người dùng và sản phẩm có thể được tận dụng. Vấn đề ""khởi đầu lạnh"" (cold-start problem) cũng được đề cập cùng với một vài kỹ thuật khắc phục. Chúc mọi người an toàn trong mùa dịch!",,,,,
"Em chào anh chị ạ,
Hiện tại em mới bắt đầu tìm hiểu về thị giác máy tính nên có một số câu hỏi muốn học hỏi thêm ạ:
Làm sao nén ảnh từ định dạng png, jpg hoặc bất cứ định dạng ảnh nào vào máy một cách tối ưu. Em có tham khảo một tập dữ liệu CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html) thì thấy file chỉ có 163MB thôi mà lưu được hẵn 60000 ảnh. Em không biết có cách nào lưu ảnh tối ưu vừa giảm được dung lượng mà vẫn giữ được thông tin ảnh trước khi đưa vào mô hình máy học không ạ?
Làm sao để so sánh một danh sách ảnh và một danh sách ảnh khác có các ảnh nào trong tập này gần giống với tập kia trong thời gian nhanh nhất mà vừa tiết kiệm bộ nhớ nhất (Ví dụ có tập A bao gồm 1,7 triệu hình, và tập B có 10k hình thì làm sao mình có thể tìm được các ảnh trong tập B giống trong tập A một cách nhanh nhất, em hy vọng có giải pháp nào đó tìm trong 30 phút). Hiện tại em đang làm bằng cách lưu ảnh dưới dạng vector và so sánh cosine similarity của từng ảnh trong tập B với danh sách ảnh trong tập A nhưng kích thước để lưu vector ảnh trong tập A quá lớn, lên đến cả trăm GB và thời gian so sánh cũng rất lâu.
Em cũng mới tìm hiểu về thị giác máy tính nên hy vọng được học hỏi từ mọi người. Em cảm ơn rất nhiều ạ.","Em chào anh chị ạ, Hiện tại em mới bắt đầu tìm hiểu về thị giác máy tính nên có một số câu hỏi muốn học hỏi thêm ạ: Làm sao nén ảnh từ định dạng png, jpg hoặc bất cứ định dạng ảnh nào vào máy một cách tối ưu. Em có tham khảo một tập dữ liệu CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html) thì thấy file chỉ có 163MB thôi mà lưu được hẵn 60000 ảnh. Em không biết có cách nào lưu ảnh tối ưu vừa giảm được dung lượng mà vẫn giữ được thông tin ảnh trước khi đưa vào mô hình máy học không ạ? Làm sao để so sánh một danh sách ảnh và một danh sách ảnh khác có các ảnh nào trong tập này gần giống với tập kia trong thời gian nhanh nhất mà vừa tiết kiệm bộ nhớ nhất (Ví dụ có tập A bao gồm 1,7 triệu hình, và tập B có 10k hình thì làm sao mình có thể tìm được các ảnh trong tập B giống trong tập A một cách nhanh nhất, em hy vọng có giải pháp nào đó tìm trong 30 phút). Hiện tại em đang làm bằng cách lưu ảnh dưới dạng vector và so sánh cosine similarity của từng ảnh trong tập B với danh sách ảnh trong tập A nhưng kích thước để lưu vector ảnh trong tập A quá lớn, lên đến cả trăm GB và thời gian so sánh cũng rất lâu. Em cũng mới tìm hiểu về thị giác máy tính nên hy vọng được học hỏi từ mọi người. Em cảm ơn rất nhiều ạ.",,,,,
"[HỎI]
Mọi người cho mình hỏi ở hàm computeSVD ở pyspark  trả về V là n*k 
còn ở hàm scipy.sparse.linalg.svds  trả về Vt là  k*n 
mình chưa hiểu lắm mong cao nhân giải đáp giúp mình với ạ",[HỎI] Mọi người cho mình hỏi ở hàm computeSVD ở pyspark trả về V là n*k còn ở hàm scipy.sparse.linalg.svds trả về Vt là k*n mình chưa hiểu lắm mong cao nhân giải đáp giúp mình với ạ,,,,,
"Mọi người cho em là về phần Information Retrieval and Web Search thì kiến thức này có giúp em làm được khi search 1 sản phẩm này trên web rồi sẽ tự đồng tìm các sản phẩm liên quan không ạ ?
(Em đang tìm hiểu về phần này để áp dụng vào phát triển web mà không biết có đúng phần kiến thức này bên AI không)",Mọi người cho em là về phần Information Retrieval and Web Search thì kiến thức này có giúp em làm được khi search 1 sản phẩm này trên web rồi sẽ tự đồng tìm các sản phẩm liên quan không ạ ? (Em đang tìm hiểu về phần này để áp dụng vào phát triển web mà không biết có đúng phần kiến thức này bên AI không),,,,,
#FeatureEngineering,,#FeatureEngineering,,,,
,nan,,,,,
"Cao nhân từng build docker sử dụng GPU và CUDA cho em hỏi, là em chạy docker với --gpus nhưng bị ERROR: ""docker: Error response from daemon: could not select device driver """" with capabilities: [[gpu]]."" là tại sao nhỉ.
Em đã cài cuda toolkit trên Root. Và trên User của em thì đang sử dụng docker rootless.
Mong các cao nhân chỉ lối. Em xin cám ơn.","Cao nhân từng build docker sử dụng GPU và CUDA cho em hỏi, là em chạy docker với --gpus nhưng bị ERROR: ""docker: Error response from daemon: could not select device driver """" with capabilities: [[gpu]]."" là tại sao nhỉ. Em đã cài cuda toolkit trên Root. Và trên User của em thì đang sử dụng docker rootless. Mong các cao nhân chỉ lối. Em xin cám ơn.",,,,,
"Hi mn, cho e hỏi chút về project structure ạ, em đã search trong group r mà hình như chưa thấy có ai nhắc tới. Hôm nay e clone 1 số winner solution ở kaggle về thì thấy một số vấn đề chưa hiểu về project structure (do e k phải dân Software và quen với code trên notebook):
1. Mọi người tổ chức cây thư mục như thế nào?
2. E thấy họ đa phần dùng argparse, mục đích là để truyền tham số trong lúc chạy trên terminal luôn đúng k ạ ??
3. File config chứa hyperparameter là dạng json, mục đích là để thuận tiện trong lúc chạy đúng k ạ??
Thanks","Hi mn, cho e hỏi chút về project structure ạ, em đã search trong group r mà hình như chưa thấy có ai nhắc tới. Hôm nay e clone 1 số winner solution ở kaggle về thì thấy một số vấn đề chưa hiểu về project structure (do e k phải dân Software và quen với code trên notebook): 1. Mọi người tổ chức cây thư mục như thế nào? 2. E thấy họ đa phần dùng argparse, mục đích là để truyền tham số trong lúc chạy trên terminal luôn đúng k ạ ?? 3. File config chứa hyperparameter là dạng json, mục đích là để thuận tiện trong lúc chạy đúng k ạ?? Thanks",,,,,
"[AI Share - Deep Learning Visuals]
Hiện nay, việc sử dụng các hình ảnh trực quan giúp cho việc truyền tải nội dung khi thuyết trình trở nên dễ dàng hơn.
Deep Learing Visuals chứa hơn 200 hình ảnh trực quan về các kiến trúc và các lớp phổ biến nhất trong deep learning. Mọi người có thể sử dụng miễn phí trong các bài đăng, slide, paper hoặc luận văn, đồ án của riêng mình.","[AI Share - Deep Learning Visuals] Hiện nay, việc sử dụng các hình ảnh trực quan giúp cho việc truyền tải nội dung khi thuyết trình trở nên dễ dàng hơn. Deep Learing Visuals chứa hơn 200 hình ảnh trực quan về các kiến trúc và các lớp phổ biến nhất trong deep learning. Mọi người có thể sử dụng miễn phí trong các bài đăng, slide, paper hoặc luận văn, đồ án của riêng mình.",,,,,
"Chào mọi người, em đang làm 1 face recognization project. Theo như em tìm hiểu thì bài toán được chia làm 2 bài toán con là:
1. Face detection
2. Classification face
Đối với bài toán 1, có nhiều phương pháp như sử dụng HoG hoặc LBP để detect ra vùng có chứa khuôn mặt. Nhưng cách này đem lại độ chính xác không cao trong các trường hợp (mặt nghiêng, khuôn mặt có râu, đeo kính, ....), nhưng thời gian tính toán nhanh. Chính vì thế em chuyển sang tìm hiểu về Deep learning, em có cài đặt 2 thuật toán dnn_face_detector của thư viện dlib và face_detector_cafee của thư viện opencv. 2 Thuật toán này về độ chính xác thì tốt trong các trường hợp mà Hog và LBP không giải quyết được. Nhưng đổi lại thì thời gian tính toán (nhất là sử dụng dnn_face_detetor của dlib, 5 đến 6 giây cho mỗi ảnh). Do vậy em muốn tham khảo mọi người:
1. Có giải pháp nào sử dụng mang deep để real-time proccess?
2. Sau khi detect được face em có nên sử dụng các thuật toán phân lớp (classification) cổ điển như (SVM, kNN...) để sử dụng cho phase face recognize không?
3. Mục tiêu cuối cùng của em là, vì GPU có giá quá đắt và đối khi không phù hợp với các máy tính nhỏ như Ras Pi. Nhưng vẫn mong muốn độ chính xác nhận dạng ở mức tốt. Và bài toán của em là áp dụng cho dữ liệu camera chứ không phải ảnh đơn.
Cám ơn mọi người nhiều!","Chào mọi người, em đang làm 1 face recognization project. Theo như em tìm hiểu thì bài toán được chia làm 2 bài toán con là: 1. Face detection 2. Classification face Đối với bài toán 1, có nhiều phương pháp như sử dụng HoG hoặc LBP để detect ra vùng có chứa khuôn mặt. Nhưng cách này đem lại độ chính xác không cao trong các trường hợp (mặt nghiêng, khuôn mặt có râu, đeo kính, ....), nhưng thời gian tính toán nhanh. Chính vì thế em chuyển sang tìm hiểu về Deep learning, em có cài đặt 2 thuật toán dnn_face_detector của thư viện dlib và face_detector_cafee của thư viện opencv. 2 Thuật toán này về độ chính xác thì tốt trong các trường hợp mà Hog và LBP không giải quyết được. Nhưng đổi lại thì thời gian tính toán (nhất là sử dụng dnn_face_detetor của dlib, 5 đến 6 giây cho mỗi ảnh). Do vậy em muốn tham khảo mọi người: 1. Có giải pháp nào sử dụng mang deep để real-time proccess? 2. Sau khi detect được face em có nên sử dụng các thuật toán phân lớp (classification) cổ điển như (SVM, kNN...) để sử dụng cho phase face recognize không? 3. Mục tiêu cuối cùng của em là, vì GPU có giá quá đắt và đối khi không phù hợp với các máy tính nhỏ như Ras Pi. Nhưng vẫn mong muốn độ chính xác nhận dạng ở mức tốt. Và bài toán của em là áp dụng cho dữ liệu camera chứ không phải ảnh đơn. Cám ơn mọi người nhiều!",,,,,
"Hiện nay mình có đọc 1 số bài viết về maximum likelihood.
Và thấy cách giải là lấy đạo hàm của likelihood và giải phuơng trình đạo hàm bằng 0.
Nhưng mình nhớ đạo hàm bằng 0 thì hoàn toàn có thể là cực tiểu hoặc là cực đại (còn phụ thuộc vào đạo hàm bậc 2 của hàm số).
Tuy nhiên mình thấy toàn bộ các tài liệu mình đọc qua đều thấy tìm max bằng phuơng pháp này.
Không biết tư duy của mình có bị sai ở điểm nào không, mọi ngưòi góp ý cho mình với ạ","Hiện nay mình có đọc 1 số bài viết về maximum likelihood. Và thấy cách giải là lấy đạo hàm của likelihood và giải phuơng trình đạo hàm bằng 0. Nhưng mình nhớ đạo hàm bằng 0 thì hoàn toàn có thể là cực tiểu hoặc là cực đại (còn phụ thuộc vào đạo hàm bậc 2 của hàm số). Tuy nhiên mình thấy toàn bộ các tài liệu mình đọc qua đều thấy tìm max bằng phuơng pháp này. Không biết tư duy của mình có bị sai ở điểm nào không, mọi ngưòi góp ý cho mình với ạ",,,,,
"Chào mọi người 😁
Em mới bắt đầu học ML và đang làm tập làm project cơ bản.
Hiện em đang muốn deploy ML model với app nhưng đa phần các khoá dạy trên mạng chỉ dạy connect app với ML model đã đc train rồi chứ không chỉ nếu có thêm data mới thì phải train tiếp làm sao.
Ví dụ như video dưới đây ạ. Nếu em add thêm data để tạo một class mới như vậy thì làm sao để retrain model với new data ạ.
Anh chị nào có thể chia sẻ kinh nghiệm hoặc khoá học nào cho em tham khảo không ạ. Em cảm ơn nhiều.",Chào mọi người Em mới bắt đầu học ML và đang làm tập làm project cơ bản. Hiện em đang muốn deploy ML model với app nhưng đa phần các khoá dạy trên mạng chỉ dạy connect app với ML model đã đc train rồi chứ không chỉ nếu có thêm data mới thì phải train tiếp làm sao. Ví dụ như video dưới đây ạ. Nếu em add thêm data để tạo một class mới như vậy thì làm sao để retrain model với new data ạ. Anh chị nào có thể chia sẻ kinh nghiệm hoặc khoá học nào cho em tham khảo không ạ. Em cảm ơn nhiều.,,,,,
"[AI Share]
DeepLearning-500-questions là một bộ sưu tập các câu hỏi từ các cuộc phỏng vấn dành cho các kĩ sư AI được một số tiến sĩ Trung Quốc biên tập lại thành sách đã được hơn 45k sao trên github. Nội dung cuốn sách này dựa trên những kiến thức được đúc kết trong quá trình học hàng ngày và các bài thi viết, câu hỏi phỏng vấn thường gặp của các công ty lớn. Tuy nhiên, sách này được viết bằng tiếng Trung Quốc. Gần đây, các câu hỏi này mới có version tiếng anh trên github với 4 phần chính sau:
Chương 1: Mathematial Basis
Chương 2: Machine Learning Foundation
Chương 3: Deep Learning Foundation
Chương 4: Classis Network","[AI Share] DeepLearning-500-questions là một bộ sưu tập các câu hỏi từ các cuộc phỏng vấn dành cho các kĩ sư AI được một số tiến sĩ Trung Quốc biên tập lại thành sách đã được hơn 45k sao trên github. Nội dung cuốn sách này dựa trên những kiến thức được đúc kết trong quá trình học hàng ngày và các bài thi viết, câu hỏi phỏng vấn thường gặp của các công ty lớn. Tuy nhiên, sách này được viết bằng tiếng Trung Quốc. Gần đây, các câu hỏi này mới có version tiếng anh trên github với 4 phần chính sau: Chương 1: Mathematial Basis Chương 2: Machine Learning Foundation Chương 3: Deep Learning Foundation Chương 4: Classis Network",,,,,
"em chào mọi người ạ. E đang gặp 1 chút vấn đề mong mọi ng giải đáp. E có train mobilenet v2 có sử dụng pretrain với imagenet cho bộ dữ liệu riêng của em. Khi e train bằng keras thì kết quả tốt nhưng khi e chuyển qua Pytorch thì kết quả lại tệ đi rất nhiều, ngay chu kì đầu thì cũng đã overffit như mọi ng có thể thấy dưới hình( dữ liệu, loss function, optimizer,... đều giống nhau, mode có khác 1 chút tham số ở phần batchnorm). E cũng đã thử điều chỉnh các hyperparameters rồi nhưng vẫn k cải thiện đc nhiều. Ko biết lí do là do đâu ạ","em chào mọi người ạ. E đang gặp 1 chút vấn đề mong mọi ng giải đáp. E có train mobilenet v2 có sử dụng pretrain với imagenet cho bộ dữ liệu riêng của em. Khi e train bằng keras thì kết quả tốt nhưng khi e chuyển qua Pytorch thì kết quả lại tệ đi rất nhiều, ngay chu kì đầu thì cũng đã overffit như mọi ng có thể thấy dưới hình( dữ liệu, loss function, optimizer,... đều giống nhau, mode có khác 1 chút tham số ở phần batchnorm). E cũng đã thử điều chỉnh các hyperparameters rồi nhưng vẫn k cải thiện đc nhiều. Ko biết lí do là do đâu ạ",,,,,
"Chào các anh chị, chả là em đang theo course Machine Learning của thầy Andrew Ng, tới phần Logistic Regression. Em có đọc cả blog của anh Tiệp để bổ trợ, cũng như hiểu rõ thêm.
Như data của anh Tiệp trong phần Logistic Regression, có thể thấy giá trị khá nhỏ, khi đó giả sử theta = 1, tính sigmoid(X dot theta) sẽ trong đoạn (0, 1). Nên không cần thiết phải rescale.
Nhưng với dữ liệu của thầy Andrew, thì khá lớn, do đó theta = [[1], [1], [1]], sigmoid(X dot theta) sẽ == 1 hoặc 0. Vậy sẽ không tìm được loss, vì không tính được log(0). Nhưng trong bài ko hề yêu cầu rescale, cũng như theta khởi tạo là [[0], [0], [0]].
Vậy có đúng là trong trường hợp này, với dữ liệu của thầy Andrew để có thể tính được loss, cũng như tìm được theta một cách thuật lợi bằng gradient descent, ta cần phải rescale? Hay là em nhầm lẫn, hoặc có lỗ hổng kiến thức nào đó ạ?
Em xin cảm ơn 🤗🤗🤗
ps: em submit bài, thì được pass. Mà khi code lại bằng python thì lại vướng ở loss function và gradient descent không hội tụ, nên hơi hoang mang.
ps2: còn đây là khi em rescale, mọi thứ có vẻ rất ổn áp https://github.com/tdbui1209/machine_learning_andrew_ng/blob/main/python/ex2/logistic_regression.ipynb","Chào các anh chị, chả là em đang theo course Machine Learning của thầy Andrew Ng, tới phần Logistic Regression. Em có đọc cả blog của anh Tiệp để bổ trợ, cũng như hiểu rõ thêm. Như data của anh Tiệp trong phần Logistic Regression, có thể thấy giá trị khá nhỏ, khi đó giả sử theta = 1, tính sigmoid(X dot theta) sẽ trong đoạn (0, 1). Nên không cần thiết phải rescale. Nhưng với dữ liệu của thầy Andrew, thì khá lớn, do đó theta = [[1], [1], [1]], sigmoid(X dot theta) sẽ == 1 hoặc 0. Vậy sẽ không tìm được loss, vì không tính được log(0). Nhưng trong bài ko hề yêu cầu rescale, cũng như theta khởi tạo là [[0], [0], [0]]. Vậy có đúng là trong trường hợp này, với dữ liệu của thầy Andrew để có thể tính được loss, cũng như tìm được theta một cách thuật lợi bằng gradient descent, ta cần phải rescale? Hay là em nhầm lẫn, hoặc có lỗ hổng kiến thức nào đó ạ? Em xin cảm ơn ps: em submit bài, thì được pass. Mà khi code lại bằng python thì lại vướng ở loss function và gradient descent không hội tụ, nên hơi hoang mang. ps2: còn đây là khi em rescale, mọi thứ có vẻ rất ổn áp https://github.com/tdbui1209/machine_learning_andrew_ng/blob/main/python/ex2/logistic_regression.ipynb",,,,,
,nan,,,,,
"Em/mình chào cả nhà,
Em đang làm một đề tài về tone của conversation giữa CEOs và analysts. Em cần thu thập transcript buổi nói chuyện (1 firm 1 lần vào q4) của S&P 500 firms qua 20 năm ~10,000 transcripts. Sau đó em tách mỗi transcript ra thành nội dung của CEOs vs analysts. Nội dung transcript giống link sau ạ.
Làm cách nào thì tốt hơn ạ?
(1)download các transcript dưới dạng txt rồi kêu python tách ra nội dung của CEOs vs analysts
(2)lưu link website của các transcript rồi kêu python đọc và tách ra nội dung của CEOs vs analysts","Em/mình chào cả nhà, Em đang làm một đề tài về tone của conversation giữa CEOs và analysts. Em cần thu thập transcript buổi nói chuyện (1 firm 1 lần vào q4) của S&P 500 firms qua 20 năm ~10,000 transcripts. Sau đó em tách mỗi transcript ra thành nội dung của CEOs vs analysts. Nội dung transcript giống link sau ạ. Làm cách nào thì tốt hơn ạ? (1)download các transcript dưới dạng txt rồi kêu python tách ra nội dung của CEOs vs analysts (2)lưu link website của các transcript rồi kêu python đọc và tách ra nội dung của CEOs vs analysts",,,,,
"Chào cả nhà, gần đây do trend Tranformer trong Computer Vision mà em đã tìm tòi và đọc được một bài báo rất hay bên Object Detection đó là DETR và thấy để là DETR ko cần NMS post-processing. Ai làm về Object Detection thì cug ít nhìu rất khó chịu về NMS 😅 nhưng thằng DETR này thì lại chả cần NMs lun @@. Em mới tìm hỉu về Transformer thui nên cug chưa hỉu lắm. Nên có cao thủ nào có thể giải đáp giúp tại sao DETR khác gì so vs các detector cũ như YOLO, SSD mà nó có thể làm đc như vậy? Em cảm ơn ạ","Chào cả nhà, gần đây do trend Tranformer trong Computer Vision mà em đã tìm tòi và đọc được một bài báo rất hay bên Object Detection đó là DETR và thấy để là DETR ko cần NMS post-processing. Ai làm về Object Detection thì cug ít nhìu rất khó chịu về NMS nhưng thằng DETR này thì lại chả cần NMs lun @@. Em mới tìm hỉu về Transformer thui nên cug chưa hỉu lắm. Nên có cao thủ nào có thể giải đáp giúp tại sao DETR khác gì so vs các detector cũ như YOLO, SSD mà nó có thể làm đc như vậy? Em cảm ơn ạ",,,,,
"Chào mọi người, mình đang tìm hiểu về các mô hình học trên dữ liệu đồ thị thì gặp thắc mắc với toán tử global max pooling. Mình có tìm đc công thức này cho các lớp pooling (h_i^K là ký hiệu cho hidden representation của nút i sau lớp graph convolutional cuối cùng - lớp thứ K). mean và sum thì có thể ngầm hiểu còn max thì được thực hiện như thế nào nhỉ? Cảm ơn mọi người đã giúp đỡ","Chào mọi người, mình đang tìm hiểu về các mô hình học trên dữ liệu đồ thị thì gặp thắc mắc với toán tử global max pooling. Mình có tìm đc công thức này cho các lớp pooling (h_i^K là ký hiệu cho hidden representation của nút i sau lớp graph convolutional cuối cùng - lớp thứ K). mean và sum thì có thể ngầm hiểu còn max thì được thực hiện như thế nào nhỉ? Cảm ơn mọi người đã giúp đỡ",,,,,
"Mọi người ơi cho em hỏi về Tensorboard.
Em có một file model.pb, em muốn hiển thị model graph trên Tensorboard. Em đã sử dụng import_pb_to_tensorboard.py ở đây https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py nhưng khi sử dụng Tensorboard thì chỉ xuất graph như ảnh dưới.
Mọi người chỉ em cách fix với, hoặc có cách nào để có thể nhìn được các layers ở trong một model tensorflow thì có thể giúp em với. ","Mọi người ơi cho em hỏi về Tensorboard. Em có một file model.pb, em muốn hiển thị model graph trên Tensorboard. Em đã sử dụng import_pb_to_tensorboard.py ở đây https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py nhưng khi sử dụng Tensorboard thì chỉ xuất graph như ảnh dưới. Mọi người chỉ em cách fix với, hoặc có cách nào để có thể nhìn được các layers ở trong một model tensorflow thì có thể giúp em với.",,,,,
"Em chào anh chị. Em đang làm bước augment cho các text dạng ảnh nhưng bị vướng phần tạo shadow cho text (như Hình 1) và bẻ cong chữ (đơn giản như hình parabol thôi như Hình 2) ạ. Mong mọi người giúp đỡ, đơn giản bằng xử lý ảnh thuần với opencv càng tốt ạ. Em cảm ơn mọi người nhiều.","Em chào anh chị. Em đang làm bước augment cho các text dạng ảnh nhưng bị vướng phần tạo shadow cho text (như Hình 1) và bẻ cong chữ (đơn giản như hình parabol thôi như Hình 2) ạ. Mong mọi người giúp đỡ, đơn giản bằng xử lý ảnh thuần với opencv càng tốt ạ. Em cảm ơn mọi người nhiều.",,,,,
"FYI: Awesome Document Understanding
https://github.com/tstanislawek/awesome-document-understanding",FYI: Awesome Document Understanding https://github.com/tstanislawek/awesome-document-understanding,,,,,
"Mọi người cho em hỏi là trong bài 12 đến đoạn nhận diện nam nữ. em đang không biết là biến total_imgs tác giả dùng để làm gì ạ. theo em nghĩ đấy là tổng số chiều tất cả các ảnh nhưng mà shape của nó lại là (700, 19800). tổng ảnh chỉ là 25 tấm. tức là shape của nó phải là (25,19800) hoặc (1,495,000) chứ ạ? mong mọi người đóng góp ý kiến ạ em cảm ơn. hoặc ai có thời gian rảnh có thể hướng dẫn e bài này với ạ.
link bài viết : https://machinelearningcoban.com/2017/02/11/binaryclassifiers/","Mọi người cho em hỏi là trong bài 12 đến đoạn nhận diện nam nữ. em đang không biết là biến total_imgs tác giả dùng để làm gì ạ. theo em nghĩ đấy là tổng số chiều tất cả các ảnh nhưng mà shape của nó lại là (700, 19800). tổng ảnh chỉ là 25 tấm. tức là shape của nó phải là (25,19800) hoặc (1,495,000) chứ ạ? mong mọi người đóng góp ý kiến ạ em cảm ơn. hoặc ai có thời gian rảnh có thể hướng dẫn e bài này với ạ. link bài viết : https://machinelearningcoban.com/2017/02/11/binaryclassifiers/",,,,,
"Mọi người ơi cho mình hỏi nếu mình dùng pytorch và torch vision để train Faster R-CNN: nếu mình có pretrained backbone sẵn rồi thì có cách nào chỉ train RPN của Faster R-CNN không nhỉ. Có pass paramater vào create model của torch vision hay mình thay đổi ở train() nhỉ
Create model():
Create your backbone from timm
backbone = timm.create_model(
""resnet50"",
pretrained=True,
num_classes=0, # remove fc layers
global_pool="""" #remove fc layers
)
#add ""out_channels"" variable to the backbone because FasterRCNN use it
backbone.out_channels = backbone.feature_info[-1][""num_chs""]
anchor_generator = AnchorGenerator(
sizes=((16, 32, 64, 128, 256),), aspect_ratios=((0.25, 0.5, 1.0, 2.0),)
)
roi_pooler = torchvision.ops.MultiScaleRoIAlign(
featmap_names=[""0""], output_size=7, sampling_ratio=2
)
fastercnn_model = FasterRCNN(
backbone=backbone,
num_classes=1000,
rpn_anchor_generator=anchor_generator,
box_roi_pool=roi_pooler,
)","Mọi người ơi cho mình hỏi nếu mình dùng pytorch và torch vision để train Faster R-CNN: nếu mình có pretrained backbone sẵn rồi thì có cách nào chỉ train RPN của Faster R-CNN không nhỉ. Có pass paramater vào create model của torch vision hay mình thay đổi ở train() nhỉ Create model(): Create your backbone from timm backbone = timm.create_model( ""resnet50"", pretrained=True, num_classes=0, # remove fc layers global_pool="""" fc layers ) ""out_channels"" variable to the backbone because FasterRCNN use it backbone.out_channels = backbone.feature_info[-1][""num_chs""] anchor_generator = AnchorGenerator( sizes=((16, 32, 64, 128, 256),), aspect_ratios=((0.25, 0.5, 1.0, 2.0),) ) roi_pooler = torchvision.ops.MultiScaleRoIAlign( featmap_names=[""0""], output_size=7, sampling_ratio=2 ) fastercnn_model = FasterRCNN( backbone=backbone, num_classes=1000, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler, )",#remove	#add,,,,
"Em chào anh chị trong group,
Em sinh đang viên năm 3, kỳ tới bên trường em có chương trình thực tập 4 tháng, nên em đang tìm một công ty hoặc một trung tâm đào tạo để được tiếp cận với mảng machine learning này.
Em bắt đầu tìm hiểu về machine learning được hơn 1 tháng nay, em bắt đầu học lại toán, thuật toán và tìm hiểu một số model cơ bản trên blog anh Tiệp và một số nguồn khác. Em có kiến thức cơ bản về thuật toán, database, ngôn ngữ lập trình (java, python và một chút về C#).
Anh chị cho em xin ý kiến em nên bắt đầu từ đâu để được tiếp cận với mảng machine learning này ngoài thực tế ạ?
Em cảm ơn Anh chị trong group.","Em chào anh chị trong group, Em sinh đang viên năm 3, kỳ tới bên trường em có chương trình thực tập 4 tháng, nên em đang tìm một công ty hoặc một trung tâm đào tạo để được tiếp cận với mảng machine learning này. Em bắt đầu tìm hiểu về machine learning được hơn 1 tháng nay, em bắt đầu học lại toán, thuật toán và tìm hiểu một số model cơ bản trên blog anh Tiệp và một số nguồn khác. Em có kiến thức cơ bản về thuật toán, database, ngôn ngữ lập trình (java, python và một chút về C#). Anh chị cho em xin ý kiến em nên bắt đầu từ đâu để được tiếp cận với mảng machine learning này ngoài thực tế ạ? Em cảm ơn Anh chị trong group.",,,,,
"Hi mng ah
Hiện tại e có đang research về active learning method thì liệu mng có thể cho e 1 vài cuốn sách thực sự chất lượng về method này hay những bài viết chất lượng ạ.E có đang gặp chút vấn đề là apply active learning tại iteration thứ 2 ko có cải thiện so với iteration 1 thì liệu vấn đề có phải do bộ unlabelled data chưa đủ lớn và đa dạng hay có hướng đi nào để giải quyết vấn để này ko ạa.À với cả phương pháp để đưa ra bộ validation data có thể đủ tốt để eval đc độ hiệu quả của phương pháp này ạ.
Tks mng nhiều ạaa 🙏",Hi mng ah Hiện tại e có đang research về active learning method thì liệu mng có thể cho e 1 vài cuốn sách thực sự chất lượng về method này hay những bài viết chất lượng ạ.E có đang gặp chút vấn đề là apply active learning tại iteration thứ 2 ko có cải thiện so với iteration 1 thì liệu vấn đề có phải do bộ unlabelled data chưa đủ lớn và đa dạng hay có hướng đi nào để giải quyết vấn để này ko ạa.À với cả phương pháp để đưa ra bộ validation data có thể đủ tốt để eval đc độ hiệu quả của phương pháp này ạ. Tks mng nhiều ạaa,,,,,
"Chào mọi người, cho mình hỏi là có ai ở Việt Nam mua colab pro chưa ạ? Dùng có ổn không ah? Do đến đoạn nhập zipcode thì mình để HCM thì nó không cho, theo câu trả lời này thì người ta có thể fake zipcode được. Nhưng mình sợ người ta phát hiện thì người ta khóa tài khoản luôn quá!
https://stackoverflow.com/questions/60240863/i-am-from-pakistan-can-i-buy-google-colab-pro-for-experiments?fbclid=IwAR1sxzuR8N0ZJfrbfWs2N8nu07AiLZYLaDRyRj4lalEXbFWuJz_FDEmad3U","Chào mọi người, cho mình hỏi là có ai ở Việt Nam mua colab pro chưa ạ? Dùng có ổn không ah? Do đến đoạn nhập zipcode thì mình để HCM thì nó không cho, theo câu trả lời này thì người ta có thể fake zipcode được. Nhưng mình sợ người ta phát hiện thì người ta khóa tài khoản luôn quá! https://stackoverflow.com/questions/60240863/i-am-from-pakistan-can-i-buy-google-colab-pro-for-experiments?fbclid=IwAR1sxzuR8N0ZJfrbfWs2N8nu07AiLZYLaDRyRj4lalEXbFWuJz_FDEmad3U",,,,,
mọi người ơi có ai có thể giúp em câu hỏi về reinforcement learning với được không ạ. Em không hiểu cái intermediate reward là gì ý ạ :(,mọi người ơi có ai có thể giúp em câu hỏi về reinforcement learning với được không ạ. Em không hiểu cái intermediate reward là gì ý ạ :(,,,,,
"Bí ẩn đằng sau giải thuật SGD. Liệu weight decay, BN, dropout... có thực sự quá quan trọng khi SGD cho rằng tự tôi cũng biết cách xử lý rồi?","Bí ẩn đằng sau giải thuật SGD. Liệu weight decay, BN, dropout... có thực sự quá quan trọng khi SGD cho rằng tự tôi cũng biết cách xử lý rồi?",,,,,
"Em chào mọi người ạ
Trong nhóm đã có ai từng làm về RNN-LSTM rồi cho em xin tài liệu mà dễ hiểu về mặt toán học một chút rồi cho e xin với ạ. Em cảm ơn mọi người nhiều",Em chào mọi người ạ Trong nhóm đã có ai từng làm về RNN-LSTM rồi cho em xin tài liệu mà dễ hiểu về mặt toán học một chút rồi cho e xin với ạ. Em cảm ơn mọi người nhiều,,"#deep_learning, #Q&A, #math",,,
"[Về cách chia train/validate/test cho bài toán Churn prediction] Mọi người cho mình hỏi làm sao để chia train/validate/test hợp lí cho bài toán churn prediction để tranh bị tình huống data leakage/nhất là trong time series. Cụ thể, mình muốn dự đoán khả năng khách nghỉ chơi với dịch vụ bên mình trong 3 tháng tiếp theo. Dữ liệu của mình có từ năm 2018 trở đi. Và tập feature được xây dựng bằng cách lấy hành vi của khách hàng trong 12 tháng trở về trước từ thời điểm đưa ra mô hình dự đoán, label là nhãn 0/1 khách hàng có rời đi trong 3 tháng từ thời điểm đưa ra dự đoán hay không. Mình có tìm trên các forum nước ngoài nhưng không thấy họ đề cập cách làm việc này hợp lí.","[Về cách chia train/validate/test cho bài toán Churn prediction] Mọi người cho mình hỏi làm sao để chia train/validate/test hợp lí cho bài toán churn prediction để tranh bị tình huống data leakage/nhất là trong time series. Cụ thể, mình muốn dự đoán khả năng khách nghỉ chơi với dịch vụ bên mình trong 3 tháng tiếp theo. Dữ liệu của mình có từ năm 2018 trở đi. Và tập feature được xây dựng bằng cách lấy hành vi của khách hàng trong 12 tháng trở về trước từ thời điểm đưa ra mô hình dự đoán, label là nhãn 0/1 khách hàng có rời đi trong 3 tháng từ thời điểm đưa ra dự đoán hay không. Mình có tìm trên các forum nước ngoài nhưng không thấy họ đề cập cách làm việc này hợp lí.",,,,,
"Nhập môn Học máy và Khai phá dữ liệu do thầy Thân Quang Khoát, Viện CNTT&TT, BKHN giảng day. Bài giảng cung cấp những khái niệm từ căn bản đến chuyên sâu, phù hợp với những người muốn tìm hiểu lĩnh vực này một cách bài bản mà chưa có nhiều kiến thức. Xin mời các bạn sinh viên quan tâm vào tìm hiểu.","Nhập môn Học máy và Khai phá dữ liệu do thầy Thân Quang Khoát, Viện CNTT&TT, BKHN giảng day. Bài giảng cung cấp những khái niệm từ căn bản đến chuyên sâu, phù hợp với những người muốn tìm hiểu lĩnh vực này một cách bài bản mà chưa có nhiều kiến thức. Xin mời các bạn sinh viên quan tâm vào tìm hiểu.",,,,,
"Mọi người cho em hỏi với ạ, em đang viết một API gọi lên server để huấn luyện model trên đó, tuy nhiên vì xử lý đồng bộ nên phía client cứ quay liên tục đến khi nào server chạy xong hàm training ra model, gửi kết quả lại mới thôi
Có cách nào để server gửi về client message mỗi lần training được 1 epoch không ạ?
Em cảm ơn ạ","Mọi người cho em hỏi với ạ, em đang viết một API gọi lên server để huấn luyện model trên đó, tuy nhiên vì xử lý đồng bộ nên phía client cứ quay liên tục đến khi nào server chạy xong hàm training ra model, gửi kết quả lại mới thôi Có cách nào để server gửi về client message mỗi lần training được 1 epoch không ạ? Em cảm ơn ạ",,,,,
"Chào mọi người,
Hiện nay có rất nhiều bạn trẻ đặt chân vào lĩnh vực AI/ML/DL. Các bài báo cũng nói rằng lĩnh vực này mới nên rất cần nhiều nguồn nhân lực.
Nhưng thực tế mình dạo trên các nhóm và trang web việc làm thì thấy những công việc liên quan thật sự không nhiều so với các lĩnh vực khác. Đặc biệt các công việc dành cho những người mới ra trường chưa có kinh nghiệm thì càng hiếm hơn nữa. Vì vậy bạn phải thực sự giỏi thì mới cạnh tranh và có việc làm được.
Thực tế này là lý do chính khiến mình phân vân có nên tiếp tục theo con đường này hay không. Mình viết bài này là mong muốn được các anh chị/ các bạn chia sẻ kinh nghiệm và tư vấn giúp mình cũng như các bạn có cùng hoàn cảnh về cơ hội việc làm và kĩ năng cần có để theo đuổi lĩnh vực này ạ.
Cảm ơn mọi người.","Chào mọi người, Hiện nay có rất nhiều bạn trẻ đặt chân vào lĩnh vực AI/ML/DL. Các bài báo cũng nói rằng lĩnh vực này mới nên rất cần nhiều nguồn nhân lực. Nhưng thực tế mình dạo trên các nhóm và trang web việc làm thì thấy những công việc liên quan thật sự không nhiều so với các lĩnh vực khác. Đặc biệt các công việc dành cho những người mới ra trường chưa có kinh nghiệm thì càng hiếm hơn nữa. Vì vậy bạn phải thực sự giỏi thì mới cạnh tranh và có việc làm được. Thực tế này là lý do chính khiến mình phân vân có nên tiếp tục theo con đường này hay không. Mình viết bài này là mong muốn được các anh chị/ các bạn chia sẻ kinh nghiệm và tư vấn giúp mình cũng như các bạn có cùng hoàn cảnh về cơ hội việc làm và kĩ năng cần có để theo đuổi lĩnh vực này ạ. Cảm ơn mọi người.",,,,,
Xin cho mình được hỏi: Khi thực hiện quá trình data visualization bộ số liệu Iris thì mình thấy SepalWidth có outliers. Nhưng trong các giải thuật ví dụ cả về ML lẫn DL đối với bộ số liệu này được trình bày trên internet mình không thấy có một chỗ nào về việc data preprocessing để xử lý các outliers này. Mọi người có thể giải thích giúp mình tại sao lại như vậy không ạ? Xin cảm ơn các ý kiến góp ý của các bạn.,Xin cho mình được hỏi: Khi thực hiện quá trình data visualization bộ số liệu Iris thì mình thấy SepalWidth có outliers. Nhưng trong các giải thuật ví dụ cả về ML lẫn DL đối với bộ số liệu này được trình bày trên internet mình không thấy có một chỗ nào về việc data preprocessing để xử lý các outliers này. Mọi người có thể giải thích giúp mình tại sao lại như vậy không ạ? Xin cảm ơn các ý kiến góp ý của các bạn.,,,,,
"Các chị em yêu công nghệ ơi, trong thời gian giãn cách xã hội căng thẳng này, hãy cùng Women Techmakers Hanoi (thuộc Google Developers Group - GDG Hà Nội) lan tỏa những điều tích cực và truyền cảm hứng cho nhau bằng cách tham gia sự kiện online cực hay ho mang tên: Women Techmakers Hanoi x Sutunam: Her Tech Story.
Her tech story là sự kiện dành cho tất cả các bạn nữ yêu thích công nghệ và đang làm trong lĩnh vực này có cơ hội chia sẻ những câu chuyện thú vị của riêng mình trong quá trình học tập và làm việc.
Sau khi nhận bài viết của bạn, chúng mình sẽ tổng hợp và đăng tất cả các bài viết lên trang Facebook của sự kiện từ ngày 13/08/2021.
Để khuyến khích những câu chuyện được lan tỏa một cách rộng rãi, chúng mình đã chuẩn bị 𝟮 𝗴𝗶𝗮̉𝗶 𝘁𝗵𝘂̛𝗼̛̉𝗻𝗴 𝗰𝗵𝗼 𝟮 𝗯𝗮̀𝗶 𝘃𝗶𝗲̂́𝘁 𝘁𝗿𝘂𝘆𝗲̂̀𝗻 𝗰𝗮̉𝗺 𝗵𝘂̛́𝗻𝗴 𝗻𝗵𝗮̂́𝘁:
🏆𝗠𝗼̂̃𝗶 𝗴𝗼́𝗶 𝗴𝗶𝗮̉𝗶 𝘁𝗵𝘂̛𝗼̛̉𝗻𝗴 𝗴𝗼̂̀𝗺:
- 1 năm học tiếng Anh miễn phí trên ELSA speak hoặc tài trợ 100% chi phí để thi lấy chứng chỉ công nghệ mà công ty đối tác sử dụng. (Tương đương với 3,000,000 VND trở lên)
- Bài thi được featured trên mọi kênh social của công ty đối tác (Facebook, Linkedin, Twitter) cũng như trang blog công nghệ chính thức, trang việc làm
- 1 ngày tham gia trải nghiệm văn hóa trực tiếp tại công ty đối tác.
- Giấy chứng nhận đạt giải từ công ty đối tác
- 1 set quà tặng kỷ niệm từ công ty đối tác
- 1 set quà tặng kỷ niệm từ Women Techmakers Hanoi
Ngoài 2 giải thưởng trên, chúng mình còn có những bất ngờ thú vị khác cho bạn trong suốt hành trình Her Tech Story. Hãy bấm theo dõi sự kiện để nhận những thông báo mới nhất từ chúng mình nhé! 😊😊
---------
Gửi bài viết thông qua Link: https://forms.gle/YfBqmZdMxtU9qSoBA
Link sự kiện:
https://www.facebook.com/events/346238727029341
Mọi thông tin thêm, thắc mắc, vui lòng inbox page hoặc liên lạc:
Ms. Khánh Linh: 0976682295","Các chị em yêu công nghệ ơi, trong thời gian giãn cách xã hội căng thẳng này, hãy cùng Women Techmakers Hanoi (thuộc Google Developers Group - GDG Hà Nội) lan tỏa những điều tích cực và truyền cảm hứng cho nhau bằng cách tham gia sự kiện online cực hay ho mang tên: Women Techmakers Hanoi x Sutunam: Her Tech Story. Her tech story là sự kiện dành cho tất cả các bạn nữ yêu thích công nghệ và đang làm trong lĩnh vực này có cơ hội chia sẻ những câu chuyện thú vị của riêng mình trong quá trình học tập và làm việc. Sau khi nhận bài viết của bạn, chúng mình sẽ tổng hợp và đăng tất cả các bài viết lên trang Facebook của sự kiện từ ngày 13/08/2021. Để khuyến khích những câu chuyện được lan tỏa một cách rộng rãi, chúng mình đã chuẩn bị ̉ ̛̛̉ ̀ ̂́ ̂̀ ̉ ̛́ ̂́: ̂̃ ́ ̉ ̛̛̉ ̂̀: - 1 năm học tiếng Anh miễn phí trên ELSA speak hoặc tài trợ 100% chi phí để thi lấy chứng chỉ công nghệ mà công ty đối tác sử dụng. (Tương đương với 3,000,000 VND trở lên) - Bài thi được featured trên mọi kênh social của công ty đối tác (Facebook, Linkedin, Twitter) cũng như trang blog công nghệ chính thức, trang việc làm - 1 ngày tham gia trải nghiệm văn hóa trực tiếp tại công ty đối tác. - Giấy chứng nhận đạt giải từ công ty đối tác - 1 set quà tặng kỷ niệm từ công ty đối tác - 1 set quà tặng kỷ niệm từ Women Techmakers Hanoi Ngoài 2 giải thưởng trên, chúng mình còn có những bất ngờ thú vị khác cho bạn trong suốt hành trình Her Tech Story. Hãy bấm theo dõi sự kiện để nhận những thông báo mới nhất từ chúng mình nhé! --------- Gửi bài viết thông qua Link: https://forms.gle/YfBqmZdMxtU9qSoBA Link sự kiện: https://www.facebook.com/events/346238727029341 Mọi thông tin thêm, thắc mắc, vui lòng inbox page hoặc liên lạc: Ms. Khánh Linh: 0976682295",,,,,
Cuốn sách này sẽ ra mắt dưới dạng ebook trong tháng 7.,Cuốn sách này sẽ ra mắt dưới dạng ebook trong tháng 7.,,,,,
Hiện mình đang làm theo hướng dẫn sử dụng deepspeech để làm NLP và mình làm trên gg colab nhưng khi mình làm tới hướng dẫn pre-train model đoạn docker file thì bị lỗi như vầy các bác có ai biết cách fix không giúp mình với ạ😢,Hiện mình đang làm theo hướng dẫn sử dụng deepspeech để làm NLP và mình làm trên gg colab nhưng khi mình làm tới hướng dẫn pre-train model đoạn docker file thì bị lỗi như vầy các bác có ai biết cách fix không giúp mình với ạ,,,,,
"Em là newbie mới tìm hiểu DL ạ. Em cũng tập tành được vài model cơ bản, em muốn hỏi :
Nếu dùng classifi model để quyết định việc xuống tay trong cổ phiếu dựa vào những data nào( dấu hiệu) để hiệu quả cao nhất ạ.
Nếu classifi model k hiệu quả thì nên dùng model nào?","Em là newbie mới tìm hiểu DL ạ. Em cũng tập tành được vài model cơ bản, em muốn hỏi : Nếu dùng classifi model để quyết định việc xuống tay trong cổ phiếu dựa vào những data nào( dấu hiệu) để hiệu quả cao nhất ạ. Nếu classifi model k hiệu quả thì nên dùng model nào?",,,,,
Mọi người đã ai mua đọc cuốn này chưa ạ? Em là người mới thì nên đọc cuốn nào ạ?,Mọi người đã ai mua đọc cuốn này chưa ạ? Em là người mới thì nên đọc cuốn nào ạ?,,,,,
"Em chào mọi người, hiện em đang tìm hiểu về dịch thủ ngữ tiếng Việt sang speech(video->text->speech), mn cho em hỏi không biết VN có dataset thủ ngữ như ASL không ạ? em cảm ơn mn!!","Em chào mọi người, hiện em đang tìm hiểu về dịch thủ ngữ tiếng Việt sang speech(video->text->speech), mn cho em hỏi không biết VN có dataset thủ ngữ như ASL không ạ? em cảm ơn mn!!",,,,,
"Chào mọi người, em đang có 1 vấn đề chưa được thông suốt và không biết mình hiểu đúng về Anchor Box trong YOLO không, mong anh chị xác nhận giúp em.
Theo em hiểu là anchor box là những box có kích thước được định nghĩa trước (có thể dùng k-mean để xác định).
Ví dụ mô hình sử dụng 5 anchor boxes, thì y dự đoán (hay còn gọi là y hat) của 1 grid cell là 5*(5 + c) phần tử, giá trị y thực tế của từng grid cell được xác định dựa trên ground truth của ảnh và kết hợp với các anchor boxes để sắp xếp thành 1 vector y có kích thước là 5*(5 + c) phải k ạ, ví dụ grid cell thứ i có 1 object có kích thước ứng với anchor box thứ 4, thì thế giá trị label object đó vào box thứ 4 của y, còn các thông số các boxes còn lại trong y là có pc = 0 là được. Và anchor box chỉ hỗ trợ trong quá trình training, còn quá trình test thì k liên quan đến anchor box.
Em cũng đọc nhiều tài liệu rồi nhưng không biết em có hiểu sai không, mong anh chị xác nhận giúp em với, hoặc cho em xin documents về anchor box với ạ. Em cảm ơn rất nhiều.","Chào mọi người, em đang có 1 vấn đề chưa được thông suốt và không biết mình hiểu đúng về Anchor Box trong YOLO không, mong anh chị xác nhận giúp em. Theo em hiểu là anchor box là những box có kích thước được định nghĩa trước (có thể dùng k-mean để xác định). Ví dụ mô hình sử dụng 5 anchor boxes, thì y dự đoán (hay còn gọi là y hat) của 1 grid cell là 5*(5 + c) phần tử, giá trị y thực tế của từng grid cell được xác định dựa trên ground truth của ảnh và kết hợp với các anchor boxes để sắp xếp thành 1 vector y có kích thước là 5*(5 + c) phải k ạ, ví dụ grid cell thứ i có 1 object có kích thước ứng với anchor box thứ 4, thì thế giá trị label object đó vào box thứ 4 của y, còn các thông số các boxes còn lại trong y là có pc = 0 là được. Và anchor box chỉ hỗ trợ trong quá trình training, còn quá trình test thì k liên quan đến anchor box. Em cũng đọc nhiều tài liệu rồi nhưng không biết em có hiểu sai không, mong anh chị xác nhận giúp em với, hoặc cho em xin documents về anchor box với ạ. Em cảm ơn rất nhiều.",,,,,
"[Dự án AI làm thơ lục bát ]
Nhằm phát triển thơ lục bát, dự án “ailamtho” đang được FPT Software AI Lab hỗ trợ phát triển giúp sinh thơ lục bát tự động nhờ trí tuệ nhân tạo. Khởi nguồn là chỉ là một project cuối kỳ tại AI for Everyone (AI4E) với sự hạn chế về dữ liệu cũng như kỹ thuật thực hiện, đến nay chúng tôi đã có bản release beta cho cộng đồng kiểm thử và đóng góp bao gồm các tính năng nổi bật sau đây:
1. Gợi ý làm thơ: Khi người dùng làm thơ, AI sẽ đưa ra các option gợi ý để hoàn thiện câu thơ hiện tại, từ đó sẽ giúp người dùng hoàn thiện bài thơ dễ hơn.
2. Sinh thơ tự động từ một từ/cụm từ cho trước: Người dùng nhập một từ/cụm từ (tối đa 6 từ), AI sẽ sinh ra bài thơ lục bát hoàn thiện.
3. Sinh thơ theo chủ đề: Người dùng chọn một trong các chủ đề có trong danh sách và nhập một vài từ mở đầu AI sẽ sinh ra bài thơ trên nền chủ đề ấy.
4. Đối thơ: người dùng nhập một câu thơ, AI sẽ đối lại một câu thơ.
Chi tiết về sản phẩm mọi người xem ở đây: https://ailamtho.com/.","[Dự án AI làm thơ lục bát ] Nhằm phát triển thơ lục bát, dự án “ailamtho” đang được FPT Software AI Lab hỗ trợ phát triển giúp sinh thơ lục bát tự động nhờ trí tuệ nhân tạo. Khởi nguồn là chỉ là một project cuối kỳ tại AI for Everyone (AI4E) với sự hạn chế về dữ liệu cũng như kỹ thuật thực hiện, đến nay chúng tôi đã có bản release beta cho cộng đồng kiểm thử và đóng góp bao gồm các tính năng nổi bật sau đây: 1. Gợi ý làm thơ: Khi người dùng làm thơ, AI sẽ đưa ra các option gợi ý để hoàn thiện câu thơ hiện tại, từ đó sẽ giúp người dùng hoàn thiện bài thơ dễ hơn. 2. Sinh thơ tự động từ một từ/cụm từ cho trước: Người dùng nhập một từ/cụm từ (tối đa 6 từ), AI sẽ sinh ra bài thơ lục bát hoàn thiện. 3. Sinh thơ theo chủ đề: Người dùng chọn một trong các chủ đề có trong danh sách và nhập một vài từ mở đầu AI sẽ sinh ra bài thơ trên nền chủ đề ấy. 4. Đối thơ: người dùng nhập một câu thơ, AI sẽ đối lại một câu thơ. Chi tiết về sản phẩm mọi người xem ở đây: https://ailamtho.com/.",,,,,
Chia sẻ đến các bạn công cụ tạo lộ trình học Machine learning chuyên biệt phù hợp với năng lực của từng người ^^,Chia sẻ đến các bạn công cụ tạo lộ trình học Machine learning chuyên biệt phù hợp với năng lực của từng người ^^,,,,,
Một comment của Khánh Đình Phạm đang nhận được rất nhiều quan tâm. Mình tạo một thread riêng để mọi người thảo luận.,Một comment của Khánh Đình Phạm đang nhận được rất nhiều quan tâm. Mình tạo một thread riêng để mọi người thảo luận.,,,,,
"Playlist giới thiệu về Colab cho bạn nào mới dùng. Nội dung bao gồm: định dạng văn bản, lưu và mở file trong Colab, command line, tương tác với google drive, xuất notebook sang html, latex, pdf,....","Playlist giới thiệu về Colab cho bạn nào mới dùng. Nội dung bao gồm: định dạng văn bản, lưu và mở file trong Colab, command line, tương tác với google drive, xuất notebook sang html, latex, pdf,....",,,,,
"Một bài viết nho nhỏ phân tích các khía cạnh giữa suy diễn Frequentist và Bayesian. Sự khác biệt giữa MLE và MAP và ứng dụng của công thức Bayes trong mô hình Naive Bayes. Mong nhận được góp ý của bạn đọc theo địa chỉ bên dưới để hoàn thiện thêm nội dung:
email: aikhanhblog@gmail.com
facebook: https://www.facebook.com/langnhin.anhtrang/
Vì một cộng đồng AI vững mạnh hơn. Fighting!",Một bài viết nho nhỏ phân tích các khía cạnh giữa suy diễn Frequentist và Bayesian. Sự khác biệt giữa MLE và MAP và ứng dụng của công thức Bayes trong mô hình Naive Bayes. Mong nhận được góp ý của bạn đọc theo địa chỉ bên dưới để hoàn thiện thêm nội dung: email: aikhanhblog@gmail.com facebook: https://www.facebook.com/langnhin.anhtrang/ Vì một cộng đồng AI vững mạnh hơn. Fighting!,,,,,
"[Hỏi về tài liệu]
Chào mọi người, em có dự định học theo hướng research nhiều hơn là engineer nên em muốn hỏi là ngoài khóa học của thầy Andrew Ng, mọi người cho em xin thêm vài khóa khác về ML, DL mà họ dạy sâu vào lý thuyết chứ ko tập trung vào sản phẩm thực tế với ạ.
Với lại theo kinh nghiệm mọi người thì để học tốt theo hướng research, thì em có nên học nhiều khóa về lý thuyết hơn không hay chỉ cần nắm được khóa của Andrew Ng và sách của anh Tiệp là đủ, và dành thời gian đó để tập trung vào cái khác ạ?
Em chưa có nhiều kinh nghiệm nên nếu câu hỏi của em có gì sai sót mong mọi người bỏ qua và chỉ bảo em ạ. Em cảm ơn mọi người nhiều","[Hỏi về tài liệu] Chào mọi người, em có dự định học theo hướng research nhiều hơn là engineer nên em muốn hỏi là ngoài khóa học của thầy Andrew Ng, mọi người cho em xin thêm vài khóa khác về ML, DL mà họ dạy sâu vào lý thuyết chứ ko tập trung vào sản phẩm thực tế với ạ. Với lại theo kinh nghiệm mọi người thì để học tốt theo hướng research, thì em có nên học nhiều khóa về lý thuyết hơn không hay chỉ cần nắm được khóa của Andrew Ng và sách của anh Tiệp là đủ, và dành thời gian đó để tập trung vào cái khác ạ? Em chưa có nhiều kinh nghiệm nên nếu câu hỏi của em có gì sai sót mong mọi người bỏ qua và chỉ bảo em ạ. Em cảm ơn mọi người nhiều",,,,,
"Mọi người cho em hỏi vẽ cái bounding box quanh text như này là mình dùng thuật toán gì ạ?
Em có nghe nói đến EAST, EAST là mình gọi model của tác giả rồi dùng, ko cần trải qua quá trình train đúng ko ạ? Thế em muốn bắt đầu xây dựng một mô hình, trải qua quá trình train mới dự đoán được, thì mọi người có thể đề xuất cho em vài thuật toán và keyword được ko ạ?
Em cảm ơn mn.
PS: Nếu có hỏi ngu gì mong mọi người chỉ bảo, đừng chửi em nhé!!","Mọi người cho em hỏi vẽ cái bounding box quanh text như này là mình dùng thuật toán gì ạ? Em có nghe nói đến EAST, EAST là mình gọi model của tác giả rồi dùng, ko cần trải qua quá trình train đúng ko ạ? Thế em muốn bắt đầu xây dựng một mô hình, trải qua quá trình train mới dự đoán được, thì mọi người có thể đề xuất cho em vài thuật toán và keyword được ko ạ? Em cảm ơn mn. PS: Nếu có hỏi ngu gì mong mọi người chỉ bảo, đừng chửi em nhé!!",,,,,
"em chào mọi người, mọi người thấy cái nào suy nghĩ logic nhiều hơn ạ?","em chào mọi người, mọi người thấy cái nào suy nghĩ logic nhiều hơn ạ?",,,,,
"Khóa học từ University of Michigan:
https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r
Người dạy khóa này cũng chính là một trong những người dạy khóa CS231n của Standford ngày xưa :3",Khóa học từ University of Michigan: https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r Người dạy khóa này cũng chính là một trong những người dạy khóa CS231n của Standford ngày xưa :3,,,,,
"mọi người cho em hỏi các ứng dụng thay đổi các bộ phận trên mặt người bằng bộ phận con vật thì họ dùng phương pháp gì ạ?
em cám ơn ạ",mọi người cho em hỏi các ứng dụng thay đổi các bộ phận trên mặt người bằng bộ phận con vật thì họ dùng phương pháp gì ạ? em cám ơn ạ,,,,,
"PyScrappy in Python
PyScrappy is an amazing Python library that can be used to collect data from websites like Flipkart, Alibaba, Snapdeal, Instagram, YouTube, Google, Yahoo, Bing, Wikipedia, and Yahoo Finance. It covers all the functions that you can easily use to collect data from websites in just a few lines of code. If you find it difficult to collect data and prepare it in the form of a DataFrame to use the collected data for further analysis then PyScrappy is for you as from collecting data to storing it to a DataFrame it does it all in just a few lines of code.","PyScrappy in Python PyScrappy is an amazing Python library that can be used to collect data from websites like Flipkart, Alibaba, Snapdeal, Instagram, YouTube, Google, Yahoo, Bing, Wikipedia, and Yahoo Finance. It covers all the functions that you can easily use to collect data from websites in just a few lines of code. If you find it difficult to collect data and prepare it in the form of a DataFrame to use the collected data for further analysis then PyScrappy is for you as from collecting data to storing it to a DataFrame it does it all in just a few lines of code.",,,,,
Gần đây có một số thảo luận khá sôi nổi về vấn đề overfitting. Hy vọng bài viết sau của Trung tâm BK.AI sẽ giới thiệu thêm cho các bạn sinh viên một số góc nhìn mới hiện đại về học máy nói chung và mạng nơ-ron nói riêng.,Gần đây có một số thảo luận khá sôi nổi về vấn đề overfitting. Hy vọng bài viết sau của Trung tâm BK.AI sẽ giới thiệu thêm cho các bạn sinh viên một số góc nhìn mới hiện đại về học máy nói chung và mạng nơ-ron nói riêng.,,,,,
"Em chào mọi người ạ !
Em đang tìm hiểu về toolbox ANN trên matlab
Em không hiểu về các giá trị bảng này ạ và hiệu chỉnh nó như thế nào để mô hình tốt ạ
Và cái "" goal "" có phải learning rate ko ạ , e thấy tăng lên thì mô hình khá tốt ạ ?
EM cảm ơn mọi người đã giúp ạ","Em chào mọi người ạ ! Em đang tìm hiểu về toolbox ANN trên matlab Em không hiểu về các giá trị bảng này ạ và hiệu chỉnh nó như thế nào để mô hình tốt ạ Và cái "" goal "" có phải learning rate ko ạ , e thấy tăng lên thì mô hình khá tốt ạ ? EM cảm ơn mọi người đã giúp ạ",,,,,
"Hôm trước team mình cùng Nguyễn Quán Anh Minh vừa win cuộc thi Coleridge Initiative - Show US the Data của Kaggle. Đây là bài toán text extraction nhưng có 1 số điểm đặc biệt là lượng unique label cực kỳ khan hiếm và buộc phải dựa vào context xung quanh để trích lấy nội dung.
Team mình đã đưa ra 2 solution hoàn toàn khác nhau nhưng đưa ra kết quả cuối cùng tương đối xêm xêm, nằm ở tầm top 1 - top 3 private leaderboard.
Đây là solution của Minh dùng metric learning https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/248253
Còn đây là version của mình dùng Causal Language Model
https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/248251
Mời mọi người tham khảo.","Hôm trước team mình cùng Nguyễn Quán Anh Minh vừa win cuộc thi Coleridge Initiative - Show US the Data của Kaggle. Đây là bài toán text extraction nhưng có 1 số điểm đặc biệt là lượng unique label cực kỳ khan hiếm và buộc phải dựa vào context xung quanh để trích lấy nội dung. Team mình đã đưa ra 2 solution hoàn toàn khác nhau nhưng đưa ra kết quả cuối cùng tương đối xêm xêm, nằm ở tầm top 1 - top 3 private leaderboard. Đây là solution của Minh dùng metric learning https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/248253 Còn đây là version của mình dùng Causal Language Model https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/discussion/248251 Mời mọi người tham khảo.",,,,,
"Cả nhà cho mình hỏi một chút. Mình cần OCR số lượng lớn check (séc ngân hàng) Tiếng Anh cả chữ in và chữ viết tay. Theo kinh nghiệm của các bạn, Python libraries nào dùng tốt? Bạn thích và không thích điểm gì nhất?
Cảm ơn cả nhà nhiều nhé!","Cả nhà cho mình hỏi một chút. Mình cần OCR số lượng lớn check (séc ngân hàng) Tiếng Anh cả chữ in và chữ viết tay. Theo kinh nghiệm của các bạn, Python libraries nào dùng tốt? Bạn thích và không thích điểm gì nhất? Cảm ơn cả nhà nhiều nhé!",,,,,
Cho mình hỏi là mình muốn sử dụng deepspeech model để train NLP. Mình mới tìm hiểu nên chưa rành lắm cho mình hỏi máy mình không có gpu thì có cách nào train trên gooogle collab hay train không cần gpu không các bác. Mình cảm ơn!,Cho mình hỏi là mình muốn sử dụng deepspeech model để train NLP. Mình mới tìm hiểu nên chưa rành lắm cho mình hỏi máy mình không có gpu thì có cách nào train trên gooogle collab hay train không cần gpu không các bác. Mình cảm ơn!,,,,,
"Chào mọi người!
Em mới học về ML và đang xây dựng cho mình mô hình học máy đầu tiên là mô hình Naive Bayes. Và em đang gặp khó khăn ở việc, lấy log của hàm Gaussian Naive Bayes.
Mọi người cho em hỏi là với ví dụ cụ thể bên dưới, thi khi lấy log, hàm P(a|x) sẽ được triển khai như thế nào ? Em lấy log rồi áp dụng theo công thức log thì có vẻ sai vì mô hình của em bị Overfitting.
Cảm ơn mọi người.
Edit: Giá trị của các xác suất tính ra rất nhỏ em nên lấy log để dễ tính toán.","Chào mọi người! Em mới học về ML và đang xây dựng cho mình mô hình học máy đầu tiên là mô hình Naive Bayes. Và em đang gặp khó khăn ở việc, lấy log của hàm Gaussian Naive Bayes. Mọi người cho em hỏi là với ví dụ cụ thể bên dưới, thi khi lấy log, hàm P(a|x) sẽ được triển khai như thế nào ? Em lấy log rồi áp dụng theo công thức log thì có vẻ sai vì mô hình của em bị Overfitting. Cảm ơn mọi người. Edit: Giá trị của các xác suất tính ra rất nhỏ em nên lấy log để dễ tính toán.",,,,,
"Dự án đang ở giai đoạn gấp rút, rất mong AE cùng đóng góp dữ liệu. Đặc biệt, nếu các bạn biết ai có F0, mong các bạn gửi link website để các bạn ấy giúp sức chống dịch. 

Các lợi ích của việc đóng góp như sau:
Nếu bạn đang khoẻ mạnh, hãy ho hàng ngày để được hệ thống đánh giá bất thường qua tiếng ho, dựa vào dữ liệu tiếng ho hàng ngày của bạn. Hệ thống đang được hoàn thiện, nhưng các bạn đã có thể record dữ liệu ngay bây giờ.
Nếu bạn có F0, hãy *Quyên góp tiếng ho* để hệ thống chuẩn đoán chính xác hơn cho người Việt, giúp khoanh vùng ổ dịch tốt hơn, và giúp những ai đang nghi ngờ mình có COVID có căn cứ tốt hơn để tăng giãn cách xã hội một cách chủ động.

Để quyên góp tiếng ho, các bạn vui lòng truy cập website: https://tiengho.aicovidvn.org. ","Dự án đang ở giai đoạn gấp rút, rất mong AE cùng đóng góp dữ liệu. Đặc biệt, nếu các bạn biết ai có F0, mong các bạn gửi link website để các bạn ấy giúp sức chống dịch. Các lợi ích của việc đóng góp như sau: Nếu bạn đang khoẻ mạnh, hãy ho hàng ngày để được hệ thống đánh giá bất thường qua tiếng ho, dựa vào dữ liệu tiếng ho hàng ngày của bạn. Hệ thống đang được hoàn thiện, nhưng các bạn đã có thể record dữ liệu ngay bây giờ. Nếu bạn có F0, hãy *Quyên góp tiếng ho* để hệ thống chuẩn đoán chính xác hơn cho người Việt, giúp khoanh vùng ổ dịch tốt hơn, và giúp những ai đang nghi ngờ mình có COVID có căn cứ tốt hơn để tăng giãn cách xã hội một cách chủ động. Để quyên góp tiếng ho, các bạn vui lòng truy cập website: https://tiengho.aicovidvn.org.",,,,,
Cho em hỏi một vấn đề về cài đặt cuda 10.0 trên nên giả lập ubuntu 18.04.5. GPU của em là gtx 950 nhưng khi cài driver của GPu driver lại cập nhật lên đến 470 và không có driver 410 để chạy cuda 10.0 nên. Khi em cài cuda 10.0 từ file tải về thì chạy nvidia-smi không nhận nên không cài được tensorflow. Có ai biết cách fix lỗi này không chỉ em với ạ. Em cảm ơn!,Cho em hỏi một vấn đề về cài đặt cuda 10.0 trên nên giả lập ubuntu 18.04.5. GPU của em là gtx 950 nhưng khi cài driver của GPu driver lại cập nhật lên đến 470 và không có driver 410 để chạy cuda 10.0 nên. Khi em cài cuda 10.0 từ file tải về thì chạy nvidia-smi không nhận nên không cài được tensorflow. Có ai biết cách fix lỗi này không chỉ em với ạ. Em cảm ơn!,,,,,
"Em chào mọi người. Hiện em đang muốn tìm hiểu về bài toán nhận dạng tiếng nói (ASR), em có đọc một số sách như HTK book, LVSR, và đọc docs của kaldi nhưng khó quá :(. Em muốn tìm một người gia sư có kinh nghiệm build model asr bằng kaldi hướng dẫn em ạ. Em xin lỗi nếu bài đăng của em vi phạm nội quy của nhóm. Em cảm ơn ạ","Em chào mọi người. Hiện em đang muốn tìm hiểu về bài toán nhận dạng tiếng nói (ASR), em có đọc một số sách như HTK book, LVSR, và đọc docs của kaldi nhưng khó quá :(. Em muốn tìm một người gia sư có kinh nghiệm build model asr bằng kaldi hướng dẫn em ạ. Em xin lỗi nếu bài đăng của em vi phạm nội quy của nhóm. Em cảm ơn ạ",,,,,
"Chào các bạn, các anh chị! Xin tự giới thiệu mình là sinh viên năm cuối ở Phần Lan.
Mình vừa nhận 1 đề tài về dữ liệu từ giáo viên. Yêu cầu đề bài xây dựng 1 hệ thống front-end và back-end cho data visualization. Platform có thể sẽ mở rộng, thêm tính năng trong tuơng lai. Data sẽ lấy từ trang http://pxnet2.stat.fi/PXWeb/pxweb/en/StatFin/.
Về chi tiết hệ thống sẽ phải tính toán raw data thu thập đuợc từ web trên, sau đấy ""computed data"" có thể lưu/cached trên database. Front end thì phải có ""interactive visualization"", có thể sử dụng các library có sẵn như react-vis hoặc d3.js
Ngôn ngữ thì tuỳ chọn cho cả front-end và back-end. Phần front-end có thể làm tối giản không cần đầy đủ chi tiết, chỉ cần hiển thị dữ liệu và tuơng tác là đuợc.
Mình đăng lên để xin lời khuyên và gợi ý về kinh nghiệm xây dựng một hệ thống như trên. Ý tuởng của mình sẽ là dùng D3.js cho front-end và typescript + node.js cho backend. Nhưng Javascript k phải thế mạnh của mình nên mình nên mình hỏi có thể hoàn thành yêu cầu bằng Python đuợc không? Mình có google và thấy Dash + plotly nhưng k biết nếu dùng Dash và plotly thì sau này mở rộng platform sẽ như thế nào? Mong các cao nhân cho mình xin ý kiến ạ !
Mình cảm on!","Chào các bạn, các anh chị! Xin tự giới thiệu mình là sinh viên năm cuối ở Phần Lan. Mình vừa nhận 1 đề tài về dữ liệu từ giáo viên. Yêu cầu đề bài xây dựng 1 hệ thống front-end và back-end cho data visualization. Platform có thể sẽ mở rộng, thêm tính năng trong tuơng lai. Data sẽ lấy từ trang http://pxnet2.stat.fi/PXWeb/pxweb/en/StatFin/. Về chi tiết hệ thống sẽ phải tính toán raw data thu thập đuợc từ web trên, sau đấy ""computed data"" có thể lưu/cached trên database. Front end thì phải có ""interactive visualization"", có thể sử dụng các library có sẵn như react-vis hoặc d3.js Ngôn ngữ thì tuỳ chọn cho cả front-end và back-end. Phần front-end có thể làm tối giản không cần đầy đủ chi tiết, chỉ cần hiển thị dữ liệu và tuơng tác là đuợc. Mình đăng lên để xin lời khuyên và gợi ý về kinh nghiệm xây dựng một hệ thống như trên. Ý tuởng của mình sẽ là dùng D3.js cho front-end và typescript + node.js cho backend. Nhưng Javascript k phải thế mạnh của mình nên mình nên mình hỏi có thể hoàn thành yêu cầu bằng Python đuợc không? Mình có google và thấy Dash + plotly nhưng k biết nếu dùng Dash và plotly thì sau này mở rộng platform sẽ như thế nào? Mong các cao nhân cho mình xin ý kiến ạ ! Mình cảm on!",,,,,
"Xin chào mọi người.
Em có hứng thú và theo học mảng ml và ds. Em đã đọc sách của a Tiệp và nắm kha khá tốt kiến thức trong sách. Em cũng đã học môn ml ở đại học nhưng không hiệu quả lắm, công thêm tình hình dịch bệnh như hiện tại cũng làm em không thể trực tiếp tham khảo từ thầy cô.
Hiện tại em đang vô cùng mông lung không biết cần học thêm gì, thực hành ra sao hay tiếp tục như nào. Vì vậy em hi vọng mọi người có thể cho em một hướng dẫn hoặc lộ trình thực sự cụ thể.
Em không có gì ngoài thời gian ạ.","Xin chào mọi người. Em có hứng thú và theo học mảng ml và ds. Em đã đọc sách của a Tiệp và nắm kha khá tốt kiến thức trong sách. Em cũng đã học môn ml ở đại học nhưng không hiệu quả lắm, công thêm tình hình dịch bệnh như hiện tại cũng làm em không thể trực tiếp tham khảo từ thầy cô. Hiện tại em đang vô cùng mông lung không biết cần học thêm gì, thực hành ra sao hay tiếp tục như nào. Vì vậy em hi vọng mọi người có thể cho em một hướng dẫn hoặc lộ trình thực sự cụ thể. Em không có gì ngoài thời gian ạ.",,,,,
"Em chào mọi người ạ
Em mới tìm hiểu về ANN và biết matlab có nntool hỗ trợ mô hình ANN . Khi em chạy thử mô hình thì được kết quả như này .
Mọi người cho em hỏi giá trị R trong regression có phải coefficient of determination không ạ ?",Em chào mọi người ạ Em mới tìm hiểu về ANN và biết matlab có nntool hỗ trợ mô hình ANN . Khi em chạy thử mô hình thì được kết quả như này . Mọi người cho em hỏi giá trị R trong regression có phải coefficient of determination không ạ ?,,,,,
"Chào mọi người,
Mình có gặp 1 bài toán liên quan đến tính xác suất nhưng giải không ra, nhờ mọi người giúp đỡ ạ :(
---
Our hero - Maga is going to make a new contest for making the best teams. He is really excited about it. There will be S students in the contest. First N students in the final standings will be awarded.
He has a list( favourite students list ) what contains the best students in informatics Olympiad. His favorite students list contains M students. Though he is excited about the contest, he will enjoy it only if at least K of students from his favourite students list are awarded. He is wondering what are the chances of that happening. He needs your help. Tell him the probability that he will enjoy. It is known that each student has equal chance of being awarded.
Input:
First line of input contains a single integer T, the number of test cases. In the next T lines, you are given 4 separated integers, S, N, M and K.
Output:
For each test case, output the required probability with 6 digits after floating point.
Constraints:
1 <= T <= 100
1 <= S <= 1000
1 <= N <= S
1 <= M <= S
0 <= K <= M
SAMPLE INPUT
3
10 10 5 3
10 4 6 4
3 2 2 1
SAMPLE OUTPUT
1.000000
0.071429
1.000000","Chào mọi người, Mình có gặp 1 bài toán liên quan đến tính xác suất nhưng giải không ra, nhờ mọi người giúp đỡ ạ :( --- Our hero - Maga is going to make a new contest for making the best teams. He is really excited about it. There will be S students in the contest. First N students in the final standings will be awarded. He has a list( favourite students list ) what contains the best students in informatics Olympiad. His favorite students list contains M students. Though he is excited about the contest, he will enjoy it only if at least K of students from his favourite students list are awarded. He is wondering what are the chances of that happening. He needs your help. Tell him the probability that he will enjoy. It is known that each student has equal chance of being awarded. Input: First line of input contains a single integer T, the number of test cases. In the next T lines, you are given 4 separated integers, S, N, M and K. Output: For each test case, output the required probability with 6 digits after floating point. Constraints: 1 <= T <= 100 1 <= S <= 1000 1 <= N <= S 1 <= M <= S 0 <= K <= M SAMPLE INPUT 3 10 10 5 3 10 4 6 4 3 2 2 1 SAMPLE OUTPUT 1.000000 0.071429 1.000000",,,,,
"Bài viết mới toanh của mình trên medium. Đây là bài thứ năm của mình liên quan đến Machine Learning. Blog của mình chuyên viết về machine learning và ứng dụng khác nhau của nó trong tài chính, marketing, sức khoẻ,..
Giới thiệu chút về bản thân, mình tên là Nguyễn Hưng Quang Khải sinh viên năm 3 khoa toán Ứng dụng ĐH Quốc Tế TPHCM. Hiện nay mình đang là Quantitative Analyst Intern tại Quỹ đầu tư VietQuant. Background của mình là mình đã từng được 1 HCV Toán Học Không biên Giới tại Bulgaria, Giải Ba HSG Toán Quốc Gia VMO, 2 HCV Olympic 30/4 và là top 8 học sinh đại diện Việt Nam tham dự trại hè Khoa học Châu Á 2018 tại Indonesia.
Trong bài đăng này, mình đã sử dụng các mô hình machine learning để xây dựng một hệ thống giúp xác định và theo dõi các nguy cơ mắc bệnh tim.
Điều đặc biệt là , trong bài viết này, thay vì tập trung vào việc fitting machine learning model, mình sẽ nhấn mạnh nhiều hơn vào bước feature engineering.
Phương pháp feature engineering của mình trong bài này khá đơn giản, nhưng hiệu quả và có thể ứng dụng được nhiều cho các project machine learning khác nhau
Theo ý kiến của mình, feature engineering là bước quan trọng nhất trong machine learning. Feature engineering tốt không chỉ giúp nâng cao hiệu suất dự đoán của mô hình mà còn giúp chúng ta có được những hiểu biết có giá trị từ dữ liệu.
Lưu ý: Project này mình chỉ lấy ra làm ví dụ cho phương pháp của mình. Để đảm báo đúng tiêu chuẩn an toàn khi ứng dụng thực tế, bài viết dưới đây không đủ.
Mùa hè dịch covid này, mình sẽ đẩy mạnh việc viết bài về machine learning/deep learning nhiều hơn trên blog medium của mình. Những bạn nào có quan tâm, có thể liên hệ trực tiếp với mình, để mình sẽ có nhiều bài viết hơn theo đúng nhu cầu của các bạn. Hơn nữa mình cũng muốn mở rộng hơn networking của mình hơn.
Hi vọng các bạn thích bài viết này của mình, và đừng quên like/share để ủng hộ mình","Bài viết mới toanh của mình trên medium. Đây là bài thứ năm của mình liên quan đến Machine Learning. Blog của mình chuyên viết về machine learning và ứng dụng khác nhau của nó trong tài chính, marketing, sức khoẻ,.. Giới thiệu chút về bản thân, mình tên là Nguyễn Hưng Quang Khải sinh viên năm 3 khoa toán Ứng dụng ĐH Quốc Tế TPHCM. Hiện nay mình đang là Quantitative Analyst Intern tại Quỹ đầu tư VietQuant. Background của mình là mình đã từng được 1 HCV Toán Học Không biên Giới tại Bulgaria, Giải Ba HSG Toán Quốc Gia VMO, 2 HCV Olympic 30/4 và là top 8 học sinh đại diện Việt Nam tham dự trại hè Khoa học Châu Á 2018 tại Indonesia. Trong bài đăng này, mình đã sử dụng các mô hình machine learning để xây dựng một hệ thống giúp xác định và theo dõi các nguy cơ mắc bệnh tim. Điều đặc biệt là , trong bài viết này, thay vì tập trung vào việc fitting machine learning model, mình sẽ nhấn mạnh nhiều hơn vào bước feature engineering. Phương pháp feature engineering của mình trong bài này khá đơn giản, nhưng hiệu quả và có thể ứng dụng được nhiều cho các project machine learning khác nhau Theo ý kiến của mình, feature engineering là bước quan trọng nhất trong machine learning. Feature engineering tốt không chỉ giúp nâng cao hiệu suất dự đoán của mô hình mà còn giúp chúng ta có được những hiểu biết có giá trị từ dữ liệu. Lưu ý: Project này mình chỉ lấy ra làm ví dụ cho phương pháp của mình. Để đảm báo đúng tiêu chuẩn an toàn khi ứng dụng thực tế, bài viết dưới đây không đủ. Mùa hè dịch covid này, mình sẽ đẩy mạnh việc viết bài về machine learning/deep learning nhiều hơn trên blog medium của mình. Những bạn nào có quan tâm, có thể liên hệ trực tiếp với mình, để mình sẽ có nhiều bài viết hơn theo đúng nhu cầu của các bạn. Hơn nữa mình cũng muốn mở rộng hơn networking của mình hơn. Hi vọng các bạn thích bài viết này của mình, và đừng quên like/share để ủng hộ mình",,,,,
"Chào các anh chị,
Tập dữ liệu của em gồm 3 phần: train, validation, test. Kết quả train và validation cho kết quả tốt trên cả AUC và AUPR nhưng trên tập test thì AUPR cực thấp.
Tập train+validation có kích thước bé hơn nhiều so với tập test. (Cái này em ko thể thay đổi vì yêu cầu của bài nó thế rồi ạ).
Như vậy vấn đề là do kích thước hay bị overfitting rồi ạ?
Em cám ơn mọi người!","Chào các anh chị, Tập dữ liệu của em gồm 3 phần: train, validation, test. Kết quả train và validation cho kết quả tốt trên cả AUC và AUPR nhưng trên tập test thì AUPR cực thấp. Tập train+validation có kích thước bé hơn nhiều so với tập test. (Cái này em ko thể thay đổi vì yêu cầu của bài nó thế rồi ạ). Như vậy vấn đề là do kích thước hay bị overfitting rồi ạ? Em cám ơn mọi người!",,,,,
"Chào các tiền bối ạ, em bên software mới chuyển sang bên aiml tìm hiểu. Vì em muốn thời gian đầu đi nhanh nên em mong muốn tìm một người thầy có thể dạy em những thuật toán như em có nói bên dưới (song hành với việc được dạy thì em cũng sẽ tự tìm hiểu để hỏi chứ không phó thác hết vào người dạy). Không chỉ thuật toán, em cũng muốn được hướng dẫn các bước ngoài như data handling. Ngoài ra em cũng muốn được chỉ giáo tips and tricks khi sử dụng những thư viện hỗ trợ. Anh chị nào có nhu cầu kiếm thêm có thể cmt em sẽ chủ động ib ạ. Em cảm ơn mn nhiều ạ ❤.
P/s: vì em mong muốn đi sâu bản chất nên em mong muốn tìm được ai có thể giải thích cho em về mặt bản chất toán học chứ không chỉ là code ạ.","Chào các tiền bối ạ, em bên software mới chuyển sang bên aiml tìm hiểu. Vì em muốn thời gian đầu đi nhanh nên em mong muốn tìm một người thầy có thể dạy em những thuật toán như em có nói bên dưới (song hành với việc được dạy thì em cũng sẽ tự tìm hiểu để hỏi chứ không phó thác hết vào người dạy). Không chỉ thuật toán, em cũng muốn được hướng dẫn các bước ngoài như data handling. Ngoài ra em cũng muốn được chỉ giáo tips and tricks khi sử dụng những thư viện hỗ trợ. Anh chị nào có nhu cầu kiếm thêm có thể cmt em sẽ chủ động ib ạ. Em cảm ơn mn nhiều ạ . P/s: vì em mong muốn đi sâu bản chất nên em mong muốn tìm được ai có thể giải thích cho em về mặt bản chất toán học chứ không chỉ là code ạ.",,,,,
"mình đang dùng k-NN để phân loại tập dữ liệu animals gồm dog, cat và pandas. Trong file knn.py mình import một số class từ các subpackage. Không biết mình sai chỗ nào mà khi run thì báo lỗi module not found. Mình kèm theo cấu trúc thư mục project để mọi người tham khảo.","mình đang dùng k-NN để phân loại tập dữ liệu animals gồm dog, cat và pandas. Trong file knn.py mình import một số class từ các subpackage. Không biết mình sai chỗ nào mà khi run thì báo lỗi module not found. Mình kèm theo cấu trúc thư mục project để mọi người tham khảo.",,,,,
Cuối cùng AlphaFold2 sau 5 năm nghiên cứu của DeepMind đã được tích hợp vào website của EMBL-EBI. Các bạn nào quan tâm tới tin sinh có thể xem tại đây,Cuối cùng AlphaFold2 sau 5 năm nghiên cứu của DeepMind đã được tích hợp vào website của EMBL-EBI. Các bạn nào quan tâm tới tin sinh có thể xem tại đây,,,,,
"Chào mọi người!
Em đang tìm hiểu về Convolutional NN và có đọc về mấy loại kiến trúc. Em thấy một số kiến trúc có dùng Depthwise Separable Convolution (SepConv2D) với mục đích giảm khối lượng tính toán và tham số (chủ yếu là kiến trúc cho thiết bị có hạn chế).
Vậy tại sao các mô hình vẫn dùng Conv2D truyền thống? Có thể dùng SepConv2D thay thế không?
Em cảm ơn!",Chào mọi người! Em đang tìm hiểu về Convolutional NN và có đọc về mấy loại kiến trúc. Em thấy một số kiến trúc có dùng Depthwise Separable Convolution (SepConv2D) với mục đích giảm khối lượng tính toán và tham số (chủ yếu là kiến trúc cho thiết bị có hạn chế). Vậy tại sao các mô hình vẫn dùng Conv2D truyền thống? Có thể dùng SepConv2D thay thế không? Em cảm ơn!,,,,,
"Bạn nào có học qua Học bổng "" FUNiX AI Connect with Cohost.AI"" hay đã học qua khoá Machine Learing của FUNiX chưa?
Cho mình xin review với","Bạn nào có học qua Học bổng "" FUNiX AI Connect with Cohost.AI"" hay đã học qua khoá Machine Learing của FUNiX chưa? Cho mình xin review với",,,,,
"Klq lắm đến ml, xin phép ad.
Mình muốn hỏi là ""vô số nghiệm"" trong tiếng anh thì nói sao ạ? Ví dụ như phương trình này có vô số nghiệm. Cảm ơn mọi người","Klq lắm đến ml, xin phép ad. Mình muốn hỏi là ""vô số nghiệm"" trong tiếng anh thì nói sao ạ? Ví dụ như phương trình này có vô số nghiệm. Cảm ơn mọi người",,,,,
"[Chia sẻ kiến thức]
Trong Machine Learning, mỗi vấn đề cụ thể luôn cần những đặc trưng quan trọng phù hợp nhất, chẳng hạn các tác vụ ML tuy có thể giống nhau trong bài toán phân loại thư rác, phân loại chữ số viết tay... nhưng các đặc trưng cần được trích xuất trong mỗi kịch bản sẽ rất khác nhau.
Vì vậy, Feature Engineering (xử lý đặc trưng) vừa là nghệ thuật, vừa là bộ môn khoa học khiến các Data Scientist thường dành 70% thời gian để chuẩn bị dữ liệu trước khi xây dựng mô hình.
Cùng tìm hiểu tầm quan trọng của Feature Engineering, tác động từ các đặc trưng này đến model + ví dụ thực tế cho câu hỏi ""Bao nhiêu features thì đủ tốt cho 1 model?"" qua chia sẻ của Ms. Trân Thiều (Research Engineer tại Trusting Social): https://bit.ly/ml2101_featureengineering
Nguồn bài gốc: https://www.facebook.com/vefacademy/posts/1018251165586293
#featureengineering #machinelearning #datascience #AI","[Chia sẻ kiến thức] Trong Machine Learning, mỗi vấn đề cụ thể luôn cần những đặc trưng quan trọng phù hợp nhất, chẳng hạn các tác vụ ML tuy có thể giống nhau trong bài toán phân loại thư rác, phân loại chữ số viết tay... nhưng các đặc trưng cần được trích xuất trong mỗi kịch bản sẽ rất khác nhau. Vì vậy, Feature Engineering (xử lý đặc trưng) vừa là nghệ thuật, vừa là bộ môn khoa học khiến các Data Scientist thường dành 70% thời gian để chuẩn bị dữ liệu trước khi xây dựng mô hình. Cùng tìm hiểu tầm quan trọng của Feature Engineering, tác động từ các đặc trưng này đến model + ví dụ thực tế cho câu hỏi ""Bao nhiêu features thì đủ tốt cho 1 model?"" qua chia sẻ của Ms. Trân Thiều (Research Engineer tại Trusting Social): https://bit.ly/ml2101_featureengineering Nguồn bài gốc: https://www.facebook.com/vefacademy/posts/1018251165586293",#featureengineering	#machinelearning	#datascience	#AI,,,,
"Chào anh chị, em mới tìm hiểu về Deep learning và có chút thắc mắc chút ạ!
Với tập dữ liệu đã được chia thành tập huấn luyện và tập kiểm thử, em có label data bằng cách dùng flow_from_dictionary, nhưng khi train cho kết quả chỉ khoảng ~ 70% ở tập test. Còn khi em label bằng cách load hết ảnh, mã hóa one-hot rồi chia bằng train_test_split thì độ chính xác lại cao hơn hẳn (khoảng 98%). Em không biết tại sao lại có sự khác nhau này, mọi người giải thích giúp em được không ạ?","Chào anh chị, em mới tìm hiểu về Deep learning và có chút thắc mắc chút ạ! Với tập dữ liệu đã được chia thành tập huấn luyện và tập kiểm thử, em có label data bằng cách dùng flow_from_dictionary, nhưng khi train cho kết quả chỉ khoảng ~ 70% ở tập test. Còn khi em label bằng cách load hết ảnh, mã hóa one-hot rồi chia bằng train_test_split thì độ chính xác lại cao hơn hẳn (khoảng 98%). Em không biết tại sao lại có sự khác nhau này, mọi người giải thích giúp em được không ạ?",,,,,
"[AI News - Course 100% free]
Deep Learning with PyTorch by Microsoft 🔥🔥.
Microsoft hợp tác với PyTorch để cung cấp một khóa học cơ bản về PyTorch HOÀN TOÀN MIỄN PHÍ. Wow, điều này thật sự quá tuyệt vời 😍
Các chủ đề bao gồm:
* Introduction to Pytorch
* Computer Vision with PyTorch
* NLP with PyTorch
* Audio Classification with PyTorch
Khóa học thiết kế để bạn có thể viết code trực tiếp trên web mà không cần cài đặt hay tải về bất cứ thứ gì.
Việc của bạn chỉ là sắp xếp thời gian để học ^^.
Tuy nhiên khóa học yêu cầu bạn phải có kiến thức cơ bản về Python, Machine Learning và cách sử dụng cơ bản Jupyter Notebooks.","[AI News - Course 100% free] Deep Learning with PyTorch by Microsoft . Microsoft hợp tác với PyTorch để cung cấp một khóa học cơ bản về PyTorch HOÀN TOÀN MIỄN PHÍ. Wow, điều này thật sự quá tuyệt vời Các chủ đề bao gồm: * Introduction to Pytorch * Computer Vision with PyTorch * NLP with PyTorch * Audio Classification with PyTorch Khóa học thiết kế để bạn có thể viết code trực tiếp trên web mà không cần cài đặt hay tải về bất cứ thứ gì. Việc của bạn chỉ là sắp xếp thời gian để học ^^. Tuy nhiên khóa học yêu cầu bạn phải có kiến thức cơ bản về Python, Machine Learning và cách sử dụng cơ bản Jupyter Notebooks.",,,,,
,nan,,,,,
"Câu hỏi về đại số tuyến tính
Em chào mọi ngưòi ạ. Chuyện là em đang học về linear algebra, cụ thể là phần inner product (tích vô hướng). Em được biết là với 2 vector u,v thuộc số thực R thì
<u,v> = tổng tích các phần tử 2 vector // <u,v> = u1.v1 + u2.v2 + ... + un.vn //
Nhưng đối với 2 vector u,v thuộc số phức C thì
<u,v> = u1*.v1 + u2*.v2 +... + un*.vn với u* là số phức liên hợp của u
Em có thắc mắc là tại sao phải là <u,v> với u,v thuộc C phải dùng u* thay vì u như của u,v thuộc R ạ.
Em xin cảm ơn.","Câu hỏi về đại số tuyến tính Em chào mọi ngưòi ạ. Chuyện là em đang học về linear algebra, cụ thể là phần inner product (tích vô hướng). Em được biết là với 2 vector u,v thuộc số thực R thì <u,v> = tổng tích các phần tử 2 vector // <u,v> = u1.v1 + u2.v2 + ... + un.vn // Nhưng đối với 2 vector u,v thuộc số phức C thì <u,v> = u1*.v1 + u2*.v2 +... + un*.vn với u* là số phức liên hợp của u Em có thắc mắc là tại sao phải là <u,v> với u,v thuộc C phải dùng u* thay vì u như của u,v thuộc R ạ. Em xin cảm ơn.",,"#Q&A, #math",,,
"Em xin chào các anh chị ạ, em có đang thử vài cái giải thuật khác nhau lên dataset của em. Em có thử dùng sgd, sgd_momentum, rmsprop và adam. Theo em được biết thì sgd_momentum sẽ hội tụ nhanh hơn và cải thiện kết quả so với sgd thuần túy. Nhưng theo như trong hình thì model dùng sgd_momentum ko học đc gì cả, trong khi sgd học được tương đối, và em đang thắc mắc có bao giờ chuyện này xảy ra ko? Em xin cảm ơn ạ.","Em xin chào các anh chị ạ, em có đang thử vài cái giải thuật khác nhau lên dataset của em. Em có thử dùng sgd, sgd_momentum, rmsprop và adam. Theo em được biết thì sgd_momentum sẽ hội tụ nhanh hơn và cải thiện kết quả so với sgd thuần túy. Nhưng theo như trong hình thì model dùng sgd_momentum ko học đc gì cả, trong khi sgd học được tương đối, và em đang thắc mắc có bao giờ chuyện này xảy ra ko? Em xin cảm ơn ạ.",,,,,
"Chào mn, cho mình hỏi liên quan đến phần cứng.
Anh Chị Em đang dùng GPU nào để train ML và AI.
￼￼1. Thuê máy để train không quan tâm đến GPU nào miễn sao train được.
2. Thuê máy nhưng GPU là: ...
3. Công ty/ cá nhân tự trang bị với GPU là: ...
4. Dùng free Google Colab và service free khác.
Anh chị đang thuê bên nào: Gcloud, Aws, Azure hay 1 bên cung cấp dịch vụ nào. Đơn giá/hour là bao nhiêu.
Xin cảm ơn ace.
#gpu #k80","Chào mn, cho mình hỏi liên quan đến phần cứng. Anh Chị Em đang dùng GPU nào để train ML và AI. 1. Thuê máy để train không quan tâm đến GPU nào miễn sao train được. 2. Thuê máy nhưng GPU là: ... 3. Công ty/ cá nhân tự trang bị với GPU là: ... 4. Dùng free Google Colab và service free khác. Anh chị đang thuê bên nào: Gcloud, Aws, Azure hay 1 bên cung cấp dịch vụ nào. Đơn giá/hour là bao nhiêu. Xin cảm ơn ace.",#gpu	#k80,,,,
"Nhóm làm việc của tôi vừa có một bài trên Elsevier Expert Systems with Applications (ESWA), một tập san khá tốt trong lĩnh vực Artificial Intelligence, Computer Science. ESWA được xếp hạng SCImago Q1, với IF=5.452. Bài báo trình bày ứng dụng Deep Learning trong chẩn đoán bệnh lao phổi, sử dụng Vision Transformer và Transfer Learning. Hiện nay bài đang trong giai đoạn được Elsevier cho tải miễn phí ở địa chỉ Share Link sau: https://authors.elsevier.com/a/1dPVb3PiGTH-6F. Trân trọng mời các bạn quan tâm ghé qua xem công trình của chúng tôi.","Nhóm làm việc của tôi vừa có một bài trên Elsevier Expert Systems with Applications (ESWA), một tập san khá tốt trong lĩnh vực Artificial Intelligence, Computer Science. ESWA được xếp hạng SCImago Q1, với IF=5.452. Bài báo trình bày ứng dụng Deep Learning trong chẩn đoán bệnh lao phổi, sử dụng Vision Transformer và Transfer Learning. Hiện nay bài đang trong giai đoạn được Elsevier cho tải miễn phí ở địa chỉ Share Link sau: https://authors.elsevier.com/a/1dPVb3PiGTH-6F. Trân trọng mời các bạn quan tâm ghé qua xem công trình của chúng tôi.",,,,,
"[AI Share - Cheat Sheets]
Tổng hợp một số cheat sheets VIP của các khóa khọc Stanford
1. CS 221: Artificial Intelligence
2. CS 229: Machine Learning
3. CS 230: Deep Learning",[AI Share - Cheat Sheets] Tổng hợp một số cheat sheets VIP của các khóa khọc Stanford 1. CS 221: Artificial Intelligence 2. CS 229: Machine Learning 3. CS 230: Deep Learning,,,,,
"Cho em/mình hỏi về split command của python
txt = ""Thanks for letting me in here. The -- are you guys seeing any strange order patterns as far as like double ordering or any kind of panic inventory rebuild? Anything -- I mean, I would imagine you'd probably see it in transportation and electronics, kind of perhaps equally in transportation and electronics of all the segments. But I'll just leave it at that. Anything on the ordering patterns that's unusual.""
Thì mình nên dùng lệnh như nào để tách ra từng individual words ạ?","Cho em/mình hỏi về split command của python txt = ""Thanks for letting me in here. The -- are you guys seeing any strange order patterns as far as like double ordering or any kind of panic inventory rebuild? Anything -- I mean, I would imagine you'd probably see it in transportation and electronics, kind of perhaps equally in transportation and electronics of all the segments. But I'll just leave it at that. Anything on the ordering patterns that's unusual."" Thì mình nên dùng lệnh như nào để tách ra từng individual words ạ?",,,,,
Có Anh/Chị/Em nào trong group được cấp quyền truy cập vào GPT-3 chưa nhỉ? Rất cần chỉ giáo và kinh nghiệm để lấy licence. Thanks,Có Anh/Chị/Em nào trong group được cấp quyền truy cập vào GPT-3 chưa nhỉ? Rất cần chỉ giáo và kinh nghiệm để lấy licence. Thanks,,,,,
"Chào mọi người
Cho mình hỏi nếu mình có data mà feature thuộc dạng list , giống trong hình, ở đây feature là interest. Nếu gặp trường hợp vậy thường mọi người xử lý sao để build ML Model (ex:logistic regression, random forest...)
Bạn nào có kinh nghiêm xin chia sẻ
Mình cảm ơn","Chào mọi người Cho mình hỏi nếu mình có data mà feature thuộc dạng list , giống trong hình, ở đây feature là interest. Nếu gặp trường hợp vậy thường mọi người xử lý sao để build ML Model (ex:logistic regression, random forest...) Bạn nào có kinh nghiêm xin chia sẻ Mình cảm ơn",,,,,
"Hỏi về Overfitting trong Decision Tree.
Em mới tìm hiểu và thực hiện thuật toán Decision Tree trên một bộ dữ liệu. Em có search và tìm hiểu thông tin trên Google về các cách tránh overfitting và đã sử dụng 2 cách : hạn chế độ sâu của cây và post purning:
Hạn chế độ sâu thì em có search trên sklearn có chỉ cách chọn độ sâu cây phù hợp. Như hình dưới em chọn max_depth=8
post purning: em có vẽ đồ thị về việc thay đổi thông số ccp_alpha so với độ chính xác trên tập training và tập test, nhưng em không biết chọn điểm giá trị tối ưu cpp_alpha, theo như em có tìm hiểu thì người ta thường chọn điểm ở vị trí có accuracy_score cao nhất, em chọn tại điểm cao nhất thì tại đó max_depth=12(độ sâu khi cây chia đến mức các lá trở nên tinh khiết nhất) và em thấy độ chính xác của training set cũng khá cao.
Hình vẽ em có để ở dưới ạ. Mong mọi người cho em nhận xét và chỉ em cách chọn thông số phù hợp. Em cảm ơn mọi người.","Hỏi về Overfitting trong Decision Tree. Em mới tìm hiểu và thực hiện thuật toán Decision Tree trên một bộ dữ liệu. Em có search và tìm hiểu thông tin trên Google về các cách tránh overfitting và đã sử dụng 2 cách : hạn chế độ sâu của cây và post purning: Hạn chế độ sâu thì em có search trên sklearn có chỉ cách chọn độ sâu cây phù hợp. Như hình dưới em chọn max_depth=8 post purning: em có vẽ đồ thị về việc thay đổi thông số ccp_alpha so với độ chính xác trên tập training và tập test, nhưng em không biết chọn điểm giá trị tối ưu cpp_alpha, theo như em có tìm hiểu thì người ta thường chọn điểm ở vị trí có accuracy_score cao nhất, em chọn tại điểm cao nhất thì tại đó max_depth=12(độ sâu khi cây chia đến mức các lá trở nên tinh khiết nhất) và em thấy độ chính xác của training set cũng khá cao. Hình vẽ em có để ở dưới ạ. Mong mọi người cho em nhận xét và chỉ em cách chọn thông số phù hợp. Em cảm ơn mọi người.",,,,,
"Mời các bạn đọc thử bài viết về thuật toán cây quyết định của cuốn machine learning algorithms to practice, đây là cuốn sách mà mình đang viết để ủng hộ cộng đồng miễn phí, và đưa thêm các góp ý để sách hoàn thiện hơn.","Mời các bạn đọc thử bài viết về thuật toán cây quyết định của cuốn machine learning algorithms to practice, đây là cuốn sách mà mình đang viết để ủng hộ cộng đồng miễn phí, và đưa thêm các góp ý để sách hoàn thiện hơn.",,"#math, #sharing, #machine_learning",,,
Cho em hỏi câu này với ạ. Em cảm ơn ạ.,Cho em hỏi câu này với ạ. Em cảm ơn ạ.,,,,,
moi huong dan hay cho cac ban nao muon xay dung mot kubernetes cluster voi raspberry de train cac model hay chay cac api container,moi huong dan hay cho cac ban nao muon xay dung mot kubernetes cluster voi raspberry de train cac model hay chay cac api container,,,,,
"Xin chào mn, e train resnet34, nếu như dùng đoạn init dưới thì model hội tụ rất chậm, còn nếu k dùng thì model hội tụ rất nhanh (loss giảm nhanh và acc tăng nhanh sau 1 vài epoch đầu), mn giải thích giùm e thế là như thế nào và phải dùng như thế nào cho đúng ạ??
Em cảm ơn","Xin chào mn, e train resnet34, nếu như dùng đoạn init dưới thì model hội tụ rất chậm, còn nếu k dùng thì model hội tụ rất nhanh (loss giảm nhanh và acc tăng nhanh sau 1 vài epoch đầu), mn giải thích giùm e thế là như thế nào và phải dùng như thế nào cho đúng ạ?? Em cảm ơn",,,,,
"Introduction to TorchShard
A Lightweight Library for Scaling-up the Training
#Pytorch #Facebookai
Author: Kaiyu Yue, Incoming Ph.D. Student at the Computer Science Department of the University of Maryland, College Park
TorchShard is a lightweight engine for slicing a PyTorch tensor into parallel shards. It can reduce GPU memory and scale up the training when the model has massive linear layers (e.g., BERT and GPT) or huge classes (millions). It has the same API design as PyTorch. In this blog, we will introduce TorchShard and illustrate how to adopt it in our projects.
https://medium.com/pytorch/torchshard-a31fcbfdc354","Introduction to TorchShard A Lightweight Library for Scaling-up the Training Author: Kaiyu Yue, Incoming Ph.D. Student at the Computer Science Department of the University of Maryland, College Park TorchShard is a lightweight engine for slicing a PyTorch tensor into parallel shards. It can reduce GPU memory and scale up the training when the model has massive linear layers (e.g., BERT and GPT) or huge classes (millions). It has the same API design as PyTorch. In this blog, we will introduce TorchShard and illustrate how to adopt it in our projects. https://medium.com/pytorch/torchshard-a31fcbfdc354",#Pytorch	#Facebookai,,,,
"Mn ơi cho e hỏi nếu trên bằng GPU thì khi validate hoặc test ko dùng gpu có được không ạ
E dùng pytorch ạ",Mn ơi cho e hỏi nếu trên bằng GPU thì khi validate hoặc test ko dùng gpu có được không ạ E dùng pytorch ạ,,,,,
"Hỏi về bài toán OCR - Understanding document
Chào mọi người. Mọi người cho em hỏi:
Em đang làm một bài toán về understanding document thì em có sử dụng dịch vụ OCR của Amazon Web Service để trích xuất các thông tin trong ảnh, nhưng mà AWS phần xử lý ngôn ngữ tự nhiên cho tiếng việt làm không được tốt, nên một số ký tự nhận dạng bị sai. Kết quả trả về của AWS là một file json, gồm các đoạn ký tự nhận dạng được.
Sau khi qua phần xử lý OCR, thì em chuẩn hóa các đoạn ký tự này về các thuộc tính lưu trữ trên database, ví dụ như địa chỉ về địa chỉ. Tuy nhiên, vì không biết đoạn ký tự đó ở thuộc vào thuộc tính nào, thì em có tiếp cận bằng cách sử dụng if else để đưa về đúng thuộc tính trong database. Cách này thì hơi củ chuối.
Mở rộng ra, giả sử như em muốn tiếp cận bài toán trên nhiều tài liệu dạng như ảnh bên dưới, nghĩa là mỗi nơi có kiểu tài liệu khác nhau thì nên tiếp cận và xử lý như thế nào để tổng quát hóa bài toán lên và phải đỡ xử lý bằng tay như build một mô hình rieng cho từng tài liệu ạ ?
Anh chị nào đã có kinh nghiệm xử lý qua bài toán này có thể chia sẻ cho em biết cách mọi người tiếp cận không ?
Em xin cảm ơn.
Ảnh dưới là minh họa cho bài toán của em ạ.","Hỏi về bài toán OCR - Understanding document Chào mọi người. Mọi người cho em hỏi: Em đang làm một bài toán về understanding document thì em có sử dụng dịch vụ OCR của Amazon Web Service để trích xuất các thông tin trong ảnh, nhưng mà AWS phần xử lý ngôn ngữ tự nhiên cho tiếng việt làm không được tốt, nên một số ký tự nhận dạng bị sai. Kết quả trả về của AWS là một file json, gồm các đoạn ký tự nhận dạng được. Sau khi qua phần xử lý OCR, thì em chuẩn hóa các đoạn ký tự này về các thuộc tính lưu trữ trên database, ví dụ như địa chỉ về địa chỉ. Tuy nhiên, vì không biết đoạn ký tự đó ở thuộc vào thuộc tính nào, thì em có tiếp cận bằng cách sử dụng if else để đưa về đúng thuộc tính trong database. Cách này thì hơi củ chuối. Mở rộng ra, giả sử như em muốn tiếp cận bài toán trên nhiều tài liệu dạng như ảnh bên dưới, nghĩa là mỗi nơi có kiểu tài liệu khác nhau thì nên tiếp cận và xử lý như thế nào để tổng quát hóa bài toán lên và phải đỡ xử lý bằng tay như build một mô hình rieng cho từng tài liệu ạ ? Anh chị nào đã có kinh nghiệm xử lý qua bài toán này có thể chia sẻ cho em biết cách mọi người tiếp cận không ? Em xin cảm ơn. Ảnh dưới là minh họa cho bài toán của em ạ.",,,,,
"Em chào mọi người! E đang code 1 model với 2 đầu vào, 1 đầu vào là ảnh, 1 đầu vào là các số. Nhưng khi em cho chạy thì bị đứng ở đoạn này và k chạy tiếp nữa, k biết có phải do cách e load data đang có vấn đề hay do model e viết đang lỗi, mong mn giúp e ạ. Em cảm ơn nhiều <3
(e dùng keras nên có vẻ như load data kiểu multiple inputs hơi khó khăn, xem trên mạng e thấy ng ta chuyển về dict rồi dùng dataset.from_generator, chi tiết hơn ở comment)","Em chào mọi người! E đang code 1 model với 2 đầu vào, 1 đầu vào là ảnh, 1 đầu vào là các số. Nhưng khi em cho chạy thì bị đứng ở đoạn này và k chạy tiếp nữa, k biết có phải do cách e load data đang có vấn đề hay do model e viết đang lỗi, mong mn giúp e ạ. Em cảm ơn nhiều <3 (e dùng keras nên có vẻ như load data kiểu multiple inputs hơi khó khăn, xem trên mạng e thấy ng ta chuyển về dict rồi dùng dataset.from_generator, chi tiết hơn ở comment)",,,,,
"Em chào anh chị ạ!
Hiện em đang có thắc mắc khi dùng thư viện spacy ạ. Như hình bên dưới khi em dùng char_span thì ba chữ đầu là ""Cần"", ""bán"", ""nhanh"" thì có thể bắt được, nhưng từ đó về phía sau char_span luôn return về None. Mặc dù index em đều đúng như s[14:17] là ""căn"", và s[18:20] là hộ, nhưng dùng char_span với index đó thì return là None. Không biết có anh chị nào gặp qua chưa ạ?
Em cảm ơn ạ.
#NLP","Em chào anh chị ạ! Hiện em đang có thắc mắc khi dùng thư viện spacy ạ. Như hình bên dưới khi em dùng char_span thì ba chữ đầu là ""Cần"", ""bán"", ""nhanh"" thì có thể bắt được, nhưng từ đó về phía sau char_span luôn return về None. Mặc dù index em đều đúng như s[14:17] là ""căn"", và s[18:20] là hộ, nhưng dùng char_span với index đó thì return là None. Không biết có anh chị nào gặp qua chưa ạ? Em cảm ơn ạ.",#NLP,,,,
"Cách đây 12h đồng hồ (lúc này 13:30 ngày 16 tháng 7 năm 2021, giờ GMT+7), có 2 công bố rất quan trọng về việc ứng dụng AI trong việc predict 3D structures của các phân tử proteins.
1/ Công bố thứ nhất của DeepMind trên Nature: Highly accurate protein structure prediction with AlphaFold (https://www.nature.com/articles/s41586-021-03819-2); với model có tên là AlphaFold và Source code viết trên JAX tại đây https://github.com/deepmind/alphafold
2/ Và nghiên cứu khác đến từ Mĩ với bài đăng trên tập san AAAS: Accurate prediction of protein structures and interactions using a three-track neural network (https://science.sciencemag.org/content/early/2021/07/14/science.abj8754); với model có tên RoseTTAFold và source code viết bằng PyTorch công bố tại đây: https://github.com/RosettaCommons/RoseTTAFold
Với những thành tựu liên ngành, hi vọng chúng ta sẽ chứng kiến nhiều đột phá trong cả khoa học máy tính và sinh y học.","Cách đây 12h đồng hồ (lúc này 13:30 ngày 16 tháng 7 năm 2021, giờ GMT+7), có 2 công bố rất quan trọng về việc ứng dụng AI trong việc predict 3D structures của các phân tử proteins. 1/ Công bố thứ nhất của DeepMind trên Nature: Highly accurate protein structure prediction with AlphaFold (https://www.nature.com/articles/s41586-021-03819-2); với model có tên là AlphaFold và Source code viết trên JAX tại đây https://github.com/deepmind/alphafold 2/ Và nghiên cứu khác đến từ Mĩ với bài đăng trên tập san AAAS: Accurate prediction of protein structures and interactions using a three-track neural network (https://science.sciencemag.org/content/early/2021/07/14/science.abj8754); với model có tên RoseTTAFold và source code viết bằng PyTorch công bố tại đây: https://github.com/RosettaCommons/RoseTTAFold Với những thành tựu liên ngành, hi vọng chúng ta sẽ chứng kiến nhiều đột phá trong cả khoa học máy tính và sinh y học.",,,,,
"Chào các anh/chị. Em có một thắc mắc khá cơ bản về feature scaling bằng normalization trong linear regression đa biến với gradient descent. 
Hiện tại em đang học course ML của GS Andrew Ng và tới được tuần 2 rồi (implement các hàm cho gradient descent).
Vấn đề em đang gặp hiện tại là nếu như em không normalize tập dữ liệu thì GD có thể converge như bình thường, còn nếu như em có normalize thì GD lại không thể converge được. Điều này có nghĩa là vấn đề nằm ở hàm featureNormalize, nhưng em lại không hiểu được là nó sai ở đâu ạ.
Em có đính kèm 2 đồ thị biểu diễn giá trị cost function sau mỗi iteration cho 2 trường hợp có và không có normalization ạ.
Đây là code của em: https://github.com/hungngocphat01/MachineLearning-AndrewNg-Coursera/tree/master/ex1-octave","Chào các anh/chị. Em có một thắc mắc khá cơ bản về feature scaling bằng normalization trong linear regression đa biến với gradient descent. Hiện tại em đang học course ML của GS Andrew Ng và tới được tuần 2 rồi (implement các hàm cho gradient descent). Vấn đề em đang gặp hiện tại là nếu như em không normalize tập dữ liệu thì GD có thể converge như bình thường, còn nếu như em có normalize thì GD lại không thể converge được. Điều này có nghĩa là vấn đề nằm ở hàm featureNormalize, nhưng em lại không hiểu được là nó sai ở đâu ạ. Em có đính kèm 2 đồ thị biểu diễn giá trị cost function sau mỗi iteration cho 2 trường hợp có và không có normalization ạ. Đây là code của em: https://github.com/hungngocphat01/MachineLearning-AndrewNg-Coursera/tree/master/ex1-octave",,,,,
"Hỏi về bài toán Regression.
Em chào mọi người, hiện em đang thực hiện một bài toán khảo sát, dựa vào số liệu đầu vào để xây dựng nên một hàm số.
- Em đang thắc mắc là trong Machine Learning mình còn dạng hàm hồi quy nào ngoài hồi quy tuyến tính, phi tuyến không ạ.
- Ngoài phương pháp chuẩn hóa trước khi huấn luyện để cải thiện độ chính xác thì còn phương pháp nào không ạ.
Em cảm ơn mọi người!","Hỏi về bài toán Regression. Em chào mọi người, hiện em đang thực hiện một bài toán khảo sát, dựa vào số liệu đầu vào để xây dựng nên một hàm số. - Em đang thắc mắc là trong Machine Learning mình còn dạng hàm hồi quy nào ngoài hồi quy tuyến tính, phi tuyến không ạ. - Ngoài phương pháp chuẩn hóa trước khi huấn luyện để cải thiện độ chính xác thì còn phương pháp nào không ạ. Em cảm ơn mọi người!",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 5/2021 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 5/2021 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
Thong tin tao tu dong qua AI cua Bao Lao Dong,Thong tin tao tu dong qua AI cua Bao Lao Dong,,,,,
"LỜI KÊU GỌI GÓP 5000 TIẾNG HO, GIÚP A.I. PHÁT HIỆN SỚM NGƯỜI MẮC COVID
Hãy giúp các chuyên gia chia sẻ thông điệp này!
Người dân Việt Nam thuộc đối tượng F0-F1-F2-F3 thu âm theo cú pháp:
1. Tôi tin mình là F...
2. (ho 4-5 tiếng)
gửi vào Nhóm Zalo cổng tiếp nhận dữ liệu: bit.ly/dulieutiengho
Con người có thể không phát hiện ra mình mắc Covid cho đến khi có triệu chứng như ho, sốt… hay tận đến khi được xét nghiệm bởi cơ quan y tế. Nhưng công nghệ A.I có thể làm được điều đó.
Nghiên cứu từ Đại học MIT cho biết khi virus mới xâm nhập cơ thể chúng chưa xâm nhập đủ sâu để tạo ra các triệu chứng như sốt hay ho nhưng đã gây ra những tổn thương nhỏ và nhẹ trong phổi. Khi được yêu cầu cố tình ho, phân tích tiếng ho này có thể nhận diện được sự hiện diện của virus. Thuật toán trí tuệ nhân tạo sẽ nghe hàng ngàn mẫu tiếng ho, qua đó, phân tích và lọc được những tín hiệu mà tai người không nghe không phân biệt được, với tỷ lệ chính xác lên đến 97% (chỉ số AUC, theo báo cáo của MIT).
Suốt 2 tháng qua, hơn 1000 chuyên gia công nghệ, chuyên gia y tế và các TNV của dự án AICOVIDVN đã xây dựng ra một Giải pháp trí tuệ nhân tạo và huấn luyện chú AI này nhận diện tiếng ho. Hiện tỉ lệ phát hiện đúng đã lên đến 90%.
AICOVIDVN kêu gọi bà con cả nước, những ai có người thân bạn bè không may nằm trong diện F0-F1-F2-F3, hãy đóng góp “tiếng ho"" của mình, để dự án nhanh chóng đạt được thêm tối thiểu 5000 tiếng ho, nạp vào tiếp tục hoàn thiện hệ thống. Lượng dữ liệu càng nhiều, AI Engine có thể chẩn đoán thông minh hơn, kỳ vọng độ chính xác trên 90%.
Sau khi hoàn thành bộ dữ liệu và đào tạo, AI Engine được dự án liên hệ, chuyển giao cho Ban chỉ đạo Quốc gia Phòng chống dịch Covid-19 nhằm phục vụ công tác chẩn đoán qua Robocall trên toàn quốc.
Công cụ này ra đời có thể giúp phát hiện được ca bệnh ở nhiều giai đoạn khác nhau, kể cả khi chưa có triệu chứng. Từ đó tìm ra những người có virus đang còn lẫn trong cộng đồng, giúp nhanh chóng khoanh vùng dịch, giảm tải cho y bác sĩ và các cán bộ tuyến đầu.
Hãy chia sẻ nội dung này!
---
CỐ VẤN & BẢO TRỢ DỰ ÁN
Anh Trần Anh Dũng, Founder & CEO, MOG Group
Anh Đào Xuân Hoàng, Founder & CEO, Monkey Junior
Anh Hùng Trần, Founder & CTO, GotIt
Anh Lợi Lưu, Founder & CEO, Kyber Network
Anh Nhân Nguyễn, Angel Investor
Anh Nguyễn Hoành Tiến, CEO, Seedcom
Anh Thức Vũ, Founder & CEO, OhmniLabs
Anh Phạm Minh Tuấn, Chairman, TFI Group
NHÂN SỰ ĐIỀU PHỐI
Tiến sĩ Vũ Xuân Sơn - Postdoctoral Fellow @ UMU, Co-founder of DopikAI Lab & AIHUB.VN
Tiến sĩ Vũ Hữu Tiệp - Machine Learning Engineer at Google, founder of MLCB.
Tiến sĩ Harry Nguyen - Asst. Prof. ĐH Glasgow, Co-founder of AIHUB.VN
Nguyễn Thùy Trinh, AI Researcher
Anh Lê Công Thành - Chairman & CEO at InfoRe Technology; Co-Founder/Producer at Dataset.vn
Tiến sĩ Phạm Minh Tuấn - Chairman, TFI Group
Anh Hưng Lê, Điều phối Dự án
Chị Trang Bùi, Điều phối Đối ngoại
Chị Thu Hà, Điều phối Marketing
Chị Bạch Dương, Điều phối Truyền thông
Cùng hàng ngàn cán bộ, chuyên gia đang tình nguyện đóng góp cho dự án
Trang thông tin chính thức
Bản giới thiệu dự án AICOVIDVN bit.ly/aicovidvn0
Fanpage https://www.facebook.com/aicovn
Group: https://www.facebook.com/groups/1264976217251463
 — với Lê Công Thành và Tiep VuHuu.","LỜI KÊU GỌI GÓP 5000 TIẾNG HO, GIÚP A.I. PHÁT HIỆN SỚM NGƯỜI MẮC COVID Hãy giúp các chuyên gia chia sẻ thông điệp này! Người dân Việt Nam thuộc đối tượng F0-F1-F2-F3 thu âm theo cú pháp: 1. Tôi tin mình là F... 2. (ho 4-5 tiếng) gửi vào Nhóm Zalo cổng tiếp nhận dữ liệu: bit.ly/dulieutiengho Con người có thể không phát hiện ra mình mắc Covid cho đến khi có triệu chứng như ho, sốt… hay tận đến khi được xét nghiệm bởi cơ quan y tế. Nhưng công nghệ A.I có thể làm được điều đó. Nghiên cứu từ Đại học MIT cho biết khi virus mới xâm nhập cơ thể chúng chưa xâm nhập đủ sâu để tạo ra các triệu chứng như sốt hay ho nhưng đã gây ra những tổn thương nhỏ và nhẹ trong phổi. Khi được yêu cầu cố tình ho, phân tích tiếng ho này có thể nhận diện được sự hiện diện của virus. Thuật toán trí tuệ nhân tạo sẽ nghe hàng ngàn mẫu tiếng ho, qua đó, phân tích và lọc được những tín hiệu mà tai người không nghe không phân biệt được, với tỷ lệ chính xác lên đến 97% (chỉ số AUC, theo báo cáo của MIT). Suốt 2 tháng qua, hơn 1000 chuyên gia công nghệ, chuyên gia y tế và các TNV của dự án AICOVIDVN đã xây dựng ra một Giải pháp trí tuệ nhân tạo và huấn luyện chú AI này nhận diện tiếng ho. Hiện tỉ lệ phát hiện đúng đã lên đến 90%. AICOVIDVN kêu gọi bà con cả nước, những ai có người thân bạn bè không may nằm trong diện F0-F1-F2-F3, hãy đóng góp “tiếng ho"" của mình, để dự án nhanh chóng đạt được thêm tối thiểu 5000 tiếng ho, nạp vào tiếp tục hoàn thiện hệ thống. Lượng dữ liệu càng nhiều, AI Engine có thể chẩn đoán thông minh hơn, kỳ vọng độ chính xác trên 90%. Sau khi hoàn thành bộ dữ liệu và đào tạo, AI Engine được dự án liên hệ, chuyển giao cho Ban chỉ đạo Quốc gia Phòng chống dịch Covid-19 nhằm phục vụ công tác chẩn đoán qua Robocall trên toàn quốc. Công cụ này ra đời có thể giúp phát hiện được ca bệnh ở nhiều giai đoạn khác nhau, kể cả khi chưa có triệu chứng. Từ đó tìm ra những người có virus đang còn lẫn trong cộng đồng, giúp nhanh chóng khoanh vùng dịch, giảm tải cho y bác sĩ và các cán bộ tuyến đầu. Hãy chia sẻ nội dung này! --- CỐ VẤN & BẢO TRỢ DỰ ÁN Anh Trần Anh Dũng, Founder & CEO, MOG Group Anh Đào Xuân Hoàng, Founder & CEO, Monkey Junior Anh Hùng Trần, Founder & CTO, GotIt Anh Lợi Lưu, Founder & CEO, Kyber Network Anh Nhân Nguyễn, Angel Investor Anh Nguyễn Hoành Tiến, CEO, Seedcom Anh Thức Vũ, Founder & CEO, OhmniLabs Anh Phạm Minh Tuấn, Chairman, TFI Group NHÂN SỰ ĐIỀU PHỐI Tiến sĩ Vũ Xuân Sơn - Postdoctoral Fellow @ UMU, Co-founder of DopikAI Lab & AIHUB.VN Tiến sĩ Vũ Hữu Tiệp - Machine Learning Engineer at Google, founder of MLCB. Tiến sĩ Harry Nguyen - Asst. Prof. ĐH Glasgow, Co-founder of AIHUB.VN Nguyễn Thùy Trinh, AI Researcher Anh Lê Công Thành - Chairman & CEO at InfoRe Technology; Co-Founder/Producer at Dataset.vn Tiến sĩ Phạm Minh Tuấn - Chairman, TFI Group Anh Hưng Lê, Điều phối Dự án Chị Trang Bùi, Điều phối Đối ngoại Chị Thu Hà, Điều phối Marketing Chị Bạch Dương, Điều phối Truyền thông Cùng hàng ngàn cán bộ, chuyên gia đang tình nguyện đóng góp cho dự án Trang thông tin chính thức Bản giới thiệu dự án AICOVIDVN bit.ly/aicovidvn0 Fanpage https://www.facebook.com/aicovn Group: https://www.facebook.com/groups/1264976217251463 — với Lê Công Thành và Tiep VuHuu.",,,,,
"Here is an implementation for QuickDraw - an online game developed by Google, combined with AirGesture - a simple gesture recognition application, written in Tensorflow
Source code: https://github.com/uvipen/QuickDraw-AirGesture-tensorflow
Demo: https://youtu.be/DkvMheb_iMc
QuickDraw là 1 bộ dataset về drawings rất nổi tiếng của Google, được thu thập dựa trên bản vẽ tay của hơn 15 triệu người. Đây cũng đồng thời là tên 1 game online được xây dựng bởi Google dựa trên bộ dataset này. Cá nhân mình đánh giá đây là 1 bộ dataset rất thú vị, và phù hợp với những người mới tìm hiểu về AI cũng như những người đã có kinh nghiệm lâu năm. Đây là project cá nhân của mình, sử dụng 1 phần nhỏ trong bộ dataset, kết hợp với 1 ứng dụng nhận dạng cử chỉ (gesture recognition) để tạo ra 1 app cho phép người dùng vẽ 1 object bất kì, và AI model sẽ dự đoán object được vẽ là gì. Let's enjoy my QuickDraw + AirGesture app","Here is an implementation for QuickDraw - an online game developed by Google, combined with AirGesture - a simple gesture recognition application, written in Tensorflow Source code: https://github.com/uvipen/QuickDraw-AirGesture-tensorflow Demo: https://youtu.be/DkvMheb_iMc QuickDraw là 1 bộ dataset về drawings rất nổi tiếng của Google, được thu thập dựa trên bản vẽ tay của hơn 15 triệu người. Đây cũng đồng thời là tên 1 game online được xây dựng bởi Google dựa trên bộ dataset này. Cá nhân mình đánh giá đây là 1 bộ dataset rất thú vị, và phù hợp với những người mới tìm hiểu về AI cũng như những người đã có kinh nghiệm lâu năm. Đây là project cá nhân của mình, sử dụng 1 phần nhỏ trong bộ dataset, kết hợp với 1 ứng dụng nhận dạng cử chỉ (gesture recognition) để tạo ra 1 app cho phép người dùng vẽ 1 object bất kì, và AI model sẽ dự đoán object được vẽ là gì. Let's enjoy my QuickDraw + AirGesture app",,,,,
"Hi mọi người, hiện tại mình đang gặp vấn đề như sau.
Mình có data từ 3 nguồn khác nhau (A, B, C). Mỗi nguồn được lưu trong 1 table riêng. Mỗi dòng ở mỗi table tương ứng với 1 user, tuy nhiên không phải lúc nào cũng có đủ dữ liệu cho các cột trong table. Ví dụ như ở bảng A chỉ có thông tin về công ty, tên, vị trí và email.
Câu hỏi của mình là với 1 user ví dụ như Samuels Jobs trong hình, có đến 3 bảng cùng lưu thông tin về người này, làm sao để gộp thông tin từ 3 nơi về, đặc biệt là khi tên người đó không được ghi đầy đủ (Sam J. hay Sam Jobs tất cả đều là 1 người Samuels Jobs)
Cảm ơn mọi người.","Hi mọi người, hiện tại mình đang gặp vấn đề như sau. Mình có data từ 3 nguồn khác nhau (A, B, C). Mỗi nguồn được lưu trong 1 table riêng. Mỗi dòng ở mỗi table tương ứng với 1 user, tuy nhiên không phải lúc nào cũng có đủ dữ liệu cho các cột trong table. Ví dụ như ở bảng A chỉ có thông tin về công ty, tên, vị trí và email. Câu hỏi của mình là với 1 user ví dụ như Samuels Jobs trong hình, có đến 3 bảng cùng lưu thông tin về người này, làm sao để gộp thông tin từ 3 nơi về, đặc biệt là khi tên người đó không được ghi đầy đủ (Sam J. hay Sam Jobs tất cả đều là 1 người Samuels Jobs) Cảm ơn mọi người.",,,,,
"#hoidap
Tình cờ lướt thấy video này, mọi người có hướng tiếp cận hay doc gì liên quan đến bài toán kiểu AI singer này không ạ?
Ai giải thích cho em về cách train cho loại bài toán này như nào được không ạ?
Em xin cảm ơn!","Tình cờ lướt thấy video này, mọi người có hướng tiếp cận hay doc gì liên quan đến bài toán kiểu AI singer này không ạ? Ai giải thích cho em về cách train cho loại bài toán này như nào được không ạ? Em xin cảm ơn!",#hoidap,,,,
"Bài giảng của MIT cho những bạn muốn nghiên cứu ứng dụng ML/DL/AI vào trong y học. https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/index.htm
Ps1. Xin lưu ý, domain chuyên ngành là rất rất quan trọng. Hôm trước có một bác sĩ challenge về định nghĩa ho/cough khi các bạn trong cộng đồng muốn làm ứng dụng AI cho chẩn đoán Covid từ tiếng ho.
Ps2. Các nghiên cứu ứng dụng trong y học cần được thiết kế nghiên cứu rất rất nghiêm ngặt để thu và gán nhãn dữ liệu theo tiêu chuẩn của từng hiệp hội chuyên ngành. Chắc chắn các bạn cũng ai “garbage in, garbage out”","Bài giảng của MIT cho những bạn muốn nghiên cứu ứng dụng ML/DL/AI vào trong y học. https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/index.htm Ps1. Xin lưu ý, domain chuyên ngành là rất rất quan trọng. Hôm trước có một bác sĩ challenge về định nghĩa ho/cough khi các bạn trong cộng đồng muốn làm ứng dụng AI cho chẩn đoán Covid từ tiếng ho. Ps2. Các nghiên cứu ứng dụng trong y học cần được thiết kế nghiên cứu rất rất nghiêm ngặt để thu và gán nhãn dữ liệu theo tiêu chuẩn của từng hiệp hội chuyên ngành. Chắc chắn các bạn cũng ai “garbage in, garbage out”",,,,,
Mn có thể giải thích công dụng của tf.keras.layers.GlobalAveragePooling2D() giúp mình vs? Đọc tài liệu mà ko hiểu nó làm cái j? Phân biệt loss 'categorical_crossentropy' vs 'sparse_categorical_crossentropy',Mn có thể giải thích công dụng của tf.keras.layers.GlobalAveragePooling2D() giúp mình vs? Đọc tài liệu mà ko hiểu nó làm cái j? Phân biệt loss 'categorical_crossentropy' vs 'sparse_categorical_crossentropy',,,,,
"Kính chào các bác! Em đang học về model Auto Encoder nên có làm một bài Fraud Detection. Em cũng làm clip để các bạn mới học tham khảo thêm. Hi vọng giúp được các bạn!
Mong các bác chỉ giáo và admin duyệt bài ạ!",Kính chào các bác! Em đang học về model Auto Encoder nên có làm một bài Fraud Detection. Em cũng làm clip để các bạn mới học tham khảo thêm. Hi vọng giúp được các bạn! Mong các bác chỉ giáo và admin duyệt bài ạ!,,,,,
"Dạ em chào anh chị và mọi người. Em đang là sinh viên, sắp phân chuyên ngành, em dự định sẽ chọn CS và ML nên có tham khảo một số tài liệu và khóa học free trên mạng từ Coursera, Geeks tới Freecodecamp nhưng không biết đâu là phù hợp. Nên em xin phép các tiền bối đi trước cho em hỏi là mới bắt đầu thì đọc tài liệu hay học trang nào và có thể cho em 1 roadmap để tham khảo không ạ. Em cảm ơn mọi người đã dành thời gian đọc ạ.","Dạ em chào anh chị và mọi người. Em đang là sinh viên, sắp phân chuyên ngành, em dự định sẽ chọn CS và ML nên có tham khảo một số tài liệu và khóa học free trên mạng từ Coursera, Geeks tới Freecodecamp nhưng không biết đâu là phù hợp. Nên em xin phép các tiền bối đi trước cho em hỏi là mới bắt đầu thì đọc tài liệu hay học trang nào và có thể cho em 1 roadmap để tham khảo không ạ. Em cảm ơn mọi người đã dành thời gian đọc ạ.",,,,,
#NLP Em chào mọi người ạ. Có cao nhân nào đã làm về trích xuất thuật ngữ trong văn bản tiếng việt rồi cho em xin ít kinh nghiệm ạ. Trong suy nghĩ của em thì để trích xuất được thuật ngữ thì mình cần có bộ data tiếng việt chứa thuật ngữ phân theo các chuyên ngành lình vực đúng k ạ? nhưng em tìm k thấy. Hay ai có cách nào khác chỉ em với ạ. Em cảm ơn,Em chào mọi người ạ. Có cao nhân nào đã làm về trích xuất thuật ngữ trong văn bản tiếng việt rồi cho em xin ít kinh nghiệm ạ. Trong suy nghĩ của em thì để trích xuất được thuật ngữ thì mình cần có bộ data tiếng việt chứa thuật ngữ phân theo các chuyên ngành lình vực đúng k ạ? nhưng em tìm k thấy. Hay ai có cách nào khác chỉ em với ạ. Em cảm ơn,#NLP,,,,
"Mn cho em hỏi vấn đề này với ạ.
1. Chuyện là em nghe thầy em nói là chỉ có ngôn ngữ C mới chạy đáp ứng thời gian thực được, vậy trong Machine Learning sau mình không dùng ngôn ngữ C để làm mà lại dùng Python ạ. Do nếu dùng Python thì khi cho vào thực tế thì nó ko nhanh bằng C dc. Mong mn thông cảm nếu câu hỏi có hơi ngốc ạ.
2. Theo mn thì dùng trình biên dịch nào là tốt nhất và nếu sử dụng Google Colab thì có ổn k ạ. Em cảm ơn mn nhiều.","Mn cho em hỏi vấn đề này với ạ. 1. Chuyện là em nghe thầy em nói là chỉ có ngôn ngữ C mới chạy đáp ứng thời gian thực được, vậy trong Machine Learning sau mình không dùng ngôn ngữ C để làm mà lại dùng Python ạ. Do nếu dùng Python thì khi cho vào thực tế thì nó ko nhanh bằng C dc. Mong mn thông cảm nếu câu hỏi có hơi ngốc ạ. 2. Theo mn thì dùng trình biên dịch nào là tốt nhất và nếu sử dụng Google Colab thì có ổn k ạ. Em cảm ơn mn nhiều.",,,,,
"chào các bác ạ, e đang học về Gradient Descent và có một chút thắc mắc nhỏ:
tại sao khoảng delta thay đổi giữa X(t) và X(T+1) lại bằng đạo hàm của hàm lr*f'(X(t)) ạ?
bản chất thuật toán thì hiểu nhưng điểm này e vẫn chưa thông ạ. Em cảm ơn","chào các bác ạ, e đang học về Gradient Descent và có một chút thắc mắc nhỏ: tại sao khoảng delta thay đổi giữa X(t) và X(T+1) lại bằng đạo hàm của hàm lr*f'(X(t)) ạ? bản chất thuật toán thì hiểu nhưng điểm này e vẫn chưa thông ạ. Em cảm ơn",,"#Q&A, #math",,,
"Chào ace
Mình mới học RL, nên nhờ ace góp ý giúp mô hình sau. Mình có 4 UAVs kết nối với mobiles dưới mặt đất cho video streaming. các users này di chuyển ngẫu nhiên. Với mô hình này thì mình nên dùng mô hình deep learning nào phù hợp để điều khiển UAVs?
Xin chân thành cảm ơn","Chào ace Mình mới học RL, nên nhờ ace góp ý giúp mô hình sau. Mình có 4 UAVs kết nối với mobiles dưới mặt đất cho video streaming. các users này di chuyển ngẫu nhiên. Với mô hình này thì mình nên dùng mô hình deep learning nào phù hợp để điều khiển UAVs? Xin chân thành cảm ơn",,,,,
"Chào mọi người, hiện tại em muốn xóa vùng màu đỏ và nối trực tiếp theo mũi tên mầu xanh thì có cách nào không ạ. Em đã thử nhiều cách mà chưa được ạ, mong mọi người giúp đỡ.","Chào mọi người, hiện tại em muốn xóa vùng màu đỏ và nối trực tiếp theo mũi tên mầu xanh thì có cách nào không ạ. Em đã thử nhiều cách mà chưa được ạ, mong mọi người giúp đỡ.",,,,,
"[HOT EVENT] TỔNG KẾT GIAI ĐOẠN 1 VÀ THUYẾT TRÌNH GIẢI PHÁP TRÍ TUỆ NHÂN TẠO (AI): PHÁT HIỆN COVID QUA TIẾNG HO VỚI GIẢI THƯỞNG LÊN TỚI 115 TRIỆU ĐỒNG 📣📣📣
🕙 Thời gian: 20:00-22:00 - Thứ Năm, 08 tháng 7 2021 (giờ Việt Nam)
☑️ Hình thức: Livestream trực tiếp
☑️ Link tham gia vào hội thảo trực tuyến: https://zoom.us/j/96842982383
Trải qua gần 2 tuần thử thách, hơn 200 đội thi của dự án là những chuyên gia AI trên khắp Việt Nam cùng với sự đồng hành của nhiều Anh/Chị là quản lý cấp cao của các Công ty/tập đoàn công nghệ lớn đã bắt đầu hoàn thành cuộc đua trong giai đoạn 1 của dự án - Giai đoạn khởi động. Dự án sẽ có một buổi tổng kết và thuyết trình các giải phát của thí sinh trong tối nay (8/7/2021) với sự đồng hành cùng đội ngũ BGK cực ""HOT"" trong giới công nghệ:
1, Tiến sĩ Phạm Minh Tuấn - Chairman, TFI Group
2, Tiến sĩ Vũ Xuân Sơn - Postdoctoral Fellow @ UMU, Co-founder of DopikAI Lab & AIHUB.VN
3, Tiến sĩ Vũ Hữu Tiệp - Machine Learning Engineer at Google, founder of MLCB.
4, Anh Lê Công Thành - Chairman & CEO at InfoRe Technology; Co-Founder/Producer at Dataset.vn
5, Tiến sĩ Harry Nguyen - Asst. Prof. ĐH Glasgow, Co-founder of AIHUB.VN
👉Giới thiệu về cuộc thi Sáng tạo giải pháp trí tuệ nhân tạo (AI): Nhận dạng Covid qua tiếng ho
""AI Covid"" là dự án cộng đồng với mục tiêu làm ra được giải pháp Trí tuệ nhân tạo (AI) để có thể phát hiện Covid 19 thông qua tiếng ho. Cuộc thi được chính thức phát động trong khuôn khổ chương trình Tập huấn “Đội hình tình nguyện số” và “Câu lạc bộ đổi mới sáng tạo Giải pháp trí tuệ nhân tạo” do Thành đoàn, Hội Sinh viên thành phố Hà Nội phối hợp tổ chức với sự đóng góp của hơn 200 chuyên gia AI và các anh/chị là quản lý cấp cao của các công ty, tập đoàn lớn tại Việt Nam.
Cuộc thi ""Sáng tạo giải pháp trí tuệ nhân tạo (AI) - Nhận dạng Covid qua tiếng ho"" được tổ chức với sự tham gia của hơn 200 đội thi với cơ cấu giải thưởng lên tới 115 triệu đồng và chia làm 2 giai đoạn:
☑️ Giai đoạn Khởi động( 22/06/2021 - 01/07/2021)
☑️ Giai đoạn về đích( 19/07/2021 - 20/08/2021)
Trong giai đoạn này, các đội thi sẽ được tiếp cận với nhiều dữ liệu thực tế được thu thập tại Việt Nam và các nước lân cận để AI sẽ có kết quả được chính xác nhất
👉 Các đội có thể tìm hiểu thêm thông tin và đăng ký tham gia giai đoạn 2 tại:
☑️ Website của cuộc thi: https://www.covid.aihub.vn/
☑️ Fanpage của dự án: https://www.facebook.com/aicovn
 — với Lê Công Thành và 4 người khác.","[HOT EVENT] TỔNG KẾT GIAI ĐOẠN 1 VÀ THUYẾT TRÌNH GIẢI PHÁP TRÍ TUỆ NHÂN TẠO (AI): PHÁT HIỆN COVID QUA TIẾNG HO VỚI GIẢI THƯỞNG LÊN TỚI 115 TRIỆU ĐỒNG Thời gian: 20:00-22:00 - Thứ Năm, 08 tháng 7 2021 (giờ Việt Nam) Hình thức: Livestream trực tiếp Link tham gia vào hội thảo trực tuyến: https://zoom.us/j/96842982383 Trải qua gần 2 tuần thử thách, hơn 200 đội thi của dự án là những chuyên gia AI trên khắp Việt Nam cùng với sự đồng hành của nhiều Anh/Chị là quản lý cấp cao của các Công ty/tập đoàn công nghệ lớn đã bắt đầu hoàn thành cuộc đua trong giai đoạn 1 của dự án - Giai đoạn khởi động. Dự án sẽ có một buổi tổng kết và thuyết trình các giải phát của thí sinh trong tối nay (8/7/2021) với sự đồng hành cùng đội ngũ BGK cực ""HOT"" trong giới công nghệ: 1, Tiến sĩ Phạm Minh Tuấn - Chairman, TFI Group 2, Tiến sĩ Vũ Xuân Sơn - Postdoctoral Fellow @ UMU, Co-founder of DopikAI Lab & AIHUB.VN 3, Tiến sĩ Vũ Hữu Tiệp - Machine Learning Engineer at Google, founder of MLCB. 4, Anh Lê Công Thành - Chairman & CEO at InfoRe Technology; Co-Founder/Producer at Dataset.vn 5, Tiến sĩ Harry Nguyen - Asst. Prof. ĐH Glasgow, Co-founder of AIHUB.VN Giới thiệu về cuộc thi Sáng tạo giải pháp trí tuệ nhân tạo (AI): Nhận dạng Covid qua tiếng ho ""AI Covid"" là dự án cộng đồng với mục tiêu làm ra được giải pháp Trí tuệ nhân tạo (AI) để có thể phát hiện Covid 19 thông qua tiếng ho. Cuộc thi được chính thức phát động trong khuôn khổ chương trình Tập huấn “Đội hình tình nguyện số” và “Câu lạc bộ đổi mới sáng tạo Giải pháp trí tuệ nhân tạo” do Thành đoàn, Hội Sinh viên thành phố Hà Nội phối hợp tổ chức với sự đóng góp của hơn 200 chuyên gia AI và các anh/chị là quản lý cấp cao của các công ty, tập đoàn lớn tại Việt Nam. Cuộc thi ""Sáng tạo giải pháp trí tuệ nhân tạo (AI) - Nhận dạng Covid qua tiếng ho"" được tổ chức với sự tham gia của hơn 200 đội thi với cơ cấu giải thưởng lên tới 115 triệu đồng và chia làm 2 giai đoạn: Giai đoạn Khởi động( 22/06/2021 - 01/07/2021) Giai đoạn về đích( 19/07/2021 - 20/08/2021) Trong giai đoạn này, các đội thi sẽ được tiếp cận với nhiều dữ liệu thực tế được thu thập tại Việt Nam và các nước lân cận để AI sẽ có kết quả được chính xác nhất Các đội có thể tìm hiểu thêm thông tin và đăng ký tham gia giai đoạn 2 tại: Website của cuộc thi: https://www.covid.aihub.vn/ Fanpage của dự án: https://www.facebook.com/aicovn — với Lê Công Thành và 4 người khác.",,,,,
"[AICovidVN-115M Challenge] Tổng kết và Thuyết trình giải pháp - Giai đoạn Khởi động
Sự kiện Trao giải và Tổng kết về Giai đoạn Khởi động của cuộc thi AICovidVN-115 Challenge
Top đội thi sẽ trình bày giải pháp của mình qua việc thuyết trình và hỏi đáp.",[AICovidVN-115M Challenge] Tổng kết và Thuyết trình giải pháp - Giai đoạn Khởi động Sự kiện Trao giải và Tổng kết về Giai đoạn Khởi động của cuộc thi AICovidVN-115 Challenge Top đội thi sẽ trình bày giải pháp của mình qua việc thuyết trình và hỏi đáp.,,,,,
"Chào mọi người!
hiện tại em đang nghiên cứu để đọc giá trị date thùng như trong ảnh. các a cho em giải pháp đọc nhanh và chính xác được không ạ",Chào mọi người! hiện tại em đang nghiên cứu để đọc giá trị date thùng như trong ảnh. các a cho em giải pháp đọc nhanh và chính xác được không ạ,,,,,
"Em có background bên Tự động hóa. Nhưng trong chương trình thực tập ở bên công ty, họ cho em 1 project về Machine learning in Computer Vision. Cho em hỏi là để học về ML mảng này thì em nên đọc tài liệu nào ạ?","Em có background bên Tự động hóa. Nhưng trong chương trình thực tập ở bên công ty, họ cho em 1 project về Machine learning in Computer Vision. Cho em hỏi là để học về ML mảng này thì em nên đọc tài liệu nào ạ?",,,,,
"Chào ace, mình có bài toán như sau:
Bài toán phân lớp của mình với 5430 phần tử thuộc tập dương, và 184155 phần tử âm, số lượng đặc trưng là 878, với 2 cách tiếp cận: 
Train trên tập gồm 5430 phần tử dương và 5430x2 phần tử âm, số lượng đặc trưng là 878.
Train trên toàn bộ 5430 phần tử dương và 184155 phần tử âm, nhưng số lượng đặc trưng rút lại còn 100.
Theo 2 cách trên thì liệu mình có dự đoán kết quả theo cách nào là khả thi hơn, nếu mình chưa bắt tay vào train ạ.
Xin mọi người chỉ giáo. Mình cám ơn ạ!","Chào ace, mình có bài toán như sau: Bài toán phân lớp của mình với 5430 phần tử thuộc tập dương, và 184155 phần tử âm, số lượng đặc trưng là 878, với 2 cách tiếp cận: Train trên tập gồm 5430 phần tử dương và 5430x2 phần tử âm, số lượng đặc trưng là 878. Train trên toàn bộ 5430 phần tử dương và 184155 phần tử âm, nhưng số lượng đặc trưng rút lại còn 100. Theo 2 cách trên thì liệu mình có dự đoán kết quả theo cách nào là khả thi hơn, nếu mình chưa bắt tay vào train ạ. Xin mọi người chỉ giáo. Mình cám ơn ạ!",,,,,
"Em chào mọi người.
Em xin phép được hỏi một câu liên quan đến training model trên GPU của server. Giả sử server của em có 4 con GPU, em muốn training trên con GPU với device_id = 3 thì mình cần làm gì ạ. Em đã set eCUDA_VISIBLE_DEVICES=""3"" và CUDAPlace(3) của paddle nhưng khi train nó chỉ dùng device = 0 như hình ạ.
Rất mong mọi người giải đáp ạ, em cảm ơn mọi người","Em chào mọi người. Em xin phép được hỏi một câu liên quan đến training model trên GPU của server. Giả sử server của em có 4 con GPU, em muốn training trên con GPU với device_id = 3 thì mình cần làm gì ạ. Em đã set eCUDA_VISIBLE_DEVICES=""3"" và CUDAPlace(3) của paddle nhưng khi train nó chỉ dùng device = 0 như hình ạ. Rất mong mọi người giải đáp ạ, em cảm ơn mọi người",,,,,
Một bài viết nho nhỏ về SVM. Mong nhận được ý kiến đóng góp từ mọi người để hoàn thiện hơn.,Một bài viết nho nhỏ về SVM. Mong nhận được ý kiến đóng góp từ mọi người để hoàn thiện hơn.,,,,,
"Chào mọi người. Mình đang làm một project về nhận dạng mặt người (face recognition) dùng opencv và Dlib. Mình có một Card NVIDIA Tesla K80. Mình muốn sử dụng GPU này thì mình đọc tài liệu là phải cài CUDA Toolkit và CuDNN.
Trên group mình có bạn nào đã cài CUDA và CuDNN trên window chưa và phải set up như thế nào để dlib dùng được GPU?
Mình nhận được phản hồi của các bạn. Thanks!",Chào mọi người. Mình đang làm một project về nhận dạng mặt người (face recognition) dùng opencv và Dlib. Mình có một Card NVIDIA Tesla K80. Mình muốn sử dụng GPU này thì mình đọc tài liệu là phải cài CUDA Toolkit và CuDNN. Trên group mình có bạn nào đã cài CUDA và CuDNN trên window chưa và phải set up như thế nào để dlib dùng được GPU? Mình nhận được phản hồi của các bạn. Thanks!,,,,,
"Thông báo dừng trang web AIviVN.com.
Kể từ khi thành lập AIviVN.com -- trang web tổ chức các cuộc thi học máy, chúng tôi rất vui khi nhận được sự ủng hộ về dữ liệu, kinh phí duy trì từ các doanh nghiệp và sự ủng hộ từ các đội tham gia các cuộc thi. Chúng tôi hy vọng AIviVN.com đã phần nào mang lại những giá trị nhất định cho cộng đồng.
Thời gian gần đây, vì những giới hạn của nguồn dữ liệu và nguồn lực duy trì trang web, chúng tôi đã cân nhắc và đưa ra quyết định đóng cửa AIviVN.com để tập trung nguồn lực vào các dự án khác có độ ưu tiên cao hơn. Việc này không ảnh hưởng tới các dự án sử dụng tên miền con, cuốn sách ""Đắm mình vào học sâu"" (d2l.aivivn.com) vẫn sẽ được duy trì.
Nhân tiện đây, tôi xin giới thiệu trang thay thế https://aihub.vn/ với một số cuộc thi khác đang diễn ra, bao gồm AICovidVN 115M. Trang này được duy trì bởi VLSP với anh Vũ Xuân Sơn làm đại diện.
Cảm ơn sự ủng hộ AIviVN của cộng đồng trong thời gian AIviVN tồn tại.","Thông báo dừng trang web AIviVN.com. Kể từ khi thành lập AIviVN.com -- trang web tổ chức các cuộc thi học máy, chúng tôi rất vui khi nhận được sự ủng hộ về dữ liệu, kinh phí duy trì từ các doanh nghiệp và sự ủng hộ từ các đội tham gia các cuộc thi. Chúng tôi hy vọng AIviVN.com đã phần nào mang lại những giá trị nhất định cho cộng đồng. Thời gian gần đây, vì những giới hạn của nguồn dữ liệu và nguồn lực duy trì trang web, chúng tôi đã cân nhắc và đưa ra quyết định đóng cửa AIviVN.com để tập trung nguồn lực vào các dự án khác có độ ưu tiên cao hơn. Việc này không ảnh hưởng tới các dự án sử dụng tên miền con, cuốn sách ""Đắm mình vào học sâu"" (d2l.aivivn.com) vẫn sẽ được duy trì. Nhân tiện đây, tôi xin giới thiệu trang thay thế https://aihub.vn/ với một số cuộc thi khác đang diễn ra, bao gồm AICovidVN 115M. Trang này được duy trì bởi VLSP với anh Vũ Xuân Sơn làm đại diện. Cảm ơn sự ủng hộ AIviVN của cộng đồng trong thời gian AIviVN tồn tại.",,,,,
"Chào mọi người. Ví dụ trong công ty mình đã có một model đang vận hành trên production cho một task nào đó. Bây giờ mình triển khai một model mới với mục đích nâng cao performance so với model cũ. Thì làm sao test được hiệu qủa của model mới so với model cũ để thuyết phục sếp triển khai mọi người nhỉ.
Cảm ơn mọi người.",Chào mọi người. Ví dụ trong công ty mình đã có một model đang vận hành trên production cho một task nào đó. Bây giờ mình triển khai một model mới với mục đích nâng cao performance so với model cũ. Thì làm sao test được hiệu qủa của model mới so với model cũ để thuyết phục sếp triển khai mọi người nhỉ. Cảm ơn mọi người.,,,,,
"Gần đây JAX nổi lên như một framework được sử dụng trong nghiên cứu mới nhất của Google Brain và DeepMind. Và các API hỗ trợ đang dần chiếm được vị trí cho JAX là Flax và Haiku. JAX hỗ trợ tốt với GPUs và TPUs nên chắc sẽ có tương lai hứa hẹn.
Dưới đây là hướng dẫn sử dụng JAX đó HuggingFace giới thiệu
YouTube:
https://youtu.be/__eG63ZP_5g?t=1545
Slides:
https://docs.google.com/presentation/d/1d8gTywMi32kA1qigOMRF2MjVdyM9e2BV7p878NPMzII/
Notebook:
https://colab.research.google.com/drive/1NV3kQAMGo4e47XkXHGvKHkpy0aJ8-Vxg",Gần đây JAX nổi lên như một framework được sử dụng trong nghiên cứu mới nhất của Google Brain và DeepMind. Và các API hỗ trợ đang dần chiếm được vị trí cho JAX là Flax và Haiku. JAX hỗ trợ tốt với GPUs và TPUs nên chắc sẽ có tương lai hứa hẹn. Dưới đây là hướng dẫn sử dụng JAX đó HuggingFace giới thiệu YouTube: https://youtu.be/__eG63ZP_5g?t=1545 Slides: https://docs.google.com/presentation/d/1d8gTywMi32kA1qigOMRF2MjVdyM9e2BV7p878NPMzII/ Notebook: https://colab.research.google.com/drive/1NV3kQAMGo4e47XkXHGvKHkpy0aJ8-Vxg,,,,,
"Chào các bạn,
Mình đang làm bài toán xây dựng mô hình đoán hành vi (đi, đứng, nằm...vv) của bò dựa trên dữ liệu cảm biến 3d sensors (accelerometer và gyroscope). Dữ liệu gán nhãn từ 10 con bò, mình lấy dữ liệu từ 5 con xong rồi chia thành 10 phần, 3 phần để test, 7 phần để train. Mô hình sau khi train thì test (1 lần) trên 3 phần test đó thì cho ra accuracy 87.35% (được xem là tốt vì có 1 vài hành vi dễ bị confusing như đứng và nằm). Mô hình đó sau khi test trên dữ liệu của một con bò khác (không nằm trong 5 con được train kia) thì cho ra accuracy thấp là 58%.
Nhận định của mình là ""Dữ liệu training từ 5 con bò kia chưa đủ mang tính đại diện, mình cần phải thêm dữ liệu cảm biến từ 1 vài con khác nữa""?!
Các bạn cho mình hỏi nhận định như vậy đã đúng chưa, và còn có thể có những vấn đề gì nào khác nữa?
Cảm ơn các bạn!","Chào các bạn, Mình đang làm bài toán xây dựng mô hình đoán hành vi (đi, đứng, nằm...vv) của bò dựa trên dữ liệu cảm biến 3d sensors (accelerometer và gyroscope). Dữ liệu gán nhãn từ 10 con bò, mình lấy dữ liệu từ 5 con xong rồi chia thành 10 phần, 3 phần để test, 7 phần để train. Mô hình sau khi train thì test (1 lần) trên 3 phần test đó thì cho ra accuracy 87.35% (được xem là tốt vì có 1 vài hành vi dễ bị confusing như đứng và nằm). Mô hình đó sau khi test trên dữ liệu của một con bò khác (không nằm trong 5 con được train kia) thì cho ra accuracy thấp là 58%. Nhận định của mình là "" Dữ liệu training từ 5 con bò kia chưa đủ mang tính đại diện, mình cần phải thêm dữ liệu cảm biến từ 1 vài con khác nữa""?! Các bạn cho mình hỏi nhận định như vậy đã đúng chưa, và còn có thể có những vấn đề gì nào khác nữa? Cảm ơn các bạn!",,,,,
"[AI Share - GitHub Copilot]
Thật là tuyệt vời! 😍😎
GitHub cùng với OpenAI vừa phát hành một hệ thống AI mới “Github Copilot: Your AI pair programmer” .
Với GitHub Copilot, bạn có thể nhận được gợi ý chocả dòng code hoặc toàn bộ hàm mà bạn định viết, giúp bạn viết code tốt hơn và nhanh hơn. Thêm vào đó, GitHub Copilot hoạt động với nhiều frameworks và nhiều ngôn ngữ nhưng đặc biệt tốt với Python, JavaScript, TypeScript, Ruby và Go.
Bạn có thể xem trước một số kĩ thuật và đăng kí trước ở đây: https://copilot.github.com/","[AI Share - GitHub Copilot] Thật là tuyệt vời! GitHub cùng với OpenAI vừa phát hành một hệ thống AI mới “Github Copilot: Your AI pair programmer” . Với GitHub Copilot, bạn có thể nhận được gợi ý chocả dòng code hoặc toàn bộ hàm mà bạn định viết, giúp bạn viết code tốt hơn và nhanh hơn. Thêm vào đó, GitHub Copilot hoạt động với nhiều frameworks và nhiều ngôn ngữ nhưng đặc biệt tốt với Python, JavaScript, TypeScript, Ruby và Go. Bạn có thể xem trước một số kĩ thuật và đăng kí trước ở đây: https://copilot.github.com/",,,,,
"Các anh chị cho em hỏi một chút với ạ.
Em có tìm hiểu recommender system, và biết rằng content-based filtering là sử dụng những item tương tự item mà user đã đánh giá để đưa ra gợi ý, không cần đến đánh giá của các user khác. Vậy nếu em sử dụng kĩ thuật của content-based (TFIDF và consine similarity) để tìm ra các user có đặc điểm (tuổi, giới tính, nghề nghiệp,...) tương tự với một user cụ thể (input) thì cách làm như vậy có được gọi là content-based filtering nữa không ạ?
Em cảm ơn ạ :D","Các anh chị cho em hỏi một chút với ạ. Em có tìm hiểu recommender system, và biết rằng content-based filtering là sử dụng những item tương tự item mà user đã đánh giá để đưa ra gợi ý, không cần đến đánh giá của các user khác. Vậy nếu em sử dụng kĩ thuật của content-based (TFIDF và consine similarity) để tìm ra các user có đặc điểm (tuổi, giới tính, nghề nghiệp,...) tương tự với một user cụ thể (input) thì cách làm như vậy có được gọi là content-based filtering nữa không ạ? Em cảm ơn ạ :D",,,,,
Hi mọi người. Mình muốn tìm sách hay nguồn đọc về deploy model to production. Ai có biết tên sách hay nguồn nào hay có thể giới thiệu được không? Cảm ơn mọi người nhiều,Hi mọi người. Mình muốn tìm sách hay nguồn đọc về deploy model to production. Ai có biết tên sách hay nguồn nào hay có thể giới thiệu được không? Cảm ơn mọi người nhiều,,,,,
"Dictionary-guided Scene Text Recognition
This might be an useful resource for future research & applications on scene text recognition, especially for Vietnamese. Our work is also accepted to CVPR 2021.
In this work, we present an approach to train and use scene text recognition models by exploiting multiple clues from a language reference. Current scene text recognition methods have used lexicons to improve recognition performance, but their naive approach of simply casting the output into a dictionary word based purely on the edit distance has many limitations. We introduce here a novel approach to incorporate a dictionary in both the training and inference stage of a scene text recognition system. We use the dictionary to generate a list of possible outcomes and find the one that is most compatible with the visual appearance of the text. The proposed method leads to a robust scene text recognition model, which is better at handling ambiguous cases encountered in the wild, and improves the overall performance of a state-of-the-art scene text spotting framework. Our work suggests that incorporating language prior is a potential approach to advance scene text detection and recognition methods. Besides, we contribute a challenging scene text dataset for Vietnamese, where some characters are equivocal in the visual form due to accent symbols. This dataset will serve as a challenging benchmark for measuring the applicability and robustness of scene text detection and recognition algorithms.
If you want to learn more about our research, you can check the video: https://youtu.be/0Cq4CBBSJII
Our paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.html
Code and dataset: https://github.com/VinAIResearch/dict-guided","Dictionary-guided Scene Text Recognition This might be an useful resource for future research & applications on scene text recognition, especially for Vietnamese. Our work is also accepted to CVPR 2021. In this work, we present an approach to train and use scene text recognition models by exploiting multiple clues from a language reference. Current scene text recognition methods have used lexicons to improve recognition performance, but their naive approach of simply casting the output into a dictionary word based purely on the edit distance has many limitations. We introduce here a novel approach to incorporate a dictionary in both the training and inference stage of a scene text recognition system. We use the dictionary to generate a list of possible outcomes and find the one that is most compatible with the visual appearance of the text. The proposed method leads to a robust scene text recognition model, which is better at handling ambiguous cases encountered in the wild, and improves the overall performance of a state-of-the-art scene text spotting framework. Our work suggests that incorporating language prior is a potential approach to advance scene text detection and recognition methods. Besides, we contribute a challenging scene text dataset for Vietnamese, where some characters are equivocal in the visual form due to accent symbols. This dataset will serve as a challenging benchmark for measuring the applicability and robustness of scene text detection and recognition algorithms. If you want to learn more about our research, you can check the video: https://youtu.be/0Cq4CBBSJII Our paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.html Code and dataset: https://github.com/VinAIResearch/dict-guided",,,,,
,nan,,,,,
"Chào mọi người,hiện em đang tìm hiểu về yolo.
Em muốn hỏi là trong yolov2 người ta có bảo có sự cải tiến là dùng anchor box.
Em có đọc được là anchor box là một bounding box cơ sở để xác định bounding box bao quanh vật thể. Theo em hiểu là trong yolov1 mỗi grid cell sẽ dự đoán bounding box và số điểm tin cậy= giá trị prediction box so với ground-truth box. Vậy thêm anchor box cụ thể là gì và có tác dụng gì  ạ ? Em vẫn chưa hiểu rõ anchor box  nó là gì và có điểm mạnh gì hơn?
Em newbie mong mọi người thông não.","Chào mọi người,hiện em đang tìm hiểu về yolo. Em muốn hỏi là trong yolov2 người ta có bảo có sự cải tiến là dùng anchor box. Em có đọc được là anchor box là một bounding box cơ sở để xác định bounding box bao quanh vật thể. Theo em hiểu là trong yolov1 mỗi grid cell sẽ dự đoán bounding box và số điểm tin cậy= giá trị prediction box so với ground-truth box. Vậy thêm anchor box cụ thể là gì và có tác dụng gì ạ ? Em vẫn chưa hiểu rõ anchor box nó là gì và có điểm mạnh gì hơn? Em newbie mong mọi người thông não.",,,,,
"Chào mọi người, hiện tại em đang tìm hiểu về thuật toán Local Outlier Factor trong bài toán Classification và có vài chỗ không hiểu nên mong mọi người giải đáp giúp em ạ.
https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html
Theo định nghĩa của sklearn: ""Unsupervised Outlier Detection using Local Outlier Factor ""
1. Cái ""Unsupervised"" ở đây em có thể hiểu là chỉ có thể fit hàm LocalOutlierFactor trên dữ liệu không có label hay thế nào ạ?
Ví dụ dataset của em có 2000 dòng (có label), em muốn kiểm tra xem trong 2000 dòng này có những dòng nào là outlier (cho contamination = 0.1, novelty=False) vậy thì em sẽ fit cái hàm LocalOutlierFactor trên dataset đã loại bỏ label hay trên toàn bộ dataset ban đầu (có label) vậy ạ?
2. Câu hỏi thứ hai là mình phải chia riêng ra hai tập train và test, sau đó fit hàm LocalOutlierFactor (novelty=False) trên mỗi tập train thôi, sau khi loại bỏ các outlier trên tập train mình dùng tập train này để sử dụng sau đó (chạy các model phân loại,... sau đó dùng model này để dự đoán trên tập test) hay là mình không cần chia riêng ra tập train và test vậy ạ?
3. Tham số 'novelty' nếu mình cho bằng False thì nó detect ra các outlier, cho bằng True thì nó detect ra các điểm mới (?) và chỉ sử dụng cho novelty = True trên mỗi tập test, em không hiểu là nếu cho novelty = True thì các điểm nó detect ra có ý nghĩa gì và các điểm này sẽ được dùng cho mục đích gì ạ?
4. Liên quan đến câu hỏi số (1), nếu trong 2000 dòng này mình bỏ ra 200 dòng mình cho là outlier, sau đó mình chỉ dùng 1800 dòng còn lại để sử dụng thì cách làm này có đúng không ạ?
Cảm ơn mọi người đã đọc và mong mọi người giải đáp giúp em ạ.","Chào mọi người, hiện tại em đang tìm hiểu về thuật toán Local Outlier Factor trong bài toán Classification và có vài chỗ không hiểu nên mong mọi người giải đáp giúp em ạ. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html Theo định nghĩa của sklearn: ""Unsupervised Outlier Detection using Local Outlier Factor "" 1. Cái ""Unsupervised"" ở đây em có thể hiểu là chỉ có thể fit hàm LocalOutlierFactor trên dữ liệu không có label hay thế nào ạ? Ví dụ dataset của em có 2000 dòng (có label), em muốn kiểm tra xem trong 2000 dòng này có những dòng nào là outlier (cho contamination = 0.1, novelty=False) vậy thì em sẽ fit cái hàm LocalOutlierFactor trên dataset đã loại bỏ label hay trên toàn bộ dataset ban đầu (có label) vậy ạ? 2. Câu hỏi thứ hai là mình phải chia riêng ra hai tập train và test, sau đó fit hàm LocalOutlierFactor (novelty=False) trên mỗi tập train thôi, sau khi loại bỏ các outlier trên tập train mình dùng tập train này để sử dụng sau đó (chạy các model phân loại,... sau đó dùng model này để dự đoán trên tập test) hay là mình không cần chia riêng ra tập train và test vậy ạ? 3. Tham số 'novelty' nếu mình cho bằng False thì nó detect ra các outlier, cho bằng True thì nó detect ra các điểm mới (?) và chỉ sử dụng cho novelty = True trên mỗi tập test, em không hiểu là nếu cho novelty = True thì các điểm nó detect ra có ý nghĩa gì và các điểm này sẽ được dùng cho mục đích gì ạ? 4. Liên quan đến câu hỏi số (1), nếu trong 2000 dòng này mình bỏ ra 200 dòng mình cho là outlier, sau đó mình chỉ dùng 1800 dòng còn lại để sử dụng thì cách làm này có đúng không ạ? Cảm ơn mọi người đã đọc và mong mọi người giải đáp giúp em ạ.",,,,,
"Mọi người ơi cho em hỏi vật to cỡ bao nhiêu thì detect train có hiệu quả nhỉ, em đọc có source bảo là ít nhất phải 20% thì mới recognition được [1]. Object của em toàn tầm này hhoặc nhỏ hơn cỡ 50x50px trong ảnh 1000x1000, thì có nên chia nhỏ ảnh ra nữa không .
Ai có kinh nghiệm preprocessing ảnh gợi ý cho em có nên equalize ánh sáng nữa không hay chuyển sang trắng đen để đạt hiểu quả hơn không nhỉ
https://openaccess.thecvf.com/content_CVPRW_2019/papers/UAVision/Unel_The_Power_of_Tiling_for_Small_Object_Detection_CVPRW_2019_paper.pdf","Mọi người ơi cho em hỏi vật to cỡ bao nhiêu thì detect train có hiệu quả nhỉ, em đọc có source bảo là ít nhất phải 20% thì mới recognition được [1]. Object của em toàn tầm này hhoặc nhỏ hơn cỡ 50x50px trong ảnh 1000x1000, thì có nên chia nhỏ ảnh ra nữa không . Ai có kinh nghiệm preprocessing ảnh gợi ý cho em có nên equalize ánh sáng nữa không hay chuyển sang trắng đen để đạt hiểu quả hơn không nhỉ https://openaccess.thecvf.com/content_CVPRW_2019/papers/UAVision/Unel_The_Power_of_Tiling_for_Small_Object_Detection_CVPRW_2019_paper.pdf",,,,,
"Hello mọi người, sắp tới em có dự định đi thực tập về lĩnh vực deep learning. Cho em hỏi các anh/chị đi trước là trong thực tế các doanh nghiệp hiện này họ dùng pretrained models nhiều hơn hay là custom models (tức models tự làm) nhiều hơn ạ?
p/s: bức ảnh mang tính chất vui nhộn :v
#DeepLearning","Hello mọi người, sắp tới em có dự định đi thực tập về lĩnh vực deep learning. Cho em hỏi các anh/chị đi trước là trong thực tế các doanh nghiệp hiện này họ dùng pretrained models nhiều hơn hay là custom models (tức models tự làm) nhiều hơn ạ? p/s: bức ảnh mang tính chất vui nhộn :v",#DeepLearning,,,,
"Em chào mọi người ạ, em muốn hỏi là có ai biết sử dụng mã hoá homomorphic trong hệ thống ML để bảo mật dữ liệu không ạ?
Em đã đọc thì thấy rằng nó có thể áp dụng vào các mô hình dự đoán, mô hình nhận diện khuôn mặt (mẫu và kết quả đều được mã hoá). Ngoài ra thì còn có thể áp dụng được các mô hình nào khác không ạ?
Em cảm ơn ạ!","Em chào mọi người ạ, em muốn hỏi là có ai biết sử dụng mã hoá homomorphic trong hệ thống ML để bảo mật dữ liệu không ạ? Em đã đọc thì thấy rằng nó có thể áp dụng vào các mô hình dự đoán, mô hình nhận diện khuôn mặt (mẫu và kết quả đều được mã hoá). Ngoài ra thì còn có thể áp dụng được các mô hình nào khác không ạ? Em cảm ơn ạ!",,,,,
"Chào mọi người, Em là người mới học về ML. Em cũng đã tham khảo qua list bài giảng của Andrew Ng và thấy cũng hiểu đc chút. Nhưng khi bước vào thực tế code thì thấy còn thiếu rất nhiều kinh nghiệm. Em có 2 câu hỏi :
Là người tự học nên rất mong mọi người cho ý kiến về lộ trình  học hay phương pháp học để tiếp cận với ngành này (chính).
Em đang muốn xây dựng model siamese, phần xử lý ảnh để tạo vector thì em đang muốn sử dụng model của người ta đã train rồi. em chỉ train các lớp khi so sánh sự khác biết giữa 2 bức ảnh và giữ nguyên weight của lớp xử lý ảnh. Em cũng nhặt ra được vài source code nhưng không biết sử dụng. Mong mọi người giải thích giúp em ý nghĩa các file trên và cách sử dụng.","Chào mọi người, Em là người mới học về ML. Em cũng đã tham khảo qua list bài giảng của Andrew Ng và thấy cũng hiểu đc chút. Nhưng khi bước vào thực tế code thì thấy còn thiếu rất nhiều kinh nghiệm. Em có 2 câu hỏi : Là người tự học nên rất mong mọi người cho ý kiến về lộ trình học hay phương pháp học để tiếp cận với ngành này (chính). Em đang muốn xây dựng model siamese, phần xử lý ảnh để tạo vector thì em đang muốn sử dụng model của người ta đã train rồi. em chỉ train các lớp khi so sánh sự khác biết giữa 2 bức ảnh và giữ nguyên weight của lớp xử lý ảnh. Em cũng nhặt ra được vài source code nhưng không biết sử dụng. Mong mọi người giải thích giúp em ý nghĩa các file trên và cách sử dụng.",,,,,
"One-way ANOVA cho feature selection.
Chào mọi người, e có một chút thắc mắc về việc chọn feature. Dataset của em gồm 10 nummerical features (có mean và std gần giống nhau, cụ thể là 0.5 và 0.38) và categorial target. ANOVA giả sử data là normal distribution. Sau khi thực hiện normalize features, e sử dụng ANOVA cho feature selection thì ra được tập features như e muốn. Tuy nhiên khi e tham khảo các example khác người ta làm, thì không thấy bước normalize data. Em muốn hỏi là
(1) Khi nhắc tới normalize data cho ANOVA, nghĩa là normalize trên features hay normalize trên target
(2) Có cần thiết phải normalize data trước khi dùng one way ANOVA cho feature selection không
Em xin cám ơn","One-way ANOVA cho feature selection. Chào mọi người, e có một chút thắc mắc về việc chọn feature. Dataset của em gồm 10 nummerical features (có mean và std gần giống nhau, cụ thể là 0.5 và 0.38) và categorial target. ANOVA giả sử data là normal distribution. Sau khi thực hiện normalize features, e sử dụng ANOVA cho feature selection thì ra được tập features như e muốn. Tuy nhiên khi e tham khảo các example khác người ta làm, thì không thấy bước normalize data. Em muốn hỏi là (1) Khi nhắc tới normalize data cho ANOVA, nghĩa là normalize trên features hay normalize trên target (2) Có cần thiết phải normalize data trước khi dùng one way ANOVA cho feature selection không Em xin cám ơn",,,,,
"Hi mọi người,
Em là thành viên mới tập tành về Machine Learning. Em có 1 câu hỏi liên quan đến ""learning rate"" trong thuật toán gradient descent nhờ mọi người giúp đỡ.
Theo em tìm hiểu được thì có nhiều cách thay đổi learning rate này để tìm được giá trị tối ưu nhất (như ở ảnh dưới). Em đang suy nghĩ liệu learning rate này có thể được học ""stochastic"" được không? Nghĩa là, em sẽ gán một phân phối cụ thể cho nó, đối với mỗi vòng lặp giá trị này sẽ thay đổi một cách ngẫu nhiên tuỳ thuộc vào cái phân phối của nó. Em đã tìm nhưng hiện tại cũng chưa ra research article nào về cái này ạ, em hiện tại đang muốn nghiên cứu thêm về thuật toán nhiều hơn. Mong mọi người chỉ giáo ạ. Many thanks!
https://en.wikipedia.org/wiki/Learning_rate","Hi mọi người, Em là thành viên mới tập tành về Machine Learning. Em có 1 câu hỏi liên quan đến ""learning rate"" trong thuật toán gradient descent nhờ mọi người giúp đỡ. Theo em tìm hiểu được thì có nhiều cách thay đổi learning rate này để tìm được giá trị tối ưu nhất (như ở ảnh dưới). Em đang suy nghĩ liệu learning rate này có thể được học ""stochastic"" được không? Nghĩa là, em sẽ gán một phân phối cụ thể cho nó, đối với mỗi vòng lặp giá trị này sẽ thay đổi một cách ngẫu nhiên tuỳ thuộc vào cái phân phối của nó. Em đã tìm nhưng hiện tại cũng chưa ra research article nào về cái này ạ, em hiện tại đang muốn nghiên cứu thêm về thuật toán nhiều hơn. Mong mọi người chỉ giáo ạ. Many thanks! https://en.wikipedia.org/wiki/Learning_rate",,,,,
"Chào các bạn.
Mình mới tìm hiểu về deep learning.
Mình có một câu hỏi là vs optimizer là GSD, thì mặc định batch size là 1, mỗi lần update weight và bias chỉ lựa chọn ngẫu nhiên 1 input đầu vào để tính.
Khi training với hàm fit, ko đặt batch size , thì mặc định batch size là 32.
Giả sử dlieu training là 120 thì sẽ tính toán thế nào để update weights và bias ạ? Batch size =32 sẽ ảnh hưởng tới việc training vs optimize là SGD như thế nào ạ?
Mình xin cảm ơn .","Chào các bạn. Mình mới tìm hiểu về deep learning. Mình có một câu hỏi là vs optimizer là GSD, thì mặc định batch size là 1, mỗi lần update weight và bias chỉ lựa chọn ngẫu nhiên 1 input đầu vào để tính. Khi training với hàm fit, ko đặt batch size , thì mặc định batch size là 32. Giả sử dlieu training là 120 thì sẽ tính toán thế nào để update weights và bias ạ? Batch size =32 sẽ ảnh hưởng tới việc training vs optimize là SGD như thế nào ạ? Mình xin cảm ơn .",,,,,
"Chào mọi người,
Trên trang Facebook của Amazon Web Services đang có chia sẻ của Suman Debnath, Principal Developer Advocate về Machine Learning, và Amazon SageMaker. Mọi người có thể tham dự buổi live và đặt câu hỏi trực tiếp cho diễn giả :)
https://www.facebook.com/watch/live/?v=827355334869097","Chào mọi người, Trên trang Facebook của Amazon Web Services đang có chia sẻ của Suman Debnath, Principal Developer Advocate về Machine Learning, và Amazon SageMaker. Mọi người có thể tham dự buổi live và đặt câu hỏi trực tiếp cho diễn giả :) https://www.facebook.com/watch/live/?v=827355334869097",,,,,
"[Nhờ cộng đồng chung tay share giúp]
Bạn hay người thân của bạn là F1 hay F0?
Mặc dù đang rất lo lắng cho sức khỏe bản thân hay người thân, nhưng nếu có thể, bạn hãy chung sức với gần 200 chuyên gia trí tuệ nhân tạo (AI) đang tham gia phát triển giải pháp “Nhận dạng Covid-19 qua tiếng ho""
Bằng cách ghi âm tiếng ho của bạn hay người thân bạn
Từ đầu tháng 6.2021, gần 200 chuyên gia trí tuệ nhân tạo (AI) đang miệt mài tham gia phát triển giải pháp “Nhận dạng Covid-19 qua tiếng ho"" do dự án cộng đồng AICovidVN tổ chức. Mục tiêu là xây dựng giải pháp AI, để các cơ quan chức năng sàng lọc nhanh Covid-19 trên diện rộng, thông qua hệ thống Callbot - gọi điện tự động cho hàng ngàn người một lúc và đề nghị ho ghi lại tiếng ho qua cuộc gọi. Đại học MIT (Mỹ) đã công bố giải pháp tương tự với độ chính xác lên đến 97% (chỉ số AUC), và đang tiến hành thủ tục xin FDA cấp phép. Cho tới nay, các đội tham gia cuộc thi tại Việt Nam đã đạt độ chính xác cao nhất là 90%, tuy nhiên vì các mẫu tiếng ho được sử dụng để huấn luyện AI còn ít, và đa số không phải dữ liệu của người Việt, nên các giải pháp có thể chưa sử dụng được ở Việt Nam.
Số ca nhiễm COVID-19 mới tại Việt Nam gần đây đã đạt mức kỷ lục hơn 700 ca mỗi ngày, và có nguy cơ bùng phát diện rộng ở một số tỉnh thành. Cả nước đang gồng hết sức để chống dịch, và một trong những hình ảnh cảm động thường thấy nhất là các y bác sĩ mặc đồ bảo hộ kín mít, giữa trời nắng 40 độ, để lấy mẫu xét nghiệm cho hàng ngàn người. Liệu có cách nào để các y bác sĩ đỡ vất vả hơn? Giải pháp này, nếu dù chỉ đạt được độ chính xác chưa cao, nhưng cũng có thể giúp ích được phần nào trong cuộc chiến chống dịch. Các nhóm đạt giải trong cuộc thi cam kết chuyển giao giải pháp cho Ban chỉ đạo quốc gia phòng chống Covid-19 để triển khai sàng lọc trên diện rộng. Nhóm tác giả có thể sử dụng giải pháp vào mục đích khác tùy nhu cầu.
Mặc dù đang rất lo lắng cho sức khỏe bản thân, nhưng nếu có thể, bạn hãy chung sức bằng cách sau:
1. Ngay bây giờ: Ghi âm tiếng ho của bạn
- Vào FB Fanpage của dự án: https://bit.ly/aicovidvn2
- Nhấn nút “Send Message”
- Nhấn nút (+) bên trái
- Nhấn nút ghi âm
- Ho 4 tiếng, chậm rãi, cách nhau 1 chút
2. Khi có kết quả xét nghiệm: Báo kết quả
- Cũng vào Fanpage trên
- Nhấn nút “Send Message”
- Nhắn tin: “Âm tính” hoặc “Dương tính”
Một số đường link khác:
FB Group của dự án cộng đồng AICovidVn: bit.ly/aicovidvn
 — với Lê Công Thành và 4 người khác.","[Nhờ cộng đồng chung tay share giúp] Bạn hay người thân của bạn là F1 hay F0? Mặc dù đang rất lo lắng cho sức khỏe bản thân hay người thân, nhưng nếu có thể, bạn hãy chung sức với gần 200 chuyên gia trí tuệ nhân tạo (AI) đang tham gia phát triển giải pháp “Nhận dạng Covid-19 qua tiếng ho"" Bằng cách ghi âm tiếng ho của bạn hay người thân bạn Từ đầu tháng 6.2021, gần 200 chuyên gia trí tuệ nhân tạo (AI) đang miệt mài tham gia phát triển giải pháp “Nhận dạng Covid-19 qua tiếng ho"" do dự án cộng đồng AICovidVN tổ chức. Mục tiêu là xây dựng giải pháp AI, để các cơ quan chức năng sàng lọc nhanh Covid-19 trên diện rộng, thông qua hệ thống Callbot - gọi điện tự động cho hàng ngàn người một lúc và đề nghị ho ghi lại tiếng ho qua cuộc gọi. Đại học MIT (Mỹ) đã công bố giải pháp tương tự với độ chính xác lên đến 97% (chỉ số AUC), và đang tiến hành thủ tục xin FDA cấp phép. Cho tới nay, các đội tham gia cuộc thi tại Việt Nam đã đạt độ chính xác cao nhất là 90%, tuy nhiên vì các mẫu tiếng ho được sử dụng để huấn luyện AI còn ít, và đa số không phải dữ liệu của người Việt, nên các giải pháp có thể chưa sử dụng được ở Việt Nam. Số ca nhiễm COVID-19 mới tại Việt Nam gần đây đã đạt mức kỷ lục hơn 700 ca mỗi ngày, và có nguy cơ bùng phát diện rộng ở một số tỉnh thành. Cả nước đang gồng hết sức để chống dịch, và một trong những hình ảnh cảm động thường thấy nhất là các y bác sĩ mặc đồ bảo hộ kín mít, giữa trời nắng 40 độ, để lấy mẫu xét nghiệm cho hàng ngàn người. Liệu có cách nào để các y bác sĩ đỡ vất vả hơn? Giải pháp này, nếu dù chỉ đạt được độ chính xác chưa cao, nhưng cũng có thể giúp ích được phần nào trong cuộc chiến chống dịch. Các nhóm đạt giải trong cuộc thi cam kết chuyển giao giải pháp cho Ban chỉ đạo quốc gia phòng chống Covid-19 để triển khai sàng lọc trên diện rộng. Nhóm tác giả có thể sử dụng giải pháp vào mục đích khác tùy nhu cầu. Mặc dù đang rất lo lắng cho sức khỏe bản thân, nhưng nếu có thể, bạn hãy chung sức bằng cách sau: 1. Ngay bây giờ: Ghi âm tiếng ho của bạn - Vào FB Fanpage của dự án: https://bit.ly/aicovidvn2 - Nhấn nút “Send Message” - Nhấn nút (+) bên trái - Nhấn nút ghi âm - Ho 4 tiếng, chậm rãi, cách nhau 1 chút 2. Khi có kết quả xét nghiệm: Báo kết quả - Cũng vào Fanpage trên - Nhấn nút “Send Message” - Nhắn tin: “Âm tính” hoặc “Dương tính” Một số đường link khác: FB Group của dự án cộng đồng AICovidVn: bit.ly/aicovidvn — với Lê Công Thành và 4 người khác.",,,,,
"Chào các anh chị ạ. Em là newbie machine learning ạ. Em mới học được chưa lâu mà hôm nay thầy em ra bài này, em không biết phải giải quyết thế nào ạ. Em mong các anh chị nhiều kinh nghiệm có thể help em hướng giải quyết với ạ. Tại vì em đọc các ví dụ, các dữ liệu training đều ở dạng số ạ. Còn bài thầy em ra thì rất nhiều dữ liệu không đơn thuần là số ( em không biết nó có ảnh hưởng gì không ạ), nên em không biết phải gán nhãn hay xử lý thế nào ạ.
Bài toán đặt ra là với tập dữ liệu (giống trong hình em ạ), hãy đưa ra dự báo thời tiết ngày mai ạ. Em cảm ơn ạ. Có cách nào đơn giản nhất và dễ hiệu nhất càng tốt ạ.","Chào các anh chị ạ. Em là newbie machine learning ạ. Em mới học được chưa lâu mà hôm nay thầy em ra bài này, em không biết phải giải quyết thế nào ạ. Em mong các anh chị nhiều kinh nghiệm có thể help em hướng giải quyết với ạ. Tại vì em đọc các ví dụ, các dữ liệu training đều ở dạng số ạ. Còn bài thầy em ra thì rất nhiều dữ liệu không đơn thuần là số ( em không biết nó có ảnh hưởng gì không ạ), nên em không biết phải gán nhãn hay xử lý thế nào ạ. Bài toán đặt ra là với tập dữ liệu (giống trong hình em ạ), hãy đưa ra dự báo thời tiết ngày mai ạ. Em cảm ơn ạ. Có cách nào đơn giản nhất và dễ hiệu nhất càng tốt ạ.",,,,,
"[Deep Learning Project]
Mình chia sẻ một số project của các bạn học sinh khóa Deep Learning.
1. Nhận diện việc đeo khẩu trang nhằm phóng chống covid.
2. Trích xuất thông tin trận đấu bóng đá như: tên hai đội, tỉ số, cầu
thủ ghi bàn,...
3. Hệ thống chấm thi trắc nghiệm tự động.
4. Khoanh vùng khí quản và nhánh phế quản chính trong ảnh CT.","[Deep Learning Project] Mình chia sẻ một số project của các bạn học sinh khóa Deep Learning. 1. Nhận diện việc đeo khẩu trang nhằm phóng chống covid. 2. Trích xuất thông tin trận đấu bóng đá như: tên hai đội, tỉ số, cầu thủ ghi bàn,... 3. Hệ thống chấm thi trắc nghiệm tự động. 4. Khoanh vùng khí quản và nhánh phế quản chính trong ảnh CT.",,,,,
"Mọi người cho mình hỏi là activation có 'linear' ạ, sao mình vào activation của Keras thì lại không thấy gì cả?
https://machinelearningcoban.com/2018/07/06/deeplearning/","Mọi người cho mình hỏi là activation có 'linear' ạ, sao mình vào activation của Keras thì lại không thấy gì cả? https://machinelearningcoban.com/2018/07/06/deeplearning/",,,,,
"[Chia sẻ] 5 nguồn dataset hữu ích cho các dự án Machine Learning.
Thấy có 2 cái dự án về Y tế là ADNI với AICovid khá thú vị. Hy vọng được cập nhật thêm về tình hình 2 dự án này.
#machinelearning #datascience #datasets #ADNI #VSAB #aicovid #Spotify #BigFivetest",[Chia sẻ] 5 nguồn dataset hữu ích cho các dự án Machine Learning. Thấy có 2 cái dự án về Y tế là ADNI với AICovid khá thú vị. Hy vọng được cập nhật thêm về tình hình 2 dự án này.,#machinelearning	#datascience	#datasets	#ADNI	#VSAB	#aicovid	#Spotify	#BigFivetest,,,,
"#quyhoachtuyentinh #LinearProgramming

Hi các a chị. E đang tìm hiểu về một số vấn đề liên quan đến tối ưu tuyến tính, cụ thể ở đây là quy hoạch tuyến tính, để tiện hơn cho việc thử nghiệm và học tập, em muốn xin gợi ý một số thư viện tối ưu tuyến tính để chạy các ví dụ một cách trực quan, với các tham số để giải bài toán dưới đây (như hình vẽ) gồm: 
+ Hàm mục tiêu ( tuyến tính ) 
+ Ràng buộc lỏng ( < hoặc = ) 
+ Ràng buộc chặt ( = )
+ Tập điều kiện chặn của biến 

Hiện tại thì em có sử dụng thư viện linprog của  scipy.optimize  để chạy ví dụ, nhưng tầm 70 biến trở lên là không hoạt động như ý, vì vậy, e muốn hỏi các a chị có kinh nghiệm về cái này: Có thư viện nào thay thế thư viện trên không, với cả nhanh 1 chút để chạy được tầm 200+ biến trở lên ạ. 
E xin cảm ơn.  ","Hi các a chị. E đang tìm hiểu về một số vấn đề liên quan đến tối ưu tuyến tính, cụ thể ở đây là quy hoạch tuyến tính, để tiện hơn cho việc thử nghiệm và học tập, em muốn xin gợi ý một số thư viện tối ưu tuyến tính để chạy các ví dụ một cách trực quan, với các tham số để giải bài toán dưới đây (như hình vẽ) gồm: + Hàm mục tiêu ( tuyến tính ) + Ràng buộc lỏng ( < hoặc = ) + Ràng buộc chặt ( = ) + Tập điều kiện chặn của biến Hiện tại thì em có sử dụng thư viện linprog của scipy.optimize để chạy ví dụ, nhưng tầm 70 biến trở lên là không hoạt động như ý, vì vậy, e muốn hỏi các a chị có kinh nghiệm về cái này: Có thư viện nào thay thế thư viện trên không, với cả nhanh 1 chút để chạy được tầm 200+ biến trở lên ạ. E xin cảm ơn.",#quyhoachtuyentinh	#LinearProgramming,"#math, #Q&A",,,
Mọi người cho mình hỏi hai từ precision và recall thì thuật ngữ tiếng Việt là gì nhỉ?,Mọi người cho mình hỏi hai từ precision và recall thì thuật ngữ tiếng Việt là gì nhỉ?,,,,,
Hy vọng bài viết về kết nối tắt sẽ có ích cho các bạn sinh viên đang làm quen với học sâu,Hy vọng bài viết về kết nối tắt sẽ có ích cho các bạn sinh viên đang làm quen với học sâu,,,,,
https://youtu.be/Boj9eD0Wug8,https://youtu.be/Boj9eD0Wug8,,,,,
"Chào mọi người,
Em đang train model nhận diện ảnh chó mèo, em có đọc một vài notebook trên kaggle thì người ta có sử dụng thêm callback. Cụ thể như giảm learning rate, rồi earlystop.... Em thì bỏ phần callback đi và em cũng có generator data thêm thì được hình như ở dưới. Em thấy hình như ở dưới thì như epoch đầu hơi bị lên xuống quá nhiều về sau thì có đỡ hơn chút. Vậy thì training model như vậy có phải là quá xấu hay xấu gì không ạ. Mình cần cải thiện gì ngoài callback... (model em cũng đã dùng drop out các thứ, em chỉ dùng thôi chứ chưa hiểu phải chỉnh sửa ra sao). Mong mọi người góp ý giúp em ạ.
Em xin chân thành cảm ơn.","Chào mọi người, Em đang train model nhận diện ảnh chó mèo, em có đọc một vài notebook trên kaggle thì người ta có sử dụng thêm callback. Cụ thể như giảm learning rate, rồi earlystop.... Em thì bỏ phần callback đi và em cũng có generator data thêm thì được hình như ở dưới. Em thấy hình như ở dưới thì như epoch đầu hơi bị lên xuống quá nhiều về sau thì có đỡ hơn chút. Vậy thì training model như vậy có phải là quá xấu hay xấu gì không ạ. Mình cần cải thiện gì ngoài callback... (model em cũng đã dùng drop out các thứ, em chỉ dùng thôi chứ chưa hiểu phải chỉnh sửa ra sao). Mong mọi người góp ý giúp em ạ. Em xin chân thành cảm ơn.",,,,,
"[YOLOv3-Deep learning-Computer vision] Chào mọi người, Mình xin giới thiệu một bộ công cụ của Intel, cho phép chúng ta triển khai nhanh các ứng dụng hoặc xử lí các vấn đề liên quan tới thị giác máy tính(Computer vision) với hiệu suất cao trên các phần cứng của Intel như CPU, GPU, VPUs, FPGA.... Sử dụng nó chúng ta thể chạy mô hình Object detection với đầu vào là Video trên CPU và cho kết quả khá tốt.
Chi tiết tại: https://ptitdeveloper.com/blog/openvino-toi-uu-hoa-hieu-suat-model-darknet-yolov3/","[YOLOv3-Deep learning-Computer vision] Chào mọi người, Mình xin giới thiệu một bộ công cụ của Intel, cho phép chúng ta triển khai nhanh các ứng dụng hoặc xử lí các vấn đề liên quan tới thị giác máy tính(Computer vision) với hiệu suất cao trên các phần cứng của Intel như CPU, GPU, VPUs, FPGA.... Sử dụng nó chúng ta thể chạy mô hình Object detection với đầu vào là Video trên CPU và cho kết quả khá tốt. Chi tiết tại: https://ptitdeveloper.com/blog/openvino-toi-uu-hoa-hieu-suat-model-darknet-yolov3/",,,,,
"Chào mọi người,
Em mới tập tành học về ML và DL, hiện tại em đang làm bài toán phân loại ảnh với Cifar100, em có follow theo một bài hướng dẫn trên mạng có yêu cầu custom data generator class (cụ thể như ở dưới hình ạ). Vài tháng trước em chạy thì không hiện thông báo lỗi gì, đến hôm nay chạy lại khi fit vào model thì nó lại có lỗi như trong hình ạ :(((
Mọi người ai biết cách fix có thể giúp em với ạ. Em cảm ơn mọi người nhiều ạ.","Chào mọi người, Em mới tập tành học về ML và DL, hiện tại em đang làm bài toán phân loại ảnh với Cifar100, em có follow theo một bài hướng dẫn trên mạng có yêu cầu custom data generator class (cụ thể như ở dưới hình ạ). Vài tháng trước em chạy thì không hiện thông báo lỗi gì, đến hôm nay chạy lại khi fit vào model thì nó lại có lỗi như trong hình ạ :((( Mọi người ai biết cách fix có thể giúp em với ạ. Em cảm ơn mọi người nhiều ạ.",,,,,
"[HELP] Làm sao chuyển tiếng Việt mã unicode sang utf-8 trên python3?
Hiện tại em đang gặp khó khăn khi muốn chuyển đổi từ unicode sang utf-8, và em cũng hơi thắc mắc là từ unicode chuyển sang utf-8 thì có đến 2 dạng, em xin mô tả bài toán của em:
Input: một chuỗi (ví dụ: xin chào mọi người)
Output: chuỗi dạng utf-8 (xin chÃ o má»�i ngÆ°á»�i)
Hiện tại em đã thử hàm encode nhưng kết quả nhận được là dưới dạng utf-8 mà em muốn. Kết quả em ra từ ví dụ trên là: \x78\x69\x6e \x63\x68\xc3\xa0\x6f \x6d\xe1\xbb\x8d\x69 \x6e\x67\xc6\xb0\xe1\xbb\x9d\x69
Hy vọng được mọi anh chị, và mọi người giúp đỡ ạ, em cảm ơn mọi người rất nhiều.","[HELP] Làm sao chuyển tiếng Việt mã unicode sang utf-8 trên python3? Hiện tại em đang gặp khó khăn khi muốn chuyển đổi từ unicode sang utf-8, và em cũng hơi thắc mắc là từ unicode chuyển sang utf-8 thì có đến 2 dạng, em xin mô tả bài toán của em: Input: một chuỗi (ví dụ: xin chào mọi người) Output: chuỗi dạng utf-8 (xin chÃ o má»i ngÆ°á»i) Hiện tại em đã thử hàm encode nhưng kết quả nhận được là dưới dạng utf-8 mà em muốn. Kết quả em ra từ ví dụ trên là: \x78\x69\x6e \x63\x68\xc3\xa0\x6f \x6d\xe1\xbb\x8d\x69 \x6e\x67\xc6\xb0\xe1\xbb\x9d\x69 Hy vọng được mọi anh chị, và mọi người giúp đỡ ạ, em cảm ơn mọi người rất nhiều.",,,,,
"Chào mọi người ạ
E đang làm 1 phần về tính toán thời gian vật xuất hiện sau khi detect+ track bằng Yolo. Em đã detect được và có ID của từng object. E test thử với 1 file video. Video detect của e được khoảng 1.5 FPS.
E tính thời gian bằng cách xét theo từng frame, nếu vật xuất hiện thì e sẽ dùng time_now() để thu được thời gian lần đầu vật xuất hiện.
Tuy nhiên, có 1 vấn đề như sau ạ:
E giả sử 1 vật xuất hiện ở frame 1. E dùng hàm time_now để bắt được thời gian nó xuất hiện lần đầu (time_start) . Giả sử nó xuất hiện đến frame 70, thì e tính thời gian tồn tại bằng time_now() - thời gian nó bắt đầu xuất hiên ( time_now - time_start ). Và e thu được kết quả nó tồn tại là 80s ( do FPS detect của e khoảng 1.5 FPS, vật xuất hiện từ frame 0 đến frame 70), e thấy khá hợp lý
Nhưng thực tế là trong video real time thì vật đó chỉ tồn tại 3-4 s
Vậy có cách nào để bắt được đúng thời gian vật xuất hiện trong file video ban đầu ko ạ. E bị vướng đoạn này mà chưa tìm được solution. Rất mong mọi người giải đáp.
Em cảm ơn mọi người nhiều","Chào mọi người ạ E đang làm 1 phần về tính toán thời gian vật xuất hiện sau khi detect+ track bằng Yolo. Em đã detect được và có ID của từng object. E test thử với 1 file video. Video detect của e được khoảng 1.5 FPS. E tính thời gian bằng cách xét theo từng frame, nếu vật xuất hiện thì e sẽ dùng time_now() để thu được thời gian lần đầu vật xuất hiện. Tuy nhiên, có 1 vấn đề như sau ạ: E giả sử 1 vật xuất hiện ở frame 1. E dùng hàm time_now để bắt được thời gian nó xuất hiện lần đầu (time_start) . Giả sử nó xuất hiện đến frame 70, thì e tính thời gian tồn tại bằng time_now() - thời gian nó bắt đầu xuất hiên ( time_now - time_start ). Và e thu được kết quả nó tồn tại là 80s ( do FPS detect của e khoảng 1.5 FPS, vật xuất hiện từ frame 0 đến frame 70), e thấy khá hợp lý Nhưng thực tế là trong video real time thì vật đó chỉ tồn tại 3-4 s Vậy có cách nào để bắt được đúng thời gian vật xuất hiện trong file video ban đầu ko ạ. E bị vướng đoạn này mà chưa tìm được solution. Rất mong mọi người giải đáp. Em cảm ơn mọi người nhiều",,,,,
"Anh chị nào có tài liệu thuật ngữ chuyên ngành về AI, NLP, PhoBert, Pretrain… Tiếng anh — > Việt cho mình xin nhé. Đa tạ.","Anh chị nào có tài liệu thuật ngữ chuyên ngành về AI, NLP, PhoBert, Pretrain… Tiếng anh — > Việt cho mình xin nhé. Đa tạ.",,,,,
"Em chào anh/chị ạ,
Em cũng là một sinh viên mới ra trường được 2 năm và đi làm về dữ liệu (chủ yếu là làm data science, phân tích dữ liệu và xây dựng mô hình máy học). Nhưng em vẫn còn băn khoăn về câu hỏi định hướng của bản thân trong 5 năm tới, 10 năm tới đối với công việc hiện tại như thế nào?
Em cũng có tự trả lời cho bản thân trước:
Dành 2 năm tiếp theo để làm thật nhiều phân tích dữ liệu, phát triển các mô hình máy học để thật sự giỏi về ngành này
Ở 2 năm tiếp theo, em mong muốn được lên làm team lead để có thể triển khai nhiều dự án, và đồng thời học cách làm việc với con người, tương tác với các thành viên trong team để đạt mục tiêu chung của sản phẩm
Ở 3 năm tiếp theo nữa, em mong muốn được làm PM về data (có thuật ngữ gọi là Data Guru), mục đích của em là được làm việc với các phòng ban khác như marketing, finance, IT, UI/UX để học cách xây dựng sản phẩm cho người dùng
Và ở 4 năm tiếp theo, em sẽ học cách tự phát triển ứng dụng phục vụ các nhu cầu về xử lý dữ liệu, xây dựng mô hình máy học để phục vụ nhu cầu của các doanh nghiệp.
Không biết anh/chị nào đã có kinh nghiệm trong ngành có thể góp ý con đường em đi có khả thi hay không? Và không biết anh chị đã đi một con đường như thế nào khi làm về dữ liệu, có thể chia sẻ với em để em học thêm được không ạ?
Em cảm ơn anh/chị rất nhiều, chúc mọi người giữ sức khỏe thật tốt trong thời gian chống dịch ạ","Em chào anh/chị ạ, Em cũng là một sinh viên mới ra trường được 2 năm và đi làm về dữ liệu (chủ yếu là làm data science, phân tích dữ liệu và xây dựng mô hình máy học). Nhưng em vẫn còn băn khoăn về câu hỏi định hướng của bản thân trong 5 năm tới, 10 năm tới đối với công việc hiện tại như thế nào? Em cũng có tự trả lời cho bản thân trước: Dành 2 năm tiếp theo để làm thật nhiều phân tích dữ liệu, phát triển các mô hình máy học để thật sự giỏi về ngành này Ở 2 năm tiếp theo, em mong muốn được lên làm team lead để có thể triển khai nhiều dự án, và đồng thời học cách làm việc với con người, tương tác với các thành viên trong team để đạt mục tiêu chung của sản phẩm Ở 3 năm tiếp theo nữa, em mong muốn được làm PM về data (có thuật ngữ gọi là Data Guru), mục đích của em là được làm việc với các phòng ban khác như marketing, finance, IT, UI/UX để học cách xây dựng sản phẩm cho người dùng Và ở 4 năm tiếp theo, em sẽ học cách tự phát triển ứng dụng phục vụ các nhu cầu về xử lý dữ liệu, xây dựng mô hình máy học để phục vụ nhu cầu của các doanh nghiệp. Không biết anh/chị nào đã có kinh nghiệm trong ngành có thể góp ý con đường em đi có khả thi hay không? Và không biết anh chị đã đi một con đường như thế nào khi làm về dữ liệu, có thể chia sẻ với em để em học thêm được không ạ? Em cảm ơn anh/chị rất nhiều, chúc mọi người giữ sức khỏe thật tốt trong thời gian chống dịch ạ",,,,,
Xin phéo chia sẻ cùng mọi người hội thảo về xây dựng mô hình AI trong xử lý ảnh y tế và xử lý bộ dữ liệu quy mô lớn,Xin phéo chia sẻ cùng mọi người hội thảo về xây dựng mô hình AI trong xử lý ảnh y tế và xử lý bộ dữ liệu quy mô lớn,,,,,
"Chia sẻ với các bạn kết quả mới của HANET Ai Camera trong tháng 2/2021, nhận diện cùng lúc 3-5 người đeo khẩu trang, đi cùng nhau. Đeo kính đen, và đăng ký hình ảnh khách hàng, nhân viên ngay trên ứng dụng HANET Connect.
Tỉ lệ nhận diện chính xác 99.9%, cam kết 1.000 người không nhận sai 1 người, ( Nhận sai từ người A sang người B )
Nhận diện khuôn mặt ngay trên camera dựa vào NPU tích hợp trong camera và lưu được 50.000 khuôn mặt.
Welcome anh em kết nối API để phát triển giải pháp của riêng mình
API document: https://developers.hanet.ai/document
Tham khảo thêm về sản phẩm: https://hanet.com/hanet-ai-camera-/","Chia sẻ với các bạn kết quả mới của HANET Ai Camera trong tháng 2/2021, nhận diện cùng lúc 3-5 người đeo khẩu trang, đi cùng nhau. Đeo kính đen, và đăng ký hình ảnh khách hàng, nhân viên ngay trên ứng dụng HANET Connect. Tỉ lệ nhận diện chính xác 99.9%, cam kết 1.000 người không nhận sai 1 người, ( Nhận sai từ người A sang người B ) Nhận diện khuôn mặt ngay trên camera dựa vào NPU tích hợp trong camera và lưu được 50.000 khuôn mặt. Welcome anh em kết nối API để phát triển giải pháp của riêng mình API document: https://developers.hanet.ai/document Tham khảo thêm về sản phẩm: https://hanet.com/hanet-ai-camera-/",,,,,
"Chào mọi người ạ, Em mới tập tành học ML chút, xong có deadline thế này mà em cần vẽ biểu đồ học tập của chương trình,Hầu như em thấy mn chạy bằng chương trình spyder trên window thì dễ để bôi đen code rồi chạy, nên mỗi lần chạy biểu đồ ko phải training lại. Em chạy vscode nó cứ chạy từ trên xuống dưói thôi, bác nào có cách cho vscode nó chạy ra biểu đồ học tập keras giúp em với được không ạ. :( em dân network qua ạ. Em cảm ơn ạ.","Chào mọi người ạ, Em mới tập tành học ML chút, xong có deadline thế này mà em cần vẽ biểu đồ học tập của chương trình,Hầu như em thấy mn chạy bằng chương trình spyder trên window thì dễ để bôi đen code rồi chạy, nên mỗi lần chạy biểu đồ ko phải training lại. Em chạy vscode nó cứ chạy từ trên xuống dưói thôi, bác nào có cách cho vscode nó chạy ra biểu đồ học tập keras giúp em với được không ạ. :( em dân network qua ạ. Em cảm ơn ạ.",,,,,
"[Nhờ cộng đồng cùng chung tay share]
CUỘC THI SÁNG TẠO GIẢI PHÁP TRÍ TUỆ NHÂN TẠO (AI)
NHẬN DẠNG COVID QUA TIẾNG HO
GIẢI THƯỞNG 115 TRIỆU ĐỒNG
Tháng 6/2021, số ca nhiễm COVID-19 mới tại Việt Nam đạt mức kỷ lục 400-500 ca mỗi ngày, và có nguy cơ bùng phát diện rộng ở một số tỉnh thành. Cả nước đang gồng hết sức để chống dịch, và một trong những hình ảnh thường thấy nhất là các y bác sĩ mặc đồ bảo hộ kín mít, giữa trời nắng 40 độ, để lấy mẫu xét nghiệm cho hàng ngàn người. Liệu có cách nào để thực hiện việc này hiệu quả hơn? Các nhà nghiên cứu trên thế giới đã chỉ ra rằng có thể phát hiện COVID-19 qua tiếng ho, qua điện thoại di động bằng trí tuệ nhân tạo (AI) với độ chính xác trên 90%.
Hiện nay TPHCM đã triển khai hệ thống Callbot, qua đó AI tự động gọi điện cho hàng triệu người dân, và đặt câu hỏi cho họ để họ tự khai báo triệu chứng COVID-19. Điều này có ý nghĩa rất lớn trong việc sàng lọc nhanh những người cần xét nghiệm kỹ hơn, và khoanh vùng các ổ dịch kịp thời. Tuy nhiên hệ thống hiện nay vẫn cần người dân tự khai báo triệu chứng, trong khi nhiều người mắc COVID-19 nhưng chưa có biểu hiện bệnh. Để tiếp tục nâng cao hiệu quả, dự án cộng đồng AICovidVN tổ chức cuộc thi “AICV-115M Challenge” kêu gọi cộng đồng tham gia xây dựng các giải pháp AI để thu âm tiếng ho của người dân qua tổng đài điện thoại, rồi đưa ra chẩn đoán COVID-19 sơ bộ. Tổng giải thưởng cuộc thi trị giá 115 triệu VND do một nhóm startup founders đóng góp.
Các nhóm đạt giải cam kết chuyển giao giải pháp cho Ban chỉ đạo quốc gia phòng chống Covid-19 để triển khai sàng lọc trên diện rộng. Nhóm tác giả có thể sử dụng giải pháp vào mục đích khác tùy nhu cầu.
Đăng ký: www.Covid.AIHub.vn
FB Group: bit.ly/aicovidvn","[Nhờ cộng đồng cùng chung tay share] CUỘC THI SÁNG TẠO GIẢI PHÁP TRÍ TUỆ NHÂN TẠO (AI) NHẬN DẠNG COVID QUA TIẾNG HO GIẢI THƯỞNG 115 TRIỆU ĐỒNG Tháng 6/2021, số ca nhiễm COVID-19 mới tại Việt Nam đạt mức kỷ lục 400-500 ca mỗi ngày, và có nguy cơ bùng phát diện rộng ở một số tỉnh thành. Cả nước đang gồng hết sức để chống dịch, và một trong những hình ảnh thường thấy nhất là các y bác sĩ mặc đồ bảo hộ kín mít, giữa trời nắng 40 độ, để lấy mẫu xét nghiệm cho hàng ngàn người. Liệu có cách nào để thực hiện việc này hiệu quả hơn? Các nhà nghiên cứu trên thế giới đã chỉ ra rằng có thể phát hiện COVID-19 qua tiếng ho, qua điện thoại di động bằng trí tuệ nhân tạo (AI) với độ chính xác trên 90%. Hiện nay TPHCM đã triển khai hệ thống Callbot, qua đó AI tự động gọi điện cho hàng triệu người dân, và đặt câu hỏi cho họ để họ tự khai báo triệu chứng COVID-19. Điều này có ý nghĩa rất lớn trong việc sàng lọc nhanh những người cần xét nghiệm kỹ hơn, và khoanh vùng các ổ dịch kịp thời. Tuy nhiên hệ thống hiện nay vẫn cần người dân tự khai báo triệu chứng, trong khi nhiều người mắc COVID-19 nhưng chưa có biểu hiện bệnh. Để tiếp tục nâng cao hiệu quả, dự án cộng đồng AICovidVN tổ chức cuộc thi “AICV-115M Challenge” kêu gọi cộng đồng tham gia xây dựng các giải pháp AI để thu âm tiếng ho của người dân qua tổng đài điện thoại, rồi đưa ra chẩn đoán COVID-19 sơ bộ. Tổng giải thưởng cuộc thi trị giá 115 triệu VND do một nhóm startup founders đóng góp. Các nhóm đạt giải cam kết chuyển giao giải pháp cho Ban chỉ đạo quốc gia phòng chống Covid-19 để triển khai sàng lọc trên diện rộng. Nhóm tác giả có thể sử dụng giải pháp vào mục đích khác tùy nhu cầu. Đăng ký: www.Covid.AIHub.vn FB Group: bit.ly/aicovidvn",,,,,
"Trong group có ai làm với thư viện pyrender trên môi trường windows mà gặp lỗi này chưa ạ? Google cái lỗi mà không ra.
Mình đang chạy code dùng pyrender để visualize 3d model. Thì lúc import thư viện đã bị lỗi rồi.
import pyrender",Trong group có ai làm với thư viện pyrender trên môi trường windows mà gặp lỗi này chưa ạ? Google cái lỗi mà không ra. Mình đang chạy code dùng pyrender để visualize 3d model. Thì lúc import thư viện đã bị lỗi rồi. import pyrender,,,,,
"Chào buổi sáng tất cả mọi người
Em đang thực hiện bài toán phân loại 2 mã sản phẩm.
Sau khi em detect vùng đặc trưng của 2 sản phẩm đó. Thì đã in ra kết quả từng bounding box ứng với mỗi items. Thì có tổng cộng là 54 items. Và show lên màn hình cũng 54 bounding box.
Bước tiếp theo, em in boxes ra thì e thấy nó lớn hơn 54. Nên khi áp dụng thuật toán Hough Circles vào thì lúc vẽ đường tròn lên thì có những items nó bị lặp lại.
Em đã thử set lại ngưỡng của NMS mà nó vẫn không hạn chế được bao nhiêu.
Mọi người trong nhóm ai đã từng làm những bài toán tương tự cũng như có hướng giải quyết có thể hướng dẫn giúp em với được không ạ?
Hình ảnh minh họa em show ở dưới cmt nha mọi người
Em xin cám ơn mọi người!!!","Chào buổi sáng tất cả mọi người Em đang thực hiện bài toán phân loại 2 mã sản phẩm. Sau khi em detect vùng đặc trưng của 2 sản phẩm đó. Thì đã in ra kết quả từng bounding box ứng với mỗi items. Thì có tổng cộng là 54 items. Và show lên màn hình cũng 54 bounding box. Bước tiếp theo, em in boxes ra thì e thấy nó lớn hơn 54. Nên khi áp dụng thuật toán Hough Circles vào thì lúc vẽ đường tròn lên thì có những items nó bị lặp lại. Em đã thử set lại ngưỡng của NMS mà nó vẫn không hạn chế được bao nhiêu. Mọi người trong nhóm ai đã từng làm những bài toán tương tự cũng như có hướng giải quyết có thể hướng dẫn giúp em với được không ạ? Hình ảnh minh họa em show ở dưới cmt nha mọi người Em xin cám ơn mọi người!!!",,,,,
"Em là sinh viên và đang học môn cơ sở AI.
Anh/chị có thể cho em hỏi đoạn code dưới thì 2 lớp ẩn sử dụng hàm kích hoạt gì được không ạ và mô hình này có sử dụng kỹ thuật tối ưu hóa không ạ?
A/c nào biết có thể chỉ cho em với ạ.",Em là sinh viên và đang học môn cơ sở AI. Anh/chị có thể cho em hỏi đoạn code dưới thì 2 lớp ẩn sử dụng hàm kích hoạt gì được không ạ và mô hình này có sử dụng kỹ thuật tối ưu hóa không ạ? A/c nào biết có thể chỉ cho em với ạ.,,,,,
"Các bạn vui lòng đăng tin tuyển dụng, sự kiện, tuyển sinh tháng 11 dưới comment của post này.
Chúc các bạn một tháng vui vẻ.","Các bạn vui lòng đăng tin tuyển dụng, sự kiện, tuyển sinh tháng 11 dưới comment của post này. Chúc các bạn một tháng vui vẻ.",,,,,
"Dạ xin chào mọi người, e tv mới vô nhóm nhờ mn giúp bài tập này hoặc có tài liệu tham khảo cho e xin với ạ, cám ơn ad và mọi người nhiều","Dạ xin chào mọi người, e tv mới vô nhóm nhờ mn giúp bài tập này hoặc có tài liệu tham khảo cho e xin với ạ, cám ơn ad và mọi người nhiều",,,,,
Em có thắc mắc là với chứng chỉ Tensorflow thì đã đủ khả năng đi làm chưa ạ. Mong các bác giải đáp giúp em,Em có thắc mắc là với chứng chỉ Tensorflow thì đã đủ khả năng đi làm chưa ạ. Mong các bác giải đáp giúp em,,,,,
"Xin phép chia sẻ cùng mọi người workshop về AI trong lĩnh vực Xử lý ảnh y tế. Hy vọng chương trình sẽ mang đến nhiều kiến thức hữu ích dành cho các bạn quan tâm đến học máy và đang tìm kiếm các nguồn dữ liệu mở, công cụ phục vụ việc đào tạo mô hình.
-------------
📢 WORKSHOP “AI TRONG XỬ LÝ ẢNH Y TẾ - KINH NGHIỆM PHÁT TRIỂN THUẬT TOÁN & XÂY DỰNG CÁC BỘ DỮ LIỆU CHUẨN HÓA”
Trong vòng 3 - 5 năm trở lại đây, các thuật toán trí tuệ nhân tạo đã mang đến những tiến bộ đột phá trong phân tích và xử lý ảnh y tế, góp phần đắc lực giúp các bác sĩ đưa ra quyết định nhanh chóng và chính xác, chẩn đoán sớm các bất thường, hướng tới sàng lọc trên diện rộng một số bệnh lý nguy hiểm. Tuy nhiên, cùng với cơ hội, ứng dụng AI trong xử lý hình ảnh y tế cũng đang phải đối diện với không ít thách thức, chẳng hạn như:
Thiếu các bộ dữ liệu chuẩn hóa, có dán nhãn bởi bác sĩ chẩn đoán hình ảnh.
Khả năng diễn giải của các mô hình AI còn hạn chế
Vậy làm thế nào để phát triển và tối ưu hóa mô hình AI, dựa trên các bộ dữ liệu quy mô lớn và đạt chuẩn?
Câu trả lời sẽ có trong workshop “AI trong xử lý ảnh y tế - Kinh nghiệm phát triển thuật toán và xây dựng các bộ dữ liệu chuẩn hóa”. Tại đây, các chuyên gia, kỹ sư công nghệ của Trung tâm Xử lý ảnh y tế, VinBigdata, những người trực tiếp tham gia phát triển sản phẩm VinDr, sẽ lần lượt trình bày:
1️⃣ Quy trình tổng quan về phát triển AI trong Xử lý ảnh y tế - TS. Phạm Huy Hiệu, Chuyên gia nghiên cứu Thị giác máy tính, Trung tâm Xử lý ảnh y tế, VinBigdata.
TS. Hiệu đã nhận bằng tiến sĩ tại Viện Đại học Toulouse, Pháp; là tác giả của hơn 20 bài báo khoa học trên các tạp chí và hội nghị quốc tế (Citations = 251; H-index = 8 ), trong đó có các tạp chí và hội nghị lớn như Computer Vision and Image Understanding (CVIU), Neurocomputing, MICCAI, MIDL.
2️⃣ Kinh nghiệm phát triển thuật toán & Xử lý dữ liệu - Kỹ sư AI Nguyễn Bá Dũng, Trưởng nhóm Phân tích ảnh y tế, Trung tâm Xử lý ảnh y tế, VinBigdata; Kaggle Competitions Grandmaster.
3️⃣ Hệ thống VinDr Lab và Các bộ dữ liệu chuẩn hóa chia sẻ cho cộng đồng - Kỹ sư AI Nguyễn Trung Nghĩa. Huy chương Bạc các cuộc thi Phát hiện xuất huyết nội sọ được tổ chức bởi Hiệp hội X‑quang Bắc Mỹ - RSNA, Phân đoạn khí màng phổi SIIM-ACR...
📍 Nếu đang quan tâm đến việc phát triển các mô hình học máy, đặc biệt trong lĩnh vực xử lý ảnh y tế, đồng thời, tìm kiếm các nguồn dữ liệu và công cụ phục vụ việc tối ưu hóa thuật toán, thì bạn đừng bỏ lỡ workshop tới đây của VinBigdata:
Thời gian: từ 15h00 – 17h00 ngày 25/6/2021
Workshop được livestream trên Fanpage VinBigdata","Xin phép chia sẻ cùng mọi người workshop về AI trong lĩnh vực Xử lý ảnh y tế. Hy vọng chương trình sẽ mang đến nhiều kiến thức hữu ích dành cho các bạn quan tâm đến học máy và đang tìm kiếm các nguồn dữ liệu mở, công cụ phục vụ việc đào tạo mô hình. ------------- WORKSHOP “AI TRONG XỬ LÝ ẢNH Y TẾ - KINH NGHIỆM PHÁT TRIỂN THUẬT TOÁN & XÂY DỰNG CÁC BỘ DỮ LIỆU CHUẨN HÓA” Trong vòng 3 - 5 năm trở lại đây, các thuật toán trí tuệ nhân tạo đã mang đến những tiến bộ đột phá trong phân tích và xử lý ảnh y tế, góp phần đắc lực giúp các bác sĩ đưa ra quyết định nhanh chóng và chính xác, chẩn đoán sớm các bất thường, hướng tới sàng lọc trên diện rộng một số bệnh lý nguy hiểm. Tuy nhiên, cùng với cơ hội, ứng dụng AI trong xử lý hình ảnh y tế cũng đang phải đối diện với không ít thách thức, chẳng hạn như: Thiếu các bộ dữ liệu chuẩn hóa, có dán nhãn bởi bác sĩ chẩn đoán hình ảnh. Khả năng diễn giải của các mô hình AI còn hạn chế Vậy làm thế nào để phát triển và tối ưu hóa mô hình AI, dựa trên các bộ dữ liệu quy mô lớn và đạt chuẩn? Câu trả lời sẽ có trong workshop “AI trong xử lý ảnh y tế - Kinh nghiệm phát triển thuật toán và xây dựng các bộ dữ liệu chuẩn hóa”. Tại đây, các chuyên gia, kỹ sư công nghệ của Trung tâm Xử lý ảnh y tế, VinBigdata, những người trực tiếp tham gia phát triển sản phẩm VinDr, sẽ lần lượt trình bày: 1⃣ Quy trình tổng quan về phát triển AI trong Xử lý ảnh y tế - TS. Phạm Huy Hiệu, Chuyên gia nghiên cứu Thị giác máy tính, Trung tâm Xử lý ảnh y tế, VinBigdata. TS. Hiệu đã nhận bằng tiến sĩ tại Viện Đại học Toulouse, Pháp; là tác giả của hơn 20 bài báo khoa học trên các tạp chí và hội nghị quốc tế (Citations = 251; H-index = 8 ), trong đó có các tạp chí và hội nghị lớn như Computer Vision and Image Understanding (CVIU), Neurocomputing, MICCAI, MIDL. 2⃣ Kinh nghiệm phát triển thuật toán & Xử lý dữ liệu - Kỹ sư AI Nguyễn Bá Dũng, Trưởng nhóm Phân tích ảnh y tế, Trung tâm Xử lý ảnh y tế, VinBigdata; Kaggle Competitions Grandmaster. 3⃣ Hệ thống VinDr Lab và Các bộ dữ liệu chuẩn hóa chia sẻ cho cộng đồng - Kỹ sư AI Nguyễn Trung Nghĩa. Huy chương Bạc các cuộc thi Phát hiện xuất huyết nội sọ được tổ chức bởi Hiệp hội X‑quang Bắc Mỹ - RSNA, Phân đoạn khí màng phổi SIIM-ACR... Nếu đang quan tâm đến việc phát triển các mô hình học máy, đặc biệt trong lĩnh vực xử lý ảnh y tế, đồng thời, tìm kiếm các nguồn dữ liệu và công cụ phục vụ việc tối ưu hóa thuật toán, thì bạn đừng bỏ lỡ workshop tới đây của VinBigdata: Thời gian: từ 15h00 – 17h00 ngày 25/6/2021 Workshop được livestream trên Fanpage VinBigdata",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2020 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 12/2020 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"hi bạn Tiệp và các bạn. Mình đang tìm hiểu về GAN và pix2pix theo bài báo https://arxiv.org/pdf/1611.07004.pdf và https://github.com/znxlwm/pytorch-pix2pix/blob/master/pytorch_pix2pix.py . Mình có một thắc mắc nhỏ nhờ mọi người giúp đỡ.
trong một step được chia làm 2 bước, bước train D (discriminator) và bước train G (generator). Tại sai khi train D người ta không frezze G (G.requires_grad(false)) và khi train G người ta không frezze D (G.requires_grad(false)).
Mình xem code thấy có set riêng optimizer cho G riêng G_optimizer = optim.Adam(G.parameters(), D riêng D_optimizer = optim.Adam(D.parameters()), nên khi gọi D_optimizer.step() hoặc G_optimizer.step() thì chỉ update weight trên D và trên G độc lập,
nên code chạy vấn đúng.
Nhưng tại sao họ không freeze G (Khi train D) với freeze D (khi train G) để đỡ phải tính grad để tăng tốc độ train?
Cám ơn ad đã duyệt bài
Ps: Sau khi thảo luận với bạn Thuận Nguyễn mình có viết một cái test nho nhỏ tại là một model gồm hai linear f1 và f1, hàm optimer chỉ lấy params ở f1 thì khi set f2.requires_grad_(False) hay f2.requires_grad_(True) thì đều ra cùng một kết quả. Hàm backwards vẫn truyền về f1 theo chain rule. Nên mình vẫn thắc mắc chỗ đó
Các bạn có thể xem code và result tại đây https://pastebin.com/hE51GkKU","hi bạn Tiệp và các bạn. Mình đang tìm hiểu về GAN và pix2pix theo bài báo https://arxiv.org/pdf/1611.07004.pdf và https://github.com/znxlwm/pytorch-pix2pix/blob/master/pytorch_pix2pix.py . Mình có một thắc mắc nhỏ nhờ mọi người giúp đỡ. trong một step được chia làm 2 bước, bước train D (discriminator) và bước train G (generator). Tại sai khi train D người ta không frezze G (G.requires_grad(false)) và khi train G người ta không frezze D (G.requires_grad(false)). Mình xem code thấy có set riêng optimizer cho G riêng G_optimizer = optim.Adam(G.parameters(), D riêng D_optimizer = optim.Adam(D.parameters()), nên khi gọi D_optimizer.step() hoặc G_optimizer.step() thì chỉ update weight trên D và trên G độc lập, nên code chạy vấn đúng. Nhưng tại sao họ không freeze G (Khi train D) với freeze D (khi train G) để đỡ phải tính grad để tăng tốc độ train? Cám ơn ad đã duyệt bài Ps: Sau khi thảo luận với bạn Thuận Nguyễn mình có viết một cái test nho nhỏ tại là một model gồm hai linear f1 và f1, hàm optimer chỉ lấy params ở f1 thì khi set f2.requires_grad_(False) hay f2.requires_grad_(True) thì đều ra cùng một kết quả. Hàm backwards vẫn truyền về f1 theo chain rule. Nên mình vẫn thắc mắc chỗ đó Các bạn có thể xem code và result tại đây https://pastebin.com/hE51GkKU",,,,,
"[AI Share - Datasets]
Một số nguồn để tìm dataset về machine learning, data science, AI.
1. Google Datasets:
Link : https://datasetsearch.research.google.com/
2. Papers with Code Datasets.
Link : https://paperswithcode.com/datasets
3. Kaggle Dataset
Link: https://www.kaggle.com/datasets
4. Big Bag NLP Datasets
Link: https://index.quantumstat.com/#/
5. Hugging Face Datasets
Link: https://huggingface.co/dataset
6. UCI Machine Learning
Link: https://archive.ics.uci.edu/ml/index.php
7. Amazin Datasets (Open Data on AWS)
Link: https:/aws.amazon.com/opendata/
8. Awesome Public Datasets
Link: https://github.com/awesomedata/awesome-public-datasets
9. Azure public datasets
Link: https://docs.microsoft.com/en-us/azure/azure-sql/public-data-sets
10. Carnegie Mellon University
Link: https://guides.library.cmu.edu/az.php
11. .gov Datasets
Link: https://data.gov.au/
https://data.gov.in/
https://data.gov.sg/
https://data.europa.eu/data/datasets?locale=en&minScoring=0
______________________","[AI Share - Datasets] Một số nguồn để tìm dataset về machine learning, data science, AI. 1. Google Datasets: Link : https://datasetsearch.research.google.com/ 2. Papers with Code Datasets. Link : https://paperswithcode.com/datasets 3. Kaggle Dataset Link: https://www.kaggle.com/datasets 4. Big Bag NLP Datasets Link: https://index.quantumstat.com/#/ 5. Hugging Face Datasets Link: https://huggingface.co/dataset 6. UCI Machine Learning Link: https://archive.ics.uci.edu/ml/index.php 7. Amazin Datasets (Open Data on AWS) Link: https:/aws.amazon.com/opendata/ 8. Awesome Public Datasets Link: https://github.com/awesomedata/awesome-public-datasets 9. Azure public datasets Link: https://docs.microsoft.com/en-us/azure/azure-sql/public-data-sets 10. Carnegie Mellon University Link: https://guides.library.cmu.edu/az.php 11. .gov Datasets Link: https://data.gov.au/ https://data.gov.in/ https://data.gov.sg/ https://data.europa.eu/data/datasets?locale=en&minScoring=0 ______________________",,,,,
"Chào cả nhà,
Mình muốn dùng Python để làm text analysis. Cụ thể là trong conference call, analysts sẽ đặt câu hỏi cho CEO và họ có thể điều chỉnh tone của câu hỏi tùy theo CEO. Mình sẽ dùng transcripts của conference để đo lường tone của câu hỏi.
Trình độ Python của mình còn ở mức basic, muốn dùng text analysis thì mình nên bắt đầu như thế nào ạ?","Chào cả nhà, Mình muốn dùng Python để làm text analysis. Cụ thể là trong conference call, analysts sẽ đặt câu hỏi cho CEO và họ có thể điều chỉnh tone của câu hỏi tùy theo CEO. Mình sẽ dùng transcripts của conference để đo lường tone của câu hỏi. Trình độ Python của mình còn ở mức basic, muốn dùng text analysis thì mình nên bắt đầu như thế nào ạ?",,,,,
"[Hỏi về cách chọn K tối ưu cho KNN] Cho mình hỏi chút về cách chọn K trong KNN , mình có làm 1 bài tập nhỏ về nhận dạng chữ số viết tay và mình chọn KNN cho dễ cài đặt thì mình thấy là mình thử K lần lượt là 3,6,7...11 thì kiểm tra thấy các trọng số tương đối rõ ràng thì lấy K, Vậy thì ngoài phương pháp đó thì có cách nào để chọn K sao cho đạt hiểu quả cao nhất không nhỉ. Code mình sử dụng thuần Go và KNN","[Hỏi về cách chọn K tối ưu cho KNN] Cho mình hỏi chút về cách chọn K trong KNN , mình có làm 1 bài tập nhỏ về nhận dạng chữ số viết tay và mình chọn KNN cho dễ cài đặt thì mình thấy là mình thử K lần lượt là 3,6,7...11 thì kiểm tra thấy các trọng số tương đối rõ ràng thì lấy K, Vậy thì ngoài phương pháp đó thì có cách nào để chọn K sao cho đạt hiểu quả cao nhất không nhỉ. Code mình sử dụng thuần Go và KNN",,,,,
"Các bác giúp với ạ
Em cài jupyter toàn bị lỗi này thì khắc phục sao ạ :(",Các bác giúp với ạ Em cài jupyter toàn bị lỗi này thì khắc phục sao ạ :(,,,,,
"OPEN CALL: Dự án cộng đồng AICovidVN
Cần cộng đồng AI góp sức chống Covid! Xây AI Engine để chẩn đoán nhanh Covid qua tiếng ho, qua điện thoại
Hiện nay TPHCM đã triển khai nhanh hệ thống Robocall, qua đó AI tự động gọi điện cho hàng triệu người dân, và đặt câu hỏi cho họ để họ tự khai báo triệu chứng Covid. Điều này có ý nghĩa rất lớn trong việc sàng lọc nhanh những người cần xét nghiệm kỹ hơn, và khoanh vùng các ổ dịch kịp thời.
Tuy nhiên hệ thống hiện nay cần người dân tự khai báo triệu chứng, trong khi nhiều người mắc Covid nhưng chưa có triệu chứng.
Để tiếp tục nâng cao hiệu quả, và triển khai diện rộng ở TPHCM và các tỉnh thành khác, chúng ta cần xây dựng thêm AI Engine để đề nghị người dân ho vào điện thoại, rồi tự động chẩn đoán Covid sơ bộ.
Các tài nguyên đã có:
1. Dataset mở: 1700 mẫu ghi âm tiếng ho của người dương tính, (từ Thụy sĩ và Ấn độ) và nhiều mẫu âm tính, có dán nhãn. Tuy nhiên còn nhiều tạp âm và vấn đề với thông tin nhãn cần xử lý.
https://bit.ly/3uQrdb1
2. Báo báo khoa học của MIT đã công bố, theo đó họ đã đạt độ chính xác 98.5% với bài toán này (với 2600 mẫu dương tính), đặc biệt là độ chính xác 100% với các cá nhân chưa có triệu chứng (việc xét nghiệm người chưa có triệu chứng là quan trọng nhất với bài toán này)
https://bit.ly/3fNsgE4
3. Một số datasets khác
https://bit.ly/3chCQRU
4. Hệ thống Robocall đã triển khai tại TPHCM, sẵn sàng lắp thêm AI Engine chẩn đoán tiếng ho
https://bit.ly/3uShXTD
5. Facebook của nhóm dự án cộng đồng AICovidVN, để cùng trao đổi chuyên môn bit.ly/aicovidvn
Hình thức tham gia:
1. Tham gia nhóm thảo luận của dự án cộng đồng AICovidVN (bit.ly/aicovidvn)
2. Tùy ý sử dụng các open datasets được chia sẻ trong group hoặc các nơi khác
3. Xây dựng AI Enging chẩn đoán Covid qua tiếng ho, độ chính xác kỳ vọng 90%+
4,. Hỗ trợ, giới thiệu đầu mối cơ quan chức năng để ghi âm thêm mẫu tiếng ho của người dương tính (có triệu chứng, chưa có triệu chứng) qua điện thoại. Cần khoảng 1000 mẫu, nhưng càng nhiều càng tốt.
5. Kết hợp với team người khiếm thị để phối hợp chẩn đoán vòng 2 qua việc nghe bằng tai (đang được training song song)
6. Chuyển giao AI Engine cho Ban chỉ đạo quốc gia phòng chống dịch covid 19 để phục vụ công tác chẩn đoán qua Robocall diện rộng
7. Tùy ý sử dụng kết quả vào các mục đích khác. Ví dụ: tư xây dựng mobile app chẩn đoán Covid qua tiếng ho cho cá nhân để thương mại hóa, hoặc chuyển giao một phần hoặc tất cả hệ thống cho Ban chỉ đạo quốc gia, hoặc công bố mã nguồn và dữ liệu mở phục vụ cộng đồng, tùy theo mong muốn.
Các doanh nghiệp, nhóm, cá nhân có nhu cầu tham gia hoạt động cộng đồng này xin mời tham gia Facebook Group của nhóm: bit.ly/aicovidvn
Nhờ anh chị em tag giúp những ai có thể tham gia hoặc hỗ trợ thu thập tiếng ho nhé.
 — với Lê Công Thành và Pham Minh Tuan.","OPEN CALL: Dự án cộng đồng AICovidVN Cần cộng đồng AI góp sức chống Covid! Xây AI Engine để chẩn đoán nhanh Covid qua tiếng ho, qua điện thoại Hiện nay TPHCM đã triển khai nhanh hệ thống Robocall, qua đó AI tự động gọi điện cho hàng triệu người dân, và đặt câu hỏi cho họ để họ tự khai báo triệu chứng Covid. Điều này có ý nghĩa rất lớn trong việc sàng lọc nhanh những người cần xét nghiệm kỹ hơn, và khoanh vùng các ổ dịch kịp thời. Tuy nhiên hệ thống hiện nay cần người dân tự khai báo triệu chứng, trong khi nhiều người mắc Covid nhưng chưa có triệu chứng. Để tiếp tục nâng cao hiệu quả, và triển khai diện rộng ở TPHCM và các tỉnh thành khác, chúng ta cần xây dựng thêm AI Engine để đề nghị người dân ho vào điện thoại, rồi tự động chẩn đoán Covid sơ bộ. Các tài nguyên đã có: 1. Dataset mở: 1700 mẫu ghi âm tiếng ho của người dương tính, (từ Thụy sĩ và Ấn độ) và nhiều mẫu âm tính, có dán nhãn. Tuy nhiên còn nhiều tạp âm và vấn đề với thông tin nhãn cần xử lý. https://bit.ly/3uQrdb1 2. Báo báo khoa học của MIT đã công bố, theo đó họ đã đạt độ chính xác 98.5% với bài toán này (với 2600 mẫu dương tính), đặc biệt là độ chính xác 100% với các cá nhân chưa có triệu chứng (việc xét nghiệm người chưa có triệu chứng là quan trọng nhất với bài toán này) https://bit.ly/3fNsgE4 3. Một số datasets khác https://bit.ly/3chCQRU 4. Hệ thống Robocall đã triển khai tại TPHCM, sẵn sàng lắp thêm AI Engine chẩn đoán tiếng ho https://bit.ly/3uShXTD 5. Facebook của nhóm dự án cộng đồng AICovidVN, để cùng trao đổi chuyên môn bit.ly/aicovidvn Hình thức tham gia: 1. Tham gia nhóm thảo luận của dự án cộng đồng AICovidVN (bit.ly/aicovidvn) 2. Tùy ý sử dụng các open datasets được chia sẻ trong group hoặc các nơi khác 3. Xây dựng AI Enging chẩn đoán Covid qua tiếng ho, độ chính xác kỳ vọng 90%+ 4,. Hỗ trợ, giới thiệu đầu mối cơ quan chức năng để ghi âm thêm mẫu tiếng ho của người dương tính (có triệu chứng, chưa có triệu chứng) qua điện thoại. Cần khoảng 1000 mẫu, nhưng càng nhiều càng tốt. 5. Kết hợp với team người khiếm thị để phối hợp chẩn đoán vòng 2 qua việc nghe bằng tai (đang được training song song) 6. Chuyển giao AI Engine cho Ban chỉ đạo quốc gia phòng chống dịch covid 19 để phục vụ công tác chẩn đoán qua Robocall diện rộng 7. Tùy ý sử dụng kết quả vào các mục đích khác. Ví dụ: tư xây dựng mobile app chẩn đoán Covid qua tiếng ho cho cá nhân để thương mại hóa, hoặc chuyển giao một phần hoặc tất cả hệ thống cho Ban chỉ đạo quốc gia, hoặc công bố mã nguồn và dữ liệu mở phục vụ cộng đồng, tùy theo mong muốn. Các doanh nghiệp, nhóm, cá nhân có nhu cầu tham gia hoạt động cộng đồng này xin mời tham gia Facebook Group của nhóm: bit.ly/aicovidvn Nhờ anh chị em tag giúp những ai có thể tham gia hoặc hỗ trợ thu thập tiếng ho nhé. — với Lê Công Thành và Pham Minh Tuan.",,,,,
"#Hỏiđáp
Mọi người cho em xin ý kiến ạ.
Hiện tại em có khoảng 1000 mẫu form về hóa đơn như thế này. Chúng được đều được định dạng dưới dạng text.
Hiện tại em muốn chuyển chúng sang dạng file excel để thống kê người dùng.
Mọi người cho em xin một số phương pháp hoặc code tham khảo để xử lý phần này được không ạ? Kiểu em muốn trích xuất Họ và Tên, số điện thoại, Gía tiền, Địa chỉ,... ra ạ.
Em cảm ơn mọi người ạ.","Mọi người cho em xin ý kiến ạ. Hiện tại em có khoảng 1000 mẫu form về hóa đơn như thế này. Chúng được đều được định dạng dưới dạng text. Hiện tại em muốn chuyển chúng sang dạng file excel để thống kê người dùng. Mọi người cho em xin một số phương pháp hoặc code tham khảo để xử lý phần này được không ạ? Kiểu em muốn trích xuất Họ và Tên, số điện thoại, Gía tiền, Địa chỉ,... ra ạ. Em cảm ơn mọi người ạ.",#Hỏiđáp,,,,
"Xin chào mọi người ạ.
Model bên dưới từ một bài báo SOTA. Họ ko ghi rõ các tham số c model, và cũng như khối màu vàng (AC - Attention Condenser). Em có đọc kỹ các paper của họ, và đoán, thì thấy khá giống cơ chế Auto Encode/ Decoder. Tuy nhiên, họ lại ghi 2 tham số (VAE thì input = output), nên em chưa biết liệu cơ chế này bằng với layer nào ạ . Hiện tại em implement chưa có AC layer thì số tham số đã gần bằng model trong paper, nên em nghĩ AV layer là tổ hợp của những layer đơn giản. (2 tham số đó, em vẫn chưa biết tương ứng với gì ạ).
Rất mong được brainstorming cùng mọi người ạ.
Attention Condenser
https://arxiv.org/abs/2104.14623","Xin chào mọi người ạ. Model bên dưới từ một bài báo SOTA. Họ ko ghi rõ các tham số c model, và cũng như khối màu vàng (AC - Attention Condenser). Em có đọc kỹ các paper của họ, và đoán, thì thấy khá giống cơ chế Auto Encode/ Decoder. Tuy nhiên, họ lại ghi 2 tham số (VAE thì input = output), nên em chưa biết liệu cơ chế này bằng với layer nào ạ . Hiện tại em implement chưa có AC layer thì số tham số đã gần bằng model trong paper, nên em nghĩ AV layer là tổ hợp của những layer đơn giản. (2 tham số đó, em vẫn chưa biết tương ứng với gì ạ). Rất mong được brainstorming cùng mọi người ạ. Attention Condenser https://arxiv.org/abs/2104.14623",,,,,
"GOOGLE CLOUD VISION API
Google Cloud Vision API là một công cụ rất mạnh có thể mang đến cho cuộc sống các khả năng ứng dụng vô tận khi kết hợp với thư viện Python. Vision API là mô hình được đào tạo trước của Google, giúp phát hiện các đối tượng, nhận dạng khuôn mặt, nhận dạng hình ảnh, phân loại, gán nhãn và trích xuất văn bản của văn bản in hoặc hình ảnh chữ viết tay. Ngoài các tính năng trên, Vision API còn một tính năng rất thực tế và thú vị đó là cho phép bạn phát hiện nội dung không lành mạnh của hình ảnh. Vision API cho phép các nhà phát triển tích hợp các tính năng thú vị vào các ứng dụng dễ dàng.
Vậy Google Cloud Vision API hỗ trợ Machine Learning bằng các tính năng gì?
🔰Hiển thị thông tin các thuộc tính của hình ảnh
🔰Phát hiện khuôn mặt
🔰Phát hiện nhãn
🔰Nhận dạng ký tự quang học
🔰Phát hiện Web
🔰Phát hiện nhiều đối tượng
🔰Phát hiện nội dung không an toàn
Những thông tin trên hi vọng giúp ích cho các bạn về các khả năng mà Google Vision API trên nền tảng GCP có thể hỗ trợ được.
Tham khảo cách trình diễn Google Cloud Vision API với Python qua Blog chia sẻ kiến thức Google Cloud: https://blog.cloud-ace.vn/.../trinh-dien-google-cloud.../
👉Cloud Ace - Google Cloud Training Partner
#googlecloudtraining #cloudacevietnam","GOOGLE CLOUD VISION API Google Cloud Vision API là một công cụ rất mạnh có thể mang đến cho cuộc sống các khả năng ứng dụng vô tận khi kết hợp với thư viện Python. Vision API là mô hình được đào tạo trước của Google, giúp phát hiện các đối tượng, nhận dạng khuôn mặt, nhận dạng hình ảnh, phân loại, gán nhãn và trích xuất văn bản của văn bản in hoặc hình ảnh chữ viết tay. Ngoài các tính năng trên, Vision API còn một tính năng rất thực tế và thú vị đó là cho phép bạn phát hiện nội dung không lành mạnh của hình ảnh. Vision API cho phép các nhà phát triển tích hợp các tính năng thú vị vào các ứng dụng dễ dàng. Vậy Google Cloud Vision API hỗ trợ Machine Learning bằng các tính năng gì? Hiển thị thông tin các thuộc tính của hình ảnh Phát hiện khuôn mặt Phát hiện nhãn Nhận dạng ký tự quang học Phát hiện Web Phát hiện nhiều đối tượng Phát hiện nội dung không an toàn Những thông tin trên hi vọng giúp ích cho các bạn về các khả năng mà Google Vision API trên nền tảng GCP có thể hỗ trợ được. Tham khảo cách trình diễn Google Cloud Vision API với Python qua Blog chia sẻ kiến thức Google Cloud: https://blog.cloud-ace.vn/.../trinh-dien-google-cloud.../ Cloud Ace - Google Cloud Training Partner",#googlecloudtraining	#cloudacevietnam,,,,
"Em xin chào tất cả mọi người, em là newbie ML, hiện đang làm đồ án liên quan đến học Giám sát ( thuật toán bradley-terry ) để dự đoán win-loss của người chơi cờ nào đó nhưng vẫn chưa định hình rõ bắt đầu từ đầu, từng bước như thế nào, với lại việc tìm tập dữ liệu cũng hạn hẹp ( đã search tìm nhưng không có nhiều ).
Em rất mong mọi người chỉ điểm, hướng dẫn! Em cảm ơn nhiều","Em xin chào tất cả mọi người, em là newbie ML, hiện đang làm đồ án liên quan đến học Giám sát ( thuật toán bradley-terry ) để dự đoán win-loss của người chơi cờ nào đó nhưng vẫn chưa định hình rõ bắt đầu từ đầu, từng bước như thế nào, với lại việc tìm tập dữ liệu cũng hạn hẹp ( đã search tìm nhưng không có nhiều ). Em rất mong mọi người chỉ điểm, hướng dẫn! Em cảm ơn nhiều",,,,,
"Nhiều bạn có thể đã biết về mạng nơ-ron nhưng không phải ai cũng tìm hiểu kỹ về tính chất bề mặt hàm mục tiêu của chúng. Bài viết mở đầu trong chuỗi các bài viết về AI của Trung tâm nghiên cứu BK.AI, Đại học Bách khoa Hà Nội hy vọng mang đến cho các bạn một số kiến thức ban đầu về lĩnh vực thú vị này.","Nhiều bạn có thể đã biết về mạng nơ-ron nhưng không phải ai cũng tìm hiểu kỹ về tính chất bề mặt hàm mục tiêu của chúng. Bài viết mở đầu trong chuỗi các bài viết về AI của Trung tâm nghiên cứu BK.AI, Đại học Bách khoa Hà Nội hy vọng mang đến cho các bạn một số kiến thức ban đầu về lĩnh vực thú vị này.",,,,,
"Mn cho em hỏi này với ạ. Em đọc trên mạng thì hàm 'Resize' chỉ có tác dụng thay đổi kích thước ảnh, em ko thấy có tác dụng chuẩn hóa giá trị ảnh về khoảng 0-1. Nhưng sau trong đây hàm 'Resize' lại chuẩn hóa được ạ. Em cảm ơn mn nhiều.","Mn cho em hỏi này với ạ. Em đọc trên mạng thì hàm 'Resize' chỉ có tác dụng thay đổi kích thước ảnh, em ko thấy có tác dụng chuẩn hóa giá trị ảnh về khoảng 0-1. Nhưng sau trong đây hàm 'Resize' lại chuẩn hóa được ạ. Em cảm ơn mn nhiều.",,,,,
"Em chào mọi người ạ.
MỌi người cho e hỏi vai trò của tham số C với cost function ở trong câu 3 trong thuật toán SVM với ạ. E tìm hiểu thì nó tương đương với tham số 1/lamda trong regularlization của logistic regression. Và khi nào thì thuật toán SVM Bị overfit và cần điều chỉnh C như nào ạ. Em cảm ơn mọi người.
Nếu ai có tài liệu liên quan đến câu hỏi có thể cho e xin link đc ko ạ.
Em xin cảm ơn mọi người",Em chào mọi người ạ. MỌi người cho e hỏi vai trò của tham số C với cost function ở trong câu 3 trong thuật toán SVM với ạ. E tìm hiểu thì nó tương đương với tham số 1/lamda trong regularlization của logistic regression. Và khi nào thì thuật toán SVM Bị overfit và cần điều chỉnh C như nào ạ. Em cảm ơn mọi người. Nếu ai có tài liệu liên quan đến câu hỏi có thể cho e xin link đc ko ạ. Em xin cảm ơn mọi người,,,,,
Hi all. Mọi người có thể share cách đọc bài báo khoa học hay đơn giản hơn là một bài viết hay về Ml một cách hiệu quả được không ạ. Cách của em là lấy giấy viết ra ghi tóm tắt lại một vài ý nhưng em cảm thấy cách đọc của mình không hiệu quả và e cũng thấy khá tốn thời gian ạ.,Hi all. Mọi người có thể share cách đọc bài báo khoa học hay đơn giản hơn là một bài viết hay về Ml một cách hiệu quả được không ạ. Cách của em là lấy giấy viết ra ghi tóm tắt lại một vài ý nhưng em cảm thấy cách đọc của mình không hiệu quả và e cũng thấy khá tốn thời gian ạ.,,,,,
"[AI Share – Hugging Face course]
Hugging Face là một cái tên khá quen thuộc trong NLP. Gần đây, nhóm Hugging Face đã phát hành khóa học miễn phí về NLP với thư viện Hugging Face. Khóa học này tập trung vào các kiến thức cơ bản về NLP bằng cách sử dụng Hugging Face. Mặc dù khóa học hướng tới những người mới bắt đầu, nhưng nó cũng sẽ hữu ích cho những người đã có kiến thức cơ bản cũng như các chuyên gia NLP theo một cách nào đó. Nội dung gồm 4 phần chính là Transformers, Datasets, Tokenizers và Accelerate.
1. Transformers là thư viện cung cấp hàng nghìn mô hình được đào tạo trước như BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet,…, để thực hiện các tác vụ trên văn bản như phân loại, trích xuất thông tin, trả lời câu hỏi, tóm tắt,…
2. Tokenizers chuyển đổi đầu vào văn bản thành dữ liệu số.
3. Datasets là một thư viện có thể dễ dàng chia sẻ và truy cập các tập dữ liệu cũng như các độ đo đánh giá trong NLP
4. Accelerate cho phép đào tạo phân tán các mô hình Pytorch trên nhiều GPU hoặc TPU.","[AI Share – Hugging Face course] Hugging Face là một cái tên khá quen thuộc trong NLP. Gần đây, nhóm Hugging Face đã phát hành khóa học miễn phí về NLP với thư viện Hugging Face. Khóa học này tập trung vào các kiến thức cơ bản về NLP bằng cách sử dụng Hugging Face. Mặc dù khóa học hướng tới những người mới bắt đầu, nhưng nó cũng sẽ hữu ích cho những người đã có kiến thức cơ bản cũng như các chuyên gia NLP theo một cách nào đó. Nội dung gồm 4 phần chính là Transformers, Datasets, Tokenizers và Accelerate. 1. Transformers là thư viện cung cấp hàng nghìn mô hình được đào tạo trước như BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet,…, để thực hiện các tác vụ trên văn bản như phân loại, trích xuất thông tin, trả lời câu hỏi, tóm tắt,… 2. Tokenizers chuyển đổi đầu vào văn bản thành dữ liệu số. 3. Datasets là một thư viện có thể dễ dàng chia sẻ và truy cập các tập dữ liệu cũng như các độ đo đánh giá trong NLP 4. Accelerate cho phép đào tạo phân tán các mô hình Pytorch trên nhiều GPU hoặc TPU.",,,,,
"Em chào mọi người ạ
Em có một câu hỏi về ML mong mọi người giải đáp ạ. Em thấy thì linear regression thì có thể dùng ở local minimum nhưng ko biết logistic thì ntn. Em cảm ơn mọi người",Em chào mọi người ạ Em có một câu hỏi về ML mong mọi người giải đáp ạ. Em thấy thì linear regression thì có thể dùng ở local minimum nhưng ko biết logistic thì ntn. Em cảm ơn mọi người,,,,,
"Mình tình cờ tìm thấy 1 notebook khá hay giải thích từ Tensor, Filters, Strides, Padding, MaxPooling, Computational Grap, AutoGrad, cũng như một số module trong bài toán Computer Vision cho PyTorch. Và một điều thú vị, các bạn làm ứng dụng hay nghiên cứu đều đặc biệt quan tâm là hiện tượng Underfitting >< Overfitting, ở notebook này cũng giải thích rất trực quan. Hi vọng nó hữu ích với mọi người. https://colab.research.google.com/github/pranjalchaubey/Deep-Learning-Notes/blob/master/PyTorch%20Image%20Classification%20in%202020/Image_Classification_practice.ipynb?fbclid=IwAR3fNr9HFt7l9q5w7VMdIBNTof6oBMZjzZVKLE2MT7UrBRI5k_DPDVHcA28#scrollTo=el1NUdaz0HrE","Mình tình cờ tìm thấy 1 notebook khá hay giải thích từ Tensor, Filters, Strides, Padding, MaxPooling, Computational Grap, AutoGrad, cũng như một số module trong bài toán Computer Vision cho PyTorch. Và một điều thú vị, các bạn làm ứng dụng hay nghiên cứu đều đặc biệt quan tâm là hiện tượng Underfitting >< Overfitting, ở notebook này cũng giải thích rất trực quan. Hi vọng nó hữu ích với mọi người. https://colab.research.google.com/github/pranjalchaubey/Deep-Learning-Notes/blob/master/PyTorch%20Image%20Classification%20in%202020/Image_Classification_practice.ipynb?fbclid=IwAR3fNr9HFt7l9q5w7VMdIBNTof6oBMZjzZVKLE2MT7UrBRI5k_DPDVHcA28#scrollTo=el1NUdaz0HrE",,,,,
#Logistic #Classification,,#Logistic	#Classification,,,,
"Em gửi mọi người clip thứ 3 em làm về Statistics ạ.
Hi vọng giúp mọi người review lại kiến thức tốt hơn.",Em gửi mọi người clip thứ 3 em làm về Statistics ạ. Hi vọng giúp mọi người review lại kiến thức tốt hơn.,,,,,
Cho các bạn muốn nghiên cứu sâu về lý thuyết deep learning.,Cho các bạn muốn nghiên cứu sâu về lý thuyết deep learning.,,,,,
"Chào mọi người!
Mọi người cho em hỏi 1 chút là Nếu so sánh Faster R-CNN với Mask-RCNN và Yolo thì có điểm gì khác biệt không ạ!
theo như em tìm hiểu thì Mask-RCNN với Yolo đều kế thừa từ Faster-RCNN và đều trả về là bounding boxes và label của đối tượng. vậy YOLO khác gì so với Mask-RCNN.
Em mới tìm hiểu , mong m,n chỉ giúp ạ!
Thanks m.n!","Chào mọi người! Mọi người cho em hỏi 1 chút là Nếu so sánh Faster R-CNN với Mask-RCNN và Yolo thì có điểm gì khác biệt không ạ! theo như em tìm hiểu thì Mask-RCNN với Yolo đều kế thừa từ Faster-RCNN và đều trả về là bounding boxes và label của đối tượng. vậy YOLO khác gì so với Mask-RCNN. Em mới tìm hiểu , mong m,n chỉ giúp ạ! Thanks m.n!",,,,,
"Giới thiệu với các bạn một mục nhỏ tiếp theo của cuốn sách ""Machine Learning cho dữ liệu dạng bảng"". Phần này giới thiệu ban đầu về việc xây dựng embedding cho dữ liệu dạng hạng mục. Mình cũng có giải thích cơ bản về word2vec và cách áp dụng ý tưởng tương tự để xây dựng embedding cho các sản phẩm dựa trên thứ tự của chúng trong các đơn hàng.
Cuốn sách sẽ còn nhiều phần khác mà embedding sẽ được áp dụng. Embedding là cách rất phổ biến giúp cải thiện chất lượng các mô hình deep learning cho dữ liệu dạng bảng.
Embedding:","Giới thiệu với các bạn một mục nhỏ tiếp theo của cuốn sách ""Machine Learning cho dữ liệu dạng bảng"". Phần này giới thiệu ban đầu về việc xây dựng embedding cho dữ liệu dạng hạng mục. Mình cũng có giải thích cơ bản về word2vec và cách áp dụng ý tưởng tương tự để xây dựng embedding cho các sản phẩm dựa trên thứ tự của chúng trong các đơn hàng. Cuốn sách sẽ còn nhiều phần khác mà embedding sẽ được áp dụng. Embedding là cách rất phổ biến giúp cải thiện chất lượng các mô hình deep learning cho dữ liệu dạng bảng. Embedding:",,,,,
Mn cho em hỏi là độ chính xác của thuật toán khoảng bao nhiêu % trở lên thì được chấp nhận ạ. Ví dụ như 40% thì có hy vọng được chấp nhận không ạ. Em cảm ơn mn nhiều.,Mn cho em hỏi là độ chính xác của thuật toán khoảng bao nhiêu % trở lên thì được chấp nhận ạ. Ví dụ như 40% thì có hy vọng được chấp nhận không ạ. Em cảm ơn mn nhiều.,,,,,
Dù đã cố gắng nhưng vẫn viết chưa hay. Mong các bạn nhận xét để hoàn thiện hơn.,Dù đã cố gắng nhưng vẫn viết chưa hay. Mong các bạn nhận xét để hoàn thiện hơn.,,,,,
,nan,,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 01/2021 vào trong comment của post này.
Hoặc https://forum.machinelearningcoban.com/c/jobs-events
Chúc các bạn năm mới mạnh khoẻ và nhiều thành công!","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 01/2021 vào trong comment của post này. Hoặc https://forum.machinelearningcoban.com/c/jobs-events Chúc các bạn năm mới mạnh khoẻ và nhiều thành công!",,,,,
"Lại là em đây ạ, câu hỏi nằm trong ảnh 1 ạ, và câu trả lời của em nằm trong ảnh 2, xin hỏi mọi người là em đã làm đúng chưa ạ, em xin cảm ơn ạ!","Lại là em đây ạ, câu hỏi nằm trong ảnh 1 ạ, và câu trả lời của em nằm trong ảnh 2, xin hỏi mọi người là em đã làm đúng chưa ạ, em xin cảm ơn ạ!",,,,,
"Chào mọi người, em đang làm estimate distance và đang không có solutions cho bài toán. Mọi người ai đã làm qua về bài toán này cho e xin ít kinh nghiệm được không ạ ?
Em chân thành cảm ơn !","Chào mọi người, em đang làm estimate distance và đang không có solutions cho bài toán. Mọi người ai đã làm qua về bài toán này cho e xin ít kinh nghiệm được không ạ ? Em chân thành cảm ơn !",,,,,
"Em đang tìm hiểu về cái recommendation engine theo link này của Code Heroku. Ở trong clip là recommendation engine theo kiểu item-item collaborative filtering.
https://youtu.be/3ecNC-So0r4
Em muốn hỏi mọi người này về tính toán 1 chút, là:
Hình 1: Ở đoạn này, từ dataset cho trước, sau khi tính cosine từng đại lượng 2 hàng ngang dọc, chúng ta có được similarities từng cặp.
Hình 2: khi có được similarities từng cặp, chúng ta nhập vô đánh giá của 1 người dùng bất kì, như ở đây có 1 người dùng đánh giá bộ phim romantic3 là 1 sao.
Làm sao để từ ma trận như hình 1 mà nó thành 6 dòng ngắn gọn như ở hình 2 vậy ạ? Em xem công thức trên code mà không hiểu
Em cảm ơn anh chị em đã xem bài ạ.
CẬP NHẬT: Em/Mình đã hiểu. Đó là chọn cột dọc hoặc hàng ngang của romantic3 đều được, vì số của cột và dòng giống nhau. Sau đó nhân từng số cho (rating-2.5, 2.5 này là mean, các bạn coi clip sẽ hiểu, khoảng phút thứ 23 24). Sau đó sắp xếp lại theo thứ tự giảm dần, ta được hình 2.","Em đang tìm hiểu về cái recommendation engine theo link này của Code Heroku. Ở trong clip là recommendation engine theo kiểu item-item collaborative filtering. https://youtu.be/3ecNC-So0r4 Em muốn hỏi mọi người này về tính toán 1 chút, là: Hình 1: Ở đoạn này, từ dataset cho trước, sau khi tính cosine từng đại lượng 2 hàng ngang dọc, chúng ta có được similarities từng cặp. Hình 2: khi có được similarities từng cặp, chúng ta nhập vô đánh giá của 1 người dùng bất kì, như ở đây có 1 người dùng đánh giá bộ phim romantic3 là 1 sao. Làm sao để từ ma trận như hình 1 mà nó thành 6 dòng ngắn gọn như ở hình 2 vậy ạ? Em xem công thức trên code mà không hiểu Em cảm ơn anh chị em đã xem bài ạ. CẬP NHẬT: Em/Mình đã hiểu. Đó là chọn cột dọc hoặc hàng ngang của romantic3 đều được, vì số của cột và dòng giống nhau. Sau đó nhân từng số cho (rating-2.5, 2.5 này là mean, các bạn coi clip sẽ hiểu, khoảng phút thứ 23 24). Sau đó sắp xếp lại theo thứ tự giảm dần, ta được hình 2.",,,,,
"Chào mọi người
Em có một câu hỏi về KL divergence giữa hai multivariate normal distribution ạ.
Em có 2 multivariate normal distribution là prior và posterior. Em dùng PCA để giảm dimension (64 xuống 3) và nhìn trực quan 2 distribution này ạ (hình đính kèm, xanh: posterior distribution, đỏ: prior distribution). Theo hình ảnh, 2 distribution này rất khác nhau vì mean của chúng tách biệt hoàn toàn.
Tuy nhiên, khi em áp dụng KL divergence cho 2 distribution này (dùng multivariate distribution gốc chứ không phải từ PCA), thì kết quả là khoảng 0.17, nghĩa là 2 distribution này lại ""giống"" nhau.

Câu hỏi: có nên đánh giá mức độ tương đồng của 2 multivariate distribution thông qua visualization từ PCA thì không?

Cảm ơn mọi người ạ","Chào mọi người Em có một câu hỏi về KL divergence giữa hai multivariate normal distribution ạ. Em có 2 multivariate normal distribution là prior và posterior. Em dùng PCA để giảm dimension (64 xuống 3) và nhìn trực quan 2 distribution này ạ (hình đính kèm, xanh: posterior distribution, đỏ: prior distribution). Theo hình ảnh, 2 distribution này rất khác nhau vì mean của chúng tách biệt hoàn toàn. Tuy nhiên, khi em áp dụng KL divergence cho 2 distribution này (dùng multivariate distribution gốc chứ không phải từ PCA), thì kết quả là khoảng 0.17, nghĩa là 2 distribution này lại ""giống"" nhau. Câu hỏi: có nên đánh giá mức độ tương đồng của 2 multivariate distribution thông qua visualization từ PCA thì không? Cảm ơn mọi người ạ",,,,,
"Em chào mọi người ạ .
Em mới bắt đầu nghiên cứu về AI và có một bài toán cần mọi người giúp đỡ để tìm ra hướng giải quyết ạ:
Bài toán đặt ra là từ một ảnh đầu vào là ảnh một vị trí hay tọa độ trên bản đồ ( ảnh google map) để phát hiện ra đường đi trong bức ảnh đó như hình dưới ạ. Mọi người có thuật toán hay code nào liên quan đến vấn đề này có thể cho em xin được không ạ. Em cũng cố gắng tìm kiếm nhiều nhưng không ra được hướng giải quyết, mông mọi người giúp đỡ ạ.","Em chào mọi người ạ . Em mới bắt đầu nghiên cứu về AI và có một bài toán cần mọi người giúp đỡ để tìm ra hướng giải quyết ạ: Bài toán đặt ra là từ một ảnh đầu vào là ảnh một vị trí hay tọa độ trên bản đồ ( ảnh google map) để phát hiện ra đường đi trong bức ảnh đó như hình dưới ạ. Mọi người có thuật toán hay code nào liên quan đến vấn đề này có thể cho em xin được không ạ. Em cũng cố gắng tìm kiếm nhiều nhưng không ra được hướng giải quyết, mông mọi người giúp đỡ ạ.",,,,,
"Em chào mn. Mn chỉ giúp em vấn đề này với ạ. Chuyện là em dùng thuật toán Random Forest trong Sklearn để giải quyết bài toán. Nhưng ngoài 'n_estimators' ra thì các 'tính chất' kia em đọc (cả thuật toán Random Forest và các giải thích trong Sklearn) nhưng ko hiểu nó sẽ có tác dụng gì và điều chỉnh ra sau để thuật toán được tốt nhất ạ. Mn cho em hỏi là trong thuật toán của em nên thêm 'tính chất' nào vào và giá trị của nó như thế nạo ạ. E cảm ơn mn nhiều.
Ps : Nên dùng GridSearchCV để tìm siêu tham số trong thuật toán này ko ạ.",Em chào mn. Mn chỉ giúp em vấn đề này với ạ. Chuyện là em dùng thuật toán Random Forest trong Sklearn để giải quyết bài toán. Nhưng ngoài 'n_estimators' ra thì các 'tính chất' kia em đọc (cả thuật toán Random Forest và các giải thích trong Sklearn) nhưng ko hiểu nó sẽ có tác dụng gì và điều chỉnh ra sau để thuật toán được tốt nhất ạ. Mn cho em hỏi là trong thuật toán của em nên thêm 'tính chất' nào vào và giá trị của nó như thế nạo ạ. E cảm ơn mn nhiều. Ps : Nên dùng GridSearchCV để tìm siêu tham số trong thuật toán này ko ạ.,,,,,
"Help!
Em cần mọi người giúp đỡ về bài tập như sau ạ:
Answer the following questions with a little Python programming.
First of all, load the faces dataset and prepare a 5-class subset using the following code.
from sklearn.datasets import fetch_olivetti_faces
from sklearn.ensemble import RandomForestClassifier
# Load the faces dataset
data = fetch_olivetti_faces()
X, y = data.data, data.target
mask = y < 5 # Limit to 5 classes
X = X[mask]
y = y[mask]
(1) [10 points] Let's use the RandomForest classifier and find out the optimal number of estimators (decision trees) from { 1000, 1500, 2000, 2500, 3000 } in terms of the OOB error. Make sure to use the default parameter setting and random_state=0 of RandomForestClassifier in scikit-learn to obtain the determinstic result.
(2) [10 points] Use the optimal number of estimators obtained in Q4-1. Then, investigate the pixel (i.e., feature) importance trained by the model. List the coordinates of the three most important pixels in an image. The upper-left corner is (0, 0), and the lower-right corner is (63, 63). That is, you need to provide three 2-dimensional coordinates (i.e., (row, col) where 0 ≤ row, col ≤ 63).
Để giải câu 1, em đã chạy code như sau và cho kết quả ở hình (1) ạ, từ đó kết luận optimal number of estimators is 2000 ạ.
Để giải câu 2, Em chạy code như hình 2, nhưng đến đoạn tìm 3 điểm quan trọng nhất và tọa độ của nó em không làm được ạ.
Nhờ mọi người xem giúp em ạ, những bước em làm đã đugns chưa ạ, và câu 2 làm thế nào ạ. Đây là bài kiểm tra của em, và được phép hỏi người khác ạ. Vì chuyên ngành của em là chuyên ngành khác nên em khác khó khăn khi học về ML ạ
Em xin cảm ơn ạ","Help! Em cần mọi người giúp đỡ về bài tập như sau ạ: Answer the following questions with a little Python programming. First of all, load the faces dataset and prepare a 5-class subset using the following code. from sklearn.datasets import fetch_olivetti_faces from sklearn.ensemble import RandomForestClassifier # Load the faces dataset data = fetch_olivetti_faces() X, y = data.data, data.target mask = y < 5 # Limit to 5 classes X = X[mask] y = y[mask] (1) [10 points] Let's use the RandomForest classifier and find out the optimal number of estimators (decision trees) from { 1000, 1500, 2000, 2500, 3000 } in terms of the OOB error. Make sure to use the default parameter setting and random_state=0 of RandomForestClassifier in scikit-learn to obtain the determinstic result. (2) [10 points] Use the optimal number of estimators obtained in Q4-1. Then, investigate the pixel (i.e., feature) importance trained by the model. List the coordinates of the three most important pixels in an image. The upper-left corner is (0, 0), and the lower-right corner is (63, 63). That is, you need to provide three 2-dimensional coordinates (i.e., (row, col) where 0 ≤ row, col ≤ 63). Để giải câu 1, em đã chạy code như sau và cho kết quả ở hình (1) ạ, từ đó kết luận optimal number of estimators is 2000 ạ. Để giải câu 2, Em chạy code như hình 2, nhưng đến đoạn tìm 3 điểm quan trọng nhất và tọa độ của nó em không làm được ạ. Nhờ mọi người xem giúp em ạ, những bước em làm đã đugns chưa ạ, và câu 2 làm thế nào ạ. Đây là bài kiểm tra của em, và được phép hỏi người khác ạ. Vì chuyên ngành của em là chuyên ngành khác nên em khác khó khăn khi học về ML ạ Em xin cảm ơn ạ",,,,,
"[AI Share]
Pytorch là một thư viện mã nguồn mở được thường được sử dụng cho Computer Vision và NLP.
Trước đây, Pytorch thường chỉ được dùng cho nghiên cứu, nhưng gần đây Pytorch cũng phát triển để hỗ trợ triển khai sản phẩm.
Mới đây, Facebook mới ra một bài viết về việc chuyển tất cả các hệ thống AI của họ sang Pytorch. Sau hơn 1 năm di chuyển, có hơn 1.700 mô hình dựa trên Pytorch ở Facebook và 93% mô hình mới về phân tích nội dung trên Facebook cũng là Pytorch.
Có thể thấy Pytorch là một trong những Deep Learning framework tốt nhất hiện tại, cho cả làm nghiên cứu lẫn phát triển sản phẩm.
Tình cờ thấy repos hướng dẫn Pytorch hơn 20k sao trên Github. Repos này dành cho người mới bắt đầu học Pytorch. Hầu hết các file chỉ chứa ít hơn 30 dòng code, đơn giản và dễ hiểu. Có các mức độ để học đi từ dễ đến khó: Cơ bản, Trung bình, Nâng cao.
Github: https://github.com/yunjey/pytorch-tutorial
Ngoài ra mọi người có thể theo dõi series về Pytorch bằng tiếng việt của mình đang viết ở đây https://nttuan8.com/category/pytorch/
Đọc thêm: https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/","[AI Share] Pytorch là một thư viện mã nguồn mở được thường được sử dụng cho Computer Vision và NLP. Trước đây, Pytorch thường chỉ được dùng cho nghiên cứu, nhưng gần đây Pytorch cũng phát triển để hỗ trợ triển khai sản phẩm. Mới đây, Facebook mới ra một bài viết về việc chuyển tất cả các hệ thống AI của họ sang Pytorch. Sau hơn 1 năm di chuyển, có hơn 1.700 mô hình dựa trên Pytorch ở Facebook và 93% mô hình mới về phân tích nội dung trên Facebook cũng là Pytorch. Có thể thấy Pytorch là một trong những Deep Learning framework tốt nhất hiện tại, cho cả làm nghiên cứu lẫn phát triển sản phẩm. Tình cờ thấy repos hướng dẫn Pytorch hơn 20k sao trên Github. Repos này dành cho người mới bắt đầu học Pytorch. Hầu hết các file chỉ chứa ít hơn 30 dòng code, đơn giản và dễ hiểu. Có các mức độ để học đi từ dễ đến khó: Cơ bản, Trung bình, Nâng cao. Github: https://github.com/yunjey/pytorch-tutorial Ngoài ra mọi người có thể theo dõi series về Pytorch bằng tiếng việt của mình đang viết ở đây https://nttuan8.com/category/pytorch/ Đọc thêm: https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/",,,,,
"#sharing
Không biết có bạn nào đã share link khóa học Machine Learning này của Stanford trên đây chưa nhỉ? Nếu có thì bỏ qua post này nhé. Stanford vừa cập nhật mới kiến thức khóa học CS229 này và các bạn có thể xem online cũng như download tất cả các lecture notes.",Không biết có bạn nào đã share link khóa học Machine Learning này của Stanford trên đây chưa nhỉ? Nếu có thì bỏ qua post này nhé. Stanford vừa cập nhật mới kiến thức khóa học CS229 này và các bạn có thể xem online cũng như download tất cả các lecture notes.,#sharing,,,,
"Xin phép admin.
Mình có 1 dự án của khách hàng, yêu cầu đọc file scan, là đáp án thi trắc nghiệm như hình dưới và trích xuất thông tin :
4 ô vuông dùng để định vị tờ trắc nghiệm, khi scan có thể tờ trắc nghiệm sẽ bị nghiêng so với ảnh.
Số báo danh ở góc trên bên phải.
Các đáp án chỉ có đúng hoặc sai, trường hợp sai thì học sinh sẽ đánh chữ X vào trong ô vuông. Vì câu hỏi có thể dài hoặc ngắn nên đáp án sẽ nằm rải rác trên tờ đáp án, không có vị trí cố định (chỉ biết là sẽ có 2 cột các câu hỏi).
Mình cũng mới học AI, chưa biết bắt đầu từ đâu, mong các bạn cho mình 1 vài solutions hoặc từ khoá để mình search với.
Thanks.","Xin phép admin. Mình có 1 dự án của khách hàng, yêu cầu đọc file scan, là đáp án thi trắc nghiệm như hình dưới và trích xuất thông tin : 4 ô vuông dùng để định vị tờ trắc nghiệm, khi scan có thể tờ trắc nghiệm sẽ bị nghiêng so với ảnh. Số báo danh ở góc trên bên phải. Các đáp án chỉ có đúng hoặc sai, trường hợp sai thì học sinh sẽ đánh chữ X vào trong ô vuông. Vì câu hỏi có thể dài hoặc ngắn nên đáp án sẽ nằm rải rác trên tờ đáp án, không có vị trí cố định (chỉ biết là sẽ có 2 cột các câu hỏi). Mình cũng mới học AI, chưa biết bắt đầu từ đâu, mong các bạn cho mình 1 vài solutions hoặc từ khoá để mình search với. Thanks.",,,,,
"Xin chào mọi người,
Mình có một vấn đề muốn thảo luận để tiếp thu thêm các góc nhìn khác nhau.
Bài toán của mình là so sánh sự giống nhau trên mặt ý nghĩa của hai văn bản, ý nghĩa ở đây là một khái niệm rộng. Nó có thể là hai văn bản nói cùng về một chủ đề, hoặc chia sẻ những ý tưởng giống nhau.
Vấn đề này mình nhận thấy sau một thời gian ngộ nhận rằng cứ việc dùng Sentence-BERT là tốt nhất rồi. Lý do mình nhận ra là vì khi để ý kỹ hơn cái văn bản mình tìm ra là giống nhất, ở những đoạn giữa hoặc kết bài thì nó chả còn liên quan gì đến văn bản tìm kiếm. Và nguyên nhân là vì Sentence-BERT hoặc BERT chỉ nhận vào 512 tokens, nghĩa là phần đầu văn bản.
Vậy có mô hình nào để tạo embedding vector ở cấp văn bản không?
Sau khi tìm kiếm mình thấy có ít nhất là 2 hướng chính sau
1. Trung bình cộng các embedding vectors ở cấp nhỏ hơn, ví dụ word-level hay sentence-level. Mình đã thí nghiệm và thú vị là kết quả tệ hơn việc dùng trực tiếp Sentence-BERT.
2. Vẫn là trung bình cộng, nhưng trong quá trình này sẽ có sự tham gia của một training task khác để đảm bảo việc tổng hợp các vectors lại có thể đại diện cho một văn bản. Ví dụ, Doc2VecC [1], hoặc dùng GLU Covolution blocks [2], hoặc một nghiên cứu rất mới của Google ở đây [3].
Câu hỏi thảo luận của mình là:
1. Lý giải khoa học nào có thể giải thích việc trung bình cộng đơn thuần các embedding vectors của cấp nhỏ hơn không thể đại diện cho một văn bản được.
2. Mình không tìm thấy một mô hình nào đã được trained và published theo hướng 2 dựa trên BERT hoặc Transformer-based model nói chung. Bạn nào có thông tin thì chia sẻ với mình nhé.
3. Nếu có giải pháp nào khác ngoài 2 hướng mình đề cập thì cũng xin mọi người chia sẻ.
[1] https://arxiv.org/pdf/1707.02377.pdf
[2] https://arxiv.org/pdf/1711.04168.pdf
[3] https://arxiv.org/pdf/2004.12297.pdf","Xin chào mọi người, Mình có một vấn đề muốn thảo luận để tiếp thu thêm các góc nhìn khác nhau. Bài toán của mình là so sánh sự giống nhau trên mặt ý nghĩa của hai văn bản, ý nghĩa ở đây là một khái niệm rộng. Nó có thể là hai văn bản nói cùng về một chủ đề, hoặc chia sẻ những ý tưởng giống nhau. Vấn đề này mình nhận thấy sau một thời gian ngộ nhận rằng cứ việc dùng Sentence-BERT là tốt nhất rồi. Lý do mình nhận ra là vì khi để ý kỹ hơn cái văn bản mình tìm ra là giống nhất, ở những đoạn giữa hoặc kết bài thì nó chả còn liên quan gì đến văn bản tìm kiếm. Và nguyên nhân là vì Sentence-BERT hoặc BERT chỉ nhận vào 512 tokens, nghĩa là phần đầu văn bản. Vậy có mô hình nào để tạo embedding vector ở cấp văn bản không? Sau khi tìm kiếm mình thấy có ít nhất là 2 hướng chính sau 1. Trung bình cộng các embedding vectors ở cấp nhỏ hơn, ví dụ word-level hay sentence-level. Mình đã thí nghiệm và thú vị là kết quả tệ hơn việc dùng trực tiếp Sentence-BERT. 2. Vẫn là trung bình cộng, nhưng trong quá trình này sẽ có sự tham gia của một training task khác để đảm bảo việc tổng hợp các vectors lại có thể đại diện cho một văn bản. Ví dụ, Doc2VecC [1], hoặc dùng GLU Covolution blocks [2], hoặc một nghiên cứu rất mới của Google ở đây [3]. Câu hỏi thảo luận của mình là: 1. Lý giải khoa học nào có thể giải thích việc trung bình cộng đơn thuần các embedding vectors của cấp nhỏ hơn không thể đại diện cho một văn bản được. 2. Mình không tìm thấy một mô hình nào đã được trained và published theo hướng 2 dựa trên BERT hoặc Transformer-based model nói chung. Bạn nào có thông tin thì chia sẻ với mình nhé. 3. Nếu có giải pháp nào khác ngoài 2 hướng mình đề cập thì cũng xin mọi người chia sẻ. [1] https://arxiv.org/pdf/1707.02377.pdf [2] https://arxiv.org/pdf/1711.04168.pdf [3] https://arxiv.org/pdf/2004.12297.pdf",,,,,
"xin phép Admin
Mình đang muốn lập team nghiên cứu và làm chủ công nghệ nhận dạng giọng nói, ae nào đang nghiên cứu và có kiến thức về phần này rồi có thể thì cùng nhau nghiên cứu cho ra ngô ra khoai nhé ae. Inbox cho mình nếu bạn nào muốn tham gia NC hoặc có team rồi thì cho mk tham gia cùng nhé, cảm ơn mn.","xin phép Admin Mình đang muốn lập team nghiên cứu và làm chủ công nghệ nhận dạng giọng nói, ae nào đang nghiên cứu và có kiến thức về phần này rồi có thể thì cùng nhau nghiên cứu cho ra ngô ra khoai nhé ae. Inbox cho mình nếu bạn nào muốn tham gia NC hoặc có team rồi thì cho mk tham gia cùng nhé, cảm ơn mn.",,,,,
"Em chào mọi người ạ !
Em lần đầu được biết đến AI ạ
Em muốn hỏi mọi người như thế này ạ :
- Dữ liệu của em là : input : 13 x 205 , target là 5 x 205
Em có thể dự đoán output bằng linear regression ko ạ ?
Và em có thể chia target ra từng 5 cái riêng lẻ để dự đoán ko ạ
Và anh chị có thể giải thích giúp em cách vẽ 2 biểu đồ này ko ạ ?
Em cảm ơn nhiều ạ","Em chào mọi người ạ ! Em lần đầu được biết đến AI ạ Em muốn hỏi mọi người như thế này ạ : - Dữ liệu của em là : input : 13 x 205 , target là 5 x 205 Em có thể dự đoán output bằng linear regression ko ạ ? Và em có thể chia target ra từng 5 cái riêng lẻ để dự đoán ko ạ Và anh chị có thể giải thích giúp em cách vẽ 2 biểu đồ này ko ạ ? Em cảm ơn nhiều ạ",,,,,
"Một bạn nhờ đăng hộ. Mình chưa download về xem nên không chịu trách nhiệm về nội dung nhé :).
""Ad ơi, Udacity có chương trình Machine Learning Nanodegree. Video bài giảng thì đều có trên youtube nhưng ko có playlist. Em đã tổng hợp tất cả video theo chủ đề đc dạy, đảm bảo đúng 100% vì e đã enroll khoá này. E muốn share vào group nhưng ko muốn lộ tên ạ :)) có gì ad share giúp e với. Bộ video này có vẻ ""legal"" vì dù sao tất cả video đều nằm trên youtube ý. Có gì ad xem xét rồi up hộ e nếu có thể ạ.""","Một bạn nhờ đăng hộ. Mình chưa download về xem nên không chịu trách nhiệm về nội dung nhé :). ""Ad ơi, Udacity có chương trình Machine Learning Nanodegree. Video bài giảng thì đều có trên youtube nhưng ko có playlist. Em đã tổng hợp tất cả video theo chủ đề đc dạy, đảm bảo đúng 100% vì e đã enroll khoá này. E muốn share vào group nhưng ko muốn lộ tên ạ :)) có gì ad share giúp e với. Bộ video này có vẻ ""legal"" vì dù sao tất cả video đều nằm trên youtube ý. Có gì ad xem xét rồi up hộ e nếu có thể ạ.""",,,,,
"#Question
Nhờ mn chỉ giúp mình hướng đi cho cách kiểm tra ảnh trùng với.
Mình có file ảnh cần so sánh bên phải do khách chụp, đôi khi ảnh bị crop, bị thừa hoặc khách dùng đt chụp lại màn máy tính. Trong khi ảnh gốc bên trái.
Mục đích là khi khách gửi hình sang thì trả về nó tương tự ảnh nào trong library.
Cám ơn các bác.","Nhờ mn chỉ giúp mình hướng đi cho cách kiểm tra ảnh trùng với. Mình có file ảnh cần so sánh bên phải do khách chụp, đôi khi ảnh bị crop, bị thừa hoặc khách dùng đt chụp lại màn máy tính. Trong khi ảnh gốc bên trái. Mục đích là khi khách gửi hình sang thì trả về nó tương tự ảnh nào trong library. Cám ơn các bác.",#Question,,,,
,nan,,,,,
"Hello mọi người, em đang có một chút thắc mắc về neural network đó là: trong bài toán binary classification thì mình có thể dùng 1 output hoặc 2 output đúng ko ạ?, 2 cách trên sẽ khác nhau ntn?. Mọi người có thể cho em 1 số key-words về vấn đề này để search dc ko ạ, em đã thử search nhưng tiếc là kết quả nhận về chưa dc mong muốn lắm :((
Thank mọi người nhiều ạ!!!
#ML #DL #NN","Hello mọi người, em đang có một chút thắc mắc về neural network đó là: trong bài toán binary classification thì mình có thể dùng 1 output hoặc 2 output đúng ko ạ?, 2 cách trên sẽ khác nhau ntn?. Mọi người có thể cho em 1 số key-words về vấn đề này để search dc ko ạ, em đã thử search nhưng tiếc là kết quả nhận về chưa dc mong muốn lắm :(( Thank mọi người nhiều ạ!!!",#ML	#DL	#NN,,,,
"Mọi người cho em hỏi, có cách nào tìm được thông tin các nhà máy/ công ty có lượng tiêu thụ điện cao không ạ. Cám ơn mọi người đã đọc tin!","Mọi người cho em hỏi, có cách nào tìm được thông tin các nhà máy/ công ty có lượng tiêu thụ điện cao không ạ. Cám ơn mọi người đã đọc tin!",,,,,
"Mọi người ơi cho em hỏi ngoài sử dụng CLAHE để equalize histogram của ảnh thì mọi người còn dùng cái gì nữa không nhỉ, em chưa biết phải preprocess ảnh ntn nữa tại dataset của em điều kiện ánh sáng rất khác nhau","Mọi người ơi cho em hỏi ngoài sử dụng CLAHE để equalize histogram của ảnh thì mọi người còn dùng cái gì nữa không nhỉ, em chưa biết phải preprocess ảnh ntn nữa tại dataset của em điều kiện ánh sáng rất khác nhau",,,,,
Xin phép chia sẻ cùng mọi người bài giảng về Ứng dụng của AI và Data Science trong giải mã gen ạ.,Xin phép chia sẻ cùng mọi người bài giảng về Ứng dụng của AI và Data Science trong giải mã gen ạ.,,,,,
"Chào mọi người mình có một bài toán như thế này
Đầu vào là những tấm ảnh lớn khoang 10.000 x 1024,
Cần kiểm tra bộ phận bên trong có bị hư không kích thước khoang 300 x 200￼
Mình đã thử với yolo5, detectron, kết quả dưới năm mươi phần trăm￼ là đúng.
Xin hỏi trong trường hợp này mọi người sẽ làm gì￼? Cảm ơn mọi người rất nhiều
￼","Chào mọi người mình có một bài toán như thế này Đầu vào là những tấm ảnh lớn khoang 10.000 x 1024, Cần kiểm tra bộ phận bên trong có bị hư không kích thước khoang 300 x 200 Mình đã thử với yolo5, detectron, kết quả dưới năm mươi phần trăm là đúng. Xin hỏi trong trường hợp này mọi người sẽ làm gì? Cảm ơn mọi người rất nhiều",,,,,
"[ACNE SCAN - ỨNG DỤNG DI ĐỘNG CHĂM SÓC DA ÁP DỤNG AI ĐẦU TIÊN TẠI VIỆT NAM .... CÓ MÀU HỒNG.]
Chào mọi người,
Chúng mình là TNT Team - sinh viên năm 4 ngành Khoa học Máy tính và tụi mình muốn giới thiệu đến mọi người ứng dụng điện thoại ACNE SCAN áp dụng công nghệ Trí tuệ nhân tạo giúp bạn:
Đánh giá tình trạng da mặt trên thang 4 mức độ thông qua các loại mụn gặp phải.
Gợi ý sản phẩm chăm sóc da theo độ yêu thích người dùng trong hệ thống.
Gợi ý lộ trình chăm sóc da
Theo dõi tình trạng da mặt qua Nhật ký da mặt
Báo thức việc sử dụng sản phẩm chăm sóc da
Để có thể tải ứng dụng và trải nghiệm:
Bạn vào CH play và tìm kiếm từ khóa Acne Scan hoặc nhấn vào đường link: https://play.google.com/store/apps/details?id=tntteam.detectacne
Thực hiện việc tải ứng dụng Acne Scan vào máy
Đăng ký tài khoản và bắt đầu trải nghiệm ứng dụng
Chúng mình tin rằng ứng dụng sẽ giúp việc chăm sóc da trở nên dễ dàng và tiện lợi hơn ngay tại nhà. Sản phẩm này cũng chính là luận văn tốt nghiệp của nhóm mình, hình của các bạn được lưu trữ bảo mật chỉ nhằm mục đích cải thiện model chứ không nhằm mục đích thương mại.
Là một trong những group có tiếng về AI/Ứng dụng điện thoại/Công nghệ, mình hy vọng các bạn có thể đóng góp ý kiến về độ chính xác mô hình phân loại mụn hay gợi ý sản phẩm của nhóm/các tính năng sản phẩm. Mọi ý kiến đóng góp của các bạn sẽ góp phần hoàn thiện sản phẩm hơn ạ.
Hiện tại ứng dụng đang có mặt trên CH play dành cho các thiết bị Android với phiên bản 2.1.0. 
Để góp ý cho tụi mình sau khi sử dụng ứng dụng, bạn có thể điền form khảo sát bên dưới:
https://tinyurl.com/feedback-tntacnescan
Trong quá trình sử dụng ứng dụng, nếu bạn gặp bất cứ vấn đề gì, bạn có thể inbox mình qua facebook cá nhân hoặc gửi mail vào hộp thư tntteam.hcmut@gmail.com.
Chúng mình xin chân thành cảm ơn và hy vọng nhận được sự góp ý của mọi người <3","[ACNE SCAN - ỨNG DỤNG DI ĐỘNG CHĂM SÓC DA ÁP DỤNG AI ĐẦU TIÊN TẠI VIỆT NAM .... CÓ MÀU HỒNG.] Chào mọi người, Chúng mình là TNT Team - sinh viên năm 4 ngành Khoa học Máy tính và tụi mình muốn giới thiệu đến mọi người ứng dụng điện thoại ACNE SCAN áp dụng công nghệ Trí tuệ nhân tạo giúp bạn: Đánh giá tình trạng da mặt trên thang 4 mức độ thông qua các loại mụn gặp phải. Gợi ý sản phẩm chăm sóc da theo độ yêu thích người dùng trong hệ thống. Gợi ý lộ trình chăm sóc da Theo dõi tình trạng da mặt qua Nhật ký da mặt Báo thức việc sử dụng sản phẩm chăm sóc da Để có thể tải ứng dụng và trải nghiệm: Bạn vào CH play và tìm kiếm từ khóa Acne Scan hoặc nhấn vào đường link: https://play.google.com/store/apps/details?id=tntteam.detectacne Thực hiện việc tải ứng dụng Acne Scan vào máy Đăng ký tài khoản và bắt đầu trải nghiệm ứng dụng Chúng mình tin rằng ứng dụng sẽ giúp việc chăm sóc da trở nên dễ dàng và tiện lợi hơn ngay tại nhà. Sản phẩm này cũng chính là luận văn tốt nghiệp của nhóm mình, hình của các bạn được lưu trữ bảo mật chỉ nhằm mục đích cải thiện model chứ không nhằm mục đích thương mại. Là một trong những group có tiếng về AI/Ứng dụng điện thoại/Công nghệ, mình hy vọng các bạn có thể đóng góp ý kiến về độ chính xác mô hình phân loại mụn hay gợi ý sản phẩm của nhóm/các tính năng sản phẩm. Mọi ý kiến đóng góp của các bạn sẽ góp phần hoàn thiện sản phẩm hơn ạ. Hiện tại ứng dụng đang có mặt trên CH play dành cho các thiết bị Android với phiên bản 2.1.0. Để góp ý cho tụi mình sau khi sử dụng ứng dụng, bạn có thể điền form khảo sát bên dưới: https://tinyurl.com/feedback-tntacnescan Trong quá trình sử dụng ứng dụng, nếu bạn gặp bất cứ vấn đề gì, bạn có thể inbox mình qua facebook cá nhân hoặc gửi mail vào hộp thư tntteam.hcmut@gmail.com. Chúng mình xin chân thành cảm ơn và hy vọng nhận được sự góp ý của mọi người <3",,,,,
"Mình xin được chia sẻ với mọi người trong nhóm về chương trình ươm tạo Lab2Market. Đây là 1 chương trình hữu ích, đồng hành và hỗ trợ các nhóm nghiên cứu, sáng chế công nghệ có thể hoàn thiện, đưa sản phẩm phát minh của mình ra thị trường.
Vào lúc 15h thứ 6 hàng tuần, Lab2Market có tổ chức buổi đối thoại trực tuyến trên facebook nhằm giải đáp thắc mắc thắc mắc về chương trình. Tham gia vào sự kiện, mọi người sẽ có cơ hội tương tác cùng những vị mentor dày dạn kinh nghiệm trên đa lĩnh vực và lắng nghe những chia sẻ kinh nghiệm qúy báu của họ. 🌟🌟","Mình xin được chia sẻ với mọi người trong nhóm về chương trình ươm tạo Lab2Market. Đây là 1 chương trình hữu ích, đồng hành và hỗ trợ các nhóm nghiên cứu, sáng chế công nghệ có thể hoàn thiện, đưa sản phẩm phát minh của mình ra thị trường. Vào lúc 15h thứ 6 hàng tuần, Lab2Market có tổ chức buổi đối thoại trực tuyến trên facebook nhằm giải đáp thắc mắc thắc mắc về chương trình. Tham gia vào sự kiện, mọi người sẽ có cơ hội tương tác cùng những vị mentor dày dạn kinh nghiệm trên đa lĩnh vực và lắng nghe những chia sẻ kinh nghiệm qúy báu của họ.",,,,,
Em chào mọi người. Hiện VinAI đang có 1 workshop trình bày về những papers được published tại CVPR 2021. Mời mọi người tham gia ạ.,Em chào mọi người. Hiện VinAI đang có 1 workshop trình bày về những papers được published tại CVPR 2021. Mời mọi người tham gia ạ.,,,,,
"Xin chào anh chị, có anh chị nào sử dụng label generated from Class Attention Map (CAM) (như hình dưới) chưa ạ? Em đang làm bài toán: brain vessel segmentation (vessel tree - small & complex shape), anh chị thấy có make sense trong trường hợp này nếu e dùng CAM để generated labels ko ạ? Em cảm ơn!
Em cảm ơn Anh/Chị Admin đã duyệt bài ạ","Xin chào anh chị, có anh chị nào sử dụng label generated from Class Attention Map (CAM) (như hình dưới) chưa ạ? Em đang làm bài toán: brain vessel segmentation (vessel tree - small & complex shape), anh chị thấy có make sense trong trường hợp này nếu e dùng CAM để generated labels ko ạ? Em cảm ơn! Em cảm ơn Anh/Chị Admin đã duyệt bài ạ",,,,,
"Mình có câu hỏi nhỏ muốn hỏi mọi người.
Đó là tại sao người ta hay dùng BatchNorm với CNN và LayerNorm với RNN ạ? Advantages và Disadvantages của hai loại này là gì?",Mình có câu hỏi nhỏ muốn hỏi mọi người. Đó là tại sao người ta hay dùng BatchNorm với CNN và LayerNorm với RNN ạ? Advantages và Disadvantages của hai loại này là gì?,,,,,
Một bài viết nho nhỏ của mình về overfitting và underfitting. Mong nhận được phản hồi từ quí bạn và các vị để được hoàn thiện hơn.,Một bài viết nho nhỏ của mình về overfitting và underfitting. Mong nhận được phản hồi từ quí bạn và các vị để được hoàn thiện hơn.,,,,,
"Chào mọi người,
Em muốn tìm mua sách ML cơ bản mà không còn thấy bán nữa. Vậy anh/chị nào có sách mà đang không dùng có thể pass lại cho em được không ạ? em cảm ơn nhiều","Chào mọi người, Em muốn tìm mua sách ML cơ bản mà không còn thấy bán nữa. Vậy anh/chị nào có sách mà đang không dùng có thể pass lại cho em được không ạ? em cảm ơn nhiều",,,,,
"CoAtNet: sự kết hợp giữa 2 kiến trúc Convolution và Attention cho tất cả các dữ liệu có kích thước khác nhau.
Kết quả đạt được hiệu năng SoTA trên các nền tảng tính toán và các datasets khác nhau.
Accuracy đạt được là 88.56% top-1 ImageNet, tương đương với ViT-huge nhưng cần ít hơn 23 lần data so với ViT-huge
Bài báo vừa mới đăng tại đây
https://arxiv.org/abs/2106.04803","CoAtNet: sự kết hợp giữa 2 kiến trúc Convolution và Attention cho tất cả các dữ liệu có kích thước khác nhau. Kết quả đạt được hiệu năng SoTA trên các nền tảng tính toán và các datasets khác nhau. Accuracy đạt được là 88.56% top-1 ImageNet, tương đương với ViT-huge nhưng cần ít hơn 23 lần data so với ViT-huge Bài báo vừa mới đăng tại đây https://arxiv.org/abs/2106.04803",,,,,
"Vào trang của tổng cục thống kê xem số liệu của Việt Nam thấy một bài tổng kết dài rất nhiều chữ và số mà không có lấy một cái biểu đồ.
Các bạn trong group thử lấy dữ liệu trong báo cáo này để vẽ các biểu đồ minh họa xem.
===================================
Cập nhật: File infographic được cung cấp ở thanh bên phải của trang https://www.gso.gov.vn/wp-content/uploads/2020/12/T12.2020-Trang-41-46.pdf. Các biểu đồ khá đẹp, hơi tiếc là chúng không được gán kèm vào nội dung web để người đọc dễ theo dõi.","Vào trang của tổng cục thống kê xem số liệu của Việt Nam thấy một bài tổng kết dài rất nhiều chữ và số mà không có lấy một cái biểu đồ. Các bạn trong group thử lấy dữ liệu trong báo cáo này để vẽ các biểu đồ minh họa xem. =================================== Cập nhật: File infographic được cung cấp ở thanh bên phải của trang https://www.gso.gov.vn/wp-content/uploads/2020/12/T12.2020-Trang-41-46.pdf. Các biểu đồ khá đẹp, hơi tiếc là chúng không được gán kèm vào nội dung web để người đọc dễ theo dõi.",,,,,
"Em mới bắt đầu học ML, có một thắc mắc nhỏ về Batch Norm mong mọi người giải đáp giúp. 
Theo em hiểu thì mục đích của BN là shift giá trị về zero mean với scale variance là 1. Vậy tại sao sau khi đã shift/scale rồi lại phải recover lại lần nữa với gamma và beta? Chẳng phải như vậy giá trị sau đó sẽ bằng chính xác với giá trị trước khi normalize sao (nghĩa là mất đi tính chất mean=0, variance=1)?
Với tanh function thì giá trị đã nằm trong khoảng zero-mean và variance là 1 rồi thì có cần dùng BN không nếu mình đảm bảo được dữ liệu không rơi vào vùng bão hòa?
Em cám ơn ạ!","Em mới bắt đầu học ML, có một thắc mắc nhỏ về Batch Norm mong mọi người giải đáp giúp. Theo em hiểu thì mục đích của BN là shift giá trị về zero mean với scale variance là 1. Vậy tại sao sau khi đã shift/scale rồi lại phải recover lại lần nữa với gamma và beta? Chẳng phải như vậy giá trị sau đó sẽ bằng chính xác với giá trị trước khi normalize sao (nghĩa là mất đi tính chất mean=0, variance=1)? Với tanh function thì giá trị đã nằm trong khoảng zero-mean và variance là 1 rồi thì có cần dùng BN không nếu mình đảm bảo được dữ liệu không rơi vào vùng bão hòa? Em cám ơn ạ!",,,,,
"Dạ em chào mọi người, cho em hỏi là tại sao fine-tune với higher resolution lại tốt hơn vậy ạ. Nếu theo như bài báo thì fine-tune với higher resolution sẽ dẫn tới position embedding của pre-train vô nghĩa vậy sao lại nói fine-tune với higher resolution lại tốt hơn","Dạ em chào mọi người, cho em hỏi là tại sao fine-tune với higher resolution lại tốt hơn vậy ạ. Nếu theo như bài báo thì fine-tune với higher resolution sẽ dẫn tới position embedding của pre-train vô nghĩa vậy sao lại nói fine-tune với higher resolution lại tốt hơn",,,,,
E chào mn . Mn cho em hỏi về số trials trong tuner hyperband với ạ. E muốn hỏi là cái hyperband này nó tính số trials theo công thức nào hay nó ngẫu nhiên ạ ? E muốn đang muốn giảm số trial nhưng chưa biết làm cách nào ạ. E cảm ơn mn ạ! Đây là bài về hyperband ạ! https://keras-team.github.io/keras-tuner/documentation/tuners/?fbclid=IwAR3j2ifKd0qCWn2GyeI2PJ8gEk-JBYn__hotkh4_AGBJ9pqNyU2cY9mjlDY,E chào mn . Mn cho em hỏi về số trials trong tuner hyperband với ạ. E muốn hỏi là cái hyperband này nó tính số trials theo công thức nào hay nó ngẫu nhiên ạ ? E muốn đang muốn giảm số trial nhưng chưa biết làm cách nào ạ. E cảm ơn mn ạ! Đây là bài về hyperband ạ! https://keras-team.github.io/keras-tuner/documentation/tuners/?fbclid=IwAR3j2ifKd0qCWn2GyeI2PJ8gEk-JBYn__hotkh4_AGBJ9pqNyU2cY9mjlDY,,,,,
"một số trang web hay tớ note lại ngày trước mà có thể sẽ có ích với một trong số các bạn.
have a nice reading :)
https://hosjiu.gitlab.io/personal-blog/2020/11/03/DeepBlogs/",một số trang web hay tớ note lại ngày trước mà có thể sẽ có ích với một trong số các bạn. have a nice reading :) https://hosjiu.gitlab.io/personal-blog/2020/11/03/DeepBlogs/,,,,,
"#ViT #VisionTransformer
Em chào mọi người. Trong bài ViT, em có thấy tác giả sử dụng thêm 1 thành phần x_class để đại diện cho ảnh đầu vào và phục vụ mục đích phân loại (ảnh 1). Mọi người cho em hỏi tại sao cần thêm thành phần đó với ạ. Và khi đó các thành phần còn lại (x_p^1, x_p^2, ...) như trong ảnh 2 đóng vai trò gì vậy ạ. Bởi vì ViT được sử dụng trong TransUNet (ảnh 3), z không cần bổ sung thành phần x_class và bản thân z lúc đó được coi là đại diện của ảnh đầu vào luôn.
Xin lỗi mọi người vì câu cú lủng củng.","Em chào mọi người. Trong bài ViT, em có thấy tác giả sử dụng thêm 1 thành phần x_class để đại diện cho ảnh đầu vào và phục vụ mục đích phân loại (ảnh 1). Mọi người cho em hỏi tại sao cần thêm thành phần đó với ạ. Và khi đó các thành phần còn lại (x_p^1, x_p^2, ...) như trong ảnh 2 đóng vai trò gì vậy ạ. Bởi vì ViT được sử dụng trong TransUNet (ảnh 3), z không cần bổ sung thành phần x_class và bản thân z lúc đó được coi là đại diện của ảnh đầu vào luôn. Xin lỗi mọi người vì câu cú lủng củng.",#ViT	#VisionTransformer,,,,
"Đây là sơ đồ NN trong word2vec.
W (hidden layer weight) sẽ được dùng để làm vector cho cái từ mà mình muốn embed.
Cho em hỏi là tại sao mình lại không dùng W’ (output layer weight)?",Đây là sơ đồ NN trong word2vec. W (hidden layer weight) sẽ được dùng để làm vector cho cái từ mà mình muốn embed. Cho em hỏi là tại sao mình lại không dùng W’ (output layer weight)?,,,,,
"Mọi người giúp em với ạ
em mới học mảng nhận diện khuôn mặt dungf thư viện face-reconigtion và báo lỗi chưa cài tool C++ trong lúc buil dlib
mà trong VS code thì e có rồi
mn có thể giúp em không ạ",Mọi người giúp em với ạ em mới học mảng nhận diện khuôn mặt dungf thư viện face-reconigtion và báo lỗi chưa cài tool C++ trong lúc buil dlib mà trong VS code thì e có rồi mn có thể giúp em không ạ,,,,,
FYI:,FYI:,,,,,
"em chào mọi người, em có thắc mắc về bản chất neural net là gì và nó đã làm gì để tìm được hàm f fit với data. em tìm hiểu thì được biết là bản chất hidden layer là việc ánh xạ sang một chiều không gian mới và với việc dùng cho bài toán phân lớp thì mỗi node trong layer này là 1 siêu phẳng phân vùng không gian layer này thành 2 nửa không gian. vậy với bài toán xor thì em hình dung không gian mới như ảnh bên dưới, vậy thì em muốn hiểu là cái siêu phẳng phân tách dữ liệu với 2 đường thẳng trong bài viết https://machinelearningcoban.com/2017/02/24/mlp/ của anh Tiệp có quan hệ gì (liệu các hiểu của em đã đúng chưa ?). và việc tính tổng các đường vào một node vd w1x1+w2x2+w3x3 +... sau đó activate rồi lại thực hiện nhiều tổng dạng như này qua các layer thì nó đã làm kiểu gì để thành 1 đường phân tách dữ liệu cuối cùng ( liệu có phải việc tính toán lồng các đường này và acivate nonliner nhiều lần đã tạo ra một hồi quy đa biến không ạ) mong mọi người giải thích giúp em, em cảm ơn.","em chào mọi người, em có thắc mắc về bản chất neural net là gì và nó đã làm gì để tìm được hàm f fit với data. em tìm hiểu thì được biết là bản chất hidden layer là việc ánh xạ sang một chiều không gian mới và với việc dùng cho bài toán phân lớp thì mỗi node trong layer này là 1 siêu phẳng phân vùng không gian layer này thành 2 nửa không gian. vậy với bài toán xor thì em hình dung không gian mới như ảnh bên dưới, vậy thì em muốn hiểu là cái siêu phẳng phân tách dữ liệu với 2 đường thẳng trong bài viết https://machinelearningcoban.com/2017/02/24/mlp/ của anh Tiệp có quan hệ gì (liệu các hiểu của em đã đúng chưa ?). và việc tính tổng các đường vào một node vd w1x1+w2x2+w3x3 +... sau đó activate rồi lại thực hiện nhiều tổng dạng như này qua các layer thì nó đã làm kiểu gì để thành 1 đường phân tách dữ liệu cuối cùng ( liệu có phải việc tính toán lồng các đường này và acivate nonliner nhiều lần đã tạo ra một hồi quy đa biến không ạ) mong mọi người giải thích giúp em, em cảm ơn.",,,,,
"Mn cho em hỏi câu này hơi ngốc ạ. Chuyện là em mới bắt đầu học Machine Learning (mới ở mức lí thuyết thôi ạ), hiện giờ em muốn làm một dự án để nâng cao khả năng thực hành. Nhưng do còn tay mơ nên em lên Github và tải một số dự án về để từ đó tập làm quen dần. Nhưng em chạy code thì bị vướng chỗ lỗi như trong ảnh ( lỗi không có tập tin hoặc thư mục ). Hiện tại, em đã có được Folder của cần thêm của Project đó nhưng làm cách nào để thêm vào Google Colab để từ đó chạy code ạ. Em cảm ơn mn nhiều.","Mn cho em hỏi câu này hơi ngốc ạ. Chuyện là em mới bắt đầu học Machine Learning (mới ở mức lí thuyết thôi ạ), hiện giờ em muốn làm một dự án để nâng cao khả năng thực hành. Nhưng do còn tay mơ nên em lên Github và tải một số dự án về để từ đó tập làm quen dần. Nhưng em chạy code thì bị vướng chỗ lỗi như trong ảnh ( lỗi không có tập tin hoặc thư mục ). Hiện tại, em đã có được Folder của cần thêm của Project đó nhưng làm cách nào để thêm vào Google Colab để từ đó chạy code ạ. Em cảm ơn mn nhiều.",,,,,
"Xin phép mọi người ạ, có thể không liên quan đến ML lắm
Ví dụ như em muốn làm 1 app tính giá nhà cho thuê. Em có được data như diện tích, số phòng ngủ, vị trí cách trung tâm và giá cho thuê
Nếu không phải đụng đến code thì có vba hay plugins cho exel hoặc app bên thứ 3 có thể xử lí số liệu như này nhỉ?","Xin phép mọi người ạ, có thể không liên quan đến ML lắm Ví dụ như em muốn làm 1 app tính giá nhà cho thuê. Em có được data như diện tích, số phòng ngủ, vị trí cách trung tâm và giá cho thuê Nếu không phải đụng đến code thì có vba hay plugins cho exel hoặc app bên thứ 3 có thể xử lí số liệu như này nhỉ?",,,,,
"Em chào mọi người, em đang làm thuật toán Wiener Filter (image restoration) và có một vấn đề là chia 1 số phức (tử số) cho một số thực(R) trong openCV
Hướng làm của em là split số phức đó ra làm 2 channel[0] và channel[1] rồi sau đó dùng hàm divide để chia 2 channel đó cho số thực R, rồi cộng 2 kết quả đó lại với nhau. Không biết rằng em đang đi có đúng hướng không ạ? Anh chị em nào biết thì chỉ em với ạ
Em cám ơn
#WienerFilter #ComplexNumber","Em chào mọi người, em đang làm thuật toán Wiener Filter (image restoration) và có một vấn đề là chia 1 số phức (tử số) cho một số thực(R) trong openCV Hướng làm của em là split số phức đó ra làm 2 channel[0] và channel[1] rồi sau đó dùng hàm divide để chia 2 channel đó cho số thực R, rồi cộng 2 kết quả đó lại với nhau. Không biết rằng em đang đi có đúng hướng không ạ? Anh chị em nào biết thì chỉ em với ạ Em cám ơn",#WienerFilter	#ComplexNumber,,,,
"Xin phép được chia sẻ thông tin khoá học lập trình Scratch/Python ONLINE Miễn Phí từ STEAM for Vietnam dành
👉👉Link đăng ký: https://bit.ly/3x1HOKc
STEAM for Vietnam đã chính thức trở lại với Summer Coding Bootcamp 2021, hãy tham gia học lập trình hoàn toàn MIỄN PHÍ cùng các chuyên gia công nghệ hàng đầu từ Google, Microsoft, Chan Zuckerberg Initiative, Twitter, Amazon và 30,000 học sinh người Việt tại 30 quốc gia.
✅CS 001- Nhập môn về Tư duy Máy tính và Ngôn ngữ Lập trình Scratch.
✅CS 101 - Nhập môn Khoa học Máy tính với Python
Hai khoá học dự kiến sẽ diễn ra theo hình thức livestream trực tuyến vào sáng Chủ nhật hàng tuần, bắt đầu từ ngày 27/6/2021.
🎉🎉Đơn đăng ký sẽ đóng vào ngày 20/6/2021, các phụ huynh hãy nhanh tay nộp nhé ạ.","Xin phép được chia sẻ thông tin khoá học lập trình Scratch/Python ONLINE Miễn Phí từ STEAM for Vietnam dành Link đăng ký: https://bit.ly/3x1HOKc STEAM for Vietnam đã chính thức trở lại với Summer Coding Bootcamp 2021, hãy tham gia học lập trình hoàn toàn MIỄN PHÍ cùng các chuyên gia công nghệ hàng đầu từ Google, Microsoft, Chan Zuckerberg Initiative, Twitter, Amazon và 30,000 học sinh người Việt tại 30 quốc gia. CS 001- Nhập môn về Tư duy Máy tính và Ngôn ngữ Lập trình Scratch. CS 101 - Nhập môn Khoa học Máy tính với Python Hai khoá học dự kiến sẽ diễn ra theo hình thức livestream trực tuyến vào sáng Chủ nhật hàng tuần, bắt đầu từ ngày 27/6/2021. Đơn đăng ký sẽ đóng vào ngày 20/6/2021, các phụ huynh hãy nhanh tay nộp nhé ạ.",,,,,
"Chào mọi người, mình là dev chính trong dự án Vietstock scraper mà bạn Khanh Pham có đăng trong nhóm ở bài này: https://www.facebook.com/groups/machinelearningcoban/permalink/1229553207502084/
Ban đầu project này chỉ để phục vụ việc nghiên cứu của bọn mình, nên code và trình bày có thể không được chỉnh chu lắm. Đồng thời, cách sử dụng nhìn có vẻ hơi phức tạp nên nếu có nhiều người quan tâm thì mình có thể viết nhanh một hướng dẫn bằng tiếng Việt để dễ tiếp cận hơn.
Về quy mô, mình thấy trên Vietstock có khoảng 3000 mã chứng khoán và khoảng 10 đầu API khác nhau (thông tin tài chính, thông tin chủ sở hữu, cấu thành doanh nghiệp, đường dẫn kéo các tài liệu dưới dạng PDF, etc.) và con scraper này technically hoàn toàn có thể kéo hết chỗ thông tin đó (giả định là bạn không bị chặn). Tuy nhiên, hiện tại mình mới chỉ code hoàn thiện cho đầu API về thông tin tài chính thôi. Nếu có nhiều người quan tâm về các đầu API khác, project này có thể sẽ tiếp tục được phát triển để kéo được nhiều thứ hơn.
Về tốc độ kéo, vì sử dụng scrapy nên con scraper này có tốc độ khá ổn cho việc kéo diện rộng và sử dụng proxy cùng lúc. Tuy nhiên, mình khuyến cáo mọi người chỉ nên kéo những mã, hoặc ngành mà các bạn quan tâm, để tránh làm quá tải server của Vietstock.
Cảm ơn mọi người!","Chào mọi người, mình là dev chính trong dự án Vietstock scraper mà bạn Khanh Pham có đăng trong nhóm ở bài này: https://www.facebook.com/groups/machinelearningcoban/permalink/1229553207502084/ Ban đầu project này chỉ để phục vụ việc nghiên cứu của bọn mình, nên code và trình bày có thể không được chỉnh chu lắm. Đồng thời, cách sử dụng nhìn có vẻ hơi phức tạp nên nếu có nhiều người quan tâm thì mình có thể viết nhanh một hướng dẫn bằng tiếng Việt để dễ tiếp cận hơn. Về quy mô, mình thấy trên Vietstock có khoảng 3000 mã chứng khoán và khoảng 10 đầu API khác nhau (thông tin tài chính, thông tin chủ sở hữu, cấu thành doanh nghiệp, đường dẫn kéo các tài liệu dưới dạng PDF, etc.) và con scraper này technically hoàn toàn có thể kéo hết chỗ thông tin đó (giả định là bạn không bị chặn). Tuy nhiên, hiện tại mình mới chỉ code hoàn thiện cho đầu API về thông tin tài chính thôi. Nếu có nhiều người quan tâm về các đầu API khác, project này có thể sẽ tiếp tục được phát triển để kéo được nhiều thứ hơn. Về tốc độ kéo, vì sử dụng scrapy nên con scraper này có tốc độ khá ổn cho việc kéo diện rộng và sử dụng proxy cùng lúc. Tuy nhiên, mình khuyến cáo mọi người chỉ nên kéo những mã, hoặc ngành mà các bạn quan tâm, để tránh làm quá tải server của Vietstock. Cảm ơn mọi người!",,,,,
"Chào mọi người, mọi người có ai từng train deeplab từ tensorflow sau đó visualize lên thì ảnh predict toàn đen không ạ? Em có dùng deeplab trên tensorflow để train thì bị vậy. Mọi người có cách nào khắc phục không ạ?
Link deeplab:
https://github.com/tensorflow/models/tree/master/research/deeplab
Link data của em:
https://drive.google.com/drive/folders/1OG6RmoXijCgxYAILyw3dVDdDshjRCHJi?usp=sharing","Chào mọi người, mọi người có ai từng train deeplab từ tensorflow sau đó visualize lên thì ảnh predict toàn đen không ạ? Em có dùng deeplab trên tensorflow để train thì bị vậy. Mọi người có cách nào khắc phục không ạ? Link deeplab: https://github.com/tensorflow/models/tree/master/research/deeplab Link data của em: https://drive.google.com/drive/folders/1OG6RmoXijCgxYAILyw3dVDdDshjRCHJi?usp=sharing",,,,,
"Em chào mọi người ạ. Hiện tại em đang làm về sentence classification với G-MLP. Tuy nhiên theo source code (https://github.com/lucidrains/g-mlp-pytorch) thì ở phần G-MLP lại tạo ra embedding cho từng word trong sentence.
Do em còn khá mới với NLP nên em muốn hỏi mọi người có cách nào để tạo ra sentence embedding để classification không ạ? Em thấy trong paper cũng có kết quả trên SST-2 nên em muốn hỏi tác giả đã tạo sentence embedding để classification bằng cách nào ạ?
Em xin cảm ơn mọi người ạ.",Em chào mọi người ạ. Hiện tại em đang làm về sentence classification với G-MLP. Tuy nhiên theo source code (https://github.com/lucidrains/g-mlp-pytorch) thì ở phần G-MLP lại tạo ra embedding cho từng word trong sentence. Do em còn khá mới với NLP nên em muốn hỏi mọi người có cách nào để tạo ra sentence embedding để classification không ạ? Em thấy trong paper cũng có kết quả trên SST-2 nên em muốn hỏi tác giả đã tạo sentence embedding để classification bằng cách nào ạ? Em xin cảm ơn mọi người ạ.,,,,,
"Chào mọi người.
Em có một thắc mắc sau mong mọi người giải đáp.
Em đang tìm hiểu về Deep metric learning, qua một thời gian tìm hiểu thì em biết được rằng Deep metric learning được sử dụng để giải quyết một số vấn đề trong sofmax function như sau:
1/ Khi dữ liệu có quá nhiều classes.
 - Ví dụ trong bài toán nhận diện khuôn mặt, phải phân loại khá nhiều khuôn mặt khác nhau (có thể là 100 hoặc 1000 classes)
2/ Khi số dữ liệu trong mỗi class là hạn chế.
3/ Có thể không cần train lại toàn bộ mô hình khi có dữ liệu thuộc class mới ( class không nằm trong bộ dữ liệu đã train).

Em có một thắc mắc là làm sao Deep metric learning có thể giải quyết được những vấn đề trên ạ? hoặc có tài liệu nào giải thích mong anh chị chia sẻ. Em cảm ơn rất nhiều ạ.
Một bài viết mà em đọc gần đây: https://towardsdatascience.com/deep-metric-learning-76fa0a5a415f","Chào mọi người. Em có một thắc mắc sau mong mọi người giải đáp. Em đang tìm hiểu về Deep metric learning, qua một thời gian tìm hiểu thì em biết được rằng Deep metric learning được sử dụng để giải quyết một số vấn đề trong sofmax function như sau: 1/ Khi dữ liệu có quá nhiều classes. - Ví dụ trong bài toán nhận diện khuôn mặt, phải phân loại khá nhiều khuôn mặt khác nhau (có thể là 100 hoặc 1000 classes) 2/ Khi số dữ liệu trong mỗi class là hạn chế. 3/ Có thể không cần train lại toàn bộ mô hình khi có dữ liệu thuộc class mới ( class không nằm trong bộ dữ liệu đã train). Em có một thắc mắc là làm sao Deep metric learning có thể giải quyết được những vấn đề trên ạ? hoặc có tài liệu nào giải thích mong anh chị chia sẻ. Em cảm ơn rất nhiều ạ. Một bài viết mà em đọc gần đây: https://towardsdatascience.com/deep-metric-learning-76fa0a5a415f",,,,,
Em muốn hỏi một chút về Hybrid Architecture được nhắc tới trong ViT (Vision Transformer). Ở đây theo em đọc là image sẽ được đưa vào CNN trước để extract feature map. Điều này có nghĩa là sao ạ tại vì trong ViT thì phải chia ảnh input ra thành từng patch nhỏ vậy thì Hybrid được sử dụng như thế nào ạ. Em cảm ơn.,Em muốn hỏi một chút về Hybrid Architecture được nhắc tới trong ViT (Vision Transformer). Ở đây theo em đọc là image sẽ được đưa vào CNN trước để extract feature map. Điều này có nghĩa là sao ạ tại vì trong ViT thì phải chia ảnh input ra thành từng patch nhỏ vậy thì Hybrid được sử dụng như thế nào ạ. Em cảm ơn.,,,,,
"Trong mô hình content base Recommendation System cho 1 trang thương mại điện tử, cụ thể là mình làm về kinh doanh điện thoại, laptop thì mọi người nghĩ lúc training mô hình mình có nên cho cái mô tả sản phẩm như hình vào để training không mọi người? Mọi người thấy cái thông tin này có hữu ích cho việc training không?
Tại hôm nay mình import dữ liệu mới, nghi là do cái nội dung này nó tràng giang quá nên lúc chạy nó bị lỗi out of range luôn nên mình sợ quá.","Trong mô hình content base Recommendation System cho 1 trang thương mại điện tử, cụ thể là mình làm về kinh doanh điện thoại, laptop thì mọi người nghĩ lúc training mô hình mình có nên cho cái mô tả sản phẩm như hình vào để training không mọi người? Mọi người thấy cái thông tin này có hữu ích cho việc training không? Tại hôm nay mình import dữ liệu mới, nghi là do cái nội dung này nó tràng giang quá nên lúc chạy nó bị lỗi out of range luôn nên mình sợ quá.",,,,,
"[ASK] OVERFITTING TRONG TRANSFER LEARNING 
Chào các anh/chị,
Như em được học, overfitting là hiện tượng dữ liệu training quá nhỏ và không đủ để khái quát hóa cho những dữ liệu mới; do đó thuật toán đã quá fit vào training set, khiến thực tế sử dụng (metrics đo trên test set) rất kém.
Em đang thử nghiệm fine-tune một pretrained model đã được train trên tập dữ liệu rất lớn với 1 task gần với task mà em mong muốn. Cụ thể, em thêm 1 linear layer với đầu ra softmax vào cuối, sau đó trên model mới end-to-end với learning rate nhỏ.
Tập dữ liệu dành cho fine-tune của em không lớn, chỉ khoảng 4000 examples.
Kết quả: khi lấy weight ở epoch có val loss thấp nhất, metrics đo được trên tập test khá tốt. Tuy nhiên hiện tượng giống như overfitting đã xảy ra: chỉ sau khoảng 3 epoch, val loss bắt đầu tăng trong khi train loss vẫn tiếp tục giảm.
Câu hỏi của em là:
Liệu hiện tượng trên đây có phải là overfitting?
Nếu đúng, tại sao lại có hiện tượng trên? Bởi pretrained model đã được train trên tập dữ liệu rất lớn, hoàn toàn đủ khả năng generalize trong phạm vi các task có liên quan. Vậy overfitting đến từ đâu?
Em có thử freeze hết phần pretrained, kết quả thu được trên test set kém đi, và hiện tượng trên vẫn xảy ra. Nói model overfit do mỗi layer cuối có vẻ không thỏa đáng lắm.
Rất mong các anh chị giải thích giúp em các câu hỏi trên. Nếu có tài liệu nào nghiên cứu về hiện tượng này và cách khắc phục nữa thì càng tốt ạ.
Em xin cảm ơn!","[ASK] OVERFITTING TRONG TRANSFER LEARNING Chào các anh/chị, Như em được học, overfitting là hiện tượng dữ liệu training quá nhỏ và không đủ để khái quát hóa cho những dữ liệu mới; do đó thuật toán đã quá fit vào training set, khiến thực tế sử dụng (metrics đo trên test set) rất kém. Em đang thử nghiệm fine-tune một pretrained model đã được train trên tập dữ liệu rất lớn với 1 task gần với task mà em mong muốn. Cụ thể, em thêm 1 linear layer với đầu ra softmax vào cuối, sau đó trên model mới end-to-end với learning rate nhỏ. Tập dữ liệu dành cho fine-tune của em không lớn, chỉ khoảng 4000 examples. Kết quả: khi lấy weight ở epoch có val loss thấp nhất, metrics đo được trên tập test khá tốt. Tuy nhiên hiện tượng giống như overfitting đã xảy ra: chỉ sau khoảng 3 epoch, val loss bắt đầu tăng trong khi train loss vẫn tiếp tục giảm. Câu hỏi của em là: Liệu hiện tượng trên đây có phải là overfitting? Nếu đúng, tại sao lại có hiện tượng trên? Bởi pretrained model đã được train trên tập dữ liệu rất lớn, hoàn toàn đủ khả năng generalize trong phạm vi các task có liên quan. Vậy overfitting đến từ đâu? Em có thử freeze hết phần pretrained, kết quả thu được trên test set kém đi, và hiện tượng trên vẫn xảy ra. Nói model overfit do mỗi layer cuối có vẻ không thỏa đáng lắm. Rất mong các anh chị giải thích giúp em các câu hỏi trên. Nếu có tài liệu nào nghiên cứu về hiện tượng này và cách khắc phục nữa thì càng tốt ạ. Em xin cảm ơn!",,,,,
"E chào mọi người, hiện tại e đang làm bài tập nhận dạng hành động sd cảm biến Android bằng LSTM. E đã xuất model frozen.pb, nhưng khi đưa vào android thì em bị lỗi này. A/c nào đã từng bị lỗi này cho em cách khắc phục ạ ! Em xin cảm ơn","E chào mọi người, hiện tại e đang làm bài tập nhận dạng hành động sd cảm biến Android bằng LSTM. E đã xuất model frozen.pb, nhưng khi đưa vào android thì em bị lỗi này. A/c nào đã từng bị lỗi này cho em cách khắc phục ạ ! Em xin cảm ơn",,,,,
"Chào mọi người,
Hôm trước anh Khanh Blog có hỏi về project nào crawl dữ liệu về tài chính ở Việt Nam, em có nhớ đến một dịp bạn Viet Anh Tran va em có làm.
Em muốn share public repo của bạn em để cho anh chị có thể sử dụng. And shout out to my friend!","Chào mọi người, Hôm trước anh Khanh Blog có hỏi về project nào crawl dữ liệu về tài chính ở Việt Nam, em có nhớ đến một dịp bạn Viet Anh Tran va em có làm. Em muốn share public repo của bạn em để cho anh chị có thể sử dụng. And shout out to my friend!",,,,,
"Chào mọi người,
Phoenix Team của FUNiX xin chia sẻ với các bạn source code của nhóm ở FPT Edu Hackathon 2021 Challenge.
Nhóm có tham khảo ý tưởng từ RANZCR CLiP - Catheter and Line Position Challenge trên nền tảng Kaggle để xây dựng một ứng dụng có thể giúp các bác sĩ kiểm tra tình trạng ống thở của các bệnh nhân nhanh hơn, chính xác hơn và hiệu quả hơn.
Source code gồm 2 phần chính: Model và backend. Phần model, nhóm có tham khảo best solutions của các solution trong 20 bạn top đầu của Kaggle Challenge:
https://github.com/thaiminhpv/Doctor-Cyclop-Hackathon-2021
Phần backend, nhóm có sử dụng một số công nghệ: MeteorJS 2.1, NodeJS 12, MongoDB 4.2, AWS S3.
https://github.com/DAN3002/Doctors-Cyclop-Webapp
Web apps có một số thiết kế và tính năng mở rộng để hướng người dùng apps (các bác sĩ và chuyên gia) có thể:
Tập trung chú ý hơn vào các cases nguy hiểm trước.
Có bảng phân tích đánh giá một số thông số cơ bản, giúp ích cho việc phân tích và đánh giá nhanh chuẩn đoán cá nhân của mình và xu hướng chuẩn đoán chung.
Ngoài ra, app có cơ chế relabel và phân cấp trình độ (thành 2 mức, chuyên viên và chuyên gia), hỗ trợ việc đưa ra chuẩn đoán chính xác hơn trong các trường hợp nghi ngờ.
Bản demo của nhóm còn xa mới đạt đến được một sản phẩm AI hoàn chỉnh, nhưng mình nghĩ sẽ là một tài liệu hữu ích cho các bạn sinh viên muốn vọc vạch tìm hiểu về AI và MLOps.
Để hiểu rõ hơn về app, các bạn có thể xem video thuyết trình về app của nhóm ở đây:
https://www.youtube.com/watch?v=6-ee4TkQDLg
Một số hướng phát triển tiếp theo của nhóm (cả nhóm sẽ team up và tiếp tục hoàn thiện sau kỳ thi THPT quốc gia):
Tích hợp phần re-training vào nối tiếp với relabeling, giúp apps có thể tự động cập nhật lại model một cách nhanh chóng và hiệu quả hơn.
Tìm hiểu và đưa thêm các phân tích có ích hơn với các bác sĩ vào apps.
Có cơ chế lưu trữ và bảo mật dữ liệu tốt hơn (tích hợp thêm smart contract vào).
…
Nhóm gồm 4 thành viên Đình Anh Nguyễn Linh Phuong Phạm Vũ Thái Minh là các bạn học sinh cấp 3, hiện tại đang học tại FUNiX . Các bạn có câu hỏi hay thắc mắc gì cứ đặt câu hỏi dưới topic này hoặc trong phần “issue” của git, chúng mình sẽ cố gắng trả lời hết các thắc mắc.
Ha Na Nguyen
P/S: Trung tâm nghiên cứu và ứng dụng AI - QAI (FPT Software Quy Nhơn) đang trao chương trình học bổng Machine Learning và Data Science dành cho 200 học viên với cam kết công việc đầu ra với mức lương và đãi ngộ hấp dẫn. Toàn bộ chương trình học bổng với QAI sẽ được phụ trách bởi đội ngũ mentor kinh nghiệm từ Funix. Các bạn quan tâm có thể thử sức ngay tại đường link này nhé:
https://forms.gle/UFZMWBfPqtjYnKtQA
Đăng ký ngay để trở thành những ứng viên AI tài năng trong thời gian sớm nhất!","Chào mọi người, Phoenix Team của FUNiX xin chia sẻ với các bạn source code của nhóm ở FPT Edu Hackathon 2021 Challenge. Nhóm có tham khảo ý tưởng từ RANZCR CLiP - Catheter and Line Position Challenge trên nền tảng Kaggle để xây dựng một ứng dụng có thể giúp các bác sĩ kiểm tra tình trạng ống thở của các bệnh nhân nhanh hơn, chính xác hơn và hiệu quả hơn. Source code gồm 2 phần chính: Model và backend. Phần model, nhóm có tham khảo best solutions của các solution trong 20 bạn top đầu của Kaggle Challenge: https://github.com/thaiminhpv/Doctor-Cyclop-Hackathon-2021 Phần backend, nhóm có sử dụng một số công nghệ: MeteorJS 2.1, NodeJS 12, MongoDB 4.2, AWS S3. https://github.com/DAN3002/Doctors-Cyclop-Webapp Web apps có một số thiết kế và tính năng mở rộng để hướng người dùng apps (các bác sĩ và chuyên gia) có thể: Tập trung chú ý hơn vào các cases nguy hiểm trước. Có bảng phân tích đánh giá một số thông số cơ bản, giúp ích cho việc phân tích và đánh giá nhanh chuẩn đoán cá nhân của mình và xu hướng chuẩn đoán chung. Ngoài ra, app có cơ chế relabel và phân cấp trình độ (thành 2 mức, chuyên viên và chuyên gia), hỗ trợ việc đưa ra chuẩn đoán chính xác hơn trong các trường hợp nghi ngờ. Bản demo của nhóm còn xa mới đạt đến được một sản phẩm AI hoàn chỉnh, nhưng mình nghĩ sẽ là một tài liệu hữu ích cho các bạn sinh viên muốn vọc vạch tìm hiểu về AI và MLOps. Để hiểu rõ hơn về app, các bạn có thể xem video thuyết trình về app của nhóm ở đây: https://www.youtube.com/watch?v=6-ee4TkQDLg Một số hướng phát triển tiếp theo của nhóm (cả nhóm sẽ team up và tiếp tục hoàn thiện sau kỳ thi THPT quốc gia): Tích hợp phần re-training vào nối tiếp với relabeling, giúp apps có thể tự động cập nhật lại model một cách nhanh chóng và hiệu quả hơn. Tìm hiểu và đưa thêm các phân tích có ích hơn với các bác sĩ vào apps. Có cơ chế lưu trữ và bảo mật dữ liệu tốt hơn (tích hợp thêm smart contract vào). … Nhóm gồm 4 thành viên Đình Anh Nguyễn Linh Phuong Phạm Vũ Thái Minh là các bạn học sinh cấp 3, hiện tại đang học tại FUNiX . Các bạn có câu hỏi hay thắc mắc gì cứ đặt câu hỏi dưới topic này hoặc trong phần “issue” của git, chúng mình sẽ cố gắng trả lời hết các thắc mắc. Ha Na Nguyen P/S: Trung tâm nghiên cứu và ứng dụng AI - QAI (FPT Software Quy Nhơn) đang trao chương trình học bổng Machine Learning và Data Science dành cho 200 học viên với cam kết công việc đầu ra với mức lương và đãi ngộ hấp dẫn. Toàn bộ chương trình học bổng với QAI sẽ được phụ trách bởi đội ngũ mentor kinh nghiệm từ Funix. Các bạn quan tâm có thể thử sức ngay tại đường link này nhé: https://forms.gle/UFZMWBfPqtjYnKtQA Đăng ký ngay để trở thành những ứng viên AI tài năng trong thời gian sớm nhất!",,,,,
"#Checkpoint #deploy 

Hi mọi người, 
Hiện tại em đang deploy một backend server - ( Đại khái là push source code lên git hub). Trong đó, source em có lưu trữ 1 "" checkpoint.pth"" ( push lên bằng git lfs ). 
Vấn đề xảy ra khi code gọi:
 model.load_state_dict(""đường dẫn tới checkpoint.pth trong github"")
Xuất hiện lỗi :
UnpicklingError at / : invalid load key, 'v'.
Em có tìm hiểu và có thể hiểu là git lfs sẽ down một file nén về khi ta truy cập file large từ code => load_state_dict lỗi.
Em vẫn chưa tìm ra hướng giải quyết. mong mọi người nếu xử lý được mong giúp em với ạ!

Chúc group sức khỏe mùa covid! 
Cảm ơn mn!","Hi mọi người, Hiện tại em đang deploy một backend server - ( Đại khái là push source code lên git hub). Trong đó, source em có lưu trữ 1 "" checkpoint.pth"" ( push lên bằng git lfs ). Vấn đề xảy ra khi code gọi: model.load_state_dict(""đường dẫn tới checkpoint.pth trong github"") Xuất hiện lỗi : UnpicklingError at / : invalid load key, 'v'. Em có tìm hiểu và có thể hiểu là git lfs sẽ down một file nén về khi ta truy cập file large từ code => load_state_dict lỗi. Em vẫn chưa tìm ra hướng giải quyết. mong mọi người nếu xử lý được mong giúp em với ạ! Chúc group sức khỏe mùa covid! Cảm ơn mn!",#Checkpoint	#deploy,,,,
"TraDeS - Track to Detect and Segment là 1 paper được accept tại CVPR năm nay, thuộc lĩnh vực Mutilple Object Tracking. Hiện TraDeS đang đứng top 2  trên Paperwithcode với 2 bộ dữ liệu MOT16 và MOT17. 
Sau 1 thời gian đọc và tìm hiểu, mình có viết 1 bài phân tích nhỏ, nêu ra những tư tưởng và cải tiến nổi bật của TraDeS. Hi vọng bài viết hữu ích với mn ^^
TraDeS giới thiệu một Tracking-conditioned-Detection end-to-end mạnh mẽ, cho phép sử dụng các kết quả tracking để cải thiện performance của detection, đồng thời cũng tăng hiệu quả tracking (Tương tự CenterTrack nhưng mạnh mẽ hơn) .
TraDeS đưa ra hướng tiếp cận mới, sử dụng Cost Volume giúp đảm bảo việc trainning đồng thời re-id embedding và detection mà không bị conflict (Chi tiết đọc phần CVA).
TraDeS trình bày về MFW, một module cho phép lan truyền các feature trong quá khứ để tăng cường các feature hiện tại.
TraDeS sử dụng data association với 2 pha, cho phép theo dõi ngắn hạn và dài hạn, giúp giảm IDSWs.
Source code của TraDeS được base và chỉnh sửa từ source code của CenterTrack: https://github.com/JialianW/TraDeS
Paper: https://arxiv.org/pdf/2103.08808v1.pdf

Chi tiết hơn, các bạn có thể đọc thêm ở link bài viết bên dưới ...
https://viblo.asia/p/centernet-centertrack-trades-tu-object-detection-den-multiple-object-tracking-jvEla9Molkw#comment-rLZDmzOx3lk","TraDeS - Track to Detect and Segment là 1 paper được accept tại CVPR năm nay, thuộc lĩnh vực Mutilple Object Tracking. Hiện TraDeS đang đứng top 2 trên Paperwithcode với 2 bộ dữ liệu MOT16 và MOT17. Sau 1 thời gian đọc và tìm hiểu, mình có viết 1 bài phân tích nhỏ, nêu ra những tư tưởng và cải tiến nổi bật của TraDeS. Hi vọng bài viết hữu ích với mn ^^ TraDeS giới thiệu một Tracking-conditioned-Detection end-to-end mạnh mẽ, cho phép sử dụng các kết quả tracking để cải thiện performance của detection, đồng thời cũng tăng hiệu quả tracking (Tương tự CenterTrack nhưng mạnh mẽ hơn) . TraDeS đưa ra hướng tiếp cận mới, sử dụng Cost Volume giúp đảm bảo việc trainning đồng thời re-id embedding và detection mà không bị conflict (Chi tiết đọc phần CVA). TraDeS trình bày về MFW, một module cho phép lan truyền các feature trong quá khứ để tăng cường các feature hiện tại. TraDeS sử dụng data association với 2 pha, cho phép theo dõi ngắn hạn và dài hạn, giúp giảm IDSWs. Source code của TraDeS được base và chỉnh sửa từ source code của CenterTrack: https://github.com/JialianW/TraDeS Paper: https://arxiv.org/pdf/2103.08808v1.pdf Chi tiết hơn, các bạn có thể đọc thêm ở link bài viết bên dưới ... https://viblo.asia/p/centernet-centertrack-trades-tu-object-detection-den-multiple-object-tracking-jvEla9Molkw#comment-rLZDmzOx3lk",,,,,
"Chào mọi người. Hiện tại em đang có dự định làm một ứng dụng cho phép tải 1 đoạn text lên và convert sang thành 1 file ảnh giống như chữ viết tay với nội dung từ đoạn text đó. Bác nào đã làm cái tương tự này có thể cho em xin một ít từ khoá hoặc giải pháp để nghiên cứu ạ.
Ví dụ:
“Nếu bạn không thể sáng tạo thì hãy thử bắt chước theo người khác…..” sẽ convert thành ảnh như sau
Ps. Các bác đừng xui em tạo font chữ rồi ánh xạ 1:1 sang ạ. Cái output em muốn trông nó phải tự nhiên - con người không phân biệt được( các chữ cái khác nhau như viết tay). Ý là mỗi một lần generate từ một input lại cho ra 2 output khác nhau, là từ thuật toán vẽ ra chữ cái chứ không phải lấy chữ cái có sẵn rồi lấy ra ghép vào với nhau. Tuy nhiên vẫn phải nhận ra là nét chữ của cùng 1 người ạ.","Chào mọi người. Hiện tại em đang có dự định làm một ứng dụng cho phép tải 1 đoạn text lên và convert sang thành 1 file ảnh giống như chữ viết tay với nội dung từ đoạn text đó. Bác nào đã làm cái tương tự này có thể cho em xin một ít từ khoá hoặc giải pháp để nghiên cứu ạ. Ví dụ: “Nếu bạn không thể sáng tạo thì hãy thử bắt chước theo người khác…..” sẽ convert thành ảnh như sau Ps. Các bác đừng xui em tạo font chữ rồi ánh xạ 1:1 sang ạ. Cái output em muốn trông nó phải tự nhiên - con người không phân biệt được( các chữ cái khác nhau như viết tay). Ý là mỗi một lần generate từ một input lại cho ra 2 output khác nhau, là từ thuật toán vẽ ra chữ cái chứ không phải lấy chữ cái có sẵn rồi lấy ra ghép vào với nhau. Tuy nhiên vẫn phải nhận ra là nét chữ của cùng 1 người ạ.",,,,,
"Chào mọi người, chuyện là em muốn tham khảo thử ý kiến của mọi người về câu hỏi sau:
Ví dụ em có bài toán Human detection dùng backbone CNN,
TH1: Data của em là các bức ảnh có người xa khoảng 50m -> model A
TH2: Data của em là các bức ảnh có người xa khoảng 100m -> model B
Câu hỏi:
1) Nếu lấy A inference trên data B và ngược lại B inference trên data A thì sao? (performance, accuracy)
2) Nếu em muốn có 1 model adapt được cả 2 loại data trên thì có phải em cần:
+ Gộp 2 loại data (không cần tăng thêm)
+ Giảm số layer để hàm số cần tìm generalizer hơn?
Cảm ơn mọi người.
(Em đã tham khảo một số ý kiến nhưng em muốn được nghe chia sẻ thêm ạ!)","Chào mọi người, chuyện là em muốn tham khảo thử ý kiến của mọi người về câu hỏi sau: Ví dụ em có bài toán Human detection dùng backbone CNN, TH1: Data của em là các bức ảnh có người xa khoảng 50m -> model A TH2: Data của em là các bức ảnh có người xa khoảng 100m -> model B Câu hỏi: 1) Nếu lấy A inference trên data B và ngược lại B inference trên data A thì sao? (performance, accuracy) 2) Nếu em muốn có 1 model adapt được cả 2 loại data trên thì có phải em cần: + Gộp 2 loại data (không cần tăng thêm) + Giảm số layer để hàm số cần tìm generalizer hơn? Cảm ơn mọi người. (Em đã tham khảo một số ý kiến nhưng em muốn được nghe chia sẻ thêm ạ!)",,,,,
"Chào các bác,
Không biết có bác nào gặp issue giống em không :(
Em cần nhúng 1 model sử dụng các layer của tensorflow2 trên phần cứng của Xilinx. Tuy nhiên bọn Vitis AI của Xilinx chỉ support hạn chế một số layers của tensorflow ( Page 62-63 của Guide này: https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf)
Vậy có cách nào tạo ra layer mới (được tensorflow2 support) dựa trên việc tổ hợp các layer được Vitis support ko ạ?
Ví dụ: Trong model của em có GlobalMaxPool2D, nhưng Vitis ko hỗ trợ, thì có thể tổ hợp ra layer trên bằng các layer mà Vitis support ko ạ?
Hình dưới là list các layers mà Vitis suport trên tersorflow2 ạ.
Cảm ơn các bác ạ!
#Replace_layers
#Xilinx
#Vai_q_tensorflow2","Chào các bác, Không biết có bác nào gặp issue giống em không :( Em cần nhúng 1 model sử dụng các layer của tensorflow2 trên phần cứng của Xilinx. Tuy nhiên bọn Vitis AI của Xilinx chỉ support hạn chế một số layers của tensorflow ( Page 62-63 của Guide này: https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf) Vậy có cách nào tạo ra layer mới (được tensorflow2 support) dựa trên việc tổ hợp các layer được Vitis support ko ạ? Ví dụ: Trong model của em có GlobalMaxPool2D, nhưng Vitis ko hỗ trợ, thì có thể tổ hợp ra layer trên bằng các layer mà Vitis support ko ạ? Hình dưới là list các layers mà Vitis suport trên tersorflow2 ạ. Cảm ơn các bác ạ!",#Replace_layers	#Xilinx	#Vai_q_tensorflow2,,,,
"Chào các bác em đang có một thắc mắc: Có cách nào để YOLO v3,4,5 nhận diện trực tiếp realtime với đầu vào từ màn hình máy tính như đối với webcam không. Ở đây em thấy một chương trình ghi màn hình nhưng không rõ chuyển làm đầu vào ntn mong các bác giúp đỡ.(https://stackoverflow.com/questions/8074595/screen-recorder-in-python)","Chào các bác em đang có một thắc mắc: Có cách nào để YOLO v3,4,5 nhận diện trực tiếp realtime với đầu vào từ màn hình máy tính như đối với webcam không. Ở đây em thấy một chương trình ghi màn hình nhưng không rõ chuyển làm đầu vào ntn mong các bác giúp đỡ.(https://stackoverflow.com/questions/8074595/screen-recorder-in-python)",,,,,
"AWS Machine Learning Scholarship Program

AWS and Udacity are collaborating to educate developers of all skill levels on machine learning concepts. We invite students 18 years of age or older who are interested in expanding their machine learning skills and expertise to enroll in the AWS Machine Learning Scholarship Program. The goal for this program is to up-level machine learning skills to all, and to cultivate the next generation of ML leaders across the world, with a focus on underrepresented groups. Through its We Power Tech Program, AWS collaborates with professional organizations that are leading initiatives to increase the diversity and talent in technical roles, including organizations like Girls In Tech and the National Society of Black Engineers.

The scholarship is open to all for registration starting May 26, 2021, and your learning will begin on June 28, 2021. Participants will have up to 3.5 months to complete the AWS Machine Learning Foundations Course. The course covers the fundamentals of machine learning, steps in machine learning process, basics of computer vision, reinforcement learning, generative AI, software engineering best practices for data science, and how to build your own python package. The foundations course is intended to help developers of all skill levels get started with machine learning.

At the end of the AWS Machine Learning Foundations Course, learners will take an assessment from which top performers will be selected for one of 425 follow-up scholarships to one of Udacity’s most popular and recently refreshed Nanodegree programs: The AWS Machine Learning Engineer Nanodegree program.
Link : https://www.udacity.com/scholarships/aws-machine-learning-scholarship-program
#machinelearning #scholarship","AWS Machine Learning Scholarship Program AWS and Udacity are collaborating to educate developers of all skill levels on machine learning concepts. We invite students 18 years of age or older who are interested in expanding their machine learning skills and expertise to enroll in the AWS Machine Learning Scholarship Program. The goal for this program is to up-level machine learning skills to all, and to cultivate the next generation of ML leaders across the world, with a focus on underrepresented groups. Through its We Power Tech Program, AWS collaborates with professional organizations that are leading initiatives to increase the diversity and talent in technical roles, including organizations like Girls In Tech and the National Society of Black Engineers. The scholarship is open to all for registration starting May 26, 2021, and your learning will begin on June 28, 2021. Participants will have up to 3.5 months to complete the AWS Machine Learning Foundations Course. The course covers the fundamentals of machine learning, steps in machine learning process, basics of computer vision, reinforcement learning, generative AI, software engineering best practices for data science, and how to build your own python package. The foundations course is intended to help developers of all skill levels get started with machine learning. At the end of the AWS Machine Learning Foundations Course, learners will take an assessment from which top performers will be selected for one of 425 follow-up scholarships to one of Udacity’s most popular and recently refreshed Nanodegree programs: The AWS Machine Learning Engineer Nanodegree program. Link : https://www.udacity.com/scholarships/aws-machine-learning-scholarship-program",#machinelearning	#scholarship,,,,
"Chào mọi người
Mình định dùng mô hình ResNet50 cùng với pretrained weights từ ImageNet để transfer learning bộ dữ liệu của mình. Mình có sửa phần Top_layers để cho phù hợp với bài toán của mình.
Lâu nay mình vẫn sử dụng và chạy bình thường nhưng vài hôm gần đây nó bị lỗi như ở dưới. M.n xem và giúp mình sửa với ạ
Cảm ơn m.n",Chào mọi người Mình định dùng mô hình ResNet50 cùng với pretrained weights từ ImageNet để transfer learning bộ dữ liệu của mình. Mình có sửa phần Top_layers để cho phù hợp với bài toán của mình. Lâu nay mình vẫn sử dụng và chạy bình thường nhưng vài hôm gần đây nó bị lỗi như ở dưới. M.n xem và giúp mình sửa với ạ Cảm ơn m.n,,,,,
"Chào mn. Hiện tại mình đang sử dụng thư viện face_recognition để làm 1 project real time attendance nho nhỏ. nhưng thấy hàm face_encodings của nó đang quá chậm khiến cho việc stream lên camera nó bị giật.
Có ai đã từng dùng thư viện này và fix được issue đấy cho em xin ý tưởng với. Thanks mn",Chào mn. Hiện tại mình đang sử dụng thư viện face_recognition để làm 1 project real time attendance nho nhỏ. nhưng thấy hàm face_encodings của nó đang quá chậm khiến cho việc stream lên camera nó bị giật. Có ai đã từng dùng thư viện này và fix được issue đấy cho em xin ý tưởng với. Thanks mn,,,,,
"Chào mọi người,
Em có vấn đề mong mọi người giải đáp.
Em đang cần train model nhưng dữ liệu bị imbalance (chênh lệch khoảng 10 lần)
Mặc dù tìm hiểu trên mạng, biết được imbalance data có thể ảnh hưởng đến kết quả train. Nhưng nếu dữ liệu của label ít nhất đủ lớn thì mô hình có học được đặc trưng mà không bị bias không ạ?
Em cảm ơn ạ.","Chào mọi người, Em có vấn đề mong mọi người giải đáp. Em đang cần train model nhưng dữ liệu bị imbalance (chênh lệch khoảng 10 lần) Mặc dù tìm hiểu trên mạng, biết được imbalance data có thể ảnh hưởng đến kết quả train. Nhưng nếu dữ liệu của label ít nhất đủ lớn thì mô hình có học được đặc trưng mà không bị bias không ạ? Em cảm ơn ạ.",,,,,
"Em chào mọi người
Hiện em đang train một model Neural  Network về classification nhưng gặp vấn đề như sau:
cost function không thể giảm xuống dưới 0.64 dù em đã thử nhiều learning_rate hay tăng epoch
Vì cost sai nên kết quả tất cả predict đều là 0
hình ảnh trên là em đang chạy trên data cat or non-cat ý ạ.
Mọi người có thể đề xuất giúp em cách để sửa tình trạng này không ạ. Em cảm ơn mọi người nhiều, chúc mọi người cuối tuần vui vẻ.","Em chào mọi người Hiện em đang train một model Neural Network về classification nhưng gặp vấn đề như sau: cost function không thể giảm xuống dưới 0.64 dù em đã thử nhiều learning_rate hay tăng epoch Vì cost sai nên kết quả tất cả predict đều là 0 hình ảnh trên là em đang chạy trên data cat or non-cat ý ạ. Mọi người có thể đề xuất giúp em cách để sửa tình trạng này không ạ. Em cảm ơn mọi người nhiều, chúc mọi người cuối tuần vui vẻ.",,,,,
Pytorch và Tensorflow. Ae thấy nào hay hơn ạ?,Pytorch và Tensorflow. Ae thấy nào hay hơn ạ?,,,,,
"Chào mọi người, em đang cần tải dataset từ baidu, nhưng hiện em không có tài khoản. Em đã thử cách tìm được trên google nhưng đều không được...
Có cao nhân nào giúp em với ạ.","Chào mọi người, em đang cần tải dataset từ baidu, nhưng hiện em không có tài khoản. Em đã thử cách tìm được trên google nhưng đều không được... Có cao nhân nào giúp em với ạ.",,,,,
"Improved YOLOv4 (Scaled-YOLOv4 and YOLOR) is still better than: PP-YOLOv2, YOLOv5, EfficientDet, SWIN Transformers: paperswithcode.com/sota/real-time…
Scaled-YOLOv4 (accepted to CVPR'21) arxiv.org/abs/2011.08036
github.com/WongKinYiu/Sca…
YOLOR arxiv.org/abs/2105.04206
https://github.com/WongKinYiu/yolor","Improved YOLOv4 (Scaled-YOLOv4 and YOLOR) is still better than: PP-YOLOv2, YOLOv5, EfficientDet, SWIN Transformers: paperswithcode.com/sota/real-time… Scaled-YOLOv4 (accepted to CVPR'21) arxiv.org/abs/2011.08036 github.com/WongKinYiu/Sca… YOLOR arxiv.org/abs/2105.04206 https://github.com/WongKinYiu/yolor",,,,,
"Chào mọi người,
Hiện tại em đang xử lí một bài toán với đầu vào là các vector số thực với số chiều cực lớn (vài chục ngàn - vài trăm ngàn chiều). Do dữ liệu ít nên em muốn làm data augmentation thêm nhưng chưa biết augment thế nào với dữ liệu vector. Mong mọi người có cách augment nào hiệu quả có thể gợi ý cho em.
Em cảm ơn!","Chào mọi người, Hiện tại em đang xử lí một bài toán với đầu vào là các vector số thực với số chiều cực lớn (vài chục ngàn - vài trăm ngàn chiều). Do dữ liệu ít nên em muốn làm data augmentation thêm nhưng chưa biết augment thế nào với dữ liệu vector. Mong mọi người có cách augment nào hiệu quả có thể gợi ý cho em. Em cảm ơn!",,,,,
Giời thiệu các bạn kinh nghiệm xin internship NVIDIA,Giời thiệu các bạn kinh nghiệm xin internship NVIDIA,,,,,
"[AI Share – AI Expert Roadmap]
AI Expert Roadmap (Lộ trình chuyên gia về AI) của công ty khởi nghiệp Đức hơn 10.000 sao trên GitHub.",[AI Share – AI Expert Roadmap] AI Expert Roadmap (Lộ trình chuyên gia về AI) của công ty khởi nghiệp Đức hơn 10.000 sao trên GitHub.,,,,,
"Chào m.n. Hiện em đang vọc vạch về project ô tô tự lái và mô phỏng trên phần mềm giả lập. Em đã lấy data về, xử lí và xây dựng Sequential model. Nhưng đến khi em thực hiện train với 10 epoch, số lượng mẫu mỗi epoch là 1000 thì máy chỉ train được epoch đầu tiên. Sau đó đứng hình và không train tiếp nữa ạ :'(
Có ai đã gặp tình trạng tương tự hay thấy em có sai thì chỉ giúp em :( . Cảm ơn m.n.","Chào m.n. Hiện em đang vọc vạch về project ô tô tự lái và mô phỏng trên phần mềm giả lập. Em đã lấy data về, xử lí và xây dựng Sequential model. Nhưng đến khi em thực hiện train với 10 epoch, số lượng mẫu mỗi epoch là 1000 thì máy chỉ train được epoch đầu tiên. Sau đó đứng hình và không train tiếp nữa ạ :'( Có ai đã gặp tình trạng tương tự hay thấy em có sai thì chỉ giúp em :( . Cảm ơn m.n.",,,,,
"[Person Reidentification]
Câu 1: Trong bài toán này dựa trên resnet50,
(https://arxiv.org/pdf/1903.07071v3.pdf)
Ý nghĩa của identity loss là để làm gì vậy ạ? Có khi nào vì ID loss này mà dẫn đến việc overfitting theo các ID bên trong dataset (train) không nhỉ?
Câu 2: Tại sao người ta lại xem bài toán person re identification như 1 bài image retrieval vậy ạ? Mình nghĩ, tuy hai bài toán có phần liên quan với nhau. Nhưng về mặt ứng dụng khác nên nó phải hơi khác nhau nhiều điểm chứ ạ? Mình đọc các bài survey thì họ thường coi 1 bài person-reid lại như là một bài image retrieval(IR), rồi sau đó áp dụng các phương pháp IR vào Person ReID. Một trong những điểm mình thấy là: Image retrieval (Known number of Identities, offline, large dataset), while Person reID (unknow identity may appear, smaller dataset (few cameras), online).
Ps: Mong tìm được các bạn đang làm về bài toán này để học hỏi thêm ạ.","[Person Reidentification] Câu 1: Trong bài toán này dựa trên resnet50, (https://arxiv.org/pdf/1903.07071v3.pdf) Ý nghĩa của identity loss là để làm gì vậy ạ? Có khi nào vì ID loss này mà dẫn đến việc overfitting theo các ID bên trong dataset (train) không nhỉ? Câu 2: Tại sao người ta lại xem bài toán person re identification như 1 bài image retrieval vậy ạ? Mình nghĩ, tuy hai bài toán có phần liên quan với nhau. Nhưng về mặt ứng dụng khác nên nó phải hơi khác nhau nhiều điểm chứ ạ? Mình đọc các bài survey thì họ thường coi 1 bài person-reid lại như là một bài image retrieval(IR), rồi sau đó áp dụng các phương pháp IR vào Person ReID. Một trong những điểm mình thấy là: Image retrieval (Known number of Identities, offline, large dataset), while Person reID (unknow identity may appear, smaller dataset (few cameras), online). Ps: Mong tìm được các bạn đang làm về bài toán này để học hỏi thêm ạ.",,,,,
Trang web TMĐT của mình mình có dùng giải thuật lọc cộng tác và lọc dựa trên nội dung cho những phần khác nhau của trang. Giờ nếu mình đánh giá mô hình thì mình sẽ dựa vào tiêu chí nào để đánh giá xem mô hình có hoạt động tốt không vậy mọi người?,Trang web TMĐT của mình mình có dùng giải thuật lọc cộng tác và lọc dựa trên nội dung cho những phần khác nhau của trang. Giờ nếu mình đánh giá mô hình thì mình sẽ dựa vào tiêu chí nào để đánh giá xem mô hình có hoạt động tốt không vậy mọi người?,,,,,
"Chào mn, hiện tại e đang train mô hình yolov5, áp dụng 10 fold cross validation. Ý của e là train mô hình 10 lần, mỗi lần 10 epochs, để chọn ra mô hình có kết quả tốt nhất và dùng toàn bộ dữ liệu để train dựa theo mô hình đó để cho ra kết quả cuối cùng. Mà thầy hướng dẫn của e không đồng ý với hướng đó, thầy bảo hãy dùng kết quả trung bình. Vậy thì mn cho e hỏi có cách nào để tính trung bình của 10 mô hình như cách mà thầy e bảo không ạ? Thanks mn.","Chào mn, hiện tại e đang train mô hình yolov5, áp dụng 10 fold cross validation. Ý của e là train mô hình 10 lần, mỗi lần 10 epochs, để chọn ra mô hình có kết quả tốt nhất và dùng toàn bộ dữ liệu để train dựa theo mô hình đó để cho ra kết quả cuối cùng. Mà thầy hướng dẫn của e không đồng ý với hướng đó, thầy bảo hãy dùng kết quả trung bình. Vậy thì mn cho e hỏi có cách nào để tính trung bình của 10 mô hình như cách mà thầy e bảo không ạ? Thanks mn.",,,,,
"Facebook AI team officially announced PyTorchVideo today!
a deep learning library for video understanding. PyTorchVideo is dedicated for both research and productization in the video domain.

For more details, please check out:
The github repo: https://lnkd.in/gJaTQbJ
The PyTorchVideo website with tutorials: https://pytorchvideo.org/
Facebook AI blog post: https://lnkd.in/gxWzADG

#deeplearning #computervision #opensource","Facebook AI team officially announced PyTorchVideo today! a deep learning library for video understanding. PyTorchVideo is dedicated for both research and productization in the video domain. For more details, please check out: The github repo: https://lnkd.in/gJaTQbJ The PyTorchVideo website with tutorials: https://pytorchvideo.org/ Facebook AI blog post: https://lnkd.in/gxWzADG",#deeplearning	#computervision	#opensource,,,,
"[AI Share]
Khóa học : The Missing Semester of Your CS Education ( Kì Học Bị Thiếu Của Giáo Trình Khoa Học Máy Tính)
“Giảng đường truyền thống dạy mọi người về các vấn đề chuyên ngành Khoa Học Máy Tính cao cấp từ hệ điều hành đến học máy. Tuy nhiên có một chủ đề rất quan trọng nhưng lại hay bị bỏ rơi để sinh viên tự mày mò, đó là khả năng sử dụng công cụ của họ. Chúng tôi sẽ dạy bạn cách làm chủ command-line, sử dụng một trình biên dịch mã nguồn (text editor) hết khả năng của nó, vô vàn các chức năng “xịn xò” của trình quản lý phiên bản (version control systems), và hơn thế nữa.
Ước chừng sinh viên sẽ dành ra hàng trăm giờ để sử dụng những công cụ nói trên trong suốt thời gian ngồi trên giảng đường (và hàng ngàn giờ khi đi làm). Vì vậy, việc đảm bảo cho họ sử dụng các công cụ này “nhanh, gọn, lẹ” là một điều vô cùng hợp lý. Làm chủ hoàn toàn được những công cụ này không những cho phép bạn tiết kiệm thời gian thao tác theo ý mình, mà còn cho phép bạn xử lý những vấn đề phức tạp, không tưởng.”
Để phần nào hạn chế những thiếu sót trên, Khóa học “The Missing Semester of Your CS Education” sẽ cung cấp những kiến thức cần thiết cho một nhà khoa học máy tính, nhà lập trình. Khóa học này vừa thực tế lại vừa cho phép bạn thực hành các công cụ, kỹ thuật mà bạn có thể áp dụng ngay lập tức vào rất nhiều trường hợp bạn sẽ gặp trong tương lai.
Link : https://missing.csail.mit.edu/
Link tiếng việt : https://missing-semester-vn.github.io/
Youtube: https://www.youtube.com/playlist?list=PLyzOVJj3bHQuloKGG59rS43e29ro7I57J
_____________________________
Nếu gặp khó khăn trong việc học các kiến thức nền tảng lĩnh vực trí tuệ nhân tạo, đừng quên các khóa học bổ ích của AI4E nhé. Khóa học Deep Learning cơ bản online sẽ được khai giảng vào 19h ngày 5/6 nhé. Nhanh tay inbox page để được tham gia lớp học bổ ích này nhé.","[AI Share] Khóa học : The Missing Semester of Your CS Education ( Kì Học Bị Thiếu Của Giáo Trình Khoa Học Máy Tính) “Giảng đường truyền thống dạy mọi người về các vấn đề chuyên ngành Khoa Học Máy Tính cao cấp từ hệ điều hành đến học máy. Tuy nhiên có một chủ đề rất quan trọng nhưng lại hay bị bỏ rơi để sinh viên tự mày mò, đó là khả năng sử dụng công cụ của họ. Chúng tôi sẽ dạy bạn cách làm chủ command-line, sử dụng một trình biên dịch mã nguồn (text editor) hết khả năng của nó, vô vàn các chức năng “xịn xò” của trình quản lý phiên bản (version control systems), và hơn thế nữa. Ước chừng sinh viên sẽ dành ra hàng trăm giờ để sử dụng những công cụ nói trên trong suốt thời gian ngồi trên giảng đường (và hàng ngàn giờ khi đi làm). Vì vậy, việc đảm bảo cho họ sử dụng các công cụ này “nhanh, gọn, lẹ” là một điều vô cùng hợp lý. Làm chủ hoàn toàn được những công cụ này không những cho phép bạn tiết kiệm thời gian thao tác theo ý mình, mà còn cho phép bạn xử lý những vấn đề phức tạp, không tưởng.” Để phần nào hạn chế những thiếu sót trên, Khóa học “The Missing Semester of Your CS Education” sẽ cung cấp những kiến thức cần thiết cho một nhà khoa học máy tính, nhà lập trình. Khóa học này vừa thực tế lại vừa cho phép bạn thực hành các công cụ, kỹ thuật mà bạn có thể áp dụng ngay lập tức vào rất nhiều trường hợp bạn sẽ gặp trong tương lai. Link : https://missing.csail.mit.edu/ Link tiếng việt : https://missing-semester-vn.github.io/ Youtube: https://www.youtube.com/playlist?list=PLyzOVJj3bHQuloKGG59rS43e29ro7I57J _____________________________ Nếu gặp khó khăn trong việc học các kiến thức nền tảng lĩnh vực trí tuệ nhân tạo, đừng quên các khóa học bổ ích của AI4E nhé. Khóa học Deep Learning cơ bản online sẽ được khai giảng vào 19h ngày 5/6 nhé. Nhanh tay inbox page để được tham gia lớp học bổ ích này nhé.",,,,,
"Cách đây khoảng hơn 1 tháng mình có viết 1 chút về Siamese Networks, hôm nay đọc paper thấy có nhóm nghiên cứu của Amazon công bố bài về SiamMOT: Siamese Multi-Object tracking tại đây https://www.amazon.science/publications/siammot-siamese-multi-object-tracking.
Code của họ tại đây: https://github.com/amazon-research/siam-mot
Hi vọng nó hữu ích với các bạn làm computer vision.","Cách đây khoảng hơn 1 tháng mình có viết 1 chút về Siamese Networks, hôm nay đọc paper thấy có nhóm nghiên cứu của Amazon công bố bài về SiamMOT: Siamese Multi-Object tracking tại đây https://www.amazon.science/publications/siammot-siamese-multi-object-tracking. Code của họ tại đây: https://github.com/amazon-research/siam-mot Hi vọng nó hữu ích với các bạn làm computer vision.",,,,,
"[Multi-class sementic segmentation]
Chào mọi người,
E hiện đang làm một bài tập về phân vùng ảnh với 5 class. E đang implement Unet để làm quen, output của e nhận được là một tensor kích thước là (5,256,256). Em muốn imshow output thì phải làm như thế nào ạ.
Em cảm ơn ạ","[Multi-class sementic segmentation] Chào mọi người, E hiện đang làm một bài tập về phân vùng ảnh với 5 class. E đang implement Unet để làm quen, output của e nhận được là một tensor kích thước là (5,256,256). Em muốn imshow output thì phải làm như thế nào ạ. Em cảm ơn ạ",,,,,
"Chào mọi người, em có hai câu hỏi về kiến trúc LSTM, mong được mọi người giải đáp ạ
1/ Các weight và bias là khác nhau giữa các cell/unit hay dùng chung cho cả 1 LSTM layer? Câu hỏi tương tự với GRU.
2/ Hiện nay có thư viện nào implement kiến trúc Convolutional Bidirectional LSTM không ạ? Em đã tìm được ít nhất 1 cài đặt trên Githubn nhưng không có documentation, ít nhất là về kích cỡ các tensor, kích cơ đầu ra.","Chào mọi người, em có hai câu hỏi về kiến trúc LSTM, mong được mọi người giải đáp ạ 1/ Các weight và bias là khác nhau giữa các cell/unit hay dùng chung cho cả 1 LSTM layer? Câu hỏi tương tự với GRU. 2/ Hiện nay có thư viện nào implement kiến trúc Convolutional Bidirectional LSTM không ạ? Em đã tìm được ít nhất 1 cài đặt trên Githubn nhưng không có documentation, ít nhất là về kích cỡ các tensor, kích cơ đầu ra.",,,,,
"[Tìm nguồn/ sách học các khái niệm cơ bản]
Hi cả nhà, e cũng mới học từ đầu xử lý dữ liệu lớn và xây các model phân loại bằng R/ Python. Em là dân tài chính nhảy ngang nên e mong được chia sẻ các cuốn sách hoặc link học các khái niệm cơ bản như vector, mảng, chiều ....
Em cám ơn các anh chị ạ.","[Tìm nguồn/ sách học các khái niệm cơ bản] Hi cả nhà, e cũng mới học từ đầu xử lý dữ liệu lớn và xây các model phân loại bằng R/ Python. Em là dân tài chính nhảy ngang nên e mong được chia sẻ các cuốn sách hoặc link học các khái niệm cơ bản như vector, mảng, chiều .... Em cám ơn các anh chị ạ.",,,,,
"Tháng tư này có lẽ không có nhiều sự kiện, nhưng có lẽ vẫn nhiều công ty tuyển nhân viên làm từ xa. Các bạn vui lòng đăng các thông tin liên quan tới sự kiện, tuyển dụng, tuyển sinh trong post này.
Chúc các bạn học thêm được nhiều thứ trong thời gian ở nhà này.","Tháng tư này có lẽ không có nhiều sự kiện, nhưng có lẽ vẫn nhiều công ty tuyển nhân viên làm từ xa. Các bạn vui lòng đăng các thông tin liên quan tới sự kiện, tuyển dụng, tuyển sinh trong post này. Chúc các bạn học thêm được nhiều thứ trong thời gian ở nhà này.",,,,,
"Hỏi về lỗi Average Precision = -1 và Average Recall = -1 khi huấn luyện mô hình SSD MobileNet bài toán Object Detection
Chào mọi người. Trong khi huấn luyện mô hình SSD MobileNet cho bài toán Object Detection em gặp một số lỗi sau xin được mọi người giúp đỡ.
Em xin nói qua về môi trường mình sử dụng
Train trên Google Colab
Tensorflow version 1
Mô hình SSD MobileNet v2
File tfrecord: Em gán nhãn bằng LabelImg và dùng App RobotFlow để chuyển thành tfrecord
Lúc trước em chuyển đổi file tfrecord trên RobotFlow và train mô hình thì các chỉ số AP và AR không có bị trừ -1 như trong ảnh, chỉ trừ một số đối tượng không có kích thước area = small như trong mô hình thì mới bằng -1. Ngoài ra thì lúc train các chỉ số ấy ban đầu là bằng 0 hết.
Tuy nhiên, sau khi em tăng cường thêm dữ liệu và chuyển đổi lại thành file tfrecord thì xảy ra hiện tượng AP và AR = -1. Em đã kiểm tra cẩn thận file configure của SSD MobileNet và các cài đặt khác, thậm chí là thay bằng file tfrecord trước đó thì mô hình đều chạy bình thường. Nhưng đối với file tfrecord sau khi tăng cường dữ liệu thì xảy ra hiện tượng như trên.
Cho em hỏi là có anh/chị nào đã từng bị trường hợp như trên, có thể cho em biết lý do tại sao và cách khắc phục như thế nào không ạ ?
Em xin cảm ơn.
File code trên colab mọi người có thể xem tại đây ạ: https://colab.research.google.com/drive/1VuBM2yoF8PMNp6z-571LtV8nExnDlEF_?usp=sharing&fbclid=IwAR2PqVW82-LYnSZF9L0Q42I4iIaxfJiztqH3qcqIj8r-AbTA9e9JeMb6-l8
Ảnh bên dưới là lỗi:
#ObjectDetection #ComputerVision #AveragePrecision","Hỏi về lỗi Average Precision = -1 và Average Recall = -1 khi huấn luyện mô hình SSD MobileNet bài toán Object Detection Chào mọi người. Trong khi huấn luyện mô hình SSD MobileNet cho bài toán Object Detection em gặp một số lỗi sau xin được mọi người giúp đỡ. Em xin nói qua về môi trường mình sử dụng Train trên Google Colab Tensorflow version 1 Mô hình SSD MobileNet v2 File tfrecord: Em gán nhãn bằng LabelImg và dùng App RobotFlow để chuyển thành tfrecord Lúc trước em chuyển đổi file tfrecord trên RobotFlow và train mô hình thì các chỉ số AP và AR không có bị trừ -1 như trong ảnh, chỉ trừ một số đối tượng không có kích thước area = small như trong mô hình thì mới bằng -1. Ngoài ra thì lúc train các chỉ số ấy ban đầu là bằng 0 hết. Tuy nhiên, sau khi em tăng cường thêm dữ liệu và chuyển đổi lại thành file tfrecord thì xảy ra hiện tượng AP và AR = -1. Em đã kiểm tra cẩn thận file configure của SSD MobileNet và các cài đặt khác, thậm chí là thay bằng file tfrecord trước đó thì mô hình đều chạy bình thường. Nhưng đối với file tfrecord sau khi tăng cường dữ liệu thì xảy ra hiện tượng như trên. Cho em hỏi là có anh/chị nào đã từng bị trường hợp như trên, có thể cho em biết lý do tại sao và cách khắc phục như thế nào không ạ ? Em xin cảm ơn. File code trên colab mọi người có thể xem tại đây ạ: https://colab.research.google.com/drive/1VuBM2yoF8PMNp6z-571LtV8nExnDlEF_?usp=sharing&fbclid=IwAR2PqVW82-LYnSZF9L0Q42I4iIaxfJiztqH3qcqIj8r-AbTA9e9JeMb6-l8 Ảnh bên dưới là lỗi:",#ObjectDetection	#ComputerVision	#AveragePrecision,,,,
"Chào mọi người, em muốn hỏi về key idea của các phương pháp word embedding như: Word2Vec, GloVe, CoVe, ELMo, BERT, GPT ạ. Điểm giống và khác nhau cơ bản giữa các pp này là gì ạ. Em cảm ơn mọi người","Chào mọi người, em muốn hỏi về key idea của các phương pháp word embedding như: Word2Vec, GloVe, CoVe, ELMo, BERT, GPT ạ. Điểm giống và khác nhau cơ bản giữa các pp này là gì ạ. Em cảm ơn mọi người",,,,,
"Chào anh em trong hội nhóm.
Chả là mình muốn cài đặt Ubuntu 20.04LTS lên máy tính chạy chip Ryzen 7 5800H để làm việc về Deep learning (Computer vision, ...) thì ở đây có ai cài ubuntu và làm AI trên máy chạy chip AMD chưa, có phải chú ý gì ko hay là có hạn chế gì ko so với chip intel. Mình cảm ơn <3","Chào anh em trong hội nhóm. Chả là mình muốn cài đặt Ubuntu 20.04LTS lên máy tính chạy chip Ryzen 7 5800H để làm việc về Deep learning (Computer vision, ...) thì ở đây có ai cài ubuntu và làm AI trên máy chạy chip AMD chưa, có phải chú ý gì ko hay là có hạn chế gì ko so với chip intel. Mình cảm ơn <3",,,,,
"Mình muốn làm NMF (Non-negative matrix factorization) một ma trận lớn với 200.000 dòng. Laptop của mình không đủ mạnh nên mất rất nhiều thời gian và xót máy.
Mn cho hỏi có dịch vụ nào cho thuế máy mạnh (nhiều RAM và CPU mạnh) không ak?
Cảm ơn mn nhiều nhé :v",Mình muốn làm NMF (Non-negative matrix factorization) một ma trận lớn với 200.000 dòng. Laptop của mình không đủ mạnh nên mất rất nhiều thời gian và xót máy. Mn cho hỏi có dịch vụ nào cho thuế máy mạnh (nhiều RAM và CPU mạnh) không ak? Cảm ơn mn nhiều nhé :v,,,,,
"xin phép admin
- Xin chào ae, hôm nay mình chia sẻ tới ae phần mềm dự đoán bệnh và lựa chọn phòng khám trong bệnh viện. Chắc hẳn đây cũng là một ứng dụng mà nhiều ae đang tìm, nghiên cứu và ứng dụng. Phần mềm này được ae #ASM trước kia từng làm cho bệnh viện làm và chia sẻ lại với ae, mong là sẽ giúp được cho ae đang cần mà chưa có hướng nghiên cứu nhé!","xin phép admin - Xin chào ae, hôm nay mình chia sẻ tới ae phần mềm dự đoán bệnh và lựa chọn phòng khám trong bệnh viện. Chắc hẳn đây cũng là một ứng dụng mà nhiều ae đang tìm, nghiên cứu và ứng dụng. Phần mềm này được ae trước kia từng làm cho bệnh viện làm và chia sẻ lại với ae, mong là sẽ giúp được cho ae đang cần mà chưa có hướng nghiên cứu nhé!",#ASM,,,,
"Chào mọi người, hiện tại tụi em đang làm khảo sát về độ hiệu quả của những mô hình xoá và tạo ảnh khác nhau.
Nếu mọi người có thời gian thì có thể tham gia khảo sát vài phút được không ạ.
Em cảm ơn
https://docs.google.com/forms/d/e/1FAIpQLSf5Z0sDgoDBEaWzBY7WJDDK5SyWFkPWClkocKqKsT5UoKwFiw/viewform?vc=0&c=0&w=1&flr=0&fbzx=5178650705536400224&fbclid=IwAR2rai7I93AfMTV2us7SyfnAl0bYFdipwksAGq2qHlYkXAvLuDTqTq-ElcI","Chào mọi người, hiện tại tụi em đang làm khảo sát về độ hiệu quả của những mô hình xoá và tạo ảnh khác nhau. Nếu mọi người có thời gian thì có thể tham gia khảo sát vài phút được không ạ. Em cảm ơn https://docs.google.com/forms/d/e/1FAIpQLSf5Z0sDgoDBEaWzBY7WJDDK5SyWFkPWClkocKqKsT5UoKwFiw/viewform?vc=0&c=0&w=1&flr=0&fbzx=5178650705536400224&fbclid=IwAR2rai7I93AfMTV2us7SyfnAl0bYFdipwksAGq2qHlYkXAvLuDTqTq-ElcI",,,,,
"[CHIA SẺ] OBJECT DETECTION TRÊN MOBILE
Tại Google I/O tuần trước, mình mới release một tutorial series hướng dẫn cách train model object detection trên dữ liệu mới và deploy trên mobile app một cách rất đơn giản. Model dùng ở đây là EfficientDet-Lite. EfficientDet là model tạo ra bằng kỹ thuật Neural Architecture Search, và đạt state-of-the-art cho object detection. Còn EfficientDet-Lite là phiên bản của EfficientDet đã được tối ưu hoá cho mobile và thiết bị IoT. Tutorial này dùng TensorFlow Lite Model Maker và Task Library là hai thư viện giúp train và deploy model trên mobile chỉ bằng vài dòng code :)

Hy vọng các tutorial này sẽ có ích cho nhiều bạn. :D

https://www.youtube.com/watch?v=sK2c66xqFDk","[CHIA SẺ] OBJECT DETECTION TRÊN MOBILE Tại Google I/O tuần trước, mình mới release một tutorial series hướng dẫn cách train model object detection trên dữ liệu mới và deploy trên mobile app một cách rất đơn giản. Model dùng ở đây là EfficientDet-Lite. EfficientDet là model tạo ra bằng kỹ thuật Neural Architecture Search, và đạt state-of-the-art cho object detection. Còn EfficientDet-Lite là phiên bản của EfficientDet đã được tối ưu hoá cho mobile và thiết bị IoT. Tutorial này dùng TensorFlow Lite Model Maker và Task Library là hai thư viện giúp train và deploy model trên mobile chỉ bằng vài dòng code :) Hy vọng các tutorial này sẽ có ích cho nhiều bạn. :D https://www.youtube.com/watch?v=sK2c66xqFDk",,,,,
"Chào mọi người, không biết mọi người có ai đã làm Try-on AR App ko ạ? Em đang làm đồ án về xây dựng AR App trên iOS, em có tham khảo MakeML Nails Segmentation và dùng deeplab để train thì được kết quả như hình, em muốn hỏi là mình muốn làm chức năng đổi kiểu móng thì mình cần làm như nào ạ? Em cảm ơn mọi người!
Link em tham khảo:
https://makeml.app/nails-segmentation-tutorial","Chào mọi người, không biết mọi người có ai đã làm Try-on AR App ko ạ? Em đang làm đồ án về xây dựng AR App trên iOS, em có tham khảo MakeML Nails Segmentation và dùng deeplab để train thì được kết quả như hình, em muốn hỏi là mình muốn làm chức năng đổi kiểu móng thì mình cần làm như nào ạ? Em cảm ơn mọi người! Link em tham khảo: https://makeml.app/nails-segmentation-tutorial",,,,,
"Xin chào mọi người, em hiện tại đang có vấn đề chưa hiểu khi thử nghiệm mô hình, mong mọi người có thể giải đáp.
Chuyện là em sử dụng hàm DynamicUnet từ thư viện fastai để build 1 cái Unet từ một cái backbone (ở đây em dùng backbone ResNet). Mọi thứ diễn ra tốt đẹp, tuy nhiên trong quá trình thử nghiệm mô hình em lại không biết tại sao mô hình vẫn có thể làm việc với những ảnh có kích thước khác với ảnh lúc em khởi tạo (256x256). Dưới đây là demo cho việc kết quả đầu ra vẫn được tính rất bình thường dù kích thước không đúng như kích thước ban đầu.
Link Doc DynamicUnet: https://docs.fast.ai/vision.models.unet.html","Xin chào mọi người, em hiện tại đang có vấn đề chưa hiểu khi thử nghiệm mô hình, mong mọi người có thể giải đáp. Chuyện là em sử dụng hàm DynamicUnet từ thư viện fastai để build 1 cái Unet từ một cái backbone (ở đây em dùng backbone ResNet). Mọi thứ diễn ra tốt đẹp, tuy nhiên trong quá trình thử nghiệm mô hình em lại không biết tại sao mô hình vẫn có thể làm việc với những ảnh có kích thước khác với ảnh lúc em khởi tạo (256x256). Dưới đây là demo cho việc kết quả đầu ra vẫn được tính rất bình thường dù kích thước không đúng như kích thước ban đầu. Link Doc DynamicUnet: https://docs.fast.ai/vision.models.unet.html",,,,,
"[Pytorch series]
Bài 6: Lưu và load model trong Pytorch.
Những bài trước mình đã học cách xây dựng và train deep learning model bằng Pytorch. Tuy nhiên, khi train xong model mình cần lưu được model đã train, để sau có thể dùng để dự đoán hoặc tiếp tục train mà không cần train lại từ đầu. Bài này mình sẽ hướng dẫn lưu và load model trong Pytorch.
https://nttuan8.com/bai-6-luu-va-load-model-trong-pytorch/","[Pytorch series] Bài 6: Lưu và load model trong Pytorch. Những bài trước mình đã học cách xây dựng và train deep learning model bằng Pytorch. Tuy nhiên, khi train xong model mình cần lưu được model đã train, để sau có thể dùng để dự đoán hoặc tiếp tục train mà không cần train lại từ đầu. Bài này mình sẽ hướng dẫn lưu và load model trong Pytorch. https://nttuan8.com/bai-6-luu-va-load-model-trong-pytorch/",,,,,
Anh chị có thể cho em hỏi là tại sao trong Shuffle Unit thì Group Convolution đầu tiên lại dùng 1/4 channels không ạ. Em xin cảm ơn.,Anh chị có thể cho em hỏi là tại sao trong Shuffle Unit thì Group Convolution đầu tiên lại dùng 1/4 channels không ạ. Em xin cảm ơn.,,,,,
"Em chào anh/chị/bạn. Chúc mọi người buổi tối vui vẻ. 
Em đang có gặp vấn đề này mà chưa giải được sau 3 tối suy nghĩ. 
Em đăng lên đây để xin mọi người keyword để em tìm hướng em làm ạ. 
(Em đã thử dùng quy hoạch tuyến tính để giải nhưng có vẻ không ăn thua)
Đề bài bằng lời:
Cho 4 biến A - B - C - D, biết:
- Số lượng: số đơn vị max của từng biến (VD: có 735 đơn vị của biến A trong kho).
- Số tiền/từng biến: giá của biến (VD: mỗi đơn vị biến A có giá 5000 => tổng giá trị của biến A trong kho = 5000*735= 3675000)
1. Tìm % giảm số tiền/từng biến (giá) để có thể ""bán"" đc số lượng tổng ABCD nhiều nhất với giá trị tổng không quá 10.000.000 và tổng % giảm giá là <= 15%
2. Tìm % giảm số tiền/từng biến (giá) để có thể ""bán được số lượng tổng ABCD nhiều nhất với giá trị tổng thấp nhất và tổng % giảm giá là <= 15%
Em cảm ơn mọi người đã dành thời gian giúp em rất nhiều ạ!","Em chào anh/chị/bạn. Chúc mọi người buổi tối vui vẻ. Em đang có gặp vấn đề này mà chưa giải được sau 3 tối suy nghĩ. Em đăng lên đây để xin mọi người keyword để em tìm hướng em làm ạ. (Em đã thử dùng quy hoạch tuyến tính để giải nhưng có vẻ không ăn thua) Đề bài bằng lời: Cho 4 biến A - B - C - D, biết: - Số lượng: số đơn vị max của từng biến (VD: có 735 đơn vị của biến A trong kho). - Số tiền/từng biến: giá của biến (VD: mỗi đơn vị biến A có giá 5000 => tổng giá trị của biến A trong kho = 5000*735= 3675000) 1. Tìm % giảm số tiền/từng biến (giá) để có thể ""bán"" đc số lượng tổng ABCD nhiều nhất với giá trị tổng không quá 10.000.000 và tổng % giảm giá là <= 15% 2. Tìm % giảm số tiền/từng biến (giá) để có thể ""bán được số lượng tổng ABCD nhiều nhất với giá trị tổng thấp nhất và tổng % giảm giá là <= 15% Em cảm ơn mọi người đã dành thời gian giúp em rất nhiều ạ!",,"#Q&A, #math",,,
"Chào các bạn, mình vừa tốt nghiệp đại học và có gần 1 năm làm việc trong phòng nghiên cứu của trường (chuyên về Natural Language Processing). Trường mình thì không mạnh về mảng Deep Learning (chỉ có học Master và PhD mới có những lớp chuyên sâu), kiến thức chung mình có được là qua 2 lớp Data Science 1 và 2 của trường, còn lại hầu như là mình tự học, hỏi thầy mình, và kinh nghiệm mình có được do làm trong phòng lab của trường. Mình có thử nộp đơn xin việc qua 1 số công ty nhưng hầu như toàn bị từ chối mặc dù mình thấy resume và skills của mình match đa số những yêu cầu của công ty.
Mình muốn tìm công việc ở vị trí NLP developer hoặc NLP Engineering. Các bạn đã và đang làm việc ở vị trí này hoặc tương tự có thể tư vấn cho mình những kỹ năng và kiến thức nào mình cần bổ sung để có thể tìm được công việc ở mảng NLP với bằng đại học được không ạ? Mình đang sống ở bang Texas, thành phố Houston ạ.
Mình sẽ gởi kèm resume của mình. Xin cám ơn các bạn.
Chúc một ngày tốt lành","Chào các bạn, mình vừa tốt nghiệp đại học và có gần 1 năm làm việc trong phòng nghiên cứu của trường (chuyên về Natural Language Processing). Trường mình thì không mạnh về mảng Deep Learning (chỉ có học Master và PhD mới có những lớp chuyên sâu), kiến thức chung mình có được là qua 2 lớp Data Science 1 và 2 của trường, còn lại hầu như là mình tự học, hỏi thầy mình, và kinh nghiệm mình có được do làm trong phòng lab của trường. Mình có thử nộp đơn xin việc qua 1 số công ty nhưng hầu như toàn bị từ chối mặc dù mình thấy resume và skills của mình match đa số những yêu cầu của công ty. Mình muốn tìm công việc ở vị trí NLP developer hoặc NLP Engineering. Các bạn đã và đang làm việc ở vị trí này hoặc tương tự có thể tư vấn cho mình những kỹ năng và kiến thức nào mình cần bổ sung để có thể tìm được công việc ở mảng NLP với bằng đại học được không ạ? Mình đang sống ở bang Texas, thành phố Houston ạ. Mình sẽ gởi kèm resume của mình. Xin cám ơn các bạn. Chúc một ngày tốt lành",,,,,
"Xin chào mọi người,
Em đang dự định làm project về bài toán Machine Reading Comprehension trên ngôn ngữ tiếng Việt. Về dataset thì em dự định sử dụng bộ corpus news, với liên hệ xin bộ UIT-ViQuAD của thầy Kiệt.
Còn về mô hình, hướng tiếp cận với cũng như tài liệu tham khảo cả MRC trên tiếng Anh hay tiếng Việt, e cũng có đọc qua 1 số, nhưng vẫn muốn nhờ mọi người chia sẻ 1 số tài liệu, hướng dẫn thêm để em nghiên cứu đi đúng hướng hơn.
Cảm ơn mọi người :)","Xin chào mọi người, Em đang dự định làm project về bài toán Machine Reading Comprehension trên ngôn ngữ tiếng Việt. Về dataset thì em dự định sử dụng bộ corpus news, với liên hệ xin bộ UIT-ViQuAD của thầy Kiệt. Còn về mô hình, hướng tiếp cận với cũng như tài liệu tham khảo cả MRC trên tiếng Anh hay tiếng Việt, e cũng có đọc qua 1 số, nhưng vẫn muốn nhờ mọi người chia sẻ 1 số tài liệu, hướng dẫn thêm để em nghiên cứu đi đúng hướng hơn. Cảm ơn mọi người :)",,,,,
"Mình có 2 câu hỏi ngu mong mọi người giúp đỡ ạ, về demography problem.
1. Tỉ lệ Nam nữ là 9:1. Khi trình bày với người không biết gì về các metrics như F1-score thì mọi người hay dùng metric nào, chứ log loss thì không thể mang đi trình bày được.
2 Về độ tuổi. Giả sử mình chia làm 4 nhóm 18-, 18-30, 30-40, 40+. Nếu dự đoán nhóm 18-30 sai thành 30-40 thì loss sẽ thấp hơn dự đoán nhóm dưới 18 thành các nhóm khác, vì khi dự đoán 18- thành nhóm cao hơn, có thể bị gợi ý phim porn cho lứa tuổi vị thành niên. Mọi người sẽ định nghĩa loss function cho model thế nào ạ.
Cảm ơn mọi người đã đọc 🤗🤗","Mình có 2 câu hỏi ngu mong mọi người giúp đỡ ạ, về demography problem. 1. Tỉ lệ Nam nữ là 9:1. Khi trình bày với người không biết gì về các metrics như F1-score thì mọi người hay dùng metric nào, chứ log loss thì không thể mang đi trình bày được. 2 Về độ tuổi. Giả sử mình chia làm 4 nhóm 18-, 18-30, 30-40, 40+. Nếu dự đoán nhóm 18-30 sai thành 30-40 thì loss sẽ thấp hơn dự đoán nhóm dưới 18 thành các nhóm khác, vì khi dự đoán 18- thành nhóm cao hơn, có thể bị gợi ý phim porn cho lứa tuổi vị thành niên. Mọi người sẽ định nghĩa loss function cho model thế nào ạ. Cảm ơn mọi người đã đọc",,,,,
"Xin chào các anh chị,
Trong quá trình học môn Đại số, thầy dạy của em có đặt câu hỏi: :""Mục đích/Lí do của việc áp dụng thuật toán PCA vào ma trận?"". Theo em nghĩ thì do tầm quan trọng của ma trận, đa số dữ liệu đều được biểu diễn bởi nó và ma trận là công cụ toán học hữu ích trong đại số tuyến tính. Không biết suy nghĩ của em đúng chưa ạ? Rất mong nhận được câu trả lời đầy đủ từ các anh chị.
Em xin cảm ơn.","Xin chào các anh chị, Trong quá trình học môn Đại số, thầy dạy của em có đặt câu hỏi: :""Mục đích/Lí do của việc áp dụng thuật toán PCA vào ma trận?"". Theo em nghĩ thì do tầm quan trọng của ma trận, đa số dữ liệu đều được biểu diễn bởi nó và ma trận là công cụ toán học hữu ích trong đại số tuyến tính. Không biết suy nghĩ của em đúng chưa ạ? Rất mong nhận được câu trả lời đầy đủ từ các anh chị. Em xin cảm ơn.",,"#Q&A, #math",,,
"Chào mọi người, mình đang tìm hiểu cơ chế multihead self attention của Transformer và có một số thắc mắc:
- Mình tham khảo trên blog http://jalammar.github.io/illustrated-transformer/ thì hiểu rằng, với mỗi head thứ i, để tạo ra ba ma trận Qi, Ki, Vi tương ứng thì ta cần nhân embedding của từ ban đầu với các ma trận Wqi, Wki và Wvi tương ứng.
- Tuy nhiên, trong paper chính https://arxiv.org/pdf/1706.03762.pdf thì mình thấy tác giả có chiếu Q, K, V qua một hàm Linear, cụ thể như ảnh phía dưới. Chỗ này mình hơi confused một chút, không biết Q, K, V ở đây được tính thế nào, và mình cảm giác nó hơi khác so với blog mình đọc ở trên.
Rất mong được các cao thủ chỉ giáo ạ, many thanks 🙂","Chào mọi người, mình đang tìm hiểu cơ chế multihead self attention của Transformer và có một số thắc mắc: - Mình tham khảo trên blog http://jalammar.github.io/illustrated-transformer/ thì hiểu rằng, với mỗi head thứ i, để tạo ra ba ma trận Qi, Ki, Vi tương ứng thì ta cần nhân embedding của từ ban đầu với các ma trận Wqi, Wki và Wvi tương ứng. - Tuy nhiên, trong paper chính https://arxiv.org/pdf/1706.03762.pdf thì mình thấy tác giả có chiếu Q, K, V qua một hàm Linear, cụ thể như ảnh phía dưới. Chỗ này mình hơi confused một chút, không biết Q, K, V ở đây được tính thế nào, và mình cảm giác nó hơi khác so với blog mình đọc ở trên. Rất mong được các cao thủ chỉ giáo ạ, many thanks",,,,,
"Chào mọi người, cho em hỏi một chút về hàm cross-entropy loss trong hình dưới ạ. Có tài liệu gì giải thích rõ ràng không ạ, em chưa rõ ràng về chỗ "" hypothesis h with respect to a distribution D""? Em xin cảm ơn.","Chào mọi người, cho em hỏi một chút về hàm cross-entropy loss trong hình dưới ạ. Có tài liệu gì giải thích rõ ràng không ạ, em chưa rõ ràng về chỗ "" hypothesis h with respect to a distribution D""? Em xin cảm ơn.",,,,,
"Em xin chào mọi người ạ.
Hiện tại em có làm đề tài về so sánh image và caption thông qua embedding. Em có làm theo phương pháp như hình dưới, tuy nhiên do lựa chọn max score nên em tính tách 2 bước FNN thành 2 neural network riêng biệt (Do caption thì cố định 2 caption nhưng bounding box trong image thì có thể có rất nhiều).
Em muốn hỏi là khi em tính loss theo (S_rand - S_match) thì lúc backward thì liệu loss có áp dụng cho cả 2 neural network không và liệu làm vậy thì có sai logic dẫn tới việc model không hội tụ không ạ?
Em xin cảm ơn ạ.","Em xin chào mọi người ạ. Hiện tại em có làm đề tài về so sánh image và caption thông qua embedding. Em có làm theo phương pháp như hình dưới, tuy nhiên do lựa chọn max score nên em tính tách 2 bước FNN thành 2 neural network riêng biệt (Do caption thì cố định 2 caption nhưng bounding box trong image thì có thể có rất nhiều). Em muốn hỏi là khi em tính loss theo (S_rand - S_match) thì lúc backward thì liệu loss có áp dụng cho cả 2 neural network không và liệu làm vậy thì có sai logic dẫn tới việc model không hội tụ không ạ? Em xin cảm ơn ạ.",,,,,
"[AI Share]
Roadmap và keyword về Natural Language Processing (NLP) dành cho sinh viên đang băn khoăn không biết học gì =))
Bao gồm Probability & Statistics, Machine Learning, Text Mining, Natural Language Processing.
Chi tiết: https://github.com/graykode/nlp-roadmap
___________________________
Nếu gặp khó khăn trong việc học các kiến thức nền tảng lĩnh vực trí tuệ nhân tạo, đừng quên các khóa học bổ ích của AI4E nhé. Khóa học Deep Learning cơ bản online sẽ được khai giảng vào 19h ngày 5/6 nhé. Nhanh tay inbox page để được tham gia lớp học bổ ích này nhé.","[AI Share] Roadmap và keyword về Natural Language Processing (NLP) dành cho sinh viên đang băn khoăn không biết học gì =)) Bao gồm Probability & Statistics, Machine Learning, Text Mining, Natural Language Processing. Chi tiết: https://github.com/graykode/nlp-roadmap ___________________________ Nếu gặp khó khăn trong việc học các kiến thức nền tảng lĩnh vực trí tuệ nhân tạo, đừng quên các khóa học bổ ích của AI4E nhé. Khóa học Deep Learning cơ bản online sẽ được khai giảng vào 19h ngày 5/6 nhé. Nhanh tay inbox page để được tham gia lớp học bổ ích này nhé.",,,,,
"Chia sẻ về AIcargo: một app phục vụhost & chia sẻ AI models
Chào các anh chị em trên diễn đàn,
Mình và các đồng nghiệp ở Aitomatic Inc hiện đang xây dựng một framework & toolchain cho việc tạo các full-stack AI Apps. Trong bước đầu của lộ trình này, bọn mình vừa tạo ra một cái công cụ miễn phí tên là AIcargo để giúp các bạn làm DS/AI dễ dàng host các models trên cloud và chia sẻ với những người khác cho những mục đích như là interactive testing, cloud-based inference API hay là làm hồ sơ tuyển dụng.
Bọn mình vừa làm xong bản beta, trước hết là hỗ trợ use case phổ biến nhất là Image Classification. Trong các releases thời gian tới đây thì sẽ tiếp tục hỗ trợ các models thông dụng khác như NLP, Time Series, Forecasting, Anomaly Detection...
Admin cho phép mình chia sẻ công cụ AIcargo này ở đây để cộng đồng cùng biết để có thể thử sử dụng và phản hồi ý kiến lại cho mình nhé!
Thanks mọi người nhiều!","Chia sẻ về AIcargo: một app phục vụ host & chia sẻ AI models Chào các anh chị em trên diễn đàn, Mình và các đồng nghiệp ở Aitomatic Inc hiện đang xây dựng một framework & toolchain cho việc tạo các full-stack AI Apps. Trong bước đầu của lộ trình này, bọn mình vừa tạo ra một cái công cụ miễn phí tên là AIcargo để giúp các bạn làm DS/AI dễ dàng host các models trên cloud và chia sẻ với những người khác cho những mục đích như là interactive testing, cloud-based inference API hay là làm hồ sơ tuyển dụng. Bọn mình vừa làm xong bản beta, trước hết là hỗ trợ use case phổ biến nhất là Image Classification. Trong các releases thời gian tới đây thì sẽ tiếp tục hỗ trợ các models thông dụng khác như NLP, Time Series, Forecasting, Anomaly Detection... Admin cho phép mình chia sẻ công cụ AIcargo này ở đây để cộng đồng cùng biết để có thể thử sử dụng và phản hồi ý kiến lại cho mình nhé! Thanks mọi người nhiều!",,,,,
RepVGG là một cải tiến của mô hình khá nổi tiếng đã ra đời khá lâu là VGG. Mình cũng đã thử dùng mô hình này làm backbone trong một vài bài toán và thu được một số kết quả tốt hơn rất nhiều so với các mô hình mà mình từng sử dụng. Gần đây mình cũng có viết một bài tìm hiểu một chút về mô hình này và chia sẻ cho mọi người. Hy vọng bài viết sẽ giúp ích nhiều cho các bạn trong quá trình tìm hiểu mô hình này ạ. :),RepVGG là một cải tiến của mô hình khá nổi tiếng đã ra đời khá lâu là VGG. Mình cũng đã thử dùng mô hình này làm backbone trong một vài bài toán và thu được một số kết quả tốt hơn rất nhiều so với các mô hình mà mình từng sử dụng. Gần đây mình cũng có viết một bài tìm hiểu một chút về mô hình này và chia sẻ cho mọi người. Hy vọng bài viết sẽ giúp ích nhiều cho các bạn trong quá trình tìm hiểu mô hình này ạ. :),,,,,
#visualization,,#visualization,,,,
"[Hỏi đáp/Tư vấn] Xin lời khuyên từ anh/chị đi trước
[Đã edit]
Em xin chào các anh/chị/bạn.
Em vừa tốt nghiệp đh ngành khmt và đang muốn tiến lên thạc sĩ. Em có tham khảo được chương trình Data Science professional của AIT - kết hợp với đại học SET (link bên dưới ạ) với học phí 15k$ (~350tr vnđ) và học chỉ trong 1 năm.
Không biết có anh/chị nào đi trước đã từng học hay biết về ctr này có thể cho lời khuyên (về chất lượng, uy tín, cơ hội sau học,...) giúp e được không ạ. Em xin cảm ơn.
Link khóa học:","[Hỏi đáp/Tư vấn] Xin lời khuyên từ anh/chị đi trước [Đã edit] Em xin chào các anh/chị/bạn. Em vừa tốt nghiệp đh ngành khmt và đang muốn tiến lên thạc sĩ. Em có tham khảo được chương trình Data Science professional của AIT - kết hợp với đại học SET (link bên dưới ạ) với học phí 15k$ (~350tr vnđ) và học chỉ trong 1 năm. Không biết có anh/chị nào đi trước đã từng học hay biết về ctr này có thể cho lời khuyên (về chất lượng, uy tín, cơ hội sau học,...) giúp e được không ạ. Em xin cảm ơn. Link khóa học:",,,,,
Giáo sư cực mạnh nhé các bạn.,Giáo sư cực mạnh nhé các bạn.,,,,,
"Chào mọi người, tình hình là mình vừa nghiên cứu về kiểu dữ liệu trong ngôn ngữ C++ và python. Mình post lên đây nhằm mục đích chia sẻ cũng như mong nhận được sự góp ý nếu mình có nghiên cứu sai.
Kích thước của các kiểu dữ liệu phổ biến trong C++:
sizeof(int) and sizeof(float) is 4 bytes
sizeof(double) is 8 bytes
sizeof(pointer) is 4 or 8 bytes
Kích thước của các kiểu dữ liệu phổ biến trong Python2:
sizeof(int), sizeof(float) is 8 bytes
Trong python2 và python3, double coi như float
Nhưng thực tế nếu bạn gõ cú pháp sys.getsizeof() thì sẽ nhận về giá trị: 16 bytes (kích thước kiểu cơ bản của python2) + kích thước kiểu dữ liệu (có thể mở rộng nếu vượt quá range: +8, + 16, ...)
Trong python3
sizeof(int) is 4 bytes, nếu vượt quá range sẽ mở rộng ( +4, +8, ...)
Nếu bạn gõ cú pháp sys.getsizeof() thì sẽ nhận về giá trị: 24 bytes (kích thước kiểu cơ bản của python2) + kích thước kiểu dữ liệu
Đặt biệt: số 0 và số ""float"" có getsizeof() là 24 + 0 (Ngay tại đây nếu có người hỏi mình kiểu float trong python3 có kích thước bao nhiêu thật sự mình không biết câu trả lời).

Cảm ơn.","Chào mọi người, tình hình là mình vừa nghiên cứu về kiểu dữ liệu trong ngôn ngữ C++ và python. Mình post lên đây nhằm mục đích chia sẻ cũng như mong nhận được sự góp ý nếu mình có nghiên cứu sai. Kích thước của các kiểu dữ liệu phổ biến trong C++: sizeof(int) and sizeof(float) is 4 bytes sizeof(double) is 8 bytes sizeof(pointer) is 4 or 8 bytes Kích thước của các kiểu dữ liệu phổ biến trong Python2: sizeof(int), sizeof(float) is 8 bytes Trong python2 và python3, double coi như float Nhưng thực tế nếu bạn gõ cú pháp sys.getsizeof() thì sẽ nhận về giá trị: 16 bytes (kích thước kiểu cơ bản của python2) + kích thước kiểu dữ liệu (có thể mở rộng nếu vượt quá range: +8, + 16, ...) Trong python3 sizeof(int) is 4 bytes, nếu vượt quá range sẽ mở rộng ( +4, +8, ...) Nếu bạn gõ cú pháp sys.getsizeof() thì sẽ nhận về giá trị: 24 bytes (kích thước kiểu cơ bản của python2) + kích thước kiểu dữ liệu Đặt biệt: số 0 và số ""float"" có getsizeof() là 24 + 0 (Ngay tại đây nếu có người hỏi mình kiểu float trong python3 có kích thước bao nhiêu thật sự mình không biết câu trả lời). Cảm ơn.",,,,,
"Công bố mới về model mới (có lẽ cách đây vài giờ) sử dụng kiến trúc Multi-layer Perceptron của Google Brain, với tên gọi là gMLP (với g ở đây ngụ ý MLP with gating). Hay ở chỗ các models mới trong bài báo hoạt động tốt cùng 1 lúc cả với tác vụ xử lí ngôn ngữ tự nhiên (Natural Language Processing), và cả với xử lí ảnh (Computer Vision).
Kiến trúc cơ bản gMLP xem hình bên dưới
Bài báo tại đây: https://arxiv.org/pdf/2105.08050.pdf","Công bố mới về model mới (có lẽ cách đây vài giờ) sử dụng kiến trúc Multi-layer Perceptron của Google Brain, với tên gọi là gMLP (với g ở đây ngụ ý MLP with gating). Hay ở chỗ các models mới trong bài báo hoạt động tốt cùng 1 lúc cả với tác vụ xử lí ngôn ngữ tự nhiên (Natural Language Processing), và cả với xử lí ảnh (Computer Vision). Kiến trúc cơ bản gMLP xem hình bên dưới Bài báo tại đây: https://arxiv.org/pdf/2105.08050.pdf",,,,,
"Nay mình mất ngủ, dậy đọc sách thấy trùng hợp ngẫu nhiên...
Thấy page 230, chương 5, sách Pattern Recognition and Machine Learning, Bishop, 2006 có viết:
""Another generalization of the network architecture is to include skip-layer connections, each of which is associated with a corresponding adaptive parameter. For instance, in a two-layer network these would go directly from inputs to outputs""
Chẳng phải trùng ý tưởng skip-layer connection của Resnet, 2015 (https://arxiv.org/pdf/1512.03385.pdf) đình đám ư. Nhiều lúc cái gì mới và rất đột phá đến từ những gì đơn giản.
Lúc mình search ảnh cho post ra 1 bài báo: ""All new ideas are made of old ideas"" (https://www.management-issues.com/opinion/4678/all-new-ideas-are-made-of-old-ideas/), mọi người đọc thêm =))","Nay mình mất ngủ, dậy đọc sách thấy trùng hợp ngẫu nhiên... Thấy page 230, chương 5, sách Pattern Recognition and Machine Learning, Bishop, 2006 có viết: ""Another generalization of the network architecture is to include skip-layer connections, each of which is associated with a corresponding adaptive parameter. For instance, in a two-layer network these would go directly from inputs to outputs"" Chẳng phải trùng ý tưởng skip-layer connection của Resnet, 2015 (https://arxiv.org/pdf/1512.03385.pdf) đình đám ư. Nhiều lúc cái gì mới và rất đột phá đến từ những gì đơn giản. Lúc mình search ảnh cho post ra 1 bài báo: ""All new ideas are made of old ideas"" (https://www.management-issues.com/opinion/4678/all-new-ideas-are-made-of-old-ideas/), mọi người đọc thêm =))",,,,,
"Xin hướng đi theo AI cho người đã đi làm.
Hello mng.
Mình đã ra trường và cũng khá thích AI. Trong thời gian sinh viên cũng có đi theo thầy làm 1 vài dự án về DL, AI.
Tuy nhiên hiện tại mình không biết phải tiếp tục sao nữa, vì những kiến thức cơ bản mình nghĩ mình đã hiểu và áp dụng được rồi. Hiện tại mình cũng chỉ loanh quanh đọc paper rồi code theo, điều này làm mình cảm giác không làm mình tăng trình độ lên được và nó không cho mình thấy mình đạt được cột mốc nào cả.
Vì ngày xưa ít nhất còn có giảng viên nói cho mình là chỗ này e làm đúng, chỗ này e làm chưa đúng ...
Vậy không biết trong forum có ai đang trong tình trạng giống em không ạ. Và có thể cho e xin lời khuyên trong trường hợp này được với.","Xin hướng đi theo AI cho người đã đi làm. Hello mng. Mình đã ra trường và cũng khá thích AI. Trong thời gian sinh viên cũng có đi theo thầy làm 1 vài dự án về DL, AI. Tuy nhiên hiện tại mình không biết phải tiếp tục sao nữa, vì những kiến thức cơ bản mình nghĩ mình đã hiểu và áp dụng được rồi. Hiện tại mình cũng chỉ loanh quanh đọc paper rồi code theo, điều này làm mình cảm giác không làm mình tăng trình độ lên được và nó không cho mình thấy mình đạt được cột mốc nào cả. Vì ngày xưa ít nhất còn có giảng viên nói cho mình là chỗ này e làm đúng, chỗ này e làm chưa đúng ... Vậy không biết trong forum có ai đang trong tình trạng giống em không ạ. Và có thể cho e xin lời khuyên trong trường hợp này được với.",,,,,
"Mọi người ơi cho em hỏi, mấy methods gần đây như là Mixer-MLP hay cũ hơn tí là Vision Transformer thường cần nhiều data hơn là CNN bởi inductive bias. Với custom dataset của mọi người dùng thì tầm bao nhiêu ảnh thì mọi người thấy hiệu quả của Vision ngang bằng CNN nhỉ?","Mọi người ơi cho em hỏi, mấy methods gần đây như là Mixer-MLP hay cũ hơn tí là Vision Transformer thường cần nhiều data hơn là CNN bởi inductive bias. Với custom dataset của mọi người dùng thì tầm bao nhiêu ảnh thì mọi người thấy hiệu quả của Vision ngang bằng CNN nhỉ?",,,,,
"[Hỏi đáp/Tư vấn] Chào mọi người,
Hiện tại em đã hoàn thành khoá học Python cơ bản và cũng như hoàn tất năm nhất (major của em là CS) tại trường em, nên em đang đang đặt cho bản thân những câu hỏi mình nên làm gì tiếp để có thể có đầy đủ kiến thức và bắt đầu theo đuổi lĩnh vực Machine Learning, và cũng như nên khởi đầu tự làm những projects dạng như thế nào để làm đẹp CV/resume của bản thân.
Em xin cảm ơn mọi người đã giúp đỡ em.","[Hỏi đáp/Tư vấn] Chào mọi người, Hiện tại em đã hoàn thành khoá học Python cơ bản và cũng như hoàn tất năm nhất (major của em là CS) tại trường em, nên em đang đang đặt cho bản thân những câu hỏi mình nên làm gì tiếp để có thể có đầy đủ kiến thức và bắt đầu theo đuổi lĩnh vực Machine Learning, và cũng như nên khởi đầu tự làm những projects dạng như thế nào để làm đẹp CV/resume của bản thân. Em xin cảm ơn mọi người đã giúp đỡ em.",,,,,
"Hello mọi người, cho em hỏi ai có kinh nghiệm về features scaling thì: anh/chị thường dùng phương pháp nào giữa ""MinMaxScaler"" và ""StandardScaler"", trong thực tế công việc hiện nay người ta thường dùng phương pháp nào và tại sao ạ. Em đã tìm nhiều tài liệu nói về 2 phương pháp này nhưng chủ yếu đều là định nghĩa và ít giải thích khi nào nên sử dụng pp nào...Em cảm ơn anh/chị rất nhiều ạ!!!!
#ML #Preprocessing #FeatureScaling","Hello mọi người, cho em hỏi ai có kinh nghiệm về features scaling thì: anh/chị thường dùng phương pháp nào giữa ""MinMaxScaler"" và ""StandardScaler"", trong thực tế công việc hiện nay người ta thường dùng phương pháp nào và tại sao ạ. Em đã tìm nhiều tài liệu nói về 2 phương pháp này nhưng chủ yếu đều là định nghĩa và ít giải thích khi nào nên sử dụng pp nào...Em cảm ơn anh/chị rất nhiều ạ!!!!",#ML	#Preprocessing	#FeatureScaling,,,,
"Em chào mọi người, hiện em đang nghiên cứu mô hình ARIMA. Em chưa hiểu kĩ về phương pháp xác định hệ số PACF. Theo em biết thì hệ số PACF thể hiện mối quan hệ giữa chuỗi hiện tại với chuỗi trễ (k). Nhưng sao trong công thức, ta lại quan tâm đến chuỗi trễ của độ trễ k vậy ạ.?
Em cảm ơn ạ.","Em chào mọi người, hiện em đang nghiên cứu mô hình ARIMA. Em chưa hiểu kĩ về phương pháp xác định hệ số PACF. Theo em biết thì hệ số PACF thể hiện mối quan hệ giữa chuỗi hiện tại với chuỗi trễ (k). Nhưng sao trong công thức, ta lại quan tâm đến chuỗi trễ của độ trễ k vậy ạ.? Em cảm ơn ạ.",,,,,
"Standford có khóa này có vẻ mới
CS221: Artificial Intelligence: Principles and Techniques
Stanford / Autumn 2019-2020
https://stanford-cs221.github.io/autumn2019/#schedule
video trên youtube:
https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX
Ngoài ra mình còn liệt kê thêm nhiều khóa khác về ML và lập trinh ở đây nếu bạn nào cần
http://ellienguyen.style/StatML/data_science_resource.html
Tính ra Stanford cũng có khá nhiều khóa online",Standford có khóa này có vẻ mới CS221: Artificial Intelligence: Principles and Techniques Stanford / Autumn 2019-2020 https://stanford-cs221.github.io/autumn2019/#schedule video trên youtube: https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX Ngoài ra mình còn liệt kê thêm nhiều khóa khác về ML và lập trinh ở đây nếu bạn nào cần http://ellienguyen.style/StatML/data_science_resource.html Tính ra Stanford cũng có khá nhiều khóa online,,,,,
"em đang làm đồ án về nhận diện khuôn mặt có một số câu hỏi mong mng giải thích giúp em ạ:
em đang áp dụng phương pháp one shot learning để nhận diện. theo bài viết của anh thắng (https://www.miai.vn/.../face-recog-2-0-nhan-dien-khuon.../) em thấy có sử dụng SVM để trích xuất vector từ data. nếu em sử dụng Data Augumentation để tạo ra nhiều ảnh mới từ 1 ảnh gốc rồi sử dụng SVM thì còn được gọi là one shot learning nữa không ạ?
hiện em đang dùng pretrained model ở đây (https://github.com/iwantooxxoox/Keras-OpenFace). em muốn train lại bằng dataset vn_celeb của bên Sun* để tăng độ chính xác khi nhận diện mặt người Việt. em có áp dụng theo blog này (https://phamdinhkhanh.github.io/2020/03/21/faceNet.html...) nhưng hiện đang bị lỗi như ảnh. em có thử xóa ""stratify=""y_labels"" thì đến lúc train loss báo là NAN. khi áp dụng dataset mà không tách mặt thì lại chạy bình thường. lỗi này có cách khắc phục nào ạ?
em dự tính sẽ deploy project lên raspberry pi. em có 2 lựa chọn camera là picamera hoặc là camera usb. cho em hỏi là nếu sử dụng cam usb thì có độ trễ hay chậm hơn là picam cắm trực tiếp lên mobo không ạ?
em cảm ơn ạ.","em đang làm đồ án về nhận diện khuôn mặt có một số câu hỏi mong mng giải thích giúp em ạ: em đang áp dụng phương pháp one shot learning để nhận diện. theo bài viết của anh thắng (https://www.miai.vn/.../face-recog-2-0-nhan-dien-khuon.../) em thấy có sử dụng SVM để trích xuất vector từ data. nếu em sử dụng Data Augumentation để tạo ra nhiều ảnh mới từ 1 ảnh gốc rồi sử dụng SVM thì còn được gọi là one shot learning nữa không ạ? hiện em đang dùng pretrained model ở đây (https://github.com/iwantooxxoox/Keras-OpenFace). em muốn train lại bằng dataset vn_celeb của bên Sun* để tăng độ chính xác khi nhận diện mặt người Việt. em có áp dụng theo blog này (https://phamdinhkhanh.github.io/2020/03/21/faceNet.html...) nhưng hiện đang bị lỗi như ảnh. em có thử xóa ""stratify=""y_labels"" thì đến lúc train loss báo là NAN. khi áp dụng dataset mà không tách mặt thì lại chạy bình thường. lỗi này có cách khắc phục nào ạ? em dự tính sẽ deploy project lên raspberry pi. em có 2 lựa chọn camera là picamera hoặc là camera usb. cho em hỏi là nếu sử dụng cam usb thì có độ trễ hay chậm hơn là picam cắm trực tiếp lên mobo không ạ? em cảm ơn ạ.",,,,,
"Cho mình hỏi là trong giải thuật content-base recommendation system lúc mình chuyển dữ liệu sang TFIDF thì thông tin mô tả của sản phẩm mình có cần ghi tiêu đề của nó vô luôn không mọi người? Ví dụ mình đặc tả sản phẩm như sau:
RAM: 8 GB
Bộ nhớ trong: 64 GB
Lúc mình convert sang TFIDF mình có nên lấy hết luôn thông tin (tức là bao gồm cả tiêu đề là RAM, bộ nhớ trong) không hay chỉ cần lấy giá trị của nó là 8 GB và 64 GB là được?
Tại mỗi sản phẩm nhiều khi đặc tả thông tin khác nhau nên mình không rõ lắm. Mong mọi người giúp đỡ.","Cho mình hỏi là trong giải thuật content-base recommendation system lúc mình chuyển dữ liệu sang TFIDF thì thông tin mô tả của sản phẩm mình có cần ghi tiêu đề của nó vô luôn không mọi người? Ví dụ mình đặc tả sản phẩm như sau: RAM: 8 GB Bộ nhớ trong: 64 GB Lúc mình convert sang TFIDF mình có nên lấy hết luôn thông tin (tức là bao gồm cả tiêu đề là RAM, bộ nhớ trong) không hay chỉ cần lấy giá trị của nó là 8 GB và 64 GB là được? Tại mỗi sản phẩm nhiều khi đặc tả thông tin khác nhau nên mình không rõ lắm. Mong mọi người giúp đỡ.",,,,,
Read papers with me,Read papers with me,,,,,
"[TÌM MENTOR]
Em chào anh/chị
Em đang cần học cách deploy model machine learning bằng Python hoặc R, nên nếu có ai dạy kèm được thì liên hệ với ạ. Trước em học kinh tế và hiện đang làm fresher DS.
Em cảm ơn mọi người đã đọc.","[TÌM MENTOR] Em chào anh/chị Em đang cần học cách deploy model machine learning bằng Python hoặc R, nên nếu có ai dạy kèm được thì liên hệ với ạ. Trước em học kinh tế và hiện đang làm fresher DS. Em cảm ơn mọi người đã đọc.",,,,,
"Em chào các anh/chị
Hiện tại em đang mắc vấn đề này, anh/chị cho em hỏi với ạ. Em chạy thuật toán DBscan với bộ dữ liệu 260 triệu dòng. Tuy nhiên vì dữ liệu quá lớn làm cho R không đủ bộ nhớ. Vậy có cách nào xử lý vấn đề này k ạ? Chuyển qua chạy trên python hay chạy song song bằng cách sử dụng Microsoft R Open có được k ạ?","Em chào các anh/chị Hiện tại em đang mắc vấn đề này, anh/chị cho em hỏi với ạ. Em chạy thuật toán DBscan với bộ dữ liệu 260 triệu dòng. Tuy nhiên vì dữ liệu quá lớn làm cho R không đủ bộ nhớ. Vậy có cách nào xử lý vấn đề này k ạ? Chuyển qua chạy trên python hay chạy song song bằng cách sử dụng Microsoft R Open có được k ạ?",,,,,
"Xin chào mọi người em có một câu hỏi về chủ đề voice-reidentification với deep learning. Bình thường thì task liên quan đến voice sẽ có 2 dạng như sau:
voice identification: Người dùng nhận diện là người A và hệ thống sẽ kiểm tra xem giọng nói có đúng với template người A đã có sẵn không.
voice recognition: Người dùng nói vào và hệ thống kiểm tra input với tất cả những mẫu đã được train trước đó để xem giống ai nhất. 
Tuy nhiên hiện tại em có 1 task hơi khác một chút, sau khi train 1 model DL với tập training (voice recording) có label. Em sẽ phải evaluate trên một test set và xác định hai voice recording có thuộc cùng một người không (i.e., đưa ra một distance matrix). Tuy nhiên em được biết trước là toàn bộ dữ liệu trong test set sẽ không thuộc về một ai trong training set cả. Em đang thử nghiệm với Siamese neural network tuy nhiên vẫn chưa thực sự thành công. Nên em lên đây hỏi về định hướng, tài liệu, hay keyword mà em nên nhìn vào.
Em cảm ơn.","Xin chào mọi người em có một câu hỏi về chủ đề voice-reidentification với deep learning. Bình thường thì task liên quan đến voice sẽ có 2 dạng như sau: voice identification: Người dùng nhận diện là người A và hệ thống sẽ kiểm tra xem giọng nói có đúng với template người A đã có sẵn không. voice recognition: Người dùng nói vào và hệ thống kiểm tra input với tất cả những mẫu đã được train trước đó để xem giống ai nhất. Tuy nhiên hiện tại em có 1 task hơi khác một chút, sau khi train 1 model DL với tập training (voice recording) có label. Em sẽ phải evaluate trên một test set và xác định hai voice recording có thuộc cùng một người không (i.e., đưa ra một distance matrix). Tuy nhiên em được biết trước là toàn bộ dữ liệu trong test set sẽ không thuộc về một ai trong training set cả. Em đang thử nghiệm với Siamese neural network tuy nhiên vẫn chưa thực sự thành công. Nên em lên đây hỏi về định hướng, tài liệu, hay keyword mà em nên nhìn vào. Em cảm ơn.",,,,,
"App này bọn mình dùng thư viện VietOCR của anh Quoc Pham.
Cảm ơn anh Quoc Pham vì thư viện rất tuyệt vời này.
Hiện tại cần cải thiện thêm ở phần chữ viết tay.
Các bạn có thể cho bọn mình xin gợi ý mảng nhận dạng chữ viết tay tiếng Việt được không ạ?",App này bọn mình dùng thư viện VietOCR của anh Quoc Pham. Cảm ơn anh Quoc Pham vì thư viện rất tuyệt vời này. Hiện tại cần cải thiện thêm ở phần chữ viết tay. Các bạn có thể cho bọn mình xin gợi ý mảng nhận dạng chữ viết tay tiếng Việt được không ạ?,,,,,
"Kính chào các bác, em đang tập nghiên cứu về Reinforcement Learning nên em mạnh dạn làm clip chia sẻ, cùng nhau tìm hiểu về Reinforcement Learning với bài toán Tìm hiểu và dạy máy tính chơi game với Q - Learning.
Hi vọng món mới này sẽ mang lại hứng khởi cho anh em mới học!
Cảm ơn các bác và cảm ơn admin duyệt bài!","Kính chào các bác, em đang tập nghiên cứu về Reinforcement Learning nên em mạnh dạn làm clip chia sẻ, cùng nhau tìm hiểu về Reinforcement Learning với bài toán Tìm hiểu và dạy máy tính chơi game với Q - Learning. Hi vọng món mới này sẽ mang lại hứng khởi cho anh em mới học! Cảm ơn các bác và cảm ơn admin duyệt bài!",,,,,
"cho mình hỏi một chút là trong lý thuyết machinelearning cơ bản thì phần recommendation System thì theo lý thuyết là phải có user đánh giá cho các bộ phim rồi ta dùng Ridge để tính w, b, đòi hỏi phải có giá trị rating. Nhưng nếu áp dụng trong cơ sở dữ liệu khác là gợi ý các sản phẩm tương tự thì ta lại không có rating này thì có phải là ta sẽ convert các thông tin của từng hàng dữ liệu sang TFIDF để ra Vector đặc trưng, sau đó sẽ áp dụng hàm cosin để tính độ tương tự được không mọi người?","cho mình hỏi một chút là trong lý thuyết machinelearning cơ bản thì phần recommendation System thì theo lý thuyết là phải có user đánh giá cho các bộ phim rồi ta dùng Ridge để tính w, b, đòi hỏi phải có giá trị rating. Nhưng nếu áp dụng trong cơ sở dữ liệu khác là gợi ý các sản phẩm tương tự thì ta lại không có rating này thì có phải là ta sẽ convert các thông tin của từng hàng dữ liệu sang TFIDF để ra Vector đặc trưng, sau đó sẽ áp dụng hàm cosin để tính độ tương tự được không mọi người?",,,,,
#information_theory,,#information_theory,,,,
"Chào mọi người, em đang train yolov5 có 4 classes, kết quả cofusion matrix như vầy, nhưng em không hiểu giá trị của cột Back Ground FP, tại sao tổng giá trị FP của 4 classes của em phải luôn bằng 1. Có anh/chị nào giúp em thắc mắc này với, em k biết em làm sai gì nữa","Chào mọi người, em đang train yolov5 có 4 classes, kết quả cofusion matrix như vầy, nhưng em không hiểu giá trị của cột Back Ground FP, tại sao tổng giá trị FP của 4 classes của em phải luôn bằng 1. Có anh/chị nào giúp em thắc mắc này với, em k biết em làm sai gì nữa",,,,,
"Tìm đối tác thương mại hoá sản phẩm phân loại tình trạng tế bào mao mạch võng mạc mắt.
Như 1 stt trước mình có demo một số kết quả về bài toán Retinal Optical Coherence Tomography (Bài báo gốc ở đây: https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5; với dataset tại đây https://data.mendeley.com/datasets/rscbjbr9sj/2)
Và mới tháng trước, bên Đức họ đang triển khai dự án Opthalmo-AI, theo mình rất nhiều ý nghĩa. Các bạn có thể xem thông cáo báo chí của dự án tại đây https://www.dfki.de/en/web/news/detail/News/ohthalmo-ai-started/?fbclid=IwAR3ffeDlV5EJ7iNRBXoZ_Q8RVrbBbxULSbwxMw-fApYvNluwbYaSZ8Qa4Qw
Kết quả của mình lặp lại thí nghiệm của họ với model của riêng mình đạt kết quả vượt trội, các bạn xem hình 1.
Tuy nhiên bài báo có vấn đề về thiết kế thí nghiệm dẫn tới hiện tượng overfiting (các bạn có thể xem Fig. 3 trong bài), nên mình nghi ngờ có vấn đề. Kết quả mình phát hiện ra hiện tượng trùng lặp dữ liệu ở test set. Mình đã xử lí và cho ra huấn luyện với model mạnh hơn (model này mình chưa fine-tune nhé và nếu emsemble nhiều models có thể sẽ cho kết quả tốt hơn) và cho ra kết quả như hình 2.
Vậy bạn/công ti nào có hứng thú và có khả năng thương mại hoá có thể liên lạc với mình để thảo luận vấn đề và giải quyết vấn đề này cho người Việt và Việt Nam. Đó là mong muốn lớn nhất của mình!","Tìm đối tác thương mại hoá sản phẩm phân loại tình trạng tế bào mao mạch võng mạc mắt. Như 1 stt trước mình có demo một số kết quả về bài toán Retinal Optical Coherence Tomography (Bài báo gốc ở đây: https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5; với dataset tại đây https://data.mendeley.com/datasets/rscbjbr9sj/2) Và mới tháng trước, bên Đức họ đang triển khai dự án Opthalmo-AI, theo mình rất nhiều ý nghĩa. Các bạn có thể xem thông cáo báo chí của dự án tại đây https://www.dfki.de/en/web/news/detail/News/ohthalmo-ai-started/?fbclid=IwAR3ffeDlV5EJ7iNRBXoZ_Q8RVrbBbxULSbwxMw-fApYvNluwbYaSZ8Qa4Qw Kết quả của mình lặp lại thí nghiệm của họ với model của riêng mình đạt kết quả vượt trội, các bạn xem hình 1. Tuy nhiên bài báo có vấn đề về thiết kế thí nghiệm dẫn tới hiện tượng overfiting (các bạn có thể xem Fig. 3 trong bài), nên mình nghi ngờ có vấn đề. Kết quả mình phát hiện ra hiện tượng trùng lặp dữ liệu ở test set. Mình đã xử lí và cho ra huấn luyện với model mạnh hơn (model này mình chưa fine-tune nhé và nếu emsemble nhiều models có thể sẽ cho kết quả tốt hơn) và cho ra kết quả như hình 2. Vậy bạn/công ti nào có hứng thú và có khả năng thương mại hoá có thể liên lạc với mình để thảo luận vấn đề và giải quyết vấn đề này cho người Việt và Việt Nam. Đó là mong muốn lớn nhất của mình!",,,,,
"Chào mọi người,
Khi tìm hiểu về tool GoogleTest thì em mò ra được cuốn này.
Em nghĩ làm ứng dụng về AI thì vẫn thuộc về phát triển phần mềm nên chia sẻ nó lên đây.
Mọi người có thể truy cập file PDF miễn phí tại https://abseil.io/resources/swe_at_google.2.pdf
List of errata https://www.oreilly.com/catalog/errata.csp?isbn=0636920296423","Chào mọi người, Khi tìm hiểu về tool GoogleTest thì em mò ra được cuốn này. Em nghĩ làm ứng dụng về AI thì vẫn thuộc về phát triển phần mềm nên chia sẻ nó lên đây. Mọi người có thể truy cập file PDF miễn phí tại https://abseil.io/resources/swe_at_google.2.pdf List of errata https://www.oreilly.com/catalog/errata.csp?isbn=0636920296423",,,,,
"[Weight Initializers]
Xin chào các bạn, mình hiện tại đang train một model chủ yếu dùng ReLU activation ở các layer đầu. Trong quá trình train mô hình, do dùng Glorot, nên mô hình không đạt độ ổn định cao nên đã chuyển sang dùng He initializers.
Tuy nhiên, trong khi nghiên cứu (đọc bài báo), search GG - DataExchange, StackOverflow các kiểu, mình vẫn không hiểu nổi tại sao lại có sự phân biệt giữa normal và uniform (với hệ số standard deviation cách biệt là căn 3). Liệu train bằng normal sẽ khác so với uniform hay chúng chỉ tạo nên sự đa dạng trong initializers khi train thôi?.
Minh xin cảm ơn các bạn","[Weight Initializers] Xin chào các bạn, mình hiện tại đang train một model chủ yếu dùng ReLU activation ở các layer đầu. Trong quá trình train mô hình, do dùng Glorot, nên mô hình không đạt độ ổn định cao nên đã chuyển sang dùng He initializers. Tuy nhiên, trong khi nghiên cứu (đọc bài báo), search GG - DataExchange, StackOverflow các kiểu, mình vẫn không hiểu nổi tại sao lại có sự phân biệt giữa normal và uniform (với hệ số standard deviation cách biệt là căn 3). Liệu train bằng normal sẽ khác so với uniform hay chúng chỉ tạo nên sự đa dạng trong initializers khi train thôi?. Minh xin cảm ơn các bạn",,,,,
"[License plate detection]
Chào các bạn, hiện tại mình đang làm bài toán nhận diện biển số xe do thiếu data nên biển số vàng và xanh cho kq ko tốt lắm. Vì vấn đề bussiness và framework mình đang sử dụng nên ko thể grayscale được. Nên mình cần thu thập thêm data. Hiện tại mình search trên mạng mới có dataset green parking. Bạn nào có nếu bán được thì ib mình nhé, mình có thể mua.
Xin cảm ơn","[License plate detection] Chào các bạn, hiện tại mình đang làm bài toán nhận diện biển số xe do thiếu data nên biển số vàng và xanh cho kq ko tốt lắm. Vì vấn đề bussiness và framework mình đang sử dụng nên ko thể grayscale được. Nên mình cần thu thập thêm data. Hiện tại mình search trên mạng mới có dataset green parking. Bạn nào có nếu bán được thì ib mình nhé, mình có thể mua. Xin cảm ơn",,,,,
Trường hợp high bias và high variance thì xem là vừa overfitting vừa underfitting hay không overfiting không underfitting mọi người nhỉ.,Trường hợp high bias và high variance thì xem là vừa overfitting vừa underfitting hay không overfiting không underfitting mọi người nhỉ.,,,,,
"EfficientNet_V2 chính thức ra mắt bản cập nhật tại đây: https://arxiv.org/pdf/2104.00298v2.pdf
Và code cho TensorFlow tại đây: https://github.com/google/automl/tree/master/efficientnetv2
Vài giờ nữa có thể có code cho Pytorch tại codebase TIMM tại đây https://github.com/rwightman/pytorch-image-models",EfficientNet_V2 chính thức ra mắt bản cập nhật tại đây: https://arxiv.org/pdf/2104.00298v2.pdf Và code cho TensorFlow tại đây: https://github.com/google/automl/tree/master/efficientnetv2 Vài giờ nữa có thể có code cho Pytorch tại codebase TIMM tại đây https://github.com/rwightman/pytorch-image-models,,,,,
"Chào mn! E có làm 1 dự án về xử lý ảnh. Đầu vào ảnh dung lượng lớn (khoảng 5000-10000 pixels hoặc hơn). Không thể để nguyên ảnh như thế để làm do dung lượng lớn tính toán rất lâu mà cũng không thể resize ảnh vì sẽ mất nội dung của ảnh. Mn cho e hỏi có cách nào để thực hiện bài toán này với ạ.
Em cảm ơn!",Chào mn! E có làm 1 dự án về xử lý ảnh. Đầu vào ảnh dung lượng lớn (khoảng 5000-10000 pixels hoặc hơn). Không thể để nguyên ảnh như thế để làm do dung lượng lớn tính toán rất lâu mà cũng không thể resize ảnh vì sẽ mất nội dung của ảnh. Mn cho e hỏi có cách nào để thực hiện bài toán này với ạ. Em cảm ơn!,,,,,
"Chào các bác, em mới nghiên cứu về phép biến đổi wavelet và wavelet coherence, đề tài của em là phân tích yếu tố thời tiết như nhiệt độ có liên quan đến số ca mắc bệnh trong khoảng thời gian từ 1998 - 2011 hay không, tập dữ liệu đầu vào , cụ thể là thời tiết và số ca mắc bệnh tại một địa phương trong khoảng thời gian 1998 - 2011 , trong bước tiền xử lý dữ liệu, em có chuẩn hóa về -1 đến 1 ở cả 2 tập dữ liệu sau đó dùng thư viện biwavelet để biến đổi, cụ thể là sử dụng hàm wtc với các thông số đầu ra như hình, em chọn sig.level là 0.95 và sig.test = 0 tức X^2. Kết quả thu được ma trận signif với 90% chỉ số < 0.95 và 10% chỉ số > 0.95, nay em muốn nhờ các bác giúp 2 vấn đề với ạ.
Có thể đánh giá mức ý nghĩa thống kế bằng ma trận signif hay không? Nếu có thì đánh giá như thế nào để ra một chỉ số P-value cuối cùng thui ạ ?
Em muốn sử dụng phương pháp kiểm thử Granger Cause Test với kết quả của hàm wavelet coherence đó thì phải làm thế nào, em có xem qua thư viện vars với VAR model và hàm causality nhưng đầu vào chỉ là tín hiệu 1D , em không biết phải sử dụng như thế nào vào bài toán của mình, mong các bác có thể giúp em với ạ ?
Do em tự học và kiến thức còn rất hạn hẹp , có điều gì sai xót mong các bác có thể thông cảm giúp em sữa chữa với ạ. Em xin chân thành cảm ơn mọi người !","Chào các bác, em mới nghiên cứu về phép biến đổi wavelet và wavelet coherence, đề tài của em là phân tích yếu tố thời tiết như nhiệt độ có liên quan đến số ca mắc bệnh trong khoảng thời gian từ 1998 - 2011 hay không, tập dữ liệu đầu vào , cụ thể là thời tiết và số ca mắc bệnh tại một địa phương trong khoảng thời gian 1998 - 2011 , trong bước tiền xử lý dữ liệu, em có chuẩn hóa về -1 đến 1 ở cả 2 tập dữ liệu sau đó dùng thư viện biwavelet để biến đổi, cụ thể là sử dụng hàm wtc với các thông số đầu ra như hình, em chọn sig.level là 0.95 và sig.test = 0 tức X^2. Kết quả thu được ma trận signif với 90% chỉ số < 0.95 và 10% chỉ số > 0.95, nay em muốn nhờ các bác giúp 2 vấn đề với ạ. Có thể đánh giá mức ý nghĩa thống kế bằng ma trận signif hay không? Nếu có thì đánh giá như thế nào để ra một chỉ số P-value cuối cùng thui ạ ? Em muốn sử dụng phương pháp kiểm thử Granger Cause Test với kết quả của hàm wavelet coherence đó thì phải làm thế nào, em có xem qua thư viện vars với VAR model và hàm causality nhưng đầu vào chỉ là tín hiệu 1D , em không biết phải sử dụng như thế nào vào bài toán của mình, mong các bác có thể giúp em với ạ ? Do em tự học và kiến thức còn rất hạn hẹp , có điều gì sai xót mong các bác có thể thông cảm giúp em sữa chữa với ạ. Em xin chân thành cảm ơn mọi người !",,,,,
"Chào mọi người, em đang tìm hiểu về thuật toán SVM. Mọi người hãy chỉ cho em những bài giảng hoặc những tài liệu chất lượng với ạ.
Em cảm ơn thiệt nhiều","Chào mọi người, em đang tìm hiểu về thuật toán SVM. Mọi người hãy chỉ cho em những bài giảng hoặc những tài liệu chất lượng với ạ. Em cảm ơn thiệt nhiều",,,,,
"Chào mọi người, em chỉ mới đang tìm hiểu về Machine Learning.
Em muốn hỏi về mạng EfficientNet, theo em tìm hiểu thì mạng EfficientNet-B0 bao gồm tổng cộng 237 layers. Tuy nhiên trong bài báo này, bảng kết quả thử nghiệm chỉ ghi có 18 layers. Vậy 18 layers đó bao gồm những layer nào ạ?
Đề tài em đang nghiên cứu là thêm bớt lớp layer để tối ưu hóa nhận dạng trong bài toán nhận dạng món ăn. Mong anh chị giúp đỡ và chỉ cho em những đề tài tương quan.
Link bài báo: https://www.researchgate.net/publication/339798572_Automated_fruit_recognition_using_EfficientNet_and_MixNet","Chào mọi người, em chỉ mới đang tìm hiểu về Machine Learning. Em muốn hỏi về mạng EfficientNet, theo em tìm hiểu thì mạng EfficientNet-B0 bao gồm tổng cộng 237 layers. Tuy nhiên trong bài báo này, bảng kết quả thử nghiệm chỉ ghi có 18 layers. Vậy 18 layers đó bao gồm những layer nào ạ? Đề tài em đang nghiên cứu là thêm bớt lớp layer để tối ưu hóa nhận dạng trong bài toán nhận dạng món ăn. Mong anh chị giúp đỡ và chỉ cho em những đề tài tương quan. Link bài báo: https://www.researchgate.net/publication/339798572_Automated_fruit_recognition_using_EfficientNet_and_MixNet",,,,,
Chào anh chị em đang dùng mobinet để đào tạo mô hình phân loại ảnh trên thiết bị di động. Không biết có loại mô hình nào cũng có mục đích tạo ra để chạy trên thiết bị di động giống mobilenet không ạ.,Chào anh chị em đang dùng mobinet để đào tạo mô hình phân loại ảnh trên thiết bị di động. Không biết có loại mô hình nào cũng có mục đích tạo ra để chạy trên thiết bị di động giống mobilenet không ạ.,,,,,
"Numpy, ML algorithms to practice","Numpy, ML algorithms to practice",,,,,
"*Góc xin dữ liệu.
Em đang làm bài toán liên quan tới nhận diện khuôn mặt nên muốn hỏi các anh chị và các bạn trong nhóm ai có bộ dữ liệu khuôn mặt hoặc link cho em xin với ạ (dữ liệu cho khu vực châu Á hay Việt Nam thì càng tốt ạ). Em xin chân thành cảm ơn.",*Góc xin dữ liệu. Em đang làm bài toán liên quan tới nhận diện khuôn mặt nên muốn hỏi các anh chị và các bạn trong nhóm ai có bộ dữ liệu khuôn mặt hoặc link cho em xin với ạ (dữ liệu cho khu vực châu Á hay Việt Nam thì càng tốt ạ). Em xin chân thành cảm ơn.,,,,,
"Xin chào các bác, hôm nay em mạnh dạn chia sẻ cách train RepVGG với dữ liệu riêng theo cả 2 hình thức là train từ đầu và transfer learning.
Em có tìm kiếm trên mạng thì nhiều bài giới thiệu về RepVGG nhưng các bài hướng dẫn train và transfer learning thì còn ít nên em viết bài này để guide các bạn mới học thôi ạ. Nếu có sai sót mong các cao thủ đi qua bỏ quá!","Xin chào các bác, hôm nay em mạnh dạn chia sẻ cách train RepVGG với dữ liệu riêng theo cả 2 hình thức là train từ đầu và transfer learning. Em có tìm kiếm trên mạng thì nhiều bài giới thiệu về RepVGG nhưng các bài hướng dẫn train và transfer learning thì còn ít nên em viết bài này để guide các bạn mới học thôi ạ. Nếu có sai sót mong các cao thủ đi qua bỏ quá!",,,,,
"Hello mọi người, em đang học đến phần Features Selection, cụ thể hơn là em dùng RFE để lựa chọn features. Câu hỏi đặt ra là mọi người thường chọn model nào để làm parameter đầu vào cho RFE (nếu mọi người đã biết về RFE thì RFE bản chất sẽ sử dụng 1 model nào đó). Ngoài ra trong thực tế người ta thường dùng selectKbest hay RFE nhiều hơn ạ? Em cảm ơn mọi người rất nhiều!!!!","Hello mọi người, em đang học đến phần Features Selection, cụ thể hơn là em dùng RFE để lựa chọn features. Câu hỏi đặt ra là mọi người thường chọn model nào để làm parameter đầu vào cho RFE (nếu mọi người đã biết về RFE thì RFE bản chất sẽ sử dụng 1 model nào đó). Ngoài ra trong thực tế người ta thường dùng selectKbest hay RFE nhiều hơn ạ? Em cảm ơn mọi người rất nhiều!!!!",,,,,
"Trong lúc cộng đồng đang hồ hởi chuyển sang dùng Transformer vào Computer Vision thì Hugging Face cũng không thể đứng ngoài cuộc chơi được. Trong bản release mới nhất 4.6.0, Hugging Face cũng chính thức hỗ trợ model cho ViT, DeiT và CLIP (đủ 3 model của 3 ông lớn, đỡ ganh tị): https://github.com/huggingface/transformers/releases/tag/v4.6.0
Anh chị em tha hồ vào nghịch 😊","Trong lúc cộng đồng đang hồ hởi chuyển sang dùng Transformer vào Computer Vision thì Hugging Face cũng không thể đứng ngoài cuộc chơi được. Trong bản release mới nhất 4.6.0, Hugging Face cũng chính thức hỗ trợ model cho ViT, DeiT và CLIP (đủ 3 model của 3 ông lớn, đỡ ganh tị): https://github.com/huggingface/transformers/releases/tag/v4.6.0 Anh chị em tha hồ vào nghịch",,,,,
"#book #Deep_Learning #Computer_Vision 
Hiện tại mình có full bộ sách:
** Deep Learning for Computer Vision with Python by Adrian Rosebrock gồm 3 quyển:
+Starter Bundle 
+Practictioner Bundle 
+ImageNet Bundle 
** Practical Python and OpenCV + Case Studies by Adrian Rosebrock (2 quyển)
**Raspberry Pi for Computer Vision by Adrian Rosebrock 1 quyển Hobbyist Bundle 
Tổng bộ này mình mua hơi bị mắc nhưng siêu hay, thuần thục có thể đi làm tốt.
Bạn nào có nhu cầu nhưng không có điều kiện, muốn pass lại với giá ưu đãi thì inbox mình ha.
Mình pass lại Full bộ trên dao động 100-150$
Cảm ơn các bạn quan tâm.
Mong ad duyệt bài","Hiện tại mình có full bộ sách: ** Deep Learning for Computer Vision with Python by Adrian Rosebrock gồm 3 quyển: +Starter Bundle +Practictioner Bundle +ImageNet Bundle ** Practical Python and OpenCV + Case Studies by Adrian Rosebrock (2 quyển) **Raspberry Pi for Computer Vision by Adrian Rosebrock 1 quyển Hobbyist Bundle Tổng bộ này mình mua hơi bị mắc nhưng siêu hay, thuần thục có thể đi làm tốt. Bạn nào có nhu cầu nhưng không có điều kiện, muốn pass lại với giá ưu đãi thì inbox mình ha. Mình pass lại Full bộ trên dao động 100-150$ Cảm ơn các bạn quan tâm. Mong ad duyệt bài",#book	#Deep_Learning	#Computer_Vision,,,,
"[Deeplearning.AI]Machine Learning Engineering for Production (MLOps) Specialization

https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops
About this Specialization

#Coursera #MLOps 

The Machine Learning Engineering for Production Specialization covers how to conceptualize, build, and maintain integrated systems that continuously operate in production. In striking contrast with standard machine learning modeling, production systems need to handle relentless evolving data. Moreover, the production system must run non-stop at the minimum cost while producing the maximum performance. In this Specialization, you will learn how to use well-established tools and methodologies for doing all of this effectively and efficiently.

Applied Learning Project
By the end, you'll be ready to

• Design an ML production system end-to-end: project scoping, data needs, modeling strategies, and deployment requirements
• Establish a model baseline, address concept drift, and prototype how to develop, deploy, and continuously improve a productionized ML application
• Build data pipelines by gathering, cleaning, and validating datasets
• Implement feature engineering, transformation, and selection with TensorFlow Extended
• Establish data lifecycle by leveraging data lineage and provenance metadata tools and follow data evolution with enterprise data schemas
• Apply techniques to manage modeling resources and best serve offline/online inference requests
• Use analytics to address model fairness, explainability issues, and mitigate bottlenecks

• Deliver deployment pipelines for model serving that require different infrastructures
• Apply best practices and progressive delivery techniques to maintain a continuously operating production system","[Deeplearning.AI]Machine Learning Engineering for Production (MLOps) Specialization https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops About this Specialization The Machine Learning Engineering for Production Specialization covers how to conceptualize, build, and maintain integrated systems that continuously operate in production. In striking contrast with standard machine learning modeling, production systems need to handle relentless evolving data. Moreover, the production system must run non-stop at the minimum cost while producing the maximum performance. In this Specialization, you will learn how to use well-established tools and methodologies for doing all of this effectively and efficiently. Applied Learning Project By the end, you'll be ready to • Design an ML production system end-to-end: project scoping, data needs, modeling strategies, and deployment requirements • Establish a model baseline, address concept drift, and prototype how to develop, deploy, and continuously improve a productionized ML application • Build data pipelines by gathering, cleaning, and validating datasets • Implement feature engineering, transformation, and selection with TensorFlow Extended • Establish data lifecycle by leveraging data lineage and provenance metadata tools and follow data evolution with enterprise data schemas • Apply techniques to manage modeling resources and best serve offline/online inference requests • Use analytics to address model fairness, explainability issues, and mitigate bottlenecks • Deliver deployment pipelines for model serving that require different infrastructures • Apply best practices and progressive delivery techniques to maintain a continuously operating production system",#Coursera	#MLOps,,,,
"Kính chào các bác, dạo này em có nghe thấy có RepVGG hay quá nên cũng tìm hiểu qua. Tuy nhiên do model mới cũng ít tài liệu và đa phần viết học thuật quá nên em mạnh dạn viết bài này chia sẻ cùng các bạn về cách hiểu sơ qua về RepVGG và ứng dụng thực tế train và inference luôn.
Mong giúp được các bạn newbie và mong admin duyệt bài!","Kính chào các bác, dạo này em có nghe thấy có RepVGG hay quá nên cũng tìm hiểu qua. Tuy nhiên do model mới cũng ít tài liệu và đa phần viết học thuật quá nên em mạnh dạn viết bài này chia sẻ cùng các bạn về cách hiểu sơ qua về RepVGG và ứng dụng thực tế train và inference luôn. Mong giúp được các bạn newbie và mong admin duyệt bài!",,,,,
"Chào các ace,
Cho em hỏi có cách nào mỗi lần chạy colab, ko phải cài lại các thư viện ko ạ. Em cài pip install torch_sparse, torch_scatter các kiểu, cứ mỗi lần chạy là colab bắt cài lại, mà mỗi lần cài khá lâu.
Xin các ace chỉ giáo, em xin cám ơn!","Chào các ace, Cho em hỏi có cách nào mỗi lần chạy colab, ko phải cài lại các thư viện ko ạ. Em cài pip install torch_sparse, torch_scatter các kiểu, cứ mỗi lần chạy là colab bắt cài lại, mà mỗi lần cài khá lâu. Xin các ace chỉ giáo, em xin cám ơn!",,,,,
"Mình đang làm dự án về AI, cũng mới tìm hiểu thôi nên còn chưa rành lắm, mong mọi người giúp đỡ.
Mình đang tìm một API hay toolkit để phân tích dữ liệu tiếng Việt (gắn label, ...), tương tự như Watson Natural Language Understanding hay Azure Text Analytics chẳng hạn.
Ai biết API nào hay cho tiếng Việt chỉ cho mình với. Watson, Azure, Google Cloud thì hầu hết không hỗ trợ tiếng Việt.
Cảm ơn mọi người.","Mình đang làm dự án về AI, cũng mới tìm hiểu thôi nên còn chưa rành lắm, mong mọi người giúp đỡ. Mình đang tìm một API hay toolkit để phân tích dữ liệu tiếng Việt (gắn label, ...), tương tự như Watson Natural Language Understanding hay Azure Text Analytics chẳng hạn. Ai biết API nào hay cho tiếng Việt chỉ cho mình với. Watson, Azure, Google Cloud thì hầu hết không hỗ trợ tiếng Việt. Cảm ơn mọi người.",,,,,
Cùng chung tay xây dựng vnquant package,Cùng chung tay xây dựng vnquant package,,,,,
"Xin chào mọi người, em đang cần muốn tính log-likelihood của classifier given data. Em tính như này có đúng không ạ? log-likelihood = sum(log probability (c | x_i)) = sum(log(probs) = sum(log(c.predict_proba(x_i))). Em xin cảm ơn","Xin chào mọi người, em đang cần muốn tính log-likelihood của classifier given data. Em tính như này có đúng không ạ? log-likelihood = sum(log probability (c | x_i)) = sum(log(probs) = sum(log(c.predict_proba(x_i))). Em xin cảm ơn",,"#math, #Q&A",,,
"Chào mọi người ạ,
Vừa qua mình có làm 1 bài thực hành về tô ảnh trắng đen. Mình có tham khảo qua paper này: https://arxiv.org/pdf/1611.07004.pdf
Mình cũng đã có kết quả khá tốt rồi. Tuy nhiên có vài điều mình chưa thực sự hiểu khi hiện thực model trong paper và làm mình hơi khó chịu và vẫn còn băn khoan. Nên mong mọi người ai từng làm qua cái này giúp mình chút giải đáp ạ
1. tại sao trong các task colorize image lại dùng không gian màu Lab thay vì dùng RGB
2. trong patch gan, mạng discriminator có cần một tầng cuối là tầng sigmoid sau khi qua tầng tích chập không? Tại thấy một số nơi implement thì có nơi có, có nơi không, mà đa số thấy là không.
Mình xin cảm ơn mọi người","Chào mọi người ạ, Vừa qua mình có làm 1 bài thực hành về tô ảnh trắng đen. Mình có tham khảo qua paper này: https://arxiv.org/pdf/1611.07004.pdf Mình cũng đã có kết quả khá tốt rồi. Tuy nhiên có vài điều mình chưa thực sự hiểu khi hiện thực model trong paper và làm mình hơi khó chịu và vẫn còn băn khoan. Nên mong mọi người ai từng làm qua cái này giúp mình chút giải đáp ạ 1. tại sao trong các task colorize image lại dùng không gian màu Lab thay vì dùng RGB 2. trong patch gan, mạng discriminator có cần một tầng cuối là tầng sigmoid sau khi qua tầng tích chập không? Tại thấy một số nơi implement thì có nơi có, có nơi không, mà đa số thấy là không. Mình xin cảm ơn mọi người",,,,,
"Chào mọi người ạ, em mới đọc về DL và làm về phân đoạn ảnh (instance segmentation vs semantic segmentation) mục đích phân đoạn ảnh y tế greyscale . Thì có 3 thuật toán em cần chọn và đưa ra ưu nhược của FCN, Mark R-CNN, Unet : Thì có vấn đề là em hiểu là FCN và Unet cùng là downsampling rồi lại upshampling lên, thì khác nhau chỗ nào ( về tính chất, thời gian xử lý, chất lượng ), ... Và sao mark r-cnn thì em ko thấy đề cập đến để xử lý ảnh y tế ạ . Em làm bên y tế nên kiến thức hẹp mong các bác thông cảm ạ . :( Em cảm ơn ạ.","Chào mọi người ạ, em mới đọc về DL và làm về phân đoạn ảnh (instance segmentation vs semantic segmentation) mục đích phân đoạn ảnh y tế greyscale . Thì có 3 thuật toán em cần chọn và đưa ra ưu nhược của FCN, Mark R-CNN, Unet : Thì có vấn đề là em hiểu là FCN và Unet cùng là downsampling rồi lại upshampling lên, thì khác nhau chỗ nào ( về tính chất, thời gian xử lý, chất lượng ), ... Và sao mark r-cnn thì em ko thấy đề cập đến để xử lý ảnh y tế ạ . Em làm bên y tế nên kiến thức hẹp mong các bác thông cảm ạ . :( Em cảm ơn ạ.",,,,,
"Hôm nay, em/mình ngồi lướt drive xem có gì để xóa cho nhẹ thì phát hiện ra có ít tài liệu mà lâu nay bỏ quên mất. Thời còn trẻ khỏe, mình và vài bạn ở lab có research về Transfer style và có note lại ý chính của 1 số paper nổi tiếng. Hồi đó em/mình nhớ là đọc các paper này rất khó hiểu nhưng lúc note ra thì thấy rất đơn giản. Hy vọng tài liệu này giúp ích cho các bạn đang quan tâm về bài toán Image to Image, GAN,vv... đặc biệt là các bạn đang làm đồ án như em/mình.
https://www.overleaf.com/read/zmtsjybppmnc
https://docs.google.com/document/d/1Eiu3-B7-wX7klu0ECDwu0QIwSiFSqZ7XfUGHVHBnxDQ/edit?usp=sharing","Hôm nay, em/mình ngồi lướt drive xem có gì để xóa cho nhẹ thì phát hiện ra có ít tài liệu mà lâu nay bỏ quên mất. Thời còn trẻ khỏe, mình và vài bạn ở lab có research về Transfer style và có note lại ý chính của 1 số paper nổi tiếng. Hồi đó em/mình nhớ là đọc các paper này rất khó hiểu nhưng lúc note ra thì thấy rất đơn giản. Hy vọng tài liệu này giúp ích cho các bạn đang quan tâm về bài toán Image to Image, GAN,vv... đặc biệt là các bạn đang làm đồ án như em/mình. https://www.overleaf.com/read/zmtsjybppmnc https://docs.google.com/document/d/1Eiu3-B7-wX7klu0ECDwu0QIwSiFSqZ7XfUGHVHBnxDQ/edit?usp=sharing",,,,,
"Em chào mn ạ,
Em có câu hỏi về Layer Normailzation ạ. E thấy trong một số kiến trúc có LayerNorm thì đôi ng ta sẽ set learnable parameter = False. Em ko biết khi nào thì nên set True hay False ạ ?
Cảm ơn mn","Em chào mn ạ, Em có câu hỏi về Layer Normailzation ạ. E thấy trong một số kiến trúc có LayerNorm thì đôi ng ta sẽ set learnable parameter = False. Em ko biết khi nào thì nên set True hay False ạ ? Cảm ơn mn",,,,,
"Xin chào mọi người ạ
Em đang tìm hiểu về bài toán Multi Label Text/Sentence classification nhằm categorize feature ạ.
Ví dụ:
'new lazy load technique for better performance.' => performance
'- Added: Youtube (or custom) video player in the ""Text Plus Image Vertical"" section;.' => social media
...
Hiện tại em có khoảng 96 categories và data set gồm > 4000 entries. Tuy nhiên khả năng predict thì chưa được như em mong muốn.
Có bác nào đã từng có kinh nghiệm trong mảng này có thể gợi ý cho em được không ạ. Hiện em đang dùng TFIDF, BOW.
Em xin thông tin thêm là một số category/label xuất hiện rất nhiều. Distribution không được đều lắm
Ảnh phía dưới là một ví dụ sai tè le
Cảm ơn các bác ạ","Xin chào mọi người ạ Em đang tìm hiểu về bài toán Multi Label Text/Sentence classification nhằm categorize feature ạ. Ví dụ: 'new lazy load technique for better performance.' => performance '- Added: Youtube (or custom) video player in the ""Text Plus Image Vertical"" section;.' => social media ... Hiện tại em có khoảng 96 categories và data set gồm > 4000 entries. Tuy nhiên khả năng predict thì chưa được như em mong muốn. Có bác nào đã từng có kinh nghiệm trong mảng này có thể gợi ý cho em được không ạ. Hiện em đang dùng TFIDF, BOW. Em xin thông tin thêm là một số category/label xuất hiện rất nhiều. Distribution không được đều lắm Ảnh phía dưới là một ví dụ sai tè le Cảm ơn các bác ạ",,,,,
"Toán - vâng vẫn là nó thôi. Ngồi miệt mài dạo khắp các clips trên youtube để hiểu hơn về PCA và LDA thì phát hiện ra được chanel của bác Luis Serrano có cách giải thích và visualize cực kỳ dễ hiểu nên mình cũng tò mò tìm profile của bác này trên Linken: 
Bác này từng tốt nghiệp tiến sĩ về Toán ở University of Michigan
Machine Learning Engineer ở Google
Head of content: AI, Data Science, Machine learning của Udacity
Lead Artificial Intelligence Educator của Apple
Đang là Quantum AI Research Scientist 
Lại có thêm một kênh tin cậy để các anh em mần mò về toán.","Toán - vâng vẫn là nó thôi. Ngồi miệt mài dạo khắp các clips trên youtube để hiểu hơn về PCA và LDA thì phát hiện ra được chanel của bác Luis Serrano có cách giải thích và visualize cực kỳ dễ hiểu nên mình cũng tò mò tìm profile của bác này trên Linken: Bác này từng tốt nghiệp tiến sĩ về Toán ở University of Michigan Machine Learning Engineer ở Google Head of content: AI, Data Science, Machine learning của Udacity Lead Artificial Intelligence Educator của Apple Đang là Quantum AI Research Scientist Lại có thêm một kênh tin cậy để các anh em mần mò về toán.",,,,,
"Chào các bạn,
Hiện nhóm dịch cuốn ""Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition""(https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) đã gần hoàn thiện nửa đầu của cuốn sách. Chúng tôi đã quyết định gửi đến bạn đọc nửa đầu của cuốn sách về nền tảng Machine Learning và thư viện Scikit-Learn trước khi tiếp tục dịch nửa sau về Keras và Tensorflow.
Đây là một cuốn sách cực hay cả về lý thuyết Machine Learning lẫn code thực tế rất bài bản. Tuy nhiên, vì vấn đề bản quyền, chúng tôi không thể chia sẻ rộng rãi cuốn sách này trong cộng đồng như hai cuốn sách trước được mà cần phân phối dưới dạng thương mại (với giá dự kiến sẽ rẻ hơn nhiều so với bản gốc tiếng Anh). Hiện nhóm đang gặp khó khăn về việc tìm nền tảng hỗ trợ tốt việc hạn chế chia sẻ file pdf. Chẳng hạn, một nền tảng cho phép hạn chế số lượng thiết bị có thể đọc được với mỗi tài khoản.
Nếu bạn nào có thông tin về một nền tảng tương tự thì ping mình nhé. Cảm ơn các bạn.
====
Update sau khi có bạn hỏi về hai cuốn trước:
Cuốn 1: https://github.com/mlbvn/ml-yearning-vn
Cuốn 2: https://d2l.aivivn.com/","Chào các bạn, Hiện nhóm dịch cuốn ""Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition""(https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) đã gần hoàn thiện nửa đầu của cuốn sách. Chúng tôi đã quyết định gửi đến bạn đọc nửa đầu của cuốn sách về nền tảng Machine Learning và thư viện Scikit-Learn trước khi tiếp tục dịch nửa sau về Keras và Tensorflow. Đây là một cuốn sách cực hay cả về lý thuyết Machine Learning lẫn code thực tế rất bài bản. Tuy nhiên, vì vấn đề bản quyền, chúng tôi không thể chia sẻ rộng rãi cuốn sách này trong cộng đồng như hai cuốn sách trước được mà cần phân phối dưới dạng thương mại (với giá dự kiến sẽ rẻ hơn nhiều so với bản gốc tiếng Anh). Hiện nhóm đang gặp khó khăn về việc tìm nền tảng hỗ trợ tốt việc hạn chế chia sẻ file pdf. Chẳng hạn, một nền tảng cho phép hạn chế số lượng thiết bị có thể đọc được với mỗi tài khoản. Nếu bạn nào có thông tin về một nền tảng tương tự thì ping mình nhé. Cảm ơn các bạn. ==== Update sau khi có bạn hỏi về hai cuốn trước: Cuốn 1: https://github.com/mlbvn/ml-yearning-vn Cuốn 2: https://d2l.aivivn.com/",,,,,
"Chào mn, mô hình học của mình nhận được AUC tạm ổn nhưng AUPR quá thấp. Xin giúp mình nguyên nhân (Mình nghĩ là do dữ liệu) và góp ý giúp mình hướng xử lý.
Cám ơn mn.","Chào mn, mô hình học của mình nhận được AUC tạm ổn nhưng AUPR quá thấp. Xin giúp mình nguyên nhân (Mình nghĩ là do dữ liệu) và góp ý giúp mình hướng xử lý. Cám ơn mn.",,,,,
"mn cho em hỏi, sau chỗ đánh dấu trong ảnh lại thêm [0] ạ. Em cảm ơn mn nhiều.","mn cho em hỏi, sau chỗ đánh dấu trong ảnh lại thêm [0] ạ. Em cảm ơn mn nhiều.",,,,,
"Trong vòng 6 tháng gần đây, lĩnh vực Computer Vision có những bước đột phá nhất định, từ việc triển khai kiến trúc Attention/Transformers, cho tới loại bỏ lớp BatchNormalization, và rồi quay lại với kiến trúc CNN với SOTA là EfficientNetV2.
Chưa dừng lại ở đây, ngày 4/5/2021, nhóm Google Brain ở Zurich và Berlin công bố SOTA mới, dựa vào kiến trúc MLP (Multi-Layer Perceptron). Models với tên gọi là MLP-Mixer hoàn toàn không sử dụng kiến trúc Convolution hay Transformer như trước. Và nhóm tác giả hi vọng bài báo này là viên gạch đầu tiên cho những bước đi tiếp theo.
Bài báo tại đây: https://arxiv.org/abs/2105.01601
Original Code được viết trên JAX và Linen tại đây: https://github.com/google-research/vision_transformer/tree/linen
Và rất nhanh chóng nó được port sang Pytorch tại đây:
https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/mlp_mixer.py
Ps. Không biết bên TensorFlow có codebases nào tập hợp nhanh chóng, chính xác, thậm trí huấn luyện lại với kết quả còn tốt hơn như codebase của anh Ross Wrightman đã làm cho framework PyTorch hay không?","Trong vòng 6 tháng gần đây, lĩnh vực Computer Vision có những bước đột phá nhất định, từ việc triển khai kiến trúc Attention/Transformers, cho tới loại bỏ lớp BatchNormalization, và rồi quay lại với kiến trúc CNN với SOTA là EfficientNetV2. Chưa dừng lại ở đây, ngày 4/5/2021, nhóm Google Brain ở Zurich và Berlin công bố SOTA mới, dựa vào kiến trúc MLP (Multi-Layer Perceptron). Models với tên gọi là MLP-Mixer hoàn toàn không sử dụng kiến trúc Convolution hay Transformer như trước. Và nhóm tác giả hi vọng bài báo này là viên gạch đầu tiên cho những bước đi tiếp theo. Bài báo tại đây: https://arxiv.org/abs/2105.01601 Original Code được viết trên JAX và Linen tại đây: https://github.com/google-research/vision_transformer/tree/linen Và rất nhanh chóng nó được port sang Pytorch tại đây: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/mlp_mixer.py Ps. Không biết bên TensorFlow có codebases nào tập hợp nhanh chóng, chính xác, thậm trí huấn luyện lại với kết quả còn tốt hơn như codebase của anh Ross Wrightman đã làm cho framework PyTorch hay không?",,,,,
"Kính chào cả nhà. Đợt rồi có một số bạn hỏi về cách tiếp cận một website để crawl dữ liệu, cách lưu trữ thông tin và visualize dữ liệu. Em cùng không pro lắm những có làm qua nên làm cái clip guide cho các bạn mới học.
Phần phân tích còn ít do chỉ muốn nhắn nhủ các bạn đừng tin vào soi cầu, báo số mà mất tiền :D
Mong admin duyệt bài và mong các bác chỉ giáo!","Kính chào cả nhà. Đợt rồi có một số bạn hỏi về cách tiếp cận một website để crawl dữ liệu, cách lưu trữ thông tin và visualize dữ liệu. Em cùng không pro lắm những có làm qua nên làm cái clip guide cho các bạn mới học. Phần phân tích còn ít do chỉ muốn nhắn nhủ các bạn đừng tin vào soi cầu, báo số mà mất tiền :D Mong admin duyệt bài và mong các bác chỉ giáo!",,,,,
Cho mình hỏi xíu là mọi người có ai từng áp dụng xử lý ngôn ngữ tự nhiên cho con chatbot của facebook chưa có thể cho mình xin tài liệu tham khảo với ạ?,Cho mình hỏi xíu là mọi người có ai từng áp dụng xử lý ngôn ngữ tự nhiên cho con chatbot của facebook chưa có thể cho mình xin tài liệu tham khảo với ạ?,,,,,
#probability,,#probability,,,,
"#food_dataset
Hi group,
Mọi người có nguồn data nào về nutrition food ( tiếng anh hoặc tiếng Việt )đã được label cho em xin với ạ !
(Dạng csv, không phải dạng image).
Cảm ơn mọi người nhiều ạ !","Hi group, Mọi người có nguồn data nào về nutrition food ( tiếng anh hoặc tiếng Việt )đã được label cho em xin với ạ ! (Dạng csv, không phải dạng image). Cảm ơn mọi người nhiều ạ !",#food_dataset,,,,
"Chào mọi người. Em đang làm bài toán Anomaly Detection bằng cách xử lý gói tin https header. Gói tin ban đầu có dạng file .pcap và em muốn chuyển sang file .csv để chạy thuật toán. Em muốn hỏi mọi người có tool nào để có thể làm việc này (bao gồm việc có thể lựa chọn các trường mong muốn) một cách dễ dàng không ạ. Em xin cảm ơn.
// Em dùng Wireshark để export ra file .csv thì nó bị thế này (cột info)",Chào mọi người. Em đang làm bài toán Anomaly Detection bằng cách xử lý gói tin https header. Gói tin ban đầu có dạng file .pcap và em muốn chuyển sang file .csv để chạy thuật toán. Em muốn hỏi mọi người có tool nào để có thể làm việc này (bao gồm việc có thể lựa chọn các trường mong muốn) một cách dễ dàng không ạ. Em xin cảm ơn. // Em dùng Wireshark để export ra file .csv thì nó bị thế này (cột info),,,,,
"Xin chào mọi người. Cho mình hỏi có cách nào để cài tensorflow gpu trên CPU không có AVX/SSE4.1 mà không cần phải dùng Bazel không? Mình phải cài trên server trường mà họ không cho cài thêm bất cứ phần mềm nào khác.
Specs của máy
Ubuntu 18.04
CUDA 10.0.130
GPU Titan X
CPU Quadcore AMD Opteron(tm) 2376
Python 3.6 hoặc 3.8
Xin cảm ơn.",Xin chào mọi người. Cho mình hỏi có cách nào để cài tensorflow gpu trên CPU không có AVX/SSE4.1 mà không cần phải dùng Bazel không? Mình phải cài trên server trường mà họ không cho cài thêm bất cứ phần mềm nào khác. Specs của máy Ubuntu 18.04 CUDA 10.0.130 GPU Titan X CPU Quadcore AMD Opteron(tm) 2376 Python 3.6 hoặc 3.8 Xin cảm ơn.,,,,,
"xin chào mọi người mình có train mô hình dịch máy vi-en (seq2seq), mình đang bị sai nhiều hoặc không nhận ra ở phần danh từ riêng. ở phần decoder nó nó dịch luôn tên danh từ riêng. Ai có kinh nghiệm hay kỹ thuật nào hay để khắc phục vấn đề này truyền cho em ít kinh nghiệm. Em xin cảm ơn","xin chào mọi người mình có train mô hình dịch máy vi-en (seq2seq), mình đang bị sai nhiều hoặc không nhận ra ở phần danh từ riêng. ở phần decoder nó nó dịch luôn tên danh từ riêng. Ai có kinh nghiệm hay kỹ thuật nào hay để khắc phục vấn đề này truyền cho em ít kinh nghiệm. Em xin cảm ơn",,,,,
"Chào mọi người, hiện giờ mình đang gặp một vấn đề khi đẩy model lên GPU của mobile sử dụng Tensorflow lite và NNAPI. Tuy nhiên có một vấn đề đó là khi chạy tensorlfow lite dùng NNAPI thì tất cả các tensor phải là fixed shape hay static shape. Bài toán mình làm về tiếng nói và độ dài audio đầu ra phụ thuộc rất nhiều vào đầu vào, nên việc sử dụng tensor với fixed length để biểu diễn là không khả thi và tốn kém trong tính toán (phải tính toán thừa nhiều).
Bạn nào trong nhóm có kinh nghiệm hoặc giải pháp có thể chia sẻ với bọn mình thì tốt quá ạ.
#TensorflowLite #NNAPI #MOBILE #GPU","Chào mọi người, hiện giờ mình đang gặp một vấn đề khi đẩy model lên GPU của mobile sử dụng Tensorflow lite và NNAPI. Tuy nhiên có một vấn đề đó là khi chạy tensorlfow lite dùng NNAPI thì tất cả các tensor phải là fixed shape hay static shape. Bài toán mình làm về tiếng nói và độ dài audio đầu ra phụ thuộc rất nhiều vào đầu vào, nên việc sử dụng tensor với fixed length để biểu diễn là không khả thi và tốn kém trong tính toán (phải tính toán thừa nhiều). Bạn nào trong nhóm có kinh nghiệm hoặc giải pháp có thể chia sẻ với bọn mình thì tốt quá ạ.",#TensorflowLite	#NNAPI	#MOBILE	#GPU,,,,
"Machine Reasoning, Video Understanding, and Human Activity Recognition are super interesting, super hot research topics in AI right now.
https://paperswithcode.com/area/reasoning
https://paperswithcode.com/task/video-understanding
https://paperswithcode.com/task/activity-recognition
And prof. Vuong Le at Deakin Uni is an expert in these topics https://vuongle2.github.io
Các bạn trẻ nhớ đăng ký theo link bên dưới và book lịch: 17h00 Thứ 5 ngày 13/5/2021 để nghe giáo sư chia sẻ.
BK.AI: làm ơn giúp puplic live stream cho cộng đồng đc theo dõi, xin cảm ơn!
https://www.facebook.com/109718987882944/posts/128407072680802/
#AI4VN #DeepReasoning #VisualUnderstanding #ActivityRecognition","Machine Reasoning, Video Understanding, and Human Activity Recognition are super interesting, super hot research topics in AI right now. https://paperswithcode.com/area/reasoning https://paperswithcode.com/task/video-understanding https://paperswithcode.com/task/activity-recognition And prof. Vuong Le at Deakin Uni is an expert in these topics https://vuongle2.github.io Các bạn trẻ nhớ đăng ký theo link bên dưới và book lịch: 17h00 Thứ 5 ngày 13/5/2021 để nghe giáo sư chia sẻ. BK.AI: làm ơn giúp puplic live stream cho cộng đồng đc theo dõi, xin cảm ơn! https://www.facebook.com/109718987882944/posts/128407072680802/",#AI4VN	#DeepReasoning	#VisualUnderstanding	#ActivityRecognition,,,,
Xử lý dữ liệu trên pandas,Xử lý dữ liệu trên pandas,,,,,
"[HELP][LÀM SAO NHẬN DIỆN ĐƯỢC NHỮNG TÊN BẤT THƯỜNG TỪ TẬP DỮ LIỆU]
Em chào anh/chị, hiện tại em đang làm một bài toán là làm sao nhận diện được đâu là cụm tên bất thường.
Input: hơn 20 triệu tên đặt cho tài khoản cá nhân (ví dụ tài khoản của em là Binh Le, và sẽ có những tên tài khoản có dạng bất kỳ như aemiiDaniela, 706807470KhanhTuanPhatTonHai, yq243tc9 thì 3 tên cuối cùng được xem là bất thường và tên bình thường có thể là Hồng Hoa, Nguyên Hà, Huy Nguyễn)
Output: Phân ra được cụm dữ liệu tên và xác định đâu là cụm tên bất thường
Hy vọng được anh/chị chia sẻ kinh nghiệm hoặc hướng đi để giải quyết tốt bài toán ạ, em cảm ơn mọi người rất nhiều.","[HELP][LÀM SAO NHẬN DIỆN ĐƯỢC NHỮNG TÊN BẤT THƯỜNG TỪ TẬP DỮ LIỆU] Em chào anh/chị, hiện tại em đang làm một bài toán là làm sao nhận diện được đâu là cụm tên bất thường. Input: hơn 20 triệu tên đặt cho tài khoản cá nhân (ví dụ tài khoản của em là Binh Le, và sẽ có những tên tài khoản có dạng bất kỳ như aemiiDaniela, 706807470KhanhTuanPhatTonHai, yq243tc9 thì 3 tên cuối cùng được xem là bất thường và tên bình thường có thể là Hồng Hoa, Nguyên Hà, Huy Nguyễn) Output: Phân ra được cụm dữ liệu tên và xác định đâu là cụm tên bất thường Hy vọng được anh/chị chia sẻ kinh nghiệm hoặc hướng đi để giải quyết tốt bài toán ạ, em cảm ơn mọi người rất nhiều.",,,,,
"App này team mình dùng U2NET để tách nền.
Mọi người thấy còn có model nào ổn hơn thì gợi ý giúp bọn mình với ạ.
Cảm ơn cả nhà nhiều ạ!",App này team mình dùng U2NET để tách nền. Mọi người thấy còn có model nào ổn hơn thì gợi ý giúp bọn mình với ạ. Cảm ơn cả nhà nhiều ạ!,,,,,
"Với những bước tiến vượt bậc của lĩnh vực computer science đã giúp chúng ta giải quyết được nhiều bài toán với dữ liệu dạng ảnh, dạng ngôn ngữ, dạng trình tự (như trình tự DNA hay protein), dạng cấu trúc (như cấu trúc bậc 3 - 4 của phân tử protein), hay như tìm kiếm thuốc mới (ứng dụng GAN và Reinforcement Learning). Tuy nhiên, dữ liệu sinh y học thường là các quan sát đơn lẻ và được ghi dưới dạng bảng (tabular) do đó ngoài phương pháp thống kê truyền thống, cho tới các model GLM hay phương pháp mới hơn liên quan tới Machine Learning (ví dụ XgBoost, KNN, SMV,...). Vậy có ai hỏi làm thế nào để ứng dụng Deep Learning vào việc xử lí dữ liệu dạng bảng này trong lĩnh vực sinh y hay không???
Rất may mắn, chúng ta có Framework PyTorch_Tabular có thể xử lí với vấn đề này! Hiện tại framework này có xây dựng sẵn một số models như: CategoryEmbeddingModel [1]; NODEModel [2]; TabNet [3]; AutoInt [4]; và các bạn có thể tuy biến models của mình thông qua thiết lập Configuration.
[1] https://arxiv.org/pdf/2104.13638.pdf
[2] https://arxiv.org/abs/1909.06312
[3] https://arxiv.org/pdf/1908.07442.pdf
[4] https://arxiv.org/abs/1810.11921
Source code của PyTorch_Tabular tại đây: https://github.com/manujosephv/pytorch_tabular","Với những bước tiến vượt bậc của lĩnh vực computer science đã giúp chúng ta giải quyết được nhiều bài toán với dữ liệu dạng ảnh, dạng ngôn ngữ, dạng trình tự (như trình tự DNA hay protein), dạng cấu trúc (như cấu trúc bậc 3 - 4 của phân tử protein), hay như tìm kiếm thuốc mới (ứng dụng GAN và Reinforcement Learning). Tuy nhiên, dữ liệu sinh y học thường là các quan sát đơn lẻ và được ghi dưới dạng bảng (tabular) do đó ngoài phương pháp thống kê truyền thống, cho tới các model GLM hay phương pháp mới hơn liên quan tới Machine Learning (ví dụ XgBoost, KNN, SMV,...). Vậy có ai hỏi làm thế nào để ứng dụng Deep Learning vào việc xử lí dữ liệu dạng bảng này trong lĩnh vực sinh y hay không??? Rất may mắn, chúng ta có Framework PyTorch_Tabular có thể xử lí với vấn đề này! Hiện tại framework này có xây dựng sẵn một số models như: CategoryEmbeddingModel [1]; NODEModel [2]; TabNet [3]; AutoInt [4]; và các bạn có thể tuy biến models của mình thông qua thiết lập Configuration. [1] https://arxiv.org/pdf/2104.13638.pdf [2] https://arxiv.org/abs/1909.06312 [3] https://arxiv.org/pdf/1908.07442.pdf [4] https://arxiv.org/abs/1810.11921 Source code của PyTorch_Tabular tại đây: https://github.com/manujosephv/pytorch_tabular",,,,,
"Xin chào mọi người, Mình là new member, base của mình là javascript mình đang tìm hiểu về computer vision cụ thể là object detection(yolov1). Mình chọn yolo v1 vì nó dễ hiểu, ít khái niệm nâng cao và tốc độ khá nhanh. Ngôn ngữ/ library mình sử dụng để tiếp cận là js/tensorflowjs. Có khá nhiều bài hướng dẫn về Yolov1 và cả code demo nhưng chỉ trên python, mình cũng đều đã xem và lấy đó làm cơ sở để hoàn thiện phiên bản trên js của mình.
Thuận lợi: viết trên ngôn ngữ mình đã có kinh nghiệm nên dễ dàng debug, tốc độ viết cũng nhanh hơn, hiểu những gì mình viết hơn.
Khó khăn: Gặp nhiều vấn đề mới khi sử dụng tensorflow (convert pretrained model hay bị lỗi, nhiều ví dụ python khó áp dụng do khác thư viện...).
Hầu hết khó khăn mình đã giải được nhưng còn một số vấn đề chính sau đây cần nhờ mọi người giúp đỡ:
1, Hệ số lambda mình chưa biết cân bằng do mình dùng model có input 512,512 và output 16x16x30(S=16) nên mình thấy lambda cho S=7 không phù hợp với mô hình của mình. Train xong Pobj bị kéo xuống gần 0(mình đoán là do imbalance data positive và negative). => làm thế nào để chọn lambda nếu S thay đổi ?
2, mình xem một số ví dụ trên mạng họ sử dụng active function leaky relu cho output, điều này khiến các xác suất Pobj, Pnoobj, Pclass sẽ có thể >1, thế nên lúc mình show kết quả ra xác xuất >1.0(tức là > 100%) nhìn nó sai sai.
3, Mình tự viết lossfunction theo ý hiểu của mình dựa vào công thức trong paper và tham khảo code python trên mạng, mọi người review giúp mình với.
--- 1 vài thông tin bổ sung về model của mình:
Extract feature : resnet50 (convert sang layer model js),
Input: 512,512,3 Output: 16x16x30 TrainVal: Voc2007","Xin chào mọi người, Mình là new member, base của mình là javascript mình đang tìm hiểu về computer vision cụ thể là object detection(yolov1). Mình chọn yolo v1 vì nó dễ hiểu, ít khái niệm nâng cao và tốc độ khá nhanh. Ngôn ngữ/ library mình sử dụng để tiếp cận là js/tensorflowjs. Có khá nhiều bài hướng dẫn về Yolov1 và cả code demo nhưng chỉ trên python, mình cũng đều đã xem và lấy đó làm cơ sở để hoàn thiện phiên bản trên js của mình. Thuận lợi: viết trên ngôn ngữ mình đã có kinh nghiệm nên dễ dàng debug, tốc độ viết cũng nhanh hơn, hiểu những gì mình viết hơn. Khó khăn: Gặp nhiều vấn đề mới khi sử dụng tensorflow (convert pretrained model hay bị lỗi, nhiều ví dụ python khó áp dụng do khác thư viện...). Hầu hết khó khăn mình đã giải được nhưng còn một số vấn đề chính sau đây cần nhờ mọi người giúp đỡ: 1, Hệ số lambda mình chưa biết cân bằng do mình dùng model có input 512,512 và output 16x16x30(S=16) nên mình thấy lambda cho S=7 không phù hợp với mô hình của mình. Train xong Pobj bị kéo xuống gần 0(mình đoán là do imbalance data positive và negative). => làm thế nào để chọn lambda nếu S thay đổi ? 2, mình xem một số ví dụ trên mạng họ sử dụng active function leaky relu cho output, điều này khiến các xác suất Pobj, Pnoobj, Pclass sẽ có thể >1, thế nên lúc mình show kết quả ra xác xuất >1.0(tức là > 100%) nhìn nó sai sai. 3, Mình tự viết lossfunction theo ý hiểu của mình dựa vào công thức trong paper và tham khảo code python trên mạng, mọi người review giúp mình với. --- 1 vài thông tin bổ sung về model của mình: Extract feature : resnet50 (convert sang layer model js), Input: 512,512,3 Output: 16x16x30 TrainVal: Voc2007",,,,,
"Tỉ lệ mắc bệnh đái tháo đường ngày một tăng cao, ước tính cso 442 triệu người mắc trên toàn Thế giới vào năm 2014 [1]. Đái tháo đường là nguyên nhân suy thận, tổn thương thần kinh và tổn thương đáy mắt.
Bên cạnh đó, việc phát hiện sớm và điều trị kịp thời có thể ngăn thoái hóa điểm vàng dẫn tới mù lòa ở người cao tuổi
Đề phát hiện các bệnh này, kĩ thuật chụp ảnh optical coherence tomography (OCT) thường xuyên được sử dụng trong ngành nhãn khoa [2]. Về cơ bản, các ảnh OCT này được phân loại vào 4 lớp là choroidal neovascularization (CNV), DME, drusen, và bình thường.
Việc ứng dụng Deep Learning vào việc hỗ trợ bác sĩ phân loại ảnh OCT này sẽ có ý nghĩa quan trọng trong bối cảnh nước ta khi tỉ lệ già hóa của dân số diễn ra nhanh, tỉ lệ bệnh nhân mắc đái tháo đường tăng cao, và đặc biệt là tình trạng thiếu bác sĩ được đào tạo chuyên sâu. Chính vì vậy, tôi nghĩ Việt Nam nên đầu tư mạnh mẽ vào nghiên cứu và ứng dụng DL trong nhiều lĩnh vực của cuộc sống cũng như y học.
Kết quả này của tôi vượt xa kết quả bài báo gốc đăng trên tạp chí chuyên ngành rất rất đỉnh là Cell. Theo như kết quả đối chứng với các bác sĩ chuyên khoa giàu kinh nghiệm thì kết quả của chúng tôi vượt qua 5/6 chuyên gia. Có 1 chuyên gia chẩn đoán tốt hơn kết quả này, bác sĩ này sai 3 ảnh trên tổng 1000 ảnh. Còn như confusion matrix của tôi, các bạn thấy model dự báo sai 6 ảnh/1000 ảnh của tập test set.
Tuy nhiên, đây là thí nghiệm nhanh (chưa finetune) nên tôi tin, kết quả của mình sẽ còn cải thiện nếu finetune đủ tốt!!!!
Bài báo gốc đăng trong top-tier chuyên ngành tại đây:https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5
Dữ liệu tại đây: Data: https://data.mendeley.com/datasets/rscbjbr9sj/2 hoặc https://www.kaggle.com/paultimothymooney/kermany2018
[1] Chan M. Global report on diabetes. World Health Organ. 2014;58:1–88.
https://doi.org/10.1128/AAC.03728-14.
[2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7082944/pdf/12886_2020_Article_1382.pdf
Ps. Do không có nhiều kiến thức về nhãn khoa nên tôi không dám dịch một số thuật ngữ chuyên ngành, mong các bạn thông cảm!","Tỉ lệ mắc bệnh đái tháo đường ngày một tăng cao, ước tính cso 442 triệu người mắc trên toàn Thế giới vào năm 2014 [1]. Đái tháo đường là nguyên nhân suy thận, tổn thương thần kinh và tổn thương đáy mắt. Bên cạnh đó, việc phát hiện sớm và điều trị kịp thời có thể ngăn thoái hóa điểm vàng dẫn tới mù lòa ở người cao tuổi Đề phát hiện các bệnh này, kĩ thuật chụp ảnh optical coherence tomography (OCT) thường xuyên được sử dụng trong ngành nhãn khoa [2]. Về cơ bản, các ảnh OCT này được phân loại vào 4 lớp là choroidal neovascularization (CNV), DME, drusen, và bình thường. Việc ứng dụng Deep Learning vào việc hỗ trợ bác sĩ phân loại ảnh OCT này sẽ có ý nghĩa quan trọng trong bối cảnh nước ta khi tỉ lệ già hóa của dân số diễn ra nhanh, tỉ lệ bệnh nhân mắc đái tháo đường tăng cao, và đặc biệt là tình trạng thiếu bác sĩ được đào tạo chuyên sâu. Chính vì vậy, tôi nghĩ Việt Nam nên đầu tư mạnh mẽ vào nghiên cứu và ứng dụng DL trong nhiều lĩnh vực của cuộc sống cũng như y học. Kết quả này của tôi vượt xa kết quả bài báo gốc đăng trên tạp chí chuyên ngành rất rất đỉnh là Cell. Theo như kết quả đối chứng với các bác sĩ chuyên khoa giàu kinh nghiệm thì kết quả của chúng tôi vượt qua 5/6 chuyên gia. Có 1 chuyên gia chẩn đoán tốt hơn kết quả này, bác sĩ này sai 3 ảnh trên tổng 1000 ảnh. Còn như confusion matrix của tôi, các bạn thấy model dự báo sai 6 ảnh/1000 ảnh của tập test set. Tuy nhiên, đây là thí nghiệm nhanh (chưa finetune) nên tôi tin, kết quả của mình sẽ còn cải thiện nếu finetune đủ tốt!!!! Bài báo gốc đăng trong top-tier chuyên ngành tại đây:https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5 Dữ liệu tại đây: Data: https://data.mendeley.com/datasets/rscbjbr9sj/2 hoặc https://www.kaggle.com/paultimothymooney/kermany2018 [1] Chan M. Global report on diabetes. World Health Organ. 2014;58:1–88. https://doi.org/10.1128/AAC.03728-14. [2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7082944/pdf/12886_2020_Article_1382.pdf Ps. Do không có nhiều kiến thức về nhãn khoa nên tôi không dám dịch một số thuật ngữ chuyên ngành, mong các bạn thông cảm!",,,,,
Hi Mọi Người. Em đang cần build bộ PC học machine learning. Tầm 30tr. Không biết có ai có thể tư vấn giúp Em được không ạ?,Hi Mọi Người. Em đang cần build bộ PC học machine learning. Tầm 30tr. Không biết có ai có thể tư vấn giúp Em được không ạ?,,,,,
"Xin phép admin
Sau thời một thời gian ""mất tích"", hôm nay mới làm đk một video về HỆ THỐNG GỢI Ý (Recommender System) và đã Up lên Youtube cho ae xem. Hi vọng sẽ có ích cho các bạn đang nghiên cứu về nó!
- Quên là còn nhiều video khác về một số bài hướng dẫn về AI, ae chưa xem có thể tìm xem trong mục video biết đâu có mục ae cần
- Mong được các cao nhân thông thạo các thứ chỉ bảo thêm ạ!
https://www.youtube.com/watch?v=HFsgbLLAVq0","Xin phép admin Sau thời một thời gian ""mất tích"", hôm nay mới làm đk một video về HỆ THỐNG GỢI Ý (Recommender System) và đã Up lên Youtube cho ae xem. Hi vọng sẽ có ích cho các bạn đang nghiên cứu về nó! - Quên là còn nhiều video khác về một số bài hướng dẫn về AI, ae chưa xem có thể tìm xem trong mục video biết đâu có mục ae cần - Mong được các cao nhân thông thạo các thứ chỉ bảo thêm ạ! https://www.youtube.com/watch?v=HFsgbLLAVq0",,,,,
,nan,,,,,
"[AI News – GAN Application]
StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing ( CVPR 21)
StyleMapGAN đề xuất một biểu diễn mới của không gian ẩn dựa trên stylemap. Ý tưởng là thay vì học cách biểu diễn ẩn dựa trên vectơ, StyleMapGAN sử dụng một tensor với các kích thước không gian rõ ràng. Khi đó, biểu diễn này được sẽ chứa thông tin từ các kích thước không gian đó, do vậy GAN dễ dàng encode đặc trưng cục bộ của hình ảnh vào không gian ẩn.
Phương pháp này cho phép chỉnh sửa các vùng cụ thể của hình ảnh. Đặc biệt, tất cả chỉnh sửa được thực hiện trong thời gian thực.","[AI News – GAN Application] StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing ( CVPR 21) StyleMapGAN đề xuất một biểu diễn mới của không gian ẩn dựa trên stylemap. Ý tưởng là thay vì học cách biểu diễn ẩn dựa trên vectơ, StyleMapGAN sử dụng một tensor với các kích thước không gian rõ ràng. Khi đó, biểu diễn này được sẽ chứa thông tin từ các kích thước không gian đó, do vậy GAN dễ dàng encode đặc trưng cục bộ của hình ảnh vào không gian ẩn. Phương pháp này cho phép chỉnh sửa các vùng cụ thể của hình ảnh. Đặc biệt, tất cả chỉnh sửa được thực hiện trong thời gian thực.",,,,,
"Chào mọi người ạ, hiện em mới đang tìm hiểu về CNN unet để phân đoạn ảnh CT phổi, thì hiện tại em đang tìm data, thì bị vướng như chỉ thấy data kiểu này. Ảnh input file tif nó bị thành thế kia mà ko phải ảnh gốc khi chụp ạ, không biết là máy mở tif bị thế hay là data train sẵn thế ạ. Mong mn giúp đỡ ạ, em cảm ơn ạ.","Chào mọi người ạ, hiện em mới đang tìm hiểu về CNN unet để phân đoạn ảnh CT phổi, thì hiện tại em đang tìm data, thì bị vướng như chỉ thấy data kiểu này. Ảnh input file tif nó bị thành thế kia mà ko phải ảnh gốc khi chụp ạ, không biết là máy mở tif bị thế hay là data train sẵn thế ạ. Mong mn giúp đỡ ạ, em cảm ơn ạ.",,,,,
"[HELP][Feature Engineering đối với dữ liệu dạng text cho Tiếng Việt]
Em chào anh chị,
Không biết anh chị nào có kinh nghiệm làm feature engineering đối với dữ liệu về name, status, feed (dữ liệu dạng text) chưa ạ.
Dữ liệu name của em thường có dạng như không viết hoa, một chuỗi ký tự bất kỳ, tên một chữ, hoặc là tên tiếng việt (ví dụ: Abcdefghik, E, bi kha, Đan Đan,... và còn nhiều dạng nữa)
Còn dữ liệu status, feed thì giống như trên Facebook, feed thì thường có text hoặc có cả text và hình, status là một đoạn text ngắn
Hy vọng được anh chị chia sẻ kinh nghiệm ạ, em cảm ơn rất nhiều và chúc mọi người ngày mới làm việc thật hiệu quả ạ.","[HELP][Feature Engineering đối với dữ liệu dạng text cho Tiếng Việt] Em chào anh chị, Không biết anh chị nào có kinh nghiệm làm feature engineering đối với dữ liệu về name, status, feed (dữ liệu dạng text) chưa ạ. Dữ liệu name của em thường có dạng như không viết hoa, một chuỗi ký tự bất kỳ, tên một chữ, hoặc là tên tiếng việt (ví dụ: Abcdefghik, E, bi kha, Đan Đan,... và còn nhiều dạng nữa) Còn dữ liệu status, feed thì giống như trên Facebook, feed thì thường có text hoặc có cả text và hình, status là một đoạn text ngắn Hy vọng được anh chị chia sẻ kinh nghiệm ạ, em cảm ơn rất nhiều và chúc mọi người ngày mới làm việc thật hiệu quả ạ.",,,,,
"Lego generator
Source code: https://github.com/uvipen/Lego-generator
Demo: https://youtu.be/uz8A1pq8CAw
Đây là mini project của mình ứng dụng Computer Vision để convert ảnh hoặc video bất kì thành output được tạo thành bởi các khối lego. Code của mình rất ngắn và đơn giản. Mong được nhận góp ý từ mọi người. Mình xin cảm ơn",Lego generator Source code: https://github.com/uvipen/Lego-generator Demo: https://youtu.be/uz8A1pq8CAw Đây là mini project của mình ứng dụng Computer Vision để convert ảnh hoặc video bất kì thành output được tạo thành bởi các khối lego. Code của mình rất ngắn và đơn giản. Mong được nhận góp ý từ mọi người. Mình xin cảm ơn,,,,,
#python #datatype,,#python	#datatype,,,,
"[AI Share - Interview]
Bạn là người mới bắt đầu hay là một chuyên gia đang tìm kiếm con đường để trở thành Data Scientist?
Hãy theo dõi series 30 ngày chuẩn bị các câu hỏi để phỏng vấn Data Science.
Repos này chứa 30 file tương ứng với 30 ngày ôn tập. Mỗi file sẽ có một bộ câu hỏi riêng nhé ^^.
Mọi người tải về và ôn tập ở đây nhé:
https://github.com/iNeuronai/interview-question-data-science-",[AI Share - Interview] Bạn là người mới bắt đầu hay là một chuyên gia đang tìm kiếm con đường để trở thành Data Scientist? Hãy theo dõi series 30 ngày chuẩn bị các câu hỏi để phỏng vấn Data Science. Repos này chứa 30 file tương ứng với 30 ngày ôn tập. Mỗi file sẽ có một bộ câu hỏi riêng nhé ^^. Mọi người tải về và ôn tập ở đây nhé: https://github.com/iNeuronai/interview-question-data-science-,,,,,
"Cho mình hỏi vài ý nha mọi người.
Mình đang làm recommendation system cho 1 trang E-C, trong lúc làm có xảy ra trường hợp là nếu người dùng đã đăng nhập, tức đã có thông tin người dùng thì sẽ dùng lọc cộng tác để recommend sp cho người dùng này. Tuy nhiên nếu ngươi dùng mới tạo tài khoản, tức là trước đây chưa từng xem sản phẩm nào cả thì mình sẽ dùng phương pháp gì để gợi ý hả mọi người?
Với lại nếu người dùng không đăng nhập thì ta có thể recommend bằng những sp có số lượt xem nhiều nhất, tuy nhiên nó không dính gì tới ML, vậy để có thể dùng thuật toán ML thì ta sẽ gợi ý theo cách nào giờ mọi người ơi?
Có gì giúp mình với, :(((.","Cho mình hỏi vài ý nha mọi người. Mình đang làm recommendation system cho 1 trang E-C, trong lúc làm có xảy ra trường hợp là nếu người dùng đã đăng nhập, tức đã có thông tin người dùng thì sẽ dùng lọc cộng tác để recommend sp cho người dùng này. Tuy nhiên nếu ngươi dùng mới tạo tài khoản, tức là trước đây chưa từng xem sản phẩm nào cả thì mình sẽ dùng phương pháp gì để gợi ý hả mọi người? Với lại nếu người dùng không đăng nhập thì ta có thể recommend bằng những sp có số lượt xem nhiều nhất, tuy nhiên nó không dính gì tới ML, vậy để có thể dùng thuật toán ML thì ta sẽ gợi ý theo cách nào giờ mọi người ơi? Có gì giúp mình với, :(((.",,,,,
"Hello mọi người, em mới bắt đầu tìm hiểu về ML được 1 tháng này. Đến phần PCA, em có một thắc mắc là mọi người thường apply PCA cho toàn bộ tập data (entire data), hay là apply cho duy nhất tập training data (fit_transform), sau đó lấy mô hình PCA học được từ tập train để transform tập test ạ? À mọi người có thể chỉ ra điểm khác biệt của 2 cách trên dc ko ạ. Em cảm ơn rất nhiều ạ!!!!
P/s: em có tham khảo (xem trộm :v) 1 số project của ổng mentor của em thì: có lúc ổng apply thẳng, có lúc ổng chia ra rồi mới apply...Mong ad duyệt ạ!!","Hello mọi người, em mới bắt đầu tìm hiểu về ML được 1 tháng này. Đến phần PCA, em có một thắc mắc là mọi người thường apply PCA cho toàn bộ tập data (entire data), hay là apply cho duy nhất tập training data (fit_transform), sau đó lấy mô hình PCA học được từ tập train để transform tập test ạ? À mọi người có thể chỉ ra điểm khác biệt của 2 cách trên dc ko ạ. Em cảm ơn rất nhiều ạ!!!! P/s: em có tham khảo (xem trộm :v) 1 số project của ổng mentor của em thì: có lúc ổng apply thẳng, có lúc ổng chia ra rồi mới apply...Mong ad duyệt ạ!!",,,,,
Mn giải thích giúp em đoạn in đậm dưới đây với ạ. Em đọc hoài mà không thông nổi. Em cảm ơn mn nhiều.,Mn giải thích giúp em đoạn in đậm dưới đây với ạ. Em đọc hoài mà không thông nổi. Em cảm ơn mn nhiều.,,,,,
Chia sẻ sách về python for data science,Chia sẻ sách về python for data science,,,,,
Một số câu lệnh cơ bản trong Linux mà các bạn nên biết,Một số câu lệnh cơ bản trong Linux mà các bạn nên biết,,,,,
"Chào mọi người.
Em hiện đang tập xây dựng cái giao diện điền khiển UAV (mô hình vui thôi) dựa trên tham khảo của viettel, các bác cho em hỏi có ai biết cách vẽ cái la bàn bằng python đưa vô không vậy ạ, sợt gu gồ toàn thấy là vẽ trên thiết bị adruno cái này thì em chịu.
Thank mọi người.","Chào mọi người. Em hiện đang tập xây dựng cái giao diện điền khiển UAV (mô hình vui thôi) dựa trên tham khảo của viettel, các bác cho em hỏi có ai biết cách vẽ cái la bàn bằng python đưa vô không vậy ạ, sợt gu gồ toàn thấy là vẽ trên thiết bị adruno cái này thì em chịu. Thank mọi người.",,,,,
"anh chị cho e hỏi chút với ạ.
em có một tập data đã segment vùng tay, hiện tại e muốn classify các hành động trong tập này. nhưng khi e train thì xuất hiện lỗi này ạ? ảnh segment là ảnh có màu (chạy với các bộ data khác k segment thì vẫn đc).
Em cảm ơn a chị ạ!","anh chị cho e hỏi chút với ạ. em có một tập data đã segment vùng tay, hiện tại e muốn classify các hành động trong tập này. nhưng khi e train thì xuất hiện lỗi này ạ? ảnh segment là ảnh có màu (chạy với các bộ data khác k segment thì vẫn đc). Em cảm ơn a chị ạ!",,,,,
"Mọi người ơi, cho em hỏi là em có 1 file Json theo CoCo format với folder ảnh, vậy có cách library nào để đọc file json rồi visualize lên ảnh để em check xem annotations đúng không nhỉ?","Mọi người ơi, cho em hỏi là em có 1 file Json theo CoCo format với folder ảnh, vậy có cách library nào để đọc file json rồi visualize lên ảnh để em check xem annotations đúng không nhỉ?",,,,,
Em xin lên tiếp clip số 2 của series về Stats ạ. Cảm ơn mọi người đã ủng hộ clip số 1 của em.,Em xin lên tiếp clip số 2 của series về Stats ạ. Cảm ơn mọi người đã ủng hộ clip số 1 của em.,,,,,
"Chào các bạn. Mình đang nghiên cứu về bài toán binary classification trên dữ liệu 1 chiều (Univariate data), hiện tại mình đang sử dụng meanshift để nhận dạng vì dữ liệu có đặc điểm không được gán nhãn trước, kết quả tương đối tốt. Các bạn vui lòng cho mình hỏi ngoài ra mình có thể sử dụng mô hình nào khác để phân loại được không ?
Xin cảm ơn rất nhiều.","Chào các bạn. Mình đang nghiên cứu về bài toán binary classification trên dữ liệu 1 chiều (Univariate data), hiện tại mình đang sử dụng meanshift để nhận dạng vì dữ liệu có đặc điểm không được gán nhãn trước, kết quả tương đối tốt. Các bạn vui lòng cho mình hỏi ngoài ra mình có thể sử dụng mô hình nào khác để phân loại được không ? Xin cảm ơn rất nhiều.",,,,,
"[AI Share]
Meta-learning là một lĩnh vực nghiên cứu nới nổi và phát triển nhanh chóng trong AI.
Dưới đây mà một số hướng dẫn về lĩnh vực Meta-learning bao gồm cơ sở toán học và các ứng dụng của nó trong AAAI 2021.",[AI Share] Meta-learning là một lĩnh vực nghiên cứu nới nổi và phát triển nhanh chóng trong AI. Dưới đây mà một số hướng dẫn về lĩnh vực Meta-learning bao gồm cơ sở toán học và các ứng dụng của nó trong AAAI 2021.,,,,,
"Chào mn, cho em hỏi việc sử dụng dataset để tìm tham số tối ưu cho mô hình bằng Bayes Optimization (sử dụng cross validation cho hàm mục tiêu), sau đó sử dụng Repeated Cross Validation để đánh giá mô hình với bộ tham số tối ưu tìm được cũng trên dataset như thế này là đúng hay sai ạ.","Chào mn, cho em hỏi việc sử dụng dataset để tìm tham số tối ưu cho mô hình bằng Bayes Optimization (sử dụng cross validation cho hàm mục tiêu), sau đó sử dụng Repeated Cross Validation để đánh giá mô hình với bộ tham số tối ưu tìm được cũng trên dataset như thế này là đúng hay sai ạ.",,,,,
"Chào mọi người. Em có một thắc mắc sau mong mọi người giúp đỡ.
Em có 2 neural netwroks có cấu trúc như nhau, có cùng số lượng parameters, nhưng vì được train trên 2 bộ data set khác nhau nên bộ trọng số của mỗi model là khác nhau. Cho em hỏi là làm sao để tính được mức độ giống nhau ( similarity ) của 2 models này ạ.
Em xin cảm ơn.","Chào mọi người. Em có một thắc mắc sau mong mọi người giúp đỡ. Em có 2 neural netwroks có cấu trúc như nhau, có cùng số lượng parameters, nhưng vì được train trên 2 bộ data set khác nhau nên bộ trọng số của mỗi model là khác nhau. Cho em hỏi là làm sao để tính được mức độ giống nhau ( similarity ) của 2 models này ạ. Em xin cảm ơn.",,,,,
"Chào mn ạ. Hiện tại em đang làm 1 project quản lý chấm công theo mô hình client angular, server là spring boot. Có 1 phần em đang vướng là tạo ảnh model nhân viên bằng opencv có cách nào để em có thể call từ angular đến python để mở camera và tạo model không ạ?
Cảm ơn mọi người nhiều.","Chào mn ạ. Hiện tại em đang làm 1 project quản lý chấm công theo mô hình client angular, server là spring boot. Có 1 phần em đang vướng là tạo ảnh model nhân viên bằng opencv có cách nào để em có thể call từ angular đến python để mở camera và tạo model không ạ? Cảm ơn mọi người nhiều.",,,,,
"Chào mọi người,
Mọi người cho em hỏi, trong mô hình transformer thì encoder pass key và value từ encoder sang decoder. Ngoài 2 matrix trên thì có pass giá trị z như hình phía dưới sang decoder ko ạ (Nếu pass thì nó sẽ sử dụng như thế nào ở decoder có phải nó dc concat với input ở encoder ko ạ, còn nếu ko thì mình tính giá trị này làm gì ạ).
Còn một câu hỏi nữa là. 2 giá trị key và value từ encoder đi vào Multi head attention còn query thì e ko thấy ạ. E thấy dấu mũi tên từ masked multi head attention đi lên nhưng ko biết nó là cái gì, có phải là vector query e đã nói ko ạ.
Mong mọi người giúp đỡ em cảm ơn","Chào mọi người, Mọi người cho em hỏi, trong mô hình transformer thì encoder pass key và value từ encoder sang decoder. Ngoài 2 matrix trên thì có pass giá trị z như hình phía dưới sang decoder ko ạ (Nếu pass thì nó sẽ sử dụng như thế nào ở decoder có phải nó dc concat với input ở encoder ko ạ, còn nếu ko thì mình tính giá trị này làm gì ạ). Còn một câu hỏi nữa là. 2 giá trị key và value từ encoder đi vào Multi head attention còn query thì e ko thấy ạ. E thấy dấu mũi tên từ masked multi head attention đi lên nhưng ko biết nó là cái gì, có phải là vector query e đã nói ko ạ. Mong mọi người giúp đỡ em cảm ơn",,,,,
"Dạ em mới tìm hiểu về machine learning, mọi người cho em hỏi máy em dùng RTX 1650 thì mình nên chạy trên máy hay dùng colab ạ, và nếu chạy trên máy thì cho em xin hướng đề cái tf + gpu với ạ. tks mn","Dạ em mới tìm hiểu về machine learning, mọi người cho em hỏi máy em dùng RTX 1650 thì mình nên chạy trên máy hay dùng colab ạ, và nếu chạy trên máy thì cho em xin hướng đề cái tf + gpu với ạ. tks mn",,,,,
"Chào mn, em đang bắt đầu đọc về graph neural net ạ
Em đang thực hành phân loại node cho data dạng bảng. Em muốn tạo node features rồi xây dựng liên kết giữa chúng đee build graph trước khi feed vào GNN thì e tính simmilarity giữa các sample rồi chọn edge theo đó có được không ạ. Em cảm ơn","Chào mn, em đang bắt đầu đọc về graph neural net ạ Em đang thực hành phân loại node cho data dạng bảng. Em muốn tạo node features rồi xây dựng liên kết giữa chúng đee build graph trước khi feed vào GNN thì e tính simmilarity giữa các sample rồi chọn edge theo đó có được không ạ. Em cảm ơn",,,,,
"Xin chào mọi người!
Hiện tại công ty em đang có nhu cầu build server vật lý để tiết kiệm chi phí cho quá trình research, mọi người có thể cho em xin một số cấu hình máy tầm trung(khoảng 120-150tr) và một vài cơ sở build máy server uy tín với ạ
Em xin cảm ơn!
P/s: Vì công việc làm khá nhiều task về training GAN model nên e dự tính sẽ dùng card T4.","Xin chào mọi người! Hiện tại công ty em đang có nhu cầu build server vật lý để tiết kiệm chi phí cho quá trình research, mọi người có thể cho em xin một số cấu hình máy tầm trung(khoảng 120-150tr) và một vài cơ sở build máy server uy tín với ạ Em xin cảm ơn! P/s: Vì công việc làm khá nhiều task về training GAN model nên e dự tính sẽ dùng card T4.",,,,,
"Anh chị ơi cho em hỏi là trên Mac M1 chưa hỗ trợ các thư viện như là numpy, pandas phải không ạ ? Em cảm ơn","Anh chị ơi cho em hỏi là trên Mac M1 chưa hỗ trợ các thư viện như là numpy, pandas phải không ạ ? Em cảm ơn",,,,,
"Mn cho em hỏi, sau từ phương trình trên lại có thể viết lại được như phương trình dưới ạ. Hai phương trình có ý nghĩa khác nhau mà ( một cái sau và một cái trước ). Em cảm ơn mn nhiều.","Mn cho em hỏi, sau từ phương trình trên lại có thể viết lại được như phương trình dưới ạ. Hai phương trình có ý nghĩa khác nhau mà ( một cái sau và một cái trước ). Em cảm ơn mn nhiều.",,,,,
"Chào mọi người ạ
Em đang làm về đề tài nhận dạng hành động thời gian thực, em sử dụng MobileNetV2 để rút trích đặc trưng và LSTM đề phân lớp đến giai đoạn training, test trên dataset thì dự đoán đúng đến trên 90% nhưng khi test real time với webcam thì dự đoán chưa được chính xác. Dataset em sử dụng là KARD và thu thập thêm một số hành động không có trong dataset (3 hành động trong KARD và 3 hành động tự thu thập thêm). Em training với 100 lần lặp thì được số liệu như hình. Mọi người phân tích giúp em mô hình học như vậy là có ổn chưa, nếu chưa ổn thì phải lặp thêm khoảng bao nhiêu lần nữa. Em cảm ơn ạ!
Nguồn tham khảo: https://github.com/peachman05/action-recognition-tutorial#dataset","Chào mọi người ạ Em đang làm về đề tài nhận dạng hành động thời gian thực, em sử dụng MobileNetV2 để rút trích đặc trưng và LSTM đề phân lớp đến giai đoạn training, test trên dataset thì dự đoán đúng đến trên 90% nhưng khi test real time với webcam thì dự đoán chưa được chính xác. Dataset em sử dụng là KARD và thu thập thêm một số hành động không có trong dataset (3 hành động trong KARD và 3 hành động tự thu thập thêm). Em training với 100 lần lặp thì được số liệu như hình. Mọi người phân tích giúp em mô hình học như vậy là có ổn chưa, nếu chưa ổn thì phải lặp thêm khoảng bao nhiêu lần nữa. Em cảm ơn ạ! Nguồn tham khảo: https://github.com/peachman05/action-recognition-tutorial#dataset",,,,,
"https://www.kaggle.com/c/mercari-price-suggestion-challenge
Đây là một bài tập dự đoán giá bằng hồi quy, cho em hỏi là việc log của price tuân theo normal distribution có ảnh hưởng thế nào đến việc giải bài toán vậy ạ?","https://www.kaggle.com/c/mercari-price-suggestion-challenge Đây là một bài tập dự đoán giá bằng hồi quy, cho em hỏi là việc log của price tuân theo normal distribution có ảnh hưởng thế nào đến việc giải bài toán vậy ạ?",,,,,
"[Tích phân riemann và định lý fubini - Đóng góp Minh Phương]
Tích phân riemann có nhiều ứng dụng trong giải tích. Nó cho phép ta tính xấp xỉ của hầu hết các tích phân mà lời giải thông thường khó tìm kiếm.
Còn định lý Fubini thì quá quen thuộc với những bài toán tìm tích phân nhiều chiều.
Cùng tìm hiểu về những khái niệm giải tích cơ bản này qua bài viết của tác giả Minh Phương trong ""Machine Learning Alogrithms to Practice"".
---------------------------------------------------------------
Với mục tiêu ""Vì một cộng đồng AI vững mạnh hơn"". Bạn có thể tham gia dự án viết sách cộng đồng của cuốn ""Machine Learning Algorithms to Practice"" tại:
https://www.facebook.com/groups/167567421900114","[Tích phân riemann và định lý fubini - Đóng góp Minh Phương] Tích phân riemann có nhiều ứng dụng trong giải tích. Nó cho phép ta tính xấp xỉ của hầu hết các tích phân mà lời giải thông thường khó tìm kiếm. Còn định lý Fubini thì quá quen thuộc với những bài toán tìm tích phân nhiều chiều. Cùng tìm hiểu về những khái niệm giải tích cơ bản này qua bài viết của tác giả Minh Phương trong ""Machine Learning Alogrithms to Practice"". --------------------------------------------------------------- Với mục tiêu ""Vì một cộng đồng AI vững mạnh hơn"". Bạn có thể tham gia dự án viết sách cộng đồng của cuốn ""Machine Learning Algorithms to Practice"" tại: https://www.facebook.com/groups/167567421900114",,,,,
"Chào mọi người.
Mình thắc mắc trong bài toán Segmantic segmentation, khi huấn luyện mình lựa chọn 1 kích thước ảnh cố định sử dụng random crop để có thể huấn luyện theo batches. Nhưng khi test, ảnh đầu vào có kích thước khác nhau và có thể lớn hơn nhiều so với lúc training liệu có ảnh hưởng tới độ chính xác của mô hình? Mn có thể giới thiệu cho mình chiến lược khi huấn luyện segmantic segmentation model khi dữ liệu ảnh có kích thước bất kì k ạ?","Chào mọi người. Mình thắc mắc trong bài toán Segmantic segmentation, khi huấn luyện mình lựa chọn 1 kích thước ảnh cố định sử dụng random crop để có thể huấn luyện theo batches. Nhưng khi test, ảnh đầu vào có kích thước khác nhau và có thể lớn hơn nhiều so với lúc training liệu có ảnh hưởng tới độ chính xác của mô hình? Mn có thể giới thiệu cho mình chiến lược khi huấn luyện segmantic segmentation model khi dữ liệu ảnh có kích thước bất kì k ạ?",,,,,
"[Xin phép Admin]
Em xin phép gửi tới các anh chị, các bạn trong group bộ dữ liệu được VinBigdata xây dựng để hỗ trợ VLSP tổ chức ASR challenge 2020, dự kiến được tổ chức trong tháng 12 ở Hà Nội tại: https://slp.vinbigdata.org/
Cụ thể, 100 giờ dữ liệu tiếng nói đã có gán nhãn sẽ được dùng làm dữ liệu huấn luyện (training dataset), giúp các đội trẻ phát triển mô hình ASR (Tự động nhận dạng tiếng nói) cho tiếng Việt. Kết quả của mô hình sẽ được đánh giá bởi Word error rate (WER) - thang đo quốc tế đối với hệ thống nhận dạng tiếng nói và dịch máy. Bên cạnh bộ dữ liệu dành cho ASR, VinBigdata cũng chia sẻ 01 bộ dữ liệu dành cho Machine Translation tại link trên.","[Xin phép Admin] Em xin phép gửi tới các anh chị, các bạn trong group bộ dữ liệu được VinBigdata xây dựng để hỗ trợ VLSP tổ chức ASR challenge 2020, dự kiến được tổ chức trong tháng 12 ở Hà Nội tại: https://slp.vinbigdata.org/ Cụ thể, 100 giờ dữ liệu tiếng nói đã có gán nhãn sẽ được dùng làm dữ liệu huấn luyện (training dataset), giúp các đội trẻ phát triển mô hình ASR (Tự động nhận dạng tiếng nói) cho tiếng Việt. Kết quả của mô hình sẽ được đánh giá bởi Word error rate (WER) - thang đo quốc tế đối với hệ thống nhận dạng tiếng nói và dịch máy. Bên cạnh bộ dữ liệu dành cho ASR, VinBigdata cũng chia sẻ 01 bộ dữ liệu dành cho Machine Translation tại link trên.",,,,,
"Mọi người cho em hỏi, em dùng email trường để học free các khóa học trên coursera thì sau khi hoàn thành em có nhận được Cerf không ạ? Giữa khóa data science của ibm trên coursera với data science trên udemy (hiện đang giảm giá còn 12 đô) thì em nên chọn học khóa nào ạ? Em cảm ơn","Mọi người cho em hỏi, em dùng email trường để học free các khóa học trên coursera thì sau khi hoàn thành em có nhận được Cerf không ạ? Giữa khóa data science của ibm trên coursera với data science trên udemy (hiện đang giảm giá còn 12 đô) thì em nên chọn học khóa nào ạ? Em cảm ơn",,,,,
"#xin_viec #xin_hoc_tap
Em xin chào mọi người.
Em mới chuyển từ kinh tế sang ML. Anh chị nào có làm dự án cần người hỗ trợ thì có thể cho em tham gia để học hỏi thêm kinh nghiệm được không ạ?
Em nắm kiến thức căn bản về một số thuật toán regression, neural network, svm, thư viện opencv tensorflow","Em xin chào mọi người. Em mới chuyển từ kinh tế sang ML. Anh chị nào có làm dự án cần người hỗ trợ thì có thể cho em tham gia để học hỏi thêm kinh nghiệm được không ạ? Em nắm kiến thức căn bản về một số thuật toán regression, neural network, svm, thư viện opencv tensorflow",#xin_viec	#xin_hoc_tap,,,,
"Mn cho em hỏi câu này hơi ngốc xíu, 1e-3 bấm trong máy tính như thế nào để ra đc 0.001 ạ. Trước em có tìm được trang nói về những kí hiệu này nhưng h tìm lại không thấy nữa. Em cảm ơn mn nhiều.","Mn cho em hỏi câu này hơi ngốc xíu, 1e-3 bấm trong máy tính như thế nào để ra đc 0.001 ạ. Trước em có tìm được trang nói về những kí hiệu này nhưng h tìm lại không thấy nữa. Em cảm ơn mn nhiều.",,,,,
"Mình sử dụng yolo train được model nặng tầm 250Mb, nhưng khi deploy lên server có cấu hình RAM 2GB, CPU AMD EPYC 7571 (như trong hình), thì có vẻ server bị quá tải dẫn tới ko detect được. Không biết như vậy là có gì bất thường không nhỉ. Nếu do cấu hình server yếu thì nên nâng cấp lên cấu hình như thế nào thì phù hợp nhỉ. Mọi người tư vấn giúp mình giải pháp xử lý vấn đề này với ạ. Cảm ơn mọi người nhiều.","Mình sử dụng yolo train được model nặng tầm 250Mb, nhưng khi deploy lên server có cấu hình RAM 2GB, CPU AMD EPYC 7571 (như trong hình), thì có vẻ server bị quá tải dẫn tới ko detect được. Không biết như vậy là có gì bất thường không nhỉ. Nếu do cấu hình server yếu thì nên nâng cấp lên cấu hình như thế nào thì phù hợp nhỉ. Mọi người tư vấn giúp mình giải pháp xử lý vấn đề này với ạ. Cảm ơn mọi người nhiều.",,,,,
"Kính chào các bác. Hôm trước có bạn trên Group hỏi về vấn đề này nên em làm thử. ở đây em chỉ làm cơ bản với OpenCV chứ ko có GAN gì hết ạ.
Chúc các bạn thành công! Hi vọng giúp được các bạn newbie và mong admin duyệt bài!",Kính chào các bác. Hôm trước có bạn trên Group hỏi về vấn đề này nên em làm thử. ở đây em chỉ làm cơ bản với OpenCV chứ ko có GAN gì hết ạ. Chúc các bạn thành công! Hi vọng giúp được các bạn newbie và mong admin duyệt bài!,,,,,
"Chào mọi người,
Mình mới train xong 1 classification model efficientnetv2, có cách nào để lắp nó vào cái rcnn architecture để thành object detection không nhỉ.
Trong trường hợp đấy thì cái classification model của mình có phải là backbone không?
Nếu là pytorch thì tốt quâ nhưng có resource gì mình đọc hết :)
Cảm ơn mọi người","Chào mọi người, Mình mới train xong 1 classification model efficientnetv2, có cách nào để lắp nó vào cái rcnn architecture để thành object detection không nhỉ. Trong trường hợp đấy thì cái classification model của mình có phải là backbone không? Nếu là pytorch thì tốt quâ nhưng có resource gì mình đọc hết :) Cảm ơn mọi người",,,,,
"Chia sẻ đến bạn 02 phương pháp Reinforcement Learning do Google AI phát triển, giúp ứng dụng để huấn luyện Robot đa tác vụ: MT-Opt và Actionable Models.
MT-Opt sử dụng dữ liệu, trials và errors có sẵn trong hệ thống để tối ưu hóa các Q-function, từ đó robot thực hiện theo.
Actionable Models nhận cấu hình mục tiêu cho trước, sau đó lấy và phân loại dữ liệu một cách tối ưu nhất hòng đạt được mục tiêu.
Cùng tham khảo bài viết do Google AI Blog đăng tải, các video giới thiệu 02 phương pháp và paper của dự án nhé.
#GoogleAI #ReinforcementLearning #AI #VEFAcademy","Chia sẻ đến bạn 02 phương pháp Reinforcement Learning do Google AI phát triển, giúp ứng dụng để huấn luyện Robot đa tác vụ: MT-Opt và Actionable Models. MT-Opt sử dụng dữ liệu, trials và errors có sẵn trong hệ thống để tối ưu hóa các Q-function, từ đó robot thực hiện theo. Actionable Models nhận cấu hình mục tiêu cho trước, sau đó lấy và phân loại dữ liệu một cách tối ưu nhất hòng đạt được mục tiêu. Cùng tham khảo bài viết do Google AI Blog đăng tải, các video giới thiệu 02 phương pháp và paper của dự án nhé.",#GoogleAI	#ReinforcementLearning	#AI	#VEFAcademy,,,,
"Hiện tại, em đang làm project nhận diện biển số xe. Em cần 400 ảnh đã đánh nhãn. Anh chị có thể cho em xin datasets được không ạ? Em cảm ơn ạ!","Hiện tại, em đang làm project nhận diện biển số xe. Em cần 400 ảnh đã đánh nhãn. Anh chị có thể cho em xin datasets được không ạ? Em cảm ơn ạ!",,,,,
"Em có code thử bài toán trong paper Meta Pseudo Labels mà không biết có đúng không ạ. Mọi người coi thử dùm em với cho em ý kiến với ạ. Tại e có làm và test trên tập TwoMoon dataset chỉ được khoảng 86% và nó chính xác giống như trong paper ạ. Hình đầu là Notation, hình thứ 2 là algorithm trong paper và hình thứ 3 là code của em ạ. Vì dữ liệu của em dạng bảng (2 cột: một là tọa độ x, hai là tọa độ y ) nên em không dùng UDA ạ.","Em có code thử bài toán trong paper Meta Pseudo Labels mà không biết có đúng không ạ. Mọi người coi thử dùm em với cho em ý kiến với ạ. Tại e có làm và test trên tập TwoMoon dataset chỉ được khoảng 86% và nó chính xác giống như trong paper ạ. Hình đầu là Notation, hình thứ 2 là algorithm trong paper và hình thứ 3 là code của em ạ. Vì dữ liệu của em dạng bảng (2 cột: một là tọa độ x, hai là tọa độ y ) nên em không dùng UDA ạ.",,,,,
"Xin chào cả nhà, VietAI muốn gửi đến mọi người code/data để train state-of-the-art Vietnamese translation systems https://github.com/vietai/sat. Đi xa hơn nữa, VietAI muốn kêu gọi cộng đồng cùng chung tay để build the best resources for Vietnamese translations for everyone to use. Mọi người có thể contribute dưới nhiều hình thức:
a. Code: build better models, e.g., using PyTorch Fairseq or advanced architecture such as Funnel Transformer. VietAI has access to Google Cloud TPUv3 cho bạn nào tâm huyết :)
b. Data: contribute data in various domains, e.g., medicine, arts, etc., or challenging examples like idioms, slangs (""thích thì chiều"").
c. Better demo: Hiện tại VietAI có demo https://demo.vietai.org/, tuy nhiên chưa scale well va còn thiếu nhiều features để make it more useful to everyone. Rất cần support từ các bạn có experience về web developments & UI/UX design.
Để tìm hiểu thêm thông tin, các bạn có thể comment ở đây; inbox cho Trieu H. Trinh Chinh Ngo (technical), Phan Bạch Xuân An (logistics); or volunteer at VietAI (link in the post below).
Cám ơn cả nhà!","Xin chào cả nhà, VietAI muốn gửi đến mọi người code/data để train state-of-the-art Vietnamese translation systems https://github.com/vietai/sat. Đi xa hơn nữa, VietAI muốn kêu gọi cộng đồng cùng chung tay để build the best resources for Vietnamese translations for everyone to use. Mọi người có thể contribute dưới nhiều hình thức: a. Code: build better models, e.g., using PyTorch Fairseq or advanced architecture such as Funnel Transformer. VietAI has access to Google Cloud TPUv3 cho bạn nào tâm huyết :) b. Data: contribute data in various domains, e.g., medicine, arts, etc., or challenging examples like idioms, slangs (""thích thì chiều""). c. Better demo: Hiện tại VietAI có demo https://demo.vietai.org/, tuy nhiên chưa scale well va còn thiếu nhiều features để make it more useful to everyone. Rất cần support từ các bạn có experience về web developments & UI/UX design. Để tìm hiểu thêm thông tin, các bạn có thể comment ở đây; inbox cho Trieu H. Trinh Chinh Ngo (technical), Phan Bạch Xuân An (logistics); or volunteer at VietAI (link in the post below). Cám ơn cả nhà!",,,,,
"Có bác nào làm data dạng text cho em hỏi tiếng việt thì mình xử lý kiểu gì nhỉ, như tiếng anh có các pp như loại punct, loại stopword, stem rồi lemmatize còn tiếng việt mình gần như chỉ có loại những từ vô nghĩa thì phải?","Có bác nào làm data dạng text cho em hỏi tiếng việt thì mình xử lý kiểu gì nhỉ, như tiếng anh có các pp như loại punct, loại stopword, stem rồi lemmatize còn tiếng việt mình gần như chỉ có loại những từ vô nghĩa thì phải?",,,,,
"Cho em hỏi về số feature map mình sẽ nhận được sau conv layer
Trường hợp nào trong 2 trường hợp sau mới là đúng:
-TH1: 1 Image -> Conv(64 filters) -> 64 feature maps -> Conv(32 filters) -> 64*32 = 2048 feature maps
-TH2: 1 Image -> Conv(64 filters) -> 64 feature maps -> Conv(32 filters) -> 32 feature maps
Nếu trường hợp 2 là đúng thì cho em hỏi chuyện gì đã xảy ra ở conv layer số 2 để từ 64 nó biến thành 32?",Cho em hỏi về số feature map mình sẽ nhận được sau conv layer Trường hợp nào trong 2 trường hợp sau mới là đúng: -TH1: 1 Image -> Conv(64 filters) -> 64 feature maps -> Conv(32 filters) -> 64*32 = 2048 feature maps -TH2: 1 Image -> Conv(64 filters) -> 64 feature maps -> Conv(32 filters) -> 32 feature maps Nếu trường hợp 2 là đúng thì cho em hỏi chuyện gì đã xảy ra ở conv layer số 2 để từ 64 nó biến thành 32?,,,,,
"Chào các bạn. Jupyter có vai trò rất lớn trong (1) giảng dạy online; (2) phân tích dữ liệu trực quan; (3) sử dụng trên colab/kaggle với GPU/TPU của server; (4) tuy ít quan trọng hơn nhưng nó là công cụ debug. Hôm nay mình sẽ chia sẻ với các bạn cách mình tạo jupyterlab trong docker nvidia/cuda + nvidia/pytorch
# Bước 1. Khởi tạo docker: $ sudo chmod 666 /var/run/docker.sock
# Bước 2. Kiểm tra docker: $ docker run hello-world
# Bước 3: Tạo file Dockerfile như sau với các thư viện cần thiết và lưu vào thư mục của bạn
""""""
FROM nvcr.io/nvidia/pytorch:21.03-py3
WORKDIR /workspace
#RUN pip install --upgrade pip
ENV CUDA_VERSION=11.1
EXPOSE 8888
RUN pip3 install jupyterlab &&\
pip3 install git+https://github.com/rwightman/pytorch-image-models.git &&\
pip3 install pandas &&\
pip3 install matplotlib &&\
pip3 install seaborn &&\
pip3 install scikit-learn &&\
pip3 install scikit-plot &&\
pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html
iile
""""""
# Bước 4: cd tới thư mục chứa file Dockerfile
# Bước 5: check docker images: $ docker images
# Bước 6: Build image: $ docker build -f /path/to/Dockerfile -t <name_of_image> . # VD: jupyterlab docker build -f Dockerfile -t jupyter .
# Bước 7: Kiểm tra xem có image jupyter tạo ra hay không: $ docker images
# Bước 8: Mount docker image
docker run --gpus all -it --ipc=host -p 8888:8888 -v ""/home/ten_may_tinh_cua_ban/Downloads/Notebooks:/Notebooks"" jupyter
# Bước 9: Run jupyter lab on local host
root@imageid:/workspace# jupyter lab
# Bước 10: click link http://localhost:8888/ or paste the link vào trình duyệt của bạn
# Bước 11: Sau đó copy và dán tonken vào token window,
#Ví dụ có 1 token: http://hostname:8888/?token=188a8eab606f81b23f9ecf24b933bef21f6c00c2230c6c62;
# copy (token/password) like this: 188a8eab606f81b23f9ecf24b933bef21f6c00c2230c6c62
#paste the token into a window and perform your notebook!!!
#Bây giờ thì bạn đã tạo thành công 1 jupyterlab, bạn có thể tự học/ phân tích code trên jupyter. Hy vọng bài viết đem lại kiến thức và giúp ích cho các bạn trong học tập và công việc. Mình xin cảm ơn các bạn đã theo dõi.
ps: trong bài tới mình sẽ tạo dockhub, hẹn gặp lại các bạn trong bài sau. Chúc các bạn có 1 tuần thật ý nghĩa.","Chào các bạn. Jupyter có vai trò rất lớn trong (1) giảng dạy online; (2) phân tích dữ liệu trực quan; (3) sử dụng trên colab/kaggle với GPU/TPU của server; (4) tuy ít quan trọng hơn nhưng nó là công cụ debug. Hôm nay mình sẽ chia sẻ với các bạn cách mình tạo jupyterlab trong docker nvidia/cuda + nvidia/pytorch # Bước 1. Khởi tạo docker: $ sudo chmod 666 /var/run/docker.sock # Bước 2. Kiểm tra docker: $ docker run hello-world # Bước 3: Tạo file Dockerfile như sau với các thư viện cần thiết và lưu vào thư mục của bạn """""" FROM nvcr.io/nvidia/pytorch:21.03-py3 WORKDIR /workspace pip install --upgrade pip ENV CUDA_VERSION=11.1 EXPOSE 8888 RUN pip3 install jupyterlab &&\ pip3 install git+https://github.com/rwightman/pytorch-image-models.git &&\ pip3 install pandas &&\ pip3 install matplotlib &&\ pip3 install seaborn &&\ pip3 install scikit-learn &&\ pip3 install scikit-plot &&\ pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html iile """""" # Bước 4: cd tới thư mục chứa file Dockerfile # Bước 5: check docker images: $ docker images # Bước 6: Build image: $ docker build -f /path/to/Dockerfile -t <name_of_image> . # VD: jupyterlab docker build -f Dockerfile -t jupyter . # Bước 7: Kiểm tra xem có image jupyter tạo ra hay không: $ docker images # Bước 8: Mount docker image docker run --gpus all -it --ipc=host -p 8888:8888 -v ""/home/ten_may_tinh_cua_ban/Downloads/Notebooks:/Notebooks"" jupyter # Bước 9: Run jupyter lab on local host root@imageid:/workspace# jupyter lab # Bước 10: click link http://localhost:8888/ or paste the link vào trình duyệt của bạn # Bước 11: Sau đó copy và dán tonken vào token window, dụ có 1 token: http://hostname:8888/?token=188a8eab606f81b23f9ecf24b933bef21f6c00c2230c6c62; # copy (token/password) like this: 188a8eab606f81b23f9ecf24b933bef21f6c00c2230c6c62 the token into a window and perform your notebook!!! giờ thì bạn đã tạo thành công 1 jupyterlab, bạn có thể tự học/ phân tích code trên jupyter. Hy vọng bài viết đem lại kiến thức và giúp ích cho các bạn trong học tập và công việc. Mình xin cảm ơn các bạn đã theo dõi. ps: trong bài tới mình sẽ tạo dockhub, hẹn gặp lại các bạn trong bài sau. Chúc các bạn có 1 tuần thật ý nghĩa.",#RUN	#Ví	#paste	#Bây,,,,
"Từ dữ liệu lớn đến dữ liệu tốt: Prof. Andrew Ng kêu gọi cộng đồng ML tập trung vào dữ liệu hơn so với model
Hiện nay, các thành tựu trong lĩnh vực ML đang được phát triển theo xu hướng tải xuống các model và cố gắng cải thiện kết quả trên các tập dataset tiêu chuẩn. Phần lớn thời gian, mọi người đang sử dụng cho việc cải thiện code, model hoặc các thuật toán. Prof. Andrew Ng cho rằng “Trong rất nhiều bài toán, tôi nhận ra rằng sẽ rất hữu ích nếu chúng ta hướng tư duy về việc không chỉ cải thiện mã mà còn cần cải thiện dữ liệu theo một cách hệ thống hơn.
Thời gian gần đây, Prof. Andrew Ng đã thu hút cộng đồng ML vào MLOps, một lĩnh vực giúp giải quyết việc xây dựng và triển khai các mô hình ML một cách có hệ thống hơn. Prof. Andrew Ng đưa ra quan điểm về việc tăng tốc việc phát triển các hệ thống học máy nếu chúng ta tập trung nhiều hơn vào dữ liệu so với việc lấy mô hình làm trung tâm. Phần mềm truyền thống chỉ được xây dựng bằng cách đoạn mã, trong khi các hệ thống AI được xây dựng bằng cách sử dụng cả mã (mô hình + thuật toán”) và dữ liệu. “Khi có một hệ thống AI làm việc chưa thực sự tốt, mọi người – theo bản năng sẽ cắm đầu vào việc cải thiện các đoạn mã. Đối với các ứng dụng thực tế, tập trung vào việc cải thiện dữ liệu sẽ hiệu quả hơn”.
Prof. Andrew Ng cho rằng tiến bộ trong ML đang được thúc đẩy bằng các nỗ lực cải thiện benchmark trên các bộ dữ liệu chuẩn. Thực tế phổ biến là các researchers thường giữ cố định dữ liệu và tập trung vào việc cải thiện mã. Tuy nhiên, với các tập dữ liệu khiêm tốn, ông cho rằng các team sẽ đạt được tiến bộ nhanh hơn nếu chúng ta có dữ liệu tốt hơn.
Người ta thường truyền tai nhau rằng 80% của 1 ML project là làm sạch dữ liệu. Prof. Andrew đặt ra câu hỏi, rằng nếu 80% thời gian là chuẩn bị dữ liệu thì tại sao chúng ta lại không đảm bảo rằng chất lượng dữ liệu là yếu tố quan trọng hàng đầu cho 1 ML project? Dường như không ai quan tâm đến điều này. Lướt qua arxiv, chúng ta có thể dễ dàng thấy các xu hướng nghiên cứu ML nào đang là “trendy”. Có một sự cạnh tranh điên cuồng trong việc đưa ra các SOTA. Nếu Google có BERT thì OpenAI có GPT-3. Tuy nhiên, những model “khổng lung” này chỉ chiếm 20% trong các vấn đề thực sự cần giải quyết. Một model tốt ở các bài toán thực tế luôn luôn có hình bóng của một tập training data chất lượng. Mọi người giờ ai cũng có thể sử dụng ngay các pretrained model hay các API một cách dễ dàng.
Một công ty internet có thể có hàng triệu user data trong 1 ngày bình thường. Nhưng hãy tưởng tượng về việc triển khai AI cho một môi trường khác, chẳng hạn như nông nghiệp hoặc chăm sóc sức khỏe, những nơi không bao giờ có đủ dữ liệu. Bạn không thể mong chờ có một triệu máy kéo hay một triệu ảnh chụp xquang! Trong khi các tập dữ liệu nhỏ hơn gặp nhiều vấn đề với outlier và noisy data, khối lượng dữ liệu lớn hơn lại gặp vấn đề trong việc gán nhãn. Làm thế nào để tiếp cận và hợp tác được với các chuyên gia trong cách chuyên ngành hẹp cũng là một trong các nút thắt cổ chai để thu thập được các tập dữ liệu lớn với chất lượng gán nhãn cao. Theo các chuyên gia, chưa kiểm định được các tập dữ liệu là một trong những thách thức lớn khi triển khai các giải pháp học máy từ phòng thí nghiệm ra sản phẩm thực tế.
Vì vậy, Prof. Andrew có đưa ra một số đề xuất giúp triển khai ML một cách hiệu quả hơn:
Nhiệm vụ quan trọng nhất của MLOps chính là cung cấp dữ liệu chất lượng cao.
Chìa khóa ở đây chính là tính nhất quán của label – làm thế nào để kiểm tra và giúp cho việc gán nhãn của các đội gán nhãn được nhất quán.
Cải thiện chất lượng dữ liệu trên basic model > chạy theo các SOTA model với dữ liệu kém chất lượng.
Trong trường hợp có lỗi xảy ra trong quá trình training, hãy lấy dữ liệu làm trung tâm.
Khi làm việc với các tập dữ liệu nhỏ, các công cụ để nâng cao chất lượng dữ liệu đóng vai trò quan trọng.
Ông muốn phát triển các công cụ MLOps giúp tạo ra các bộ dữ liệu và hệ thống AI tốt hơn trong tương lai mà không chỉ dựa vào các kỹ sư để tìm ra cách tốt nhất để cải thiện tập dữ liệu. Có vẻ như MLOps sẽ là một lĩnh vực mới và rất triển vọng để giải quyết bài toán về dữ liệu.
Đây là một bản lược dịch của mình, các bạn có thể xem bản dịch gốc ở đây:
https://analyticsindiamag.com/big-data-to-good-data.../
Tuần trước, Prof. Andrew Ng cũng có 1 buổi streaming dài 1 tiếng về chủ đề này, các bạn quan tâm hơn có thể xem video ở đây:
https://www.youtube.com/watch?v=06-AZXmwHjo
Translator: Ha Na Nguyen
P/s: Hiện tại Trung tâm nghiên cứu và ứng dụng AI - QAI (FPT Software Quy Nhơn) giới thiệu chương trình học bổng Machine Learning và Data Science dành cho 200 học viên với mức lương và đãi ngộ hấp dẫn nhất thị trường, ngay tại FPT Software Quy Nhơn.
Đăng ký để trở thành ứng viên tiềm năng
https://forms.gle/UFZMWBfPqtjYnKtQA","Từ dữ liệu lớn đến dữ liệu tốt: Prof. Andrew Ng kêu gọi cộng đồng ML tập trung vào dữ liệu hơn so với model Hiện nay, các thành tựu trong lĩnh vực ML đang được phát triển theo xu hướng tải xuống các model và cố gắng cải thiện kết quả trên các tập dataset tiêu chuẩn. Phần lớn thời gian, mọi người đang sử dụng cho việc cải thiện code, model hoặc các thuật toán. Prof. Andrew Ng cho rằng “Trong rất nhiều bài toán, tôi nhận ra rằng sẽ rất hữu ích nếu chúng ta hướng tư duy về việc không chỉ cải thiện mã mà còn cần cải thiện dữ liệu theo một cách hệ thống hơn. Thời gian gần đây, Prof. Andrew Ng đã thu hút cộng đồng ML vào MLOps, một lĩnh vực giúp giải quyết việc xây dựng và triển khai các mô hình ML một cách có hệ thống hơn. Prof. Andrew Ng đưa ra quan điểm về việc tăng tốc việc phát triển các hệ thống học máy nếu chúng ta tập trung nhiều hơn vào dữ liệu so với việc lấy mô hình làm trung tâm. Phần mềm truyền thống chỉ được xây dựng bằng cách đoạn mã, trong khi các hệ thống AI được xây dựng bằng cách sử dụng cả mã (mô hình + thuật toán”) và dữ liệu. “Khi có một hệ thống AI làm việc chưa thực sự tốt, mọi người – theo bản năng sẽ cắm đầu vào việc cải thiện các đoạn mã. Đối với các ứng dụng thực tế, tập trung vào việc cải thiện dữ liệu sẽ hiệu quả hơn”. Prof. Andrew Ng cho rằng tiến bộ trong ML đang được thúc đẩy bằng các nỗ lực cải thiện benchmark trên các bộ dữ liệu chuẩn. Thực tế phổ biến là các researchers thường giữ cố định dữ liệu và tập trung vào việc cải thiện mã. Tuy nhiên, với các tập dữ liệu khiêm tốn, ông cho rằng các team sẽ đạt được tiến bộ nhanh hơn nếu chúng ta có dữ liệu tốt hơn. Người ta thường truyền tai nhau rằng 80% của 1 ML project là làm sạch dữ liệu. Prof. Andrew đặt ra câu hỏi, rằng nếu 80% thời gian là chuẩn bị dữ liệu thì tại sao chúng ta lại không đảm bảo rằng chất lượng dữ liệu là yếu tố quan trọng hàng đầu cho 1 ML project? Dường như không ai quan tâm đến điều này. Lướt qua arxiv, chúng ta có thể dễ dàng thấy các xu hướng nghiên cứu ML nào đang là “trendy”. Có một sự cạnh tranh điên cuồng trong việc đưa ra các SOTA. Nếu Google có BERT thì OpenAI có GPT-3. Tuy nhiên, những model “khổng lung” này chỉ chiếm 20% trong các vấn đề thực sự cần giải quyết. Một model tốt ở các bài toán thực tế luôn luôn có hình bóng của một tập training data chất lượng. Mọi người giờ ai cũng có thể sử dụng ngay các pretrained model hay các API một cách dễ dàng. Một công ty internet có thể có hàng triệu user data trong 1 ngày bình thường. Nhưng hãy tưởng tượng về việc triển khai AI cho một môi trường khác, chẳng hạn như nông nghiệp hoặc chăm sóc sức khỏe, những nơi không bao giờ có đủ dữ liệu. Bạn không thể mong chờ có một triệu máy kéo hay một triệu ảnh chụp xquang! Trong khi các tập dữ liệu nhỏ hơn gặp nhiều vấn đề với outlier và noisy data, khối lượng dữ liệu lớn hơn lại gặp vấn đề trong việc gán nhãn. Làm thế nào để tiếp cận và hợp tác được với các chuyên gia trong cách chuyên ngành hẹp cũng là một trong các nút thắt cổ chai để thu thập được các tập dữ liệu lớn với chất lượng gán nhãn cao. Theo các chuyên gia, chưa kiểm định được các tập dữ liệu là một trong những thách thức lớn khi triển khai các giải pháp học máy từ phòng thí nghiệm ra sản phẩm thực tế. Vì vậy, Prof. Andrew có đưa ra một số đề xuất giúp triển khai ML một cách hiệu quả hơn: Nhiệm vụ quan trọng nhất của MLOps chính là cung cấp dữ liệu chất lượng cao. Chìa khóa ở đây chính là tính nhất quán của label – làm thế nào để kiểm tra và giúp cho việc gán nhãn của các đội gán nhãn được nhất quán. Cải thiện chất lượng dữ liệu trên basic model > chạy theo các SOTA model với dữ liệu kém chất lượng. Trong trường hợp có lỗi xảy ra trong quá trình training, hãy lấy dữ liệu làm trung tâm. Khi làm việc với các tập dữ liệu nhỏ, các công cụ để nâng cao chất lượng dữ liệu đóng vai trò quan trọng. Ông muốn phát triển các công cụ MLOps giúp tạo ra các bộ dữ liệu và hệ thống AI tốt hơn trong tương lai mà không chỉ dựa vào các kỹ sư để tìm ra cách tốt nhất để cải thiện tập dữ liệu. Có vẻ như MLOps sẽ là một lĩnh vực mới và rất triển vọng để giải quyết bài toán về dữ liệu. Đây là một bản lược dịch của mình, các bạn có thể xem bản dịch gốc ở đây: https://analyticsindiamag.com/big-data-to-good-data.../ Tuần trước, Prof. Andrew Ng cũng có 1 buổi streaming dài 1 tiếng về chủ đề này, các bạn quan tâm hơn có thể xem video ở đây: https://www.youtube.com/watch?v=06-AZXmwHjo Translator: Ha Na Nguyen P/s: Hiện tại Trung tâm nghiên cứu và ứng dụng AI - QAI (FPT Software Quy Nhơn) giới thiệu chương trình học bổng Machine Learning và Data Science dành cho 200 học viên với mức lương và đãi ngộ hấp dẫn nhất thị trường, ngay tại FPT Software Quy Nhơn. Đăng ký để trở thành ứng viên tiềm năng https://forms.gle/UFZMWBfPqtjYnKtQA",,,,,
"[AI Share]
Tổng hợp các cheat sheats cho người mới học về Data Science để dễ dàng tìm hiểu và tra cứu.
Hôm trước AI4E đã chia sẻ các cheat sheets về Python basic, sklearn, numpy, pandas. Hôm nay AI4E sẽ chia sẻ thêm một số cheat sheets nâng cao hơn.
• Data Science for Business Learders: tổng quan về Khoa học dữ liệu, cách xây dựng team và các bước phổ biến trong workflow khoa học dữ liệu
• Data Manipulation ( Thao tác với dữ liệu)
- Pandas Data Wrangling Cheat Sheet
• Data Visualization ( Trực quan hóa dữ liệu)
- Matplotlib Cheat Sheet
- Seaborn Cheat Sheet
- Bokeh Cheat Sheet
• Scipy : thư viện hỗ trợ các phép toán trong đại số tuyến tính ( ví dụ ma trận)
- Scipy Linear Algebra Cheat Sheet
• Keras : thư viện Deep Learning
- Keras Cheat Sheet
• spaCy : thư viện NLP ( cách load model, xử lí text,…)
• Importing data ( một số cách lấy dữ liệu từ các tệp txt, csv, các tệp tạo ra từ Excel, Matlab hoặc SQL)
• IDE :
- Jupyter Notebook Cheat Sheet","[AI Share] Tổng hợp các cheat sheats cho người mới học về Data Science để dễ dàng tìm hiểu và tra cứu. Hôm trước AI4E đã chia sẻ các cheat sheets về Python basic, sklearn, numpy, pandas. Hôm nay AI4E sẽ chia sẻ thêm một số cheat sheets nâng cao hơn. • Data Science for Business Learders: tổng quan về Khoa học dữ liệu, cách xây dựng team và các bước phổ biến trong workflow khoa học dữ liệu • Data Manipulation ( Thao tác với dữ liệu) - Pandas Data Wrangling Cheat Sheet • Data Visualization ( Trực quan hóa dữ liệu) - Matplotlib Cheat Sheet - Seaborn Cheat Sheet - Bokeh Cheat Sheet • Scipy : thư viện hỗ trợ các phép toán trong đại số tuyến tính ( ví dụ ma trận) - Scipy Linear Algebra Cheat Sheet • Keras : thư viện Deep Learning - Keras Cheat Sheet • spaCy : thư viện NLP ( cách load model, xử lí text,…) • Importing data ( một số cách lấy dữ liệu từ các tệp txt, csv, các tệp tạo ra từ Excel, Matlab hoặc SQL) • IDE : - Jupyter Notebook Cheat Sheet",,,,,
"Cuốn ""Machine Learning với dữ liệu dạng bảng"" đã có thêm một vài mục mới về làm sạch dữ liệu và các kỹ thuật xây dựng đặc trưng cho dữ liệu dạng hạng mục. Mời các bạn đọc và đánh giá.","Cuốn ""Machine Learning với dữ liệu dạng bảng"" đã có thêm một vài mục mới về làm sạch dữ liệu và các kỹ thuật xây dựng đặc trưng cho dữ liệu dạng hạng mục. Mời các bạn đọc và đánh giá.",,,,,
"Xin chào mọi người!
Hiện tại em đang cần bộ dataset Megaface nguyên bản mà web dưới đây họ không còn cho download nữa.
http://megaface.cs.washington.edu/
Mọi người nếu ai down rồi thì share cho em với ạ.
Em cảm ơn ạ.",Xin chào mọi người! Hiện tại em đang cần bộ dataset Megaface nguyên bản mà web dưới đây họ không còn cho download nữa. http://megaface.cs.washington.edu/ Mọi người nếu ai down rồi thì share cho em với ạ. Em cảm ơn ạ.,,,,,
"[Series Pytorch]
Bài 5: Transfer Learning trong Pytorch
Bài này mình sẽ hướng dẫn sử dụng transfer learning trong Pytorch. Phần đầu mình sẽ hướng dẫn sử dụng pre-trained model để dự đoán, phần sau mình sẽ hướng dẫn fine-tune pre-trained model.","[Series Pytorch] Bài 5: Transfer Learning trong Pytorch Bài này mình sẽ hướng dẫn sử dụng transfer learning trong Pytorch. Phần đầu mình sẽ hướng dẫn sử dụng pre-trained model để dự đoán, phần sau mình sẽ hướng dẫn fine-tune pre-trained model.",,,,,
"Sau khi tìm hiểu về mạng Nơ-ron nhân tạo trong dự báo Time-Series Anh chị cho em hỏi một số câu hỏi e đang thắc mắc với ạ.
1) Ưu điểm của mạng Nơ-ron (cụ thể là mạng RNN LSTM, GRU) trong dự báo dữ liệu time-series ?
2) Nhược điểm của mạng nơ-ron nói chung và nhược điểm của RNN, LSTM, GRU nói riêng.
3) Mạng Nơ-ron có gì tốt hơn khi làm việc với time-series so với các phương pháp machine learning khác ( có 12 phương pháp khác nhau)
Rất mong các anh chị giúp em với ạ.","Sau khi tìm hiểu về mạng Nơ-ron nhân tạo trong dự báo Time-Series Anh chị cho em hỏi một số câu hỏi e đang thắc mắc với ạ. 1) Ưu điểm của mạng Nơ-ron (cụ thể là mạng RNN LSTM, GRU) trong dự báo dữ liệu time-series ? 2) Nhược điểm của mạng nơ-ron nói chung và nhược điểm của RNN, LSTM, GRU nói riêng. 3) Mạng Nơ-ron có gì tốt hơn khi làm việc với time-series so với các phương pháp machine learning khác ( có 12 phương pháp khác nhau) Rất mong các anh chị giúp em với ạ.",,,,,
Mn cho em hỏi có cách nào tải bộ dữ liệu MNIST nữa ko ạ. Em lên trang chủ nhưng bị lỗi như này. Em cảm ơn mn nhiều.,Mn cho em hỏi có cách nào tải bộ dữ liệu MNIST nữa ko ạ. Em lên trang chủ nhưng bị lỗi như này. Em cảm ơn mn nhiều.,,,,,
#Book,,#Book,,,,
"Dear all, trước đó có đọc bài của anh Tuấn Linh về docker, hiện tại mình cũng mới tìm hiểu về docker. Hôm nay mình xin chia sẻ với các bạn cách cài docker (thay vì cài pytorch và cuda-toolkit cho GPU) , cài nvidia-docker và cách huấn luyện model phân loại chó/mèo bên trong docker nividia/pytorch:21.03-py3
#Bước 1. Cài docker theo hướng dẫn tại đây
https://linuxhint.com/install_configure_docker_ubuntu/
#Bước 2: pull images from NIVIDIA CHO PYTORCH
$ docker pull nvcr.io/nvidia/pytorch:21.03-py3
#Bước 3: PULL nvidia/cuda:11.1-base images (maybe cuda:10.0; cuda:11.0, ect)
$ docker pull nvidia/cuda:11.0-base #for lastest image
$ nvidia-smi
# Bước 4: kiểm tra các images
$ docker images
# Bước 5: tải, giải nén 1 repo trên github.com, địa chỉ tại đây: https://github.com/ardamavi/Dog-Cat-Classifier. Sau đó, chỉnh lại tên folder thành dogcat, chỉnh đường dẫn trong file train.py (dòng 22 data_dir=""/dogcat/data_dir/ và tại dòng 246 torch.save(checkpoint, '/dogcat/mod_densenet.pth')
# Bước 6: tải, giải nén dataset từ đây: https://www.kaggle.com/tongpython/cat-and-dog
# Lưu ý chuyển folder ra thành dạng data_dir chứa 2 tập train và test sets (trong mỗi folder train và test sets chứa 2 folders là dog và cat). Sau đó chuyển toàn bộ folder data_dir này vào trong folder dogcat để tiện cho việc mount workspace.
# Bước 7: mount folder dogcat chứa code và dataset vào trong docker image là pytorch/nvidia:21.03-py3
$ docker run -it --rm --ipc=host -v /home/ten_may_cua_ban/Downloads/dogcat:/dogcat -v /home/ten_may_cua_ban/dogcat nvcr.io/nvidia/pytorch:21.03-py3
# lưu ý: vì lúc này folder được mount giống như ""root"" hay còn được gọi là workspace nên tất cả mọi thứ đều bắt đầu được xác định là /dogcat/ (nó tương tự như /home/ trong Ubuntu)
# Bước 8: Run repo
root@imageid:/workspace# python /dogcat/train.py
Vậy là chúng ta đã hoàn tất các bước cơ bản để bắt đầu khám phá sâu hơn về docker. Cảm ơn các bạn đã đọc bài viết. Chúc mọi người cuối tuần vui vẻ.","Dear all, trước đó có đọc bài của anh Tuấn Linh về docker, hiện tại mình cũng mới tìm hiểu về docker. Hôm nay mình xin chia sẻ với các bạn cách cài docker (thay vì cài pytorch và cuda-toolkit cho GPU) , cài nvidia-docker và cách huấn luyện model phân loại chó/mèo bên trong docker nividia/pytorch:21.03-py3 1. Cài docker theo hướng dẫn tại đây https://linuxhint.com/install_configure_docker_ubuntu/ 2: pull images from NIVIDIA CHO PYTORCH $ docker pull nvcr.io/nvidia/pytorch:21.03-py3 3: PULL nvidia/cuda:11.1-base images (maybe cuda:10.0; cuda:11.0, ect) $ docker pull nvidia/cuda:11.0-base lastest image $ nvidia-smi # Bước 4: kiểm tra các images $ docker images # Bước 5: tải, giải nén 1 repo trên github.com, địa chỉ tại đây: https://github.com/ardamavi/Dog-Cat-Classifier. Sau đó, chỉnh lại tên folder thành dogcat, chỉnh đường dẫn trong file train.py (dòng 22 data_dir=""/dogcat/data_dir/ và tại dòng 246 torch.save(checkpoint, '/dogcat/mod_densenet.pth') # Bước 6: tải, giải nén dataset từ đây: https://www.kaggle.com/tongpython/cat-and-dog # Lưu ý chuyển folder ra thành dạng data_dir chứa 2 tập train và test sets (trong mỗi folder train và test sets chứa 2 folders là dog và cat). Sau đó chuyển toàn bộ folder data_dir này vào trong folder dogcat để tiện cho việc mount workspace. # Bước 7: mount folder dogcat chứa code và dataset vào trong docker image là pytorch/nvidia:21.03-py3 $ docker run -it --rm --ipc=host -v /home/ten_may_cua_ban/Downloads/dogcat:/dogcat -v /home/ten_may_cua_ban/dogcat nvcr.io/nvidia/pytorch:21.03-py3 # lưu ý: vì lúc này folder được mount giống như ""root"" hay còn được gọi là workspace nên tất cả mọi thứ đều bắt đầu được xác định là /dogcat/ (nó tương tự như /home/ trong Ubuntu) # Bước 8: Run repo root@imageid:/workspace# python /dogcat/train.py Vậy là chúng ta đã hoàn tất các bước cơ bản để bắt đầu khám phá sâu hơn về docker. Cảm ơn các bạn đã đọc bài viết. Chúc mọi người cuối tuần vui vẻ.",#Bước	#Bước	#Bước	#for,,,,
"Xin chào các anh/chị/bạn,
Em xin giới thiệu clip đầu tiên trong playlist em tính làm về thống kê ạ. Do bản thân đi làm và đi học em thấy thống kê là một môn khó và nền tảng cho nhiều thứ, tuy nhiên theo em biết thì còn ít người làm clip về mảng này, nên em quyết tâm thử ạ. Vì bản thân em còn đang trau dồi mảng này nên sai sót là khó tránh, nếu em có truyền tải kiến thức nào sai mong các anh chị góp ý giúp em.
Playlist này em sẽ làm theo các mục lý thuyết, bài tập thống kê bình thường, code. Nội dung và ý tưởng em tham khảo từ nhiều nguồn, các anh chị cần học gấp có thể tham khảo: Penn state university Statistic online, kênh youtube StatQuest.
Em xin cảm ơn ạ.","Xin chào các anh/chị/bạn, Em xin giới thiệu clip đầu tiên trong playlist em tính làm về thống kê ạ. Do bản thân đi làm và đi học em thấy thống kê là một môn khó và nền tảng cho nhiều thứ, tuy nhiên theo em biết thì còn ít người làm clip về mảng này, nên em quyết tâm thử ạ. Vì bản thân em còn đang trau dồi mảng này nên sai sót là khó tránh, nếu em có truyền tải kiến thức nào sai mong các anh chị góp ý giúp em. Playlist này em sẽ làm theo các mục lý thuyết, bài tập thống kê bình thường, code. Nội dung và ý tưởng em tham khảo từ nhiều nguồn, các anh chị cần học gấp có thể tham khảo: Penn state university Statistic online, kênh youtube StatQuest. Em xin cảm ơn ạ.",,,,,
"Chắc chắn các bạn ở đây rất quen thuộc với 1 số công cụ để tạo môi trường làm việc như dùng conda, pipenv, hay docker (tạm gọi thế đi). Mình có suy nghĩ về việc viết 1 số bài về trải nghiệm thực tế của mình trong việc sử dụng docker! Có lẽ để hiểu docker, các bạn có thể dễ dàng google và sẽ nhận được câu giải thích là giống ""môi trường/env"", container, máy ảo/virtual machine, nhưng mình thấy nó là 1 ảnh/image có vẻ hợp lí nhất. Tại sao lại là 1 ảnh???? Bạn thử hình dung 1 chút, bạn có tấm ảnh chụp chung với đại gia đình của bạn 10 năm về trước: gồm ông, bà, cha, mẹ, anh, chị, em, con, cháu. Tất nhiên, vì là thành viên trong gia đình nên đều ""phụ thuộc"" vào nhau. Câu hỏi đặt ra là làm sao bạn quay về được quá khứ 10 năm về trước với đúng bối cảnh đó, có lẽ cần cỗ máy thời gian của Doremon rồi. Quay lại câu chuyện thực tế, là bạn có 1 dự án rất rất thành côngcách đây 5 năm, với các bản tensorflow alpha, pandas beta, numpy gamma,... tất cả đều là dependencies packages, chưa kể phần cứng lúc đó. Nay dữ liệu nhiều hơn, khách hàng muốn bạn huấn luyện lại mô hình để nâng cao trải nghiệm khách hàng. Có 2 cách, (1) là quay về tuổi thơ 5 năm trước với các packages alpha, beta,... câu hỏi đặt ra là làm sao cài được đúng các packages dependencies đó; (2) viết lại code, mà codebase rất lớn, tiêu tốn nguồn lực kinh khủng!. Vậy hướng giải quyết là gì???? Câu trả lời là docker image! Vậy Docker image là gì? Tại sao docker image lại giải quyết được vấn đề trên?,... mình sẽ để thời gian để chúng ta cùng thảo luận trước khi đi tới những câu trả lời","Chắc chắn các bạn ở đây rất quen thuộc với 1 số công cụ để tạo môi trường làm việc như dùng conda, pipenv, hay docker (tạm gọi thế đi). Mình có suy nghĩ về việc viết 1 số bài về trải nghiệm thực tế của mình trong việc sử dụng docker! Có lẽ để hiểu docker, các bạn có thể dễ dàng google và sẽ nhận được câu giải thích là giống ""môi trường/env"", container, máy ảo/virtual machine, nhưng mình thấy nó là 1 ảnh/image có vẻ hợp lí nhất. Tại sao lại là 1 ảnh???? Bạn thử hình dung 1 chút, bạn có tấm ảnh chụp chung với đại gia đình của bạn 10 năm về trước: gồm ông, bà, cha, mẹ, anh, chị, em, con, cháu. Tất nhiên, vì là thành viên trong gia đình nên đều ""phụ thuộc"" vào nhau. Câu hỏi đặt ra là làm sao bạn quay về được quá khứ 10 năm về trước với đúng bối cảnh đó, có lẽ cần cỗ máy thời gian của Doremon rồi. Quay lại câu chuyện thực tế, là bạn có 1 dự án rất rất thành côngcách đây 5 năm, với các bản tensorflow alpha, pandas beta, numpy gamma,... tất cả đều là dependencies packages, chưa kể phần cứng lúc đó. Nay dữ liệu nhiều hơn, khách hàng muốn bạn huấn luyện lại mô hình để nâng cao trải nghiệm khách hàng. Có 2 cách, (1) là quay về tuổi thơ 5 năm trước với các packages alpha, beta,... câu hỏi đặt ra là làm sao cài được đúng các packages dependencies đó; (2) viết lại code, mà codebase rất lớn, tiêu tốn nguồn lực kinh khủng!. Vậy hướng giải quyết là gì???? Câu trả lời là docker image! Vậy Docker image là gì? Tại sao docker image lại giải quyết được vấn đề trên?,... mình sẽ để thời gian để chúng ta cùng thảo luận trước khi đi tới những câu trả lời",,,,,
,nan,,,,,
"Có ai đã train models sử dụng docker của NVIDIA hay chưa cho mình hỏi 1 chút! Mình đã pull, attach images, train models ngon lành, nhưng chỉ có điều tìm mãi không thấy trained weights đâu cả, dù message thông báo là đã saved. Mình sử dụng Ubuntu 20.04, docker ngc 21.03. Cảm ơn trước các bạn nhé!","Có ai đã train models sử dụng docker của NVIDIA hay chưa cho mình hỏi 1 chút! Mình đã pull, attach images, train models ngon lành, nhưng chỉ có điều tìm mãi không thấy trained weights đâu cả, dù message thông báo là đã saved. Mình sử dụng Ubuntu 20.04, docker ngc 21.03. Cảm ơn trước các bạn nhé!",,,,,
Chi sẻ với mọi người lỗi Ethics khi làm nghiên cứu của University of Minnesota.,Chi sẻ với mọi người lỗi Ethics khi làm nghiên cứu của University of Minnesota.,,,,,
"Xin chào mọi người.
Mình có một thắc mắc từ sáng tới giờ cần mọi người giải đáp giúp ạ.
Chả là lúc sáng thầy mình vừa nói về ưu điểm của VGG so với mạng khác là dùng hai kernel 3x3 thay vì 5x5 để tiết kiệm tham số của mạng. Nên mình nảy sinh một câu hỏi như sau:
Như hình 1 (kiến trúc mạng VGG), số lượng tham số (bỏ qua bias) của hai khối convolution 3x3 (là 64 khối 3x3x3 và 64 khối 3x3x64) lần lượt là 3x3x3x64 + 3x3x64x64 = 38K params. Ngược lại nếu dùng một lần kernel 5x5 (nghĩa là khối 64 khối 5x5x3) thì chỉ là 5x5x3x64 = 4.8K params. Output của cả hai cách convolution trên đều bằng 224x224x64 (giả sử có padding). Vậy có thực sự là số tham số đã được giảm khi dùng kernel 3x3 thay vì 5x5 hay không?
Mình cũng đã thử suy nghĩ theo ý của thầy là: sử dụng hai kernel 3x3 nghĩa là 2x(3x3x3x64) thay vì 5x5x3x64. Tuy nhiên nếu nhân tích chập theo kiểu 2x(3x3x3x64) thì nó sẽ nhân như thế nào? Dựa vào hình 2 (cách nhân convolution 3D), nếu nhân lần lượt từng khối convolution 3x3x3x64 thì output lần nhân đầu tiên là 224x244x64, còn output lần nhân thứ hai không tính được vì depth khác nhau (64 và 3).
Did I misunderstand something??? :(
Cảm ơn mọi người đã đọc.","Xin chào mọi người. Mình có một thắc mắc từ sáng tới giờ cần mọi người giải đáp giúp ạ. Chả là lúc sáng thầy mình vừa nói về ưu điểm của VGG so với mạng khác là dùng hai kernel 3x3 thay vì 5x5 để tiết kiệm tham số của mạng. Nên mình nảy sinh một câu hỏi như sau: Như hình 1 (kiến trúc mạng VGG), số lượng tham số (bỏ qua bias) của hai khối convolution 3x3 (là 64 khối 3x3x3 và 64 khối 3x3x64) lần lượt là 3x3x3x64 + 3x3x64x64 = 38K params. Ngược lại nếu dùng một lần kernel 5x5 (nghĩa là khối 64 khối 5x5x3) thì chỉ là 5x5x3x64 = 4.8K params. Output của cả hai cách convolution trên đều bằng 224x224x64 (giả sử có padding). Vậy có thực sự là số tham số đã được giảm khi dùng kernel 3x3 thay vì 5x5 hay không? Mình cũng đã thử suy nghĩ theo ý của thầy là: sử dụng hai kernel 3x3 nghĩa là 2x(3x3x3x64) thay vì 5x5x3x64. Tuy nhiên nếu nhân tích chập theo kiểu 2x(3x3x3x64) thì nó sẽ nhân như thế nào? Dựa vào hình 2 (cách nhân convolution 3D), nếu nhân lần lượt từng khối convolution 3x3x3x64 thì output lần nhân đầu tiên là 224x244x64, còn output lần nhân thứ hai không tính được vì depth khác nhau (64 và 3). Did I misunderstand something??? :( Cảm ơn mọi người đã đọc.",,,,,
"🎉 F8 Refresh Hackathon Registration is open! 🎉
👇 Register for this year's F8 Hackathon at the link below:
 https://fb.me/F8-Refresh-Hackathon-Registration 
F8 has always brought together an incredible community of people who are building, innovating, and looking for what’s next. Over the past year, developers have been enabling businesses of all sizes to adapt and accelerate their digital transformation.
We’re drawing inspiration from F8’s early days and bringing the conference back to its roots: a place to celebrate, inspire and help developers grow. We’re excited to introduce a new virtual event format, F8 Refresh. 🌐✨
F8 Refresh will feature the latest product tools to help you build across our family of apps, as well as technical deep-dive sessions, demos – all with the goal of enabling your growth. Join us at the F8 Refresh: Hackathon for prizes, bragging rights, and more as you show off your skills to help make the world a better place.
We'll be welcoming global developers and creators to build immersive AR and AI solutions for the chance to win up to USD$100,000 in cash prizes.
Running online from May 3-10, participants in the F8 Refresh Hackathon will form teams with fellow innovators and choose between two product challenge areas - Spark AR and Wit.ai - while receiving special access to resources and experts.
Register now to be among the first to receive specific details on the challenges as soon as they're released on May 3!
#F8 #F8Refresh #Hackathon  #BuildwithFacebook","F8 Refresh Hackathon Registration is open! Register for this year's F8 Hackathon at the link below: https://fb.me/F8-Refresh-Hackathon-Registration F8 has always brought together an incredible community of people who are building, innovating, and looking for what’s next. Over the past year, developers have been enabling businesses of all sizes to adapt and accelerate their digital transformation. We’re drawing inspiration from F8’s early days and bringing the conference back to its roots: a place to celebrate, inspire and help developers grow. We’re excited to introduce a new virtual event format, F8 Refresh. F8 Refresh will feature the latest product tools to help you build across our family of apps, as well as technical deep-dive sessions, demos – all with the goal of enabling your growth. Join us at the F8 Refresh: Hackathon for prizes, bragging rights, and more as you show off your skills to help make the world a better place. We'll be welcoming global developers and creators to build immersive AR and AI solutions for the chance to win up to USD$100,000 in cash prizes. Running online from May 3-10, participants in the F8 Refresh Hackathon will form teams with fellow innovators and choose between two product challenge areas - Spark AR and Wit.ai - while receiving special access to resources and experts. Register now to be among the first to receive specific details on the challenges as soon as they're released on May 3!",#F8	#F8Refresh	#Hackathon	#BuildwithFacebook,,,,
"Kính chào các bác. Đợt rồi em tìm hiểu về Principal Component Analysis (PCA) mà đọc mấy bài trên mạng khó hiểu và nhiều toán quá (chắc do trình còn non) nên em mạnh dạn viết bài này chia sẻ cùng cả nhà mong giúp được các bạn newbie mới học.
Mong admin duyệt bài và mong nhận được chỉ giáo của các bác!",Kính chào các bác. Đợt rồi em tìm hiểu về Principal Component Analysis (PCA) mà đọc mấy bài trên mạng khó hiểu và nhiều toán quá (chắc do trình còn non) nên em mạnh dạn viết bài này chia sẻ cùng cả nhà mong giúp được các bạn newbie mới học. Mong admin duyệt bài và mong nhận được chỉ giáo của các bác!,,,,,
"Xin chào mọi người!
Hiện tại em đang cần bộ dataset Megaface nguyên bản mà web dưới đây họ không còn cho download nữa.
http://megaface.cs.washington.edu/
Mọi người nếu ai có thì cho em xin với ạ.
Em cảm ơn ạ.",Xin chào mọi người! Hiện tại em đang cần bộ dataset Megaface nguyên bản mà web dưới đây họ không còn cho download nữa. http://megaface.cs.washington.edu/ Mọi người nếu ai có thì cho em xin với ạ. Em cảm ơn ạ.,,,,,
"LÀM SAO TÌM ABNORMAL TỪ DỮ LIỆU CHUỖI THỜI GIAN (TIME SERIES)
Em chào anh/chị,
Hiện tại em đang làm một bài toán làm sao phát hiện abnormal từ dữ liệu chuỗi thời gian (time series). Và em có tham khảo bài phía dưới thì mới biết là mình cần làm qua các phần như: Trend, Seasonality, Cycle và Irregular remainder.
Không biết anh/chị nào đã từng học qua có thể giới thiêu em project nào nổi bật để em làm và học nhanh để giải bài toán tìm abnormal từ dữ liệu time series được không ạ? Em cũng chỉ mới làm dữ liệu gần đây, nên hy vọng được học hỏi từ mọi người nhiều ạ.
Em cảm ơn anh/chị rất nhiều.
Bài tham khảo:","LÀM SAO TÌM ABNORMAL TỪ DỮ LIỆU CHUỖI THỜI GIAN (TIME SERIES) Em chào anh/chị, Hiện tại em đang làm một bài toán làm sao phát hiện abnormal từ dữ liệu chuỗi thời gian (time series). Và em có tham khảo bài phía dưới thì mới biết là mình cần làm qua các phần như: Trend, Seasonality, Cycle và Irregular remainder. Không biết anh/chị nào đã từng học qua có thể giới thiêu em project nào nổi bật để em làm và học nhanh để giải bài toán tìm abnormal từ dữ liệu time series được không ạ? Em cũng chỉ mới làm dữ liệu gần đây, nên hy vọng được học hỏi từ mọi người nhiều ạ. Em cảm ơn anh/chị rất nhiều. Bài tham khảo:",,,,,
"Mọi người ơi, cho em hỏi là giá trị của từng phần tử trong ma trận kernel channel 3x3 trong hình được tạo ra theo quy tắc nào ạ.
Em đang xài mạng VGG11 pretrained của pytorch trên tập ImageNet để nhận diện vật thể thì những giá trị của kernel này có phải đã được quy định sẵn không ạ? Nếu có thì em có thể xem ở đâu ạ?","Mọi người ơi, cho em hỏi là giá trị của từng phần tử trong ma trận kernel channel 3x3 trong hình được tạo ra theo quy tắc nào ạ. Em đang xài mạng VGG11 pretrained của pytorch trên tập ImageNet để nhận diện vật thể thì những giá trị của kernel này có phải đã được quy định sẵn không ạ? Nếu có thì em có thể xem ở đâu ạ?",,,,,
"[Giới thiệu sách]
Giới thiệu với các bạn 2 quyển MIT Press vừa chuyển sang giấy phép CC BY-NC-ND 4.0 cho phép tải về miễn phí.
1) Algorithms for Optimization: https://algorithmsbook.com/optimization/ (Mykel J. Kochenderfer, Tim A. Wheeler)
2) Algorithms for Decision Making: https://algorithmsbook.com/ (Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray)
Hai quyển đều đi song song lý thuyết và thực hành (có mã nguồn Julia kèm theo), bố cục sách rõ ràng, ví dụ có kèm nhiều minh họa.
Quyển Algorithms for Optimization giới thiệu nhiều phương pháp tối ưu trong các bài toán khác nhau. Những chủ đề trong quyển này rất rộng, mỗi bài toán đều được đặt vấn đề kỹ lưỡng, từ những bài có thể làm một ít phân tích để giải được, đến những bài cần tí yếu tố ngẫu nhiên để tìm một chút may mắn, hay các bài tối ưu có ràng buộc đơn giản, tối ưu đa mục tiêu, và các nhóm phương pháp xây dựng hàm thay thế để chỉ dẫn cho quá trình tối ưu... và các chủ đề nâng cao như tối ưu hàm mục tiêu phi tất định, tối ưu các bài không rõ số biến biển diễn (ví dụ cấu trúc), và các bài toán thiết kế các hệ thống đa lĩnh vực (multi-disciplinary) .
Quyển Algorithms for Decision Making tập trung thiên về các bài toán lập kế hoạch và học tăng cường để giải quyết các bài toán ra quyết định trong một thế giới không chắc chắn.
Hy vọng hai quyển sẽ hữu ích với bạn đọc!","[Giới thiệu sách] Giới thiệu với các bạn 2 quyển MIT Press vừa chuyển sang giấy phép CC BY-NC-ND 4.0 cho phép tải về miễn phí. 1) Algorithms for Optimization: https://algorithmsbook.com/optimization/ (Mykel J. Kochenderfer, Tim A. Wheeler) 2) Algorithms for Decision Making: https://algorithmsbook.com/ (Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray) Hai quyển đều đi song song lý thuyết và thực hành (có mã nguồn Julia kèm theo), bố cục sách rõ ràng, ví dụ có kèm nhiều minh họa. Quyển Algorithms for Optimization giới thiệu nhiều phương pháp tối ưu trong các bài toán khác nhau. Những chủ đề trong quyển này rất rộng, mỗi bài toán đều được đặt vấn đề kỹ lưỡng, từ những bài có thể làm một ít phân tích để giải được, đến những bài cần tí yếu tố ngẫu nhiên để tìm một chút may mắn, hay các bài tối ưu có ràng buộc đơn giản, tối ưu đa mục tiêu, và các nhóm phương pháp xây dựng hàm thay thế để chỉ dẫn cho quá trình tối ưu... và các chủ đề nâng cao như tối ưu hàm mục tiêu phi tất định, tối ưu các bài không rõ số biến biển diễn (ví dụ cấu trúc), và các bài toán thiết kế các hệ thống đa lĩnh vực (multi-disciplinary) . Quyển Algorithms for Decision Making tập trung thiên về các bài toán lập kế hoạch và học tăng cường để giải quyết các bài toán ra quyết định trong một thế giới không chắc chắn. Hy vọng hai quyển sẽ hữu ích với bạn đọc!",,,,,
"Mọi người cho em hỏi tý ạ. Em có 1 đoạn video em muốn xử lí detection cụ thể yolov5 ạ. Em muốn là tận dụng tối đa tài nguyên để xử lí nhanh nhất có thể. Em thử đa luồng thì thấy kết quả nó chưa tốt lắm, hiện tại em xử lí là cho tiền xử lí 1 thể xong cho batch ảnh vào model thì cái bước tiền xử lí nó khá chậm. Vậy cho em hỏi là việc tiền xử lí cpu có thể song song được không và có cách nào xử lí bài toán này không ạ. Vì em thấy chưa tận dụng hết tài nguyên mà cần phải ưu tiên tốc độ xử lý","Mọi người cho em hỏi tý ạ. Em có 1 đoạn video em muốn xử lí detection cụ thể yolov5 ạ. Em muốn là tận dụng tối đa tài nguyên để xử lí nhanh nhất có thể. Em thử đa luồng thì thấy kết quả nó chưa tốt lắm, hiện tại em xử lí là cho tiền xử lí 1 thể xong cho batch ảnh vào model thì cái bước tiền xử lí nó khá chậm. Vậy cho em hỏi là việc tiền xử lí cpu có thể song song được không và có cách nào xử lí bài toán này không ạ. Vì em thấy chưa tận dụng hết tài nguyên mà cần phải ưu tiên tốc độ xử lý",,,,,
"Em chào mọi người.
Hiện tại em có đang thực hiện trainning mô hình. Tuy nhiên em gặp vấn đề khi config lên colab thì code đôi khi chạy sai làm loss (MSE) tăng cao (như trong hình 1). Sau đó em khởi động lại toàn bộ thì đôi lúc lại chạy đúng (loss MSE giảm hình 2). Khi train trên máy ở local thì không gặp lỗi hàm loss tăng quá cao.
Em đã config ramdon seed để reproducible và consistency version packages.
Mọi người có ai biết lý do tại sao và cách khắc phục như nào hay không ạ.
Em cảm ơn.",Em chào mọi người. Hiện tại em có đang thực hiện trainning mô hình. Tuy nhiên em gặp vấn đề khi config lên colab thì code đôi khi chạy sai làm loss (MSE) tăng cao (như trong hình 1). Sau đó em khởi động lại toàn bộ thì đôi lúc lại chạy đúng (loss MSE giảm hình 2). Khi train trên máy ở local thì không gặp lỗi hàm loss tăng quá cao. Em đã config ramdon seed để reproducible và consistency version packages. Mọi người có ai biết lý do tại sao và cách khắc phục như nào hay không ạ. Em cảm ơn.,,,,,
"Khoa CNTT Đại học Đại Nam FIT-DNU sẽ có 05 postdoctoral fellowships toàn thời gian cho niên học 2021-2022, bắt đầu từ tháng 9/2021. Hoc bổng có thể gia hạn tối đa 3 năm, nếu ứng viên đáp ứng được yêu cầu chuyên môn. Ứng viên được chọn có thể tuỳ ý chọn đề tài, hoặc làm việc dưới sự hướng dẫn của các giáo sư hàng đầu là cố vấn khoa học của FIT-DNU, được khuyến khích tham gia giảng dạy và có thể đăng ký Đề tài nghiên cứu sau tiến sĩ tới $50.000. Trong thời gian nghiên cứu sau TS, ứng viên có thể đi nghiên cứu nước ngoài với sự chấp thuận của Khoa không quá 3 tháng.
Ứng viên phải bảo vệ luận án Tiến sĩ trong hoặc ngoài nước, sau 1/1/2017 hoặc đã nộp luận án Tiến sĩ trước 1/6/2021. Hồ sơ đăng ký gồm:
- Luận án TS
- CV
- Kế hoạch nghiên cứu
- 2 thư giới thiệu
Gửi về Prof. Aiviet Nguyen, Dean FIT-DNU fit@dainam.edu.vn.","Khoa CNTT Đại học Đại Nam FIT-DNU sẽ có 05 postdoctoral fellowships toàn thời gian cho niên học 2021-2022, bắt đầu từ tháng 9/2021. Hoc bổng có thể gia hạn tối đa 3 năm, nếu ứng viên đáp ứng được yêu cầu chuyên môn. Ứng viên được chọn có thể tuỳ ý chọn đề tài, hoặc làm việc dưới sự hướng dẫn của các giáo sư hàng đầu là cố vấn khoa học của FIT-DNU, được khuyến khích tham gia giảng dạy và có thể đăng ký Đề tài nghiên cứu sau tiến sĩ tới $50.000. Trong thời gian nghiên cứu sau TS, ứng viên có thể đi nghiên cứu nước ngoài với sự chấp thuận của Khoa không quá 3 tháng. Ứng viên phải bảo vệ luận án Tiến sĩ trong hoặc ngoài nước, sau 1/1/2017 hoặc đã nộp luận án Tiến sĩ trước 1/6/2021. Hồ sơ đăng ký gồm: - Luận án TS - CV - Kế hoạch nghiên cứu - 2 thư giới thiệu Gửi về Prof. Aiviet Nguyen, Dean FIT-DNU fit@dainam.edu.vn.",,,,,
"Chao moi nguoi,
Minh muon hoi: cac ban xai computer tower gi cho data science projects?
Minh dag nghi se mua: computer tower ma co 16gbs, 512 ssd, nvida card, icore 9 or AMD rayzen 7 or 9 processor.
Cac ban cho minh y kien nhe.
Cam on. :)","Chao moi nguoi, Minh muon hoi: cac ban xai computer tower gi cho data science projects? Minh dag nghi se mua: computer tower ma co 16gbs, 512 ssd, nvida card, icore 9 or AMD rayzen 7 or 9 processor. Cac ban cho minh y kien nhe. Cam on. :)",,,,,
"Hellu mọi người ạ
Em đang cài đặt rasa chatbot với một vài chức năng cơ bản
trong đó có dùng api -> trích thông tin
Em đang test trên mess của fb thì nó không work và báo lỗi, nó đã chạy(kết nối được với fb) nhưng chức năng có action dùng api thì báo như bên dưới
Wrong fb secret! Make sure this matches the secret in your facebook app settings
Em test trên local thì ok, và chạy rất bình thường với các action
Nếu ai biết hương khắc phục thì chỉ em với ạ.
Em có mô tả chi tiết hơn từng ảnh ở phần bình luận. mỗi ảnh","Hellu mọi người ạ Em đang cài đặt rasa chatbot với một vài chức năng cơ bản trong đó có dùng api -> trích thông tin Em đang test trên mess của fb thì nó không work và báo lỗi, nó đã chạy(kết nối được với fb) nhưng chức năng có action dùng api thì báo như bên dưới Wrong fb secret! Make sure this matches the secret in your facebook app settings Em test trên local thì ok, và chạy rất bình thường với các action Nếu ai biết hương khắc phục thì chỉ em với ạ. Em có mô tả chi tiết hơn từng ảnh ở phần bình luận. mỗi ảnh",,,,,
Cho em hỏi về lọc feature theo Predictive Power Score mình nên lọc thủ công từ ma trận trả về hay có cách nào lọc tự động không ạ? Và có thể kết hợp với các mô hình đánh giá khác hỗ trợ không ạ? Em cảm ơn.,Cho em hỏi về lọc feature theo Predictive Power Score mình nên lọc thủ công từ ma trận trả về hay có cách nào lọc tự động không ạ? Và có thể kết hợp với các mô hình đánh giá khác hỗ trợ không ạ? Em cảm ơn.,,,,,
#Pytorch #ComputerVision,,#Pytorch	#ComputerVision,,,,
#Pytorch #ComputerVision,,#Pytorch	#ComputerVision,,,,
"Ứng dụng DL/AI trong y học mặc dù có rất nhiều tiềm năng nhưng đi kèm với đó là nhiều rủi ro/bất định và khoảng trống về pháp lí. Tuy nhiên trong thời đại dịch Covid-19 diễn ra gây bao tổn thất về người và của, cũng như tình hình bệnh lao phổi luôn là gánh nặng bệnh tật rất lớn cho những nước có tỉ lệ lưu hành dịch lao phổi cao như ở Việt Nam, chúng tôi, nhóm nghiên cứu Đại học Duy Tân, xin chung tay góp sức đẩy lùi dịch bệnh nói trên bằng việc ứng dụng AI trong công tác chẩn đoán các bệnh này qua film X Quang và computed tomography qua ứng dụng http://co2c.duytan.edu.vn/. Các bạn có xem thêm thông tin báo chí tại đây https://m.thanhnien.vn/giao-duc/dh-duy-tan-phat-trien-ung-dung-phat-hien-covid-19-qua-xu-ly-phim-xquang-ct-1367959.html?zarsrc=1303&utm_source=zalo&utm_medium=zalo&utm_campaign=zalo","Ứng dụng DL/AI trong y học mặc dù có rất nhiều tiềm năng nhưng đi kèm với đó là nhiều rủi ro/bất định và khoảng trống về pháp lí. Tuy nhiên trong thời đại dịch Covid-19 diễn ra gây bao tổn thất về người và của, cũng như tình hình bệnh lao phổi luôn là gánh nặng bệnh tật rất lớn cho những nước có tỉ lệ lưu hành dịch lao phổi cao như ở Việt Nam, chúng tôi, nhóm nghiên cứu Đại học Duy Tân, xin chung tay góp sức đẩy lùi dịch bệnh nói trên bằng việc ứng dụng AI trong công tác chẩn đoán các bệnh này qua film X Quang và computed tomography qua ứng dụng http://co2c.duytan.edu.vn/. Các bạn có xem thêm thông tin báo chí tại đây https://m.thanhnien.vn/giao-duc/dh-duy-tan-phat-trien-ung-dung-phat-hien-covid-19-qua-xu-ly-phim-xquang-ct-1367959.html?zarsrc=1303&utm_source=zalo&utm_medium=zalo&utm_campaign=zalo",,,,,
"[Chương 2: Giải tích - Machine Learning Algorithms to Practice ]
Cùng ôn lại các khái niệm giải tích cơ bản trong machine learning thông qua chương 2, bản MVP của cuốn ""Machine Learning Algorithms to Practice"".
☑️ Tại chương này bạn sẽ làm quen với các khái niệm về đạo hàm, gradient descent.
☑️ Các nguyên tắc tính vi phân.
☑️ Khai triển Taylor, định lý Fermat.
☑️ Lan truyền thuận, lan truyền ngược.
Nội dung được tóm lược để đơn giản, dễ hiểu cho người đọc và kèm theo bài tập thực hành.
------------------------------------------------------------------------------------
Với mục tiêu ""Vì một cộng đồng AI vững mạnh hơn"". Bạn có thể tham gia dự án viết sách cộng đồng của cuốn ""Machine Learning Algorithms to Practice"" tại:
https://www.facebook.com/groups/167567421900114
https://phamdinhkhanh.github.io/deepai-book/ch_calculus/appendix_calculus.html","[Chương 2: Giải tích - Machine Learning Algorithms to Practice ] Cùng ôn lại các khái niệm giải tích cơ bản trong machine learning thông qua chương 2, bản MVP của cuốn ""Machine Learning Algorithms to Practice"". Tại chương này bạn sẽ làm quen với các khái niệm về đạo hàm, gradient descent. Các nguyên tắc tính vi phân. Khai triển Taylor, định lý Fermat. Lan truyền thuận, lan truyền ngược. Nội dung được tóm lược để đơn giản, dễ hiểu cho người đọc và kèm theo bài tập thực hành. ------------------------------------------------------------------------------------ Với mục tiêu ""Vì một cộng đồng AI vững mạnh hơn"". Bạn có thể tham gia dự án viết sách cộng đồng của cuốn ""Machine Learning Algorithms to Practice"" tại: https://www.facebook.com/groups/167567421900114 https://phamdinhkhanh.github.io/deepai-book/ch_calculus/appendix_calculus.html",,"#sharing, #math, #machine_learning",,,
"#hoidap
Chào mọi người, hiện em đang làm project về facial recognition. Hướng em chọn là sử dụng pretrained model insightface. Sau khi face detection, face alignment em sẽ lấy vector embedding, sau đó tìm vector trung bình cho từng class, tìm ngưỡng phân loại theo distance euclidean và detect unknown. Em làm trên tập dataset khoảng 5 người mỗi người tầm 20 ảnh. Kết quả cho ra được khá là ok tuy nhiên vẫn bị sai đôi khi.
Hiện em đang cải thiện model bằng cách thêm data augmentation với noise, rotation, etc... và dùng tính siamese net. Do lượng data ít nên em tính dùng siamese net nhưng thay vì là hình đầu vào thì em sẽ truyền vector embedding thay vì model cnn share weight thì em sẽ dùng model fully connected share weight để tạo ra vector để tính similarity score.
Mong mọi người cho em ý kiến về việc thay cnn bằng lớp fully connected cũng như gợi ý giúp em hướng nào đó để phát triển thêm. Em xin cảm ơn.","Chào mọi người, hiện em đang làm project về facial recognition. Hướng em chọn là sử dụng pretrained model insightface. Sau khi face detection, face alignment em sẽ lấy vector embedding, sau đó tìm vector trung bình cho từng class, tìm ngưỡng phân loại theo distance euclidean và detect unknown. Em làm trên tập dataset khoảng 5 người mỗi người tầm 20 ảnh. Kết quả cho ra được khá là ok tuy nhiên vẫn bị sai đôi khi. Hiện em đang cải thiện model bằng cách thêm data augmentation với noise, rotation, etc... và dùng tính siamese net. Do lượng data ít nên em tính dùng siamese net nhưng thay vì là hình đầu vào thì em sẽ truyền vector embedding thay vì model cnn share weight thì em sẽ dùng model fully connected share weight để tạo ra vector để tính similarity score. Mong mọi người cho em ý kiến về việc thay cnn bằng lớp fully connected cũng như gợi ý giúp em hướng nào đó để phát triển thêm. Em xin cảm ơn.",#hoidap,,,,
"Em chào mọi người, hiện tại em đang nghiên cứu về mô hình dự báo chuỗi thời gian, sau khi em đọc blog của anh Khánh và 1 số bài viết của nước ngoài em có vài câu hỏi thắc mắc mong các cao nhân giúp em ạ:
1. Trong mô hình arima thì nhiễu trắng là thành phần bắt buộc phải có ko ạ. Em có tham khảo vài bài viết, thì chuỗi nhiễu trắng rất khó dự đoán. Nhưng sau khi sai phần dữ liệu (để cải thiện tính ổn định) thì xuất hiện while noise (mô hình MA khắc phục vấn đề này). Để chọn tham số cho MA, xem xét biểu đồ ACF vậy mình lấy giá trị lag đầu tiên về 0, nghĩa là từ giá trị mà chuỗi nhiễu trắng bắt đầu xuất hiện đúng ko ạ.
2. Tại sao mô hình dự báo cần dữ liệu ko có sự tương quan (các ria của ACF nằm trong khoảng =0) ạ.
3. Tại sao biểu đồ PACF để xác định tham số cho AR lại loại bỏ các thời điểm giữa ạ. Em hiểu là nó chỉ lấy các thời điểm có tác động mạnh đến giá thời điệm cần dự đoán, nhưng tại sao ko sử dụng nhiều thời điểm để độ chính xác cao hơn ạ.
Dạ em cảm ơn ad.","Em chào mọi người, hiện tại em đang nghiên cứu về mô hình dự báo chuỗi thời gian, sau khi em đọc blog của anh Khánh và 1 số bài viết của nước ngoài em có vài câu hỏi thắc mắc mong các cao nhân giúp em ạ: 1. Trong mô hình arima thì nhiễu trắng là thành phần bắt buộc phải có ko ạ. Em có tham khảo vài bài viết, thì chuỗi nhiễu trắng rất khó dự đoán. Nhưng sau khi sai phần dữ liệu (để cải thiện tính ổn định) thì xuất hiện while noise (mô hình MA khắc phục vấn đề này). Để chọn tham số cho MA, xem xét biểu đồ ACF vậy mình lấy giá trị lag đầu tiên về 0, nghĩa là từ giá trị mà chuỗi nhiễu trắng bắt đầu xuất hiện đúng ko ạ. 2. Tại sao mô hình dự báo cần dữ liệu ko có sự tương quan (các ria của ACF nằm trong khoảng =0) ạ. 3. Tại sao biểu đồ PACF để xác định tham số cho AR lại loại bỏ các thời điểm giữa ạ. Em hiểu là nó chỉ lấy các thời điểm có tác động mạnh đến giá thời điệm cần dự đoán, nhưng tại sao ko sử dụng nhiều thời điểm để độ chính xác cao hơn ạ. Dạ em cảm ơn ad.",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 8/2020 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 8/2020 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"Gentle reminder: Production ML conf live TODAY
https://www.facebook.com/curiousAI/posts/2387205408090381",Gentle reminder: Production ML conf live TODAY https://www.facebook.com/curiousAI/posts/2387205408090381,,,,,
"Xin chào mọi người, mình đang tìm kiếm dữ liệu để thử nghiệm bài toán tìm kiếm văn bản (tiếng Anh) nhưng vẫn chưa đáp ứng được yêu cầu, vậy anh chị biết có thể share giúp được không ạ. 
Yêu cầu: tập dữ liệu văn bản (pdf hoặc txt đều được), 100 file, mỗi file có độ dài xấp xỉ 10 trang (nếu ít hơn nhưng không đáng kể cũng được).  
ps: mình đã thử tìm kiếm datasets trên mạng nhưng các file có độ dài khá ngắn. :( Xin cảm ơn.","Xin chào mọi người, mình đang tìm kiếm dữ liệu để thử nghiệm bài toán tìm kiếm văn bản (tiếng Anh) nhưng vẫn chưa đáp ứng được yêu cầu, vậy anh chị biết có thể share giúp được không ạ. Yêu cầu: tập dữ liệu văn bản (pdf hoặc txt đều được), 100 file, mỗi file có độ dài xấp xỉ 10 trang (nếu ít hơn nhưng không đáng kể cũng được). ps: mình đã thử tìm kiếm datasets trên mạng nhưng các file có độ dài khá ngắn. :( Xin cảm ơn.",,,,,
"Chào các anh chị, em có base là IT Android App mới bẻ ngang sang con đường này vì thấy khá là thích thú.
Em có một vấn đề là lúc đi apply intern hoặc trainee thì hầu như bị fail rất nặng. Được một anh bảo là nên làm một vài project đã bỏ vào CV rồi hẳn đi apply lại. Em có google qua nhưng ko biết là do keyword chưa đúng hay thế nào nên ko có kết quả như mong muốn. Em viết post này nhờ các anh chị tư vấn xem là đi kiếm thông tin/gợi ý về các project mình có thể làm ở đâu, trang web nào ạ.","Chào các anh chị, em có base là IT Android App mới bẻ ngang sang con đường này vì thấy khá là thích thú. Em có một vấn đề là lúc đi apply intern hoặc trainee thì hầu như bị fail rất nặng. Được một anh bảo là nên làm một vài project đã bỏ vào CV rồi hẳn đi apply lại. Em có google qua nhưng ko biết là do keyword chưa đúng hay thế nào nên ko có kết quả như mong muốn. Em viết post này nhờ các anh chị tư vấn xem là đi kiếm thông tin/gợi ý về các project mình có thể làm ở đâu, trang web nào ạ.",,,,,
,nan,,,,,
"Em chào mọi người ạ, cho phép em xin hỏi bằng tiếng Anh bởi em sợ em hỏi tiếng Việt không bị rõ ý làm ơn không ném đá ạ
Ultimately, what is the practicality of image classification?
Wouldn't the goal of image classification in the end overlap with image recognition since in real life scenarios, you would not likely to have a perfectly cropped image of an animal/object/human to put through the model.
This question might seem so basic you would think I'm trolling but I'm not. If the image isn't well cropped and get put through the program after the deployment of model, wouldn't it need to be detected that there is an object in the picture first before classifier can work?
Say that you train a model with 99% accuracy with input shape of (3, 224, 224), for the model to work in real life after deployment, would you need to cut up an input images to multiple square image with RGB mode and feed it through the algorithm for it to work as a classifier?","Em chào mọi người ạ, cho phép em xin hỏi bằng tiếng Anh bởi em sợ em hỏi tiếng Việt không bị rõ ý làm ơn không ném đá ạ Ultimately, what is the practicality of image classification? Wouldn't the goal of image classification in the end overlap with image recognition since in real life scenarios, you would not likely to have a perfectly cropped image of an animal/object/human to put through the model. This question might seem so basic you would think I'm trolling but I'm not. If the image isn't well cropped and get put through the program after the deployment of model, wouldn't it need to be detected that there is an object in the picture first before classifier can work? Say that you train a model with 99% accuracy with input shape of (3, 224, 224), for the model to work in real life after deployment, would you need to cut up an input images to multiple square image with RGB mode and feed it through the algorithm for it to work as a classifier?",,,,,
"SAT - Styled Augmented Translation released! (better than Google Translate on some test sets & keep improving)
Here is state-of-the-art Neural Machine Translation model for Vietnamese community project made by VietAI Alumni (Chinh Ngo) & Google Advisor (Trieu Trinh). Special thanks to invaluable guidances from VietAI advisors Thang Luong, Thu Nguyen, and Phat Hoang on the project development, Dung Le for Web support, and An Tong for the Artwork Design.
TEST SET 1
Input:
Edward chose his words carefully. ""It is a great honor, honey, and I'm sure it's not one they would offer lightly. They must have had good reason for choosing you."" He hesitated. ""We have to think about this very carefully. About what it would do to our lives.""
Google Translate:
Edward lựa chọn lời nói của mình một cách cẩn thận. ""Đó là một vinh dự lớn, em yêu, và anh chắc chắn đó không phải là một vinh dự mà họ sẽ đưa ra một cách nhẹ nhàng. Họ phải có lý do chính đáng để chọn bạn."" Anh ngập ngừng. ""Chúng tôi phải suy nghĩ về điều này rất cẩn thận. Về những gì nó sẽ làm cho cuộc sống của chúng tôi.""
Our model:
Edward cẩn thận lựa chọn từ ngữ của mình. ""Đó là một vinh dự lớn lao, con yêu, và bố chắc chắn rằng đó không phải là thứ mà họ sẽ đề nghị một cách nhẹ nhàng. Họ hẳn phải có lý do chính đáng để chọn con."" Anh ngập ngừng. ""Chúng ta phải suy nghĩ thật cẩn thận về điều này. Về những gì nó sẽ làm với cuộc sống của chúng ta.""
Blog: https://blog.vietai.org/sat/
Try it out: https://demo.vietai.org/","SAT - Styled Augmented Translation released! (better than Google Translate on some test sets & keep improving) Here is state-of-the-art Neural Machine Translation model for Vietnamese community project made by VietAI Alumni (Chinh Ngo) & Google Advisor (Trieu Trinh). Special thanks to invaluable guidances from VietAI advisors Thang Luong, Thu Nguyen, and Phat Hoang on the project development, Dung Le for Web support, and An Tong for the Artwork Design. TEST SET 1 Input: Edward chose his words carefully. ""It is a great honor, honey, and I'm sure it's not one they would offer lightly. They must have had good reason for choosing you."" He hesitated. ""We have to think about this very carefully. About what it would do to our lives."" Google Translate: Edward lựa chọn lời nói của mình một cách cẩn thận. ""Đó là một vinh dự lớn, em yêu, và anh chắc chắn đó không phải là một vinh dự mà họ sẽ đưa ra một cách nhẹ nhàng. Họ phải có lý do chính đáng để chọn bạn."" Anh ngập ngừng. ""Chúng tôi phải suy nghĩ về điều này rất cẩn thận. Về những gì nó sẽ làm cho cuộc sống của chúng tôi."" Our model: Edward cẩn thận lựa chọn từ ngữ của mình. ""Đó là một vinh dự lớn lao, con yêu, và bố chắc chắn rằng đó không phải là thứ mà họ sẽ đề nghị một cách nhẹ nhàng. Họ hẳn phải có lý do chính đáng để chọn con."" Anh ngập ngừng. ""Chúng ta phải suy nghĩ thật cẩn thận về điều này. Về những gì nó sẽ làm với cuộc sống của chúng ta."" Blog: https://blog.vietai.org/sat/ Try it out: https://demo.vietai.org/",,,,,
"Cho em hỏi trong paper Meta Pseudo Labels (https://arxiv.org/abs/2003.10580) ở phần A. Tại sao tác giả lại ghi câu ""In expectation, the student's ... "" (câu khoanh đỏ ạ). Cụ thể là:
- Tại sao lại có expectation ạ ?
- Tại sao lại có đạo hàm theo learning rate ạ?
Dạ em xin cảm ơn mọi người.","Cho em hỏi trong paper Meta Pseudo Labels (https://arxiv.org/abs/2003.10580) ở phần A. Tại sao tác giả lại ghi câu ""In expectation, the student's ... "" (câu khoanh đỏ ạ). Cụ thể là: - Tại sao lại có expectation ạ ? - Tại sao lại có đạo hàm theo learning rate ạ? Dạ em xin cảm ơn mọi người.",,,,,
"Dự án của NVIDIA cho việc sử dụng CUDA (hỗ trợ tính toán trên GPU) tăng tốc xử lí dữ liệu dạng bảng gọi là RAPID với package cudf. Theo thông tin chính thức từ RAPID, cudf cho tốc độ tính toán nhanh hơn 35 lần so với PANDAS và Scikit-learn, hai packages rất phổ biến với việc các bạn làm việc với dữ liệu dạng bảng và Machine Learning, hiên mới chỉ hỗ trợ tính toán trên CPU. Ngoài Python ra, cudf còn hỗ trợ rất cho cả Java. Tutorials cũng khá hấp dẫn, các bạn có thể tham khảo tại đây:","Dự án của NVIDIA cho việc sử dụng CUDA (hỗ trợ tính toán trên GPU) tăng tốc xử lí dữ liệu dạng bảng gọi là RAPID với package cudf. Theo thông tin chính thức từ RAPID, cudf cho tốc độ tính toán nhanh hơn 35 lần so với PANDAS và Scikit-learn, hai packages rất phổ biến với việc các bạn làm việc với dữ liệu dạng bảng và Machine Learning, hiên mới chỉ hỗ trợ tính toán trên CPU. Ngoài Python ra, cudf còn hỗ trợ rất cho cả Java. Tutorials cũng khá hấp dẫn, các bạn có thể tham khảo tại đây:",,,,,
"Sharing:
Cách thức Jukebox training model để sáng tác âm nhạc, đặc biệt là nguồn data đầu vào, khá thú vị. Rất trông chờ vào tương lai của dự án này.
#AI #jukebox #machinelearning #VEFAcademy","Sharing: Cách thức Jukebox training model để sáng tác âm nhạc, đặc biệt là nguồn data đầu vào, khá thú vị. Rất trông chờ vào tương lai của dự án này.",#AI	#jukebox	#machinelearning	#VEFAcademy,,,,
Cho mình hỏi là trong group có ai làm về nhận dạng chữ ký (từ ảnh hoặc trong file pdf) không nhỉ?,Cho mình hỏi là trong group có ai làm về nhận dạng chữ ký (từ ảnh hoặc trong file pdf) không nhỉ?,,,,,
"Dạ em chào các anh/chị ạ.
Em có một thắc mắc mong được các anh/chị giải đáp giúp em với ạ. Đó là việc các nền tảng mạng xã hội sử dụng trí tuệ nhân tạo phân tích dữ liệu tìm kiếm của người dùng để đề xuất quảng cáo sẽ mang lại những lợi ích vậy ạ? Và việc này có xâm phạm đến thông tin cá nhân của người dùng không ạ?",Dạ em chào các anh/chị ạ. Em có một thắc mắc mong được các anh/chị giải đáp giúp em với ạ. Đó là việc các nền tảng mạng xã hội sử dụng trí tuệ nhân tạo phân tích dữ liệu tìm kiếm của người dùng để đề xuất quảng cáo sẽ mang lại những lợi ích vậy ạ? Và việc này có xâm phạm đến thông tin cá nhân của người dùng không ạ?,,,,,
"Hi mọi người, hiện mình có một dự án sử dụng Telematics data. Một trong những objective của dự án là phân tích ảnh hưởng của tác động môi trường xung quanh đến driving behavior. Vì lý do bảo mật mình không thể share data lên đây được, nhưng đại khái trông như sau:
trip_id: uuid of a trip
driver_id: uuid of a driver
timestamp_start: when trip started
timestamp_end: when trip stopped
gps_lat: gps coordinate
gps_long: gps coordinate
events: telematics captured event (breaking, acceleration, speeding, distraction)
timestamp_event: when event happened
road_type: highway, primary, secondary, etc
speed_limit
...
Target variable là counts(event) per trip. Mình đang nghĩ là sử dụng ANOVA để test xem có sự khác biệt nào giữa các road_type (for example) hay không. Nhưng những events này ở một timestamp nhất định (cung đường nhất định) cũng không phải là independent. Thế thì trong trường hợp này statistical test nào là thích hợp nhất? Có bạn nào có kinh nghiệm có thể cho ít feedback không ạ. Mình cám ơn","Hi mọi người, hiện mình có một dự án sử dụng Telematics data. Một trong những objective của dự án là phân tích ảnh hưởng của tác động môi trường xung quanh đến driving behavior. Vì lý do bảo mật mình không thể share data lên đây được, nhưng đại khái trông như sau: trip_id: uuid of a trip driver_id: uuid of a driver timestamp_start: when trip started timestamp_end: when trip stopped gps_lat: gps coordinate gps_long: gps coordinate events: telematics captured event (breaking, acceleration, speeding, distraction) timestamp_event: when event happened road_type: highway, primary, secondary, etc speed_limit ... Target variable là counts(event) per trip. Mình đang nghĩ là sử dụng ANOVA để test xem có sự khác biệt nào giữa các road_type (for example) hay không. Nhưng những events này ở một timestamp nhất định (cung đường nhất định) cũng không phải là independent. Thế thì trong trường hợp này statistical test nào là thích hợp nhất? Có bạn nào có kinh nghiệm có thể cho ít feedback không ạ. Mình cám ơn",,,,,
,nan,,,,,
"Giáo sư Mehdi Bennis, IEEE Fellow (big name), từ University of Oulu, Phần Lan, đang tìm các bạn PhD students và Posdoc người Việt giỏi về machine learning (lý thuyết và thực nghiệm) để join team của bác. Bác vừa mới kiếm được fund, muốn làm research về
Machine learning for Edge Networks/Fog Networks
Edge Networks/Fog Networks for Machine Learning
Đối với PhD students, bác yêu cầu điểm cao (GPA) và đã có publications, bác ok luôn với hội nghị trong nước của VN, tuy nhiên bác không rush trong việc tìm sinh viên, miễn là năng lực tốt.
Đối với Posdoc, bác mong tìm researcher người Việt có publications trên top-tier journals/conferences.
Nếu cần biết thêm thông tin thì có thể liên hệ trực tiếp với bác mehdi dot bennis at oulu dot fi hoặc là email mình thinhdinh at ieee dot org","Giáo sư Mehdi Bennis, IEEE Fellow (big name), từ University of Oulu, Phần Lan, đang tìm các bạn PhD students và Posdoc người Việt giỏi về machine learning (lý thuyết và thực nghiệm) để join team của bác. Bác vừa mới kiếm được fund, muốn làm research về Machine learning for Edge Networks/Fog Networks Edge Networks/Fog Networks for Machine Learning Đối với PhD students, bác yêu cầu điểm cao (GPA) và đã có publications, bác ok luôn với hội nghị trong nước của VN, tuy nhiên bác không rush trong việc tìm sinh viên, miễn là năng lực tốt. Đối với Posdoc, bác mong tìm researcher người Việt có publications trên top-tier journals/conferences. Nếu cần biết thêm thông tin thì có thể liên hệ trực tiếp với bác mehdi dot bennis at oulu dot fi hoặc là email mình thinhdinh at ieee dot org",,,,,
"[ MACHINE LEARNING 2 MONTHS ]
Xin chào mọi người 😄. Đây là một dự án nho nhỏ mình đã muốn thực hiện từ rất lâu và quyết tâm sẽ làm trong vòng 2 tháng tới.
Mục đích là để học, củng cố những gì mình sẽ học trong thời gian sắp tới và chia sẻ cho mọi người, cộng đồng, những bạn đang và muốn tìm hiểu lĩnh vực này cùng với mình. Ở đây mình sẽ tóm tắt lại những gì mình hiểu qua phần lý thuyết và có cả phần code cho từng phần để các bạn có thể tham khảo và đóng góp ý kiến để mình cải thiện thêm. Mình đặt giới hạn 2 tháng để tạo áp lực thời gian cho bản thân vì mình muốn hoàn thành trong một khoảng thời gian không quá ngắn cũng không phải quá dài vừa đủ để có một nền tảng tốt.
Mình sẽ cập thường xuyên tiến độ rất mong được mọi người giúp đỡ và ủng hộ trong thời gian sắp tới. 😃","[ MACHINE LEARNING 2 MONTHS ] Xin chào mọi người . Đây là một dự án nho nhỏ mình đã muốn thực hiện từ rất lâu và quyết tâm sẽ làm trong vòng 2 tháng tới. Mục đích là để học, củng cố những gì mình sẽ học trong thời gian sắp tới và chia sẻ cho mọi người, cộng đồng, những bạn đang và muốn tìm hiểu lĩnh vực này cùng với mình. Ở đây mình sẽ tóm tắt lại những gì mình hiểu qua phần lý thuyết và có cả phần code cho từng phần để các bạn có thể tham khảo và đóng góp ý kiến để mình cải thiện thêm. Mình đặt giới hạn 2 tháng để tạo áp lực thời gian cho bản thân vì mình muốn hoàn thành trong một khoảng thời gian không quá ngắn cũng không phải quá dài vừa đủ để có một nền tảng tốt. Mình sẽ cập thường xuyên tiến độ rất mong được mọi người giúp đỡ và ủng hộ trong thời gian sắp tới.",,,,,
mọi người cho mình hỏi là nếu dùng giải thuật lọc cộng tác thì có khi nào xảy ra tình trạng là nếu một người dùng chưa bình chọn cho sản phẩm nào thì ta sẽ không gợi ý được sản phẩm mà có thể người dùng đó sẽ thích không mọi người?,mọi người cho mình hỏi là nếu dùng giải thuật lọc cộng tác thì có khi nào xảy ra tình trạng là nếu một người dùng chưa bình chọn cho sản phẩm nào thì ta sẽ không gợi ý được sản phẩm mà có thể người dùng đó sẽ thích không mọi người?,,,,,
"Chào mọi người,
Bữa trước mình thấy có một bài chia sẻ khá hay về stacking. Vì stacking cũng là một dạng ensemble, nên hôm nay mình xin giới thiệu thêm về project mình đang làm, trong đó nếu không có ensemble thì không thể dùng được.
Bài toán ở đây là khi thiết kế một mẫu xe ô tô mới (cụ thể ở đây là 3 hãng mà mình dùng data: Renault, BMW, Volvo), các kỹ sư có rất nhiều option: đèn sẽ đặt ở đâu, loa, camera, sensor, …. Mỗi thiết bị sẽ phải kết nối vào một bộ xử lý (ECU - Electronic Control Unit). Có khoảng hơn 100 ECU cho một chiếc xe, để xử lý khoảng vài ngàn function. Những ECU này sẽ giao tiếp với nhau và tạo ra data stream. Yêu cầu tiên quyết là không được để stream nào trễ deadline (vì vậy ngành này gọi là real-time system, nghĩa là đúng thời gian). Một network không có stream nào trễ deadline là một feasible network.
Quy trình là các kỹ sư phải dựa vào kinh nghiệm để chọn ra một vài network, kiểm tra xem có feasible hay không và đưa lên cho cấp trên duyệt. Lặp đi lặp lại như vậy khoảng vài tháng thì xong phần thiết kế communication network cho một mẫu xe.
Vấn đề là có hàng triệu network có thể có. Nếu chỉ dựa vào kinh nghiệm thì sẽ bỏ lỡ rất nhiều network tốt hơn. Nếu có thể kiểm tra 1 triệu network coi thử có feasible hay không thì hay biết mấy. Việc đó không khả thi với công nghệ hiện tại (ước tính mất vài tuần là ít nhất).
Giải pháp mình đưa ra là dùng Deep Learning để dự đoán xem network nào là feasible. Cụ thể ở đây là mình dùng Graph Neural Network (GNN) để encode các real-time network này và dùng khoảng 10.000 network có label để train. Sau này thì mình tăng lên thành 20.000 labelled network và cộng thêm một chút cải tiến với mạng GNN.
Nhưng đời không như là mơ. Trong các bài toán công nghiệp, cái quan trọng nhất với ML không phải là training data, mà là testing data. Mình nhắc lại nhé: testing data là cái quan trọng số một, vì đó chính là cái thực tế. Mình có thể train với bất kì data nào mình thích, nhưng các hãng xe chỉ đưa ra một số network của họ và mình không thể hi vọng gì thêm. Tổng cộng mình có được 13 testing network. Với mỗi testing network mình tạo ra 1000 sample (giữ nguyên topology nhưng thay đổi data stream), và tất nhiên phải report độ chính xác của từng testing network một.
Ác mộng ở đây là mình có train tốt tới đâu thì một model GNN cũng chỉ có thể đạt độ chính xác cao với vài testing network nhưng không phải là tất cả. Điều này rất thực tế, mình hi vọng gì một GNN model có thể đạt độ chính xác cao ở tất cả mọi topology như hình đính kèm? Giải pháp? Ensemble. Mình sẽ tạo ra 32 GNN model, initialized weight khác nhau, và train hoàn toàn như nhau (điều này tiết kiệm rất nhiều công sức khi code như mình sẽ nói sau đây). 32 GNN model này sẽ dự đoán độc lập với nhau, và với mỗi network mình chỉ cần lấy theo ý kiến số đông (majority voting) để quyết định là network có feasible hay không. Với lý thuyết như vậy, mình đã xí 32 cái GPU V100 từ HPC của trường và làm đi làm lại. Ở đây việc training các model giống nhau giúp cho mình không cần phải code thêm gì cả, chỉ việc để cho các GPU hoạt động song song.
Kết quả cuối cùng (như hình đính kèm) là ensemble giúp cho độ chính xác của bất kì testing network nào đều nằm trong khoảng 80-90%. Sau này mình đã cải tiến để tăng lên 85-95%. Ensemble của 32 GNN model nhanh hơn phương pháp hiện có khoảng vài ngàn lần là ít nhất. Nếu không có ensemble thì mình cũng không nghĩ ra được cách nào khác để kiểm soát độ tin cậy của mạng GNN trong bài toán của mình.
Đây là một ví dụ với ensemble cho các bạn tham khảo khi dùng ML trong công nghiệp. Hi vọng các bạn có thêm một góc nhìn nữa về ML nói chung. Chú ý là hình các hình ảnh đính kèm có bản quyền và không được sử dụng nếu không được cấp phép.
Cảm ơn các bạn đã quan tâm.","Chào mọi người, Bữa trước mình thấy có một bài chia sẻ khá hay về stacking. Vì stacking cũng là một dạng ensemble, nên hôm nay mình xin giới thiệu thêm về project mình đang làm, trong đó nếu không có ensemble thì không thể dùng được. Bài toán ở đây là khi thiết kế một mẫu xe ô tô mới (cụ thể ở đây là 3 hãng mà mình dùng data: Renault, BMW, Volvo), các kỹ sư có rất nhiều option: đèn sẽ đặt ở đâu, loa, camera, sensor, …. Mỗi thiết bị sẽ phải kết nối vào một bộ xử lý (ECU - Electronic Control Unit). Có khoảng hơn 100 ECU cho một chiếc xe, để xử lý khoảng vài ngàn function. Những ECU này sẽ giao tiếp với nhau và tạo ra data stream. Yêu cầu tiên quyết là không được để stream nào trễ deadline (vì vậy ngành này gọi là real-time system, nghĩa là đúng thời gian). Một network không có stream nào trễ deadline là một feasible network. Quy trình là các kỹ sư phải dựa vào kinh nghiệm để chọn ra một vài network, kiểm tra xem có feasible hay không và đưa lên cho cấp trên duyệt. Lặp đi lặp lại như vậy khoảng vài tháng thì xong phần thiết kế communication network cho một mẫu xe. Vấn đề là có hàng triệu network có thể có. Nếu chỉ dựa vào kinh nghiệm thì sẽ bỏ lỡ rất nhiều network tốt hơn. Nếu có thể kiểm tra 1 triệu network coi thử có feasible hay không thì hay biết mấy. Việc đó không khả thi với công nghệ hiện tại (ước tính mất vài tuần là ít nhất). Giải pháp mình đưa ra là dùng Deep Learning để dự đoán xem network nào là feasible. Cụ thể ở đây là mình dùng Graph Neural Network (GNN) để encode các real-time network này và dùng khoảng 10.000 network có label để train. Sau này thì mình tăng lên thành 20.000 labelled network và cộng thêm một chút cải tiến với mạng GNN. Nhưng đời không như là mơ. Trong các bài toán công nghiệp, cái quan trọng nhất với ML không phải là training data, mà là testing data. Mình nhắc lại nhé: testing data là cái quan trọng số một, vì đó chính là cái thực tế. Mình có thể train với bất kì data nào mình thích, nhưng các hãng xe chỉ đưa ra một số network của họ và mình không thể hi vọng gì thêm. Tổng cộng mình có được 13 testing network. Với mỗi testing network mình tạo ra 1000 sample (giữ nguyên topology nhưng thay đổi data stream), và tất nhiên phải report độ chính xác của từng testing network một. Ác mộng ở đây là mình có train tốt tới đâu thì một model GNN cũng chỉ có thể đạt độ chính xác cao với vài testing network nhưng không phải là tất cả. Điều này rất thực tế, mình hi vọng gì một GNN model có thể đạt độ chính xác cao ở tất cả mọi topology như hình đính kèm? Giải pháp? Ensemble. Mình sẽ tạo ra 32 GNN model, initialized weight khác nhau, và train hoàn toàn như nhau (điều này tiết kiệm rất nhiều công sức khi code như mình sẽ nói sau đây). 32 GNN model này sẽ dự đoán độc lập với nhau, và với mỗi network mình chỉ cần lấy theo ý kiến số đông (majority voting) để quyết định là network có feasible hay không. Với lý thuyết như vậy, mình đã xí 32 cái GPU V100 từ HPC của trường và làm đi làm lại. Ở đây việc training các model giống nhau giúp cho mình không cần phải code thêm gì cả, chỉ việc để cho các GPU hoạt động song song. Kết quả cuối cùng (như hình đính kèm) là ensemble giúp cho độ chính xác của bất kì testing network nào đều nằm trong khoảng 80-90%. Sau này mình đã cải tiến để tăng lên 85-95%. Ensemble của 32 GNN model nhanh hơn phương pháp hiện có khoảng vài ngàn lần là ít nhất. Nếu không có ensemble thì mình cũng không nghĩ ra được cách nào khác để kiểm soát độ tin cậy của mạng GNN trong bài toán của mình. Đây là một ví dụ với ensemble cho các bạn tham khảo khi dùng ML trong công nghiệp. Hi vọng các bạn có thêm một góc nhìn nữa về ML nói chung. Chú ý là hình các hình ảnh đính kèm có bản quyền và không được sử dụng nếu không được cấp phép. Cảm ơn các bạn đã quan tâm.",,,,,
Mình đang cần tìm cộng tác viên để cùng viết cuốn Machine Learning Algorithms to Practice.,Mình đang cần tìm cộng tác viên để cùng viết cuốn Machine Learning Algorithms to Practice.,,,,,
[Cách đóng góp cho sách Machine Learning Algorithms to Practice],[Cách đóng góp cho sách Machine Learning Algorithms to Practice],,,,,
"Làm sao chuyển 1 câu thành 1 vector theo dạng word2vec nhỉ các bác, em thấy trên mạng thì nó dùng cách là word tokenize ra mean sum các vector từ lại nhưng cách đó liệu có khả quan không nhỉ ?","Làm sao chuyển 1 câu thành 1 vector theo dạng word2vec nhỉ các bác, em thấy trên mạng thì nó dùng cách là word tokenize ra mean sum các vector từ lại nhưng cách đó liệu có khả quan không nhỉ ?",,,,,
"#relax
Mất gốc toán đến nơi rồi các ông ạ",Mất gốc toán đến nơi rồi các ông ạ,#relax,,,,
"Cách đây không lâu, Google Brain đã public mã nguồn và  papers chi tiết về EfficientNetV2 - một phiên bản ""hoàn thiện và tối ưu"" hơn của EfficientNet, mô hình CNN đang thống trị trong các bảng xếp hạng SOTA ở các hạng mục và challenge khác nhau về computer vision trong 2 năm gần đây. 
Một số điểm đáng chú ý của EfficientNet V2 so với V1:
1 cấu trúc layer mới so với V1: Fused-MBConv.
Sử dụng NAS (1 mảng mà anh Quốc Lê đã có rất nhiều publications) và scaling để tối ưu tốc độ và hiệu quả của việc training. 
Sử dụng adaptively adjusts regularization (kết hợp Dropout, RandAugment và Mixup) với các network và input size khác nhau.
So sánh với EfficientNetB5, V2s nhanh hơn 12 %, ít params hơn 17% và có số lượng FLOP tính toán ít hơn 12%. Đối với các mạng lớn hơn, tốc độ tính toán và số lượng params càng thể hiện rõ sự tối ưu. Kiến thức EfficientNetV2-L bỏ xa SOTA hiện tại (NFNet-F4) cho tập ImageNet 21k tận 0.9% (86.8 vs 85.9), với số lượng params chỉ bằng 1/3 và số lượng FLOP thậm chí không bằng 1/4!
Để tìm hiểu kỹ hơn về EfficientNet, các bạn có thể tìm hiểu chi tiết ở các bài viết/video sao:
Official paper: https://arxiv.org/pdf/2104.00298.pdf
Video giải thích chi tiết về paper: https://www.youtube.com/watch?v=CTsSrOKSPNo
Source code của nhóm tác giả papers: https://github.com/google/automl/efficientnetv2
Source code Pytorch đã tích hợp EfficentNetV2: https://github.com/rwightman/pytorch-image-models
  P/s: Hiện tại Trung tâm nghiên cứu và ứng dụng AI - QAI (FPT Software Quy Nhơn) giới thiệu chương trình học bổng Machine Learning và Data Science dành cho 200 học viên với cam kết công việc đầu ra với mức lương và đãi ngộ hấp dẫn nhất thị trường, ngay tại FPT Software Quy Nhơn.
 Đăng ký ngay để trở thành những ứng viên tiềm năng 
https://forms.gle/UFZMWBfPqtjYnKtQA","Cách đây không lâu, Google Brain đã public mã nguồn và papers chi tiết về EfficientNetV2 - một phiên bản ""hoàn thiện và tối ưu"" hơn của EfficientNet, mô hình CNN đang thống trị trong các bảng xếp hạng SOTA ở các hạng mục và challenge khác nhau về computer vision trong 2 năm gần đây. Một số điểm đáng chú ý của EfficientNet V2 so với V1: 1 cấu trúc layer mới so với V1: Fused-MBConv. Sử dụng NAS (1 mảng mà anh Quốc Lê đã có rất nhiều publications) và scaling để tối ưu tốc độ và hiệu quả của việc training. Sử dụng adaptively adjusts regularization (kết hợp Dropout, RandAugment và Mixup) với các network và input size khác nhau. So sánh với EfficientNetB5, V2s nhanh hơn 12 %, ít params hơn 17% và có số lượng FLOP tính toán ít hơn 12%. Đối với các mạng lớn hơn, tốc độ tính toán và số lượng params càng thể hiện rõ sự tối ưu. Kiến thức EfficientNetV2-L bỏ xa SOTA hiện tại (NFNet-F4) cho tập ImageNet 21k tận 0.9% (86.8 vs 85.9), với số lượng params chỉ bằng 1/3 và số lượng FLOP thậm chí không bằng 1/4! Để tìm hiểu kỹ hơn về EfficientNet, các bạn có thể tìm hiểu chi tiết ở các bài viết/video sao: Official paper: https://arxiv.org/pdf/2104.00298.pdf Video giải thích chi tiết về paper: https://www.youtube.com/watch?v=CTsSrOKSPNo Source code của nhóm tác giả papers: https://github.com/google/automl/efficientnetv2 Source code Pytorch đã tích hợp EfficentNetV2: https://github.com/rwightman/pytorch-image-models P/s: Hiện tại Trung tâm nghiên cứu và ứng dụng AI - QAI (FPT Software Quy Nhơn) giới thiệu chương trình học bổng Machine Learning và Data Science dành cho 200 học viên với cam kết công việc đầu ra với mức lương và đãi ngộ hấp dẫn nhất thị trường, ngay tại FPT Software Quy Nhơn. Đăng ký ngay để trở thành những ứng viên tiềm năng https://forms.gle/UFZMWBfPqtjYnKtQA",,,,,
Huggingface mới giới thiệu package accelerate giúp việc viết training loop sử dụng nhiều GPUs và TPUs với framework Pytorch trở nên dễ dàng hơn. Các bạn có thể tham khảo tại đây https://t.co/12l2JYZL0w. Chúc các bạn cuối tuần vui vẻ,Huggingface mới giới thiệu package accelerate giúp việc viết training loop sử dụng nhiều GPUs và TPUs với framework Pytorch trở nên dễ dàng hơn. Các bạn có thể tham khảo tại đây https://t.co/12l2JYZL0w. Chúc các bạn cuối tuần vui vẻ,,,,,
"LIGHTLY IS A COMPUTER VISION FRAMEWORK FOR SELF-SUPERVISED LEARNING.
Lightly offers features like:
modular framework
support for multi-gpu training using PyTorch Lightning
easy to use and written in a PyTorch like style
supports custom backbone models for self-supervised pre-training
Link github: https://github.com/lightly-ai/lightly",LIGHTLY IS A COMPUTER VISION FRAMEWORK FOR SELF-SUPERVISED LEARNING. Lightly offers features like: modular framework support for multi-gpu training using PyTorch Lightning easy to use and written in a PyTorch like style supports custom backbone models for self-supervised pre-training Link github: https://github.com/lightly-ai/lightly,,,,,
"Xin phép Admin duyệt giúp
#xulyanh nền tảng tiếp cận Computer Vision",Xin phép Admin duyệt giúp nền tảng tiếp cận Computer Vision,#xulyanh,,,,
"Mọi người cho mình hỏi xíu với. Trong mô hình colaborative filter hôm nay mình xem lý thuyết để làm thì thấy phần này ta sẽ chia movielen ra làm 2 phần training và testing, nếu mình làm như vậy thì khi áp dụng với bài toán thực tế như là gợi ý sản phẩm cho một người dùng nào đó thì mình phải training như thế nào vậy mọi người, có phải là mình sẽ training toàn bộ Database thông tin người dùng, lúc recommend mình sẽ lấy dữ liệu của người dùng đó và tính rồi xuất kết quả phải không mn?","Mọi người cho mình hỏi xíu với. Trong mô hình colaborative filter hôm nay mình xem lý thuyết để làm thì thấy phần này ta sẽ chia movielen ra làm 2 phần training và testing, nếu mình làm như vậy thì khi áp dụng với bài toán thực tế như là gợi ý sản phẩm cho một người dùng nào đó thì mình phải training như thế nào vậy mọi người, có phải là mình sẽ training toàn bộ Database thông tin người dùng, lúc recommend mình sẽ lấy dữ liệu của người dùng đó và tính rồi xuất kết quả phải không mn?",,,,,
"Nhờ các bác bắt mạch cho em với:
Em đang làm theo 1 tut Facenet của anh Phạm Đình Khánh. Đến đoạn loadmodel vào hàm thì bị lỗi "" Unsupported Lua Type"" như này.
Kiến thức về framework của em còn hạn chế nên không hiểu lỗi này là gì. Nhờ các bác tư vấn cho em làm sao để giải quyết cái này với ạ.","Nhờ các bác bắt mạch cho em với: Em đang làm theo 1 tut Facenet của anh Phạm Đình Khánh. Đến đoạn loadmodel vào hàm thì bị lỗi "" Unsupported Lua Type"" như này. Kiến thức về framework của em còn hạn chế nên không hiểu lỗi này là gì. Nhờ các bác tư vấn cho em làm sao để giải quyết cái này với ạ.",,,,,
Dành cho các bạn quan tâm đến xin việc trong ngành Data Science và Machine Learning. Đây là trang web tổng hợp câu hỏi phổng vấn (chủ yếu tại Mỹ). Bạn có thể học khoá Machine Learning Design tại dây.,Dành cho các bạn quan tâm đến xin việc trong ngành Data Science và Machine Learning. Đây là trang web tổng hợp câu hỏi phổng vấn (chủ yếu tại Mỹ). Bạn có thể học khoá Machine Learning Design tại dây.,,,,,
"Hi mọi người, 

Cho e hỏi xíu là với dữ liệu là AUDIO, thì có những bài toán gì trong machine learning với ạ + hướng giải quết ạ . E thì chỉ tìm được 2 bài toán cho dữ liêu  audio là :

Bài toán classification audio --> cái này dùng extraction features, rồi dùng một model classification để phân loại thui ạ
Bài toán speech to text và text to speech ---> Dùng Fourier Transform để tính toán features MFFC (Mel frequency Cepstrum)  rồi đưa features này vào một model.
  ","Hi mọi người, Cho e hỏi xíu là với dữ liệu là AUDIO, thì có những bài toán gì trong machine learning với ạ + hướng giải quết ạ . E thì chỉ tìm được 2 bài toán cho dữ liêu audio là : Bài toán classification audio --> cái này dùng extraction features, rồi dùng một model classification để phân loại thui ạ Bài toán speech to text và text to speech ---> Dùng Fourier Transform để tính toán features MFFC (Mel frequency Cepstrum) rồi đưa features này vào một model.",,,,,
"#dataset_food
Hi group, hiện giờ mình đang tìm 1 bộ dataset liên quan đến thực phẩm gồm các thông tin :
Tên thực phẩm + thành phần chính sản phẩm + thành phần dinh dưỡng sản phẩm (ngoài tên sản phẩm,có thể chỉ cần thêm 1 trong 2 yếu tố còn lại)
Rất mong nhận được sự giúp đỡ từu cộng đồng. Thanks","Hi group, hiện giờ mình đang tìm 1 bộ dataset liên quan đến thực phẩm gồm các thông tin : Tên thực phẩm + thành phần chính sản phẩm + thành phần dinh dưỡng sản phẩm (ngoài tên sản phẩm,có thể chỉ cần thêm 1 trong 2 yếu tố còn lại) Rất mong nhận được sự giúp đỡ từu cộng đồng. Thanks",#dataset_food,,,,
"Cho e hỏi chút ạ
Với MFCC dạng vec tơ 2 chiều (n hệ số x số frame) thì sử dụng KNN như thế nào để phân loại ạ.E cám ơn mn",Cho e hỏi chút ạ Với MFCC dạng vec tơ 2 chiều (n hệ số x số frame) thì sử dụng KNN như thế nào để phân loại ạ.E cám ơn mn,,,,,
"Nhân dịp cuốn sách Deep Learning của Ian Goodfellow, Yoshua Bengio và Aaron Courville được dịch ra tiếng việt.
Các bạn có thể xem thêm loạt bài của hadrienj. Nó ứng với chương 2 của cuốn sách và giải thích các khái niệm thông qua coding trên python với numpy.
“Everything became much clearer when I started writing code.” Nên theo mình vừa đọc vừa code sẽ dễ hiểu hơn. :D","Nhân dịp cuốn sách Deep Learning của Ian Goodfellow, Yoshua Bengio và Aaron Courville được dịch ra tiếng việt. Các bạn có thể xem thêm loạt bài của hadrienj. Nó ứng với chương 2 của cuốn sách và giải thích các khái niệm thông qua coding trên python với numpy. “Everything became much clearer when I started writing code.” Nên theo mình vừa đọc vừa code sẽ dễ hiểu hơn. :D",,,,,
"[VIDEO: HƯỚNG DẪN THÀNH THẠO NUMPY TỪ A ĐẾN Z]
Xin chào cả nhà, với mong muốn giúp các bạn có một nền tảng vững chắc trong quá trình học về Machine Learning và Data Science, nhóm mình có làm video ""Hướng Dẫn Thành Thạo NumPy từ A đến Z"".
NumPy (Numeric Python) là một thư viện toán học mạnh mẽ của Python, cho phép làm việc hiệu quả trên các Cấu Trúc Dữ Liệu thường dùng trong Machine Learning như: Vector, Ma trận và Mảng, đặc biệt với những mảng đa chiều (tensors) với tốc độ xử lý nhanh hơn nhiều lần so với “core Python” đơn thuần.
Chỉ 1️⃣ video Duy Nhất, các bạn có thể hiểu và nắm rõ được về mảng NumPy 1 chiều và đa chiều cũng như cách sử dụng và tra cứu các hàm sẵn có (built-in) thông dụng trong NumPy như: Random, Sort, Statistics và Linear Algebra.
Cảm ơn mọi người và Admin đã duyệt bài !
Link: https://www.youtube.com/watch?v=1eSmR2EJjYM","[VIDEO: HƯỚNG DẪN THÀNH THẠO NUMPY TỪ A ĐẾN Z] Xin chào cả nhà, với mong muốn giúp các bạn có một nền tảng vững chắc trong quá trình học về Machine Learning và Data Science, nhóm mình có làm video ""Hướng Dẫn Thành Thạo NumPy từ A đến Z"". NumPy (Numeric Python) là một thư viện toán học mạnh mẽ của Python, cho phép làm việc hiệu quả trên các Cấu Trúc Dữ Liệu thường dùng trong Machine Learning như: Vector, Ma trận và Mảng, đặc biệt với những mảng đa chiều (tensors) với tốc độ xử lý nhanh hơn nhiều lần so với “core Python” đơn thuần. Chỉ 1⃣ video Duy Nhất, các bạn có thể hiểu và nắm rõ được về mảng NumPy 1 chiều và đa chiều cũng như cách sử dụng và tra cứu các hàm sẵn có (built-in) thông dụng trong NumPy như: Random, Sort, Statistics và Linear Algebra. Cảm ơn mọi người và Admin đã duyệt bài ! Link: https://www.youtube.com/watch?v=1eSmR2EJjYM",,,,,
"Chào a/c. Mọi người có thể cho em biết một vài cộng đồng trao đổi về học thuật trong lĩnh vực ML ở các nước khác mà anh/chị đang follow được không ạ. Em rất vui vì nước mình có group này phát triển rất nhanh và trở thành nơi chia sẻ, dẫn dắt cho những người mới như em. Nên em có thắc mắc là các nước khác đi trước về mảng này chắc hẳn là cũng có cộng đồng cũng mạnh chẳng kém. Mà e tìm mãi chẳng thấy nên mong nhận được giải đáp của mọi người ạ :D Cảm ơn rất nhiều ạ!","Chào a/c. Mọi người có thể cho em biết một vài cộng đồng trao đổi về học thuật trong lĩnh vực ML ở các nước khác mà anh/chị đang follow được không ạ. Em rất vui vì nước mình có group này phát triển rất nhanh và trở thành nơi chia sẻ, dẫn dắt cho những người mới như em. Nên em có thắc mắc là các nước khác đi trước về mảng này chắc hẳn là cũng có cộng đồng cũng mạnh chẳng kém. Mà e tìm mãi chẳng thấy nên mong nhận được giải đáp của mọi người ạ :D Cảm ơn rất nhiều ạ!",,,,,
"Nhóm mình vừa hoàn thành một nghiên cứu trong paper COVID-19 named entity recognition for Vietnamese (https://arxiv.org/abs/2104.03879) và release bộ dữ liệu PhoNER cho Tiếng Việt trong domain COVID-19.
Bạn nào quan tâm có thể download corpus từ: https://github.com/VinAIResearch/PhoNER_COVID19",Nhóm mình vừa hoàn thành một nghiên cứu trong paper COVID-19 named entity recognition for Vietnamese (https://arxiv.org/abs/2104.03879) và release bộ dữ liệu PhoNER cho Tiếng Việt trong domain COVID-19. Bạn nào quan tâm có thể download corpus từ: https://github.com/VinAIResearch/PhoNER_COVID19,,,,,
"[AI News]
Thay thế background là một nhiệm vụ nổi bật trong lĩnh vực video, tạo ra các hiệu ứng đặc biệt và phát trực tiếp. Trong thời kì dịch covid, việc học online hay làm việc từ xa trở nên phổ biến hơn bao giờ hết. Có thể dễ dàng thấy trong các công cụ để học trực tuyến hay làm việc từ xa như Zoom và Google Meets, việc thay thế background được sử dụng khi nhiều hội nghị, cuộc họp yêu cầu background. Ngoài ra, việc thay thế background có thể dử dụng để giải trí tạo ra các video đẹp hơn.
BackgroundMattingV2 là một kĩ thuật thay thế background có độ phân giải cao theo thời gian thực đầu tiên mà có kết quả hiện đài ở chế độ 4K là 30fps và chế độ HD là 60fps.","[AI News] Thay thế background là một nhiệm vụ nổi bật trong lĩnh vực video, tạo ra các hiệu ứng đặc biệt và phát trực tiếp. Trong thời kì dịch covid, việc học online hay làm việc từ xa trở nên phổ biến hơn bao giờ hết. Có thể dễ dàng thấy trong các công cụ để học trực tuyến hay làm việc từ xa như Zoom và Google Meets, việc thay thế background được sử dụng khi nhiều hội nghị, cuộc họp yêu cầu background. Ngoài ra, việc thay thế background có thể dử dụng để giải trí tạo ra các video đẹp hơn. BackgroundMattingV2 là một kĩ thuật thay thế background có độ phân giải cao theo thời gian thực đầu tiên mà có kết quả hiện đài ở chế độ 4K là 30fps và chế độ HD là 60fps.",,,,,
"[AI Share]
Người ta thường nói “ Học phải đi đôi với hành”, hôm nay AI4E sẽ share một nguồn tài liệu hay để mọi người có thể vừa học vừa thực hành code^^.
Repo này tổng hợp hơn 500 project về AI, Machine Learning, Deep Learning, Computer Vision, NLP. Đặc biệt, các project này có code đi kèm ^^.
Hãy chọn các project về topic mà mình quan tâm để học hỏi thêm nhé^^","[AI Share] Người ta thường nói “ Học phải đi đôi với hành”, hôm nay AI4E sẽ share một nguồn tài liệu hay để mọi người có thể vừa học vừa thực hành code^^. Repo này tổng hợp hơn 500 project về AI, Machine Learning, Deep Learning, Computer Vision, NLP. Đặc biệt, các project này có code đi kèm ^^. Hãy chọn các project về topic mà mình quan tâm để học hỏi thêm nhé^^",,,,,
"Chào mọi người,
Hôm nọ có bài hỏi về ensemble khá hot và có nhiều ý kiến tranh luận trên 2 topic chính:
- Có nên gộp các model có variance lớn lại với nhau hay không
- Quy luật của số lớn có ảnh hưởng thế nào đến kết quả của ensemble
Trước khi trả lời thì mình có trình bày test của mình trên các tập như sau.
- Cho một model với accuracy là x = 0.51 (gọi là weak model) chạy 1000 lần, tính accuracy trung bình (TEST 1)
- Cho một tập các weak model ở trên với các size khác nhau (10, 50, 100, 500, 1000), tính major vote và tính accuracy trung bình của 1000 run. Test này để xem số lượng model ảnh hưởng thế nào đến accuracy của model (TEST 2)
- Cho một tập các model với size là 10, chạy với các model có acc vari khác nhau xoay quanh ngưỡng kì vọng 0.75 với các vari (0, 0.05, 0.1, 0.2), i.e, 10 models với vari từ -0.2 đến 0.2; để xem kết quả ra sao. (TEST 3)
Sau đây là kết quả:
TEST 1
Accuracy of one model: 0.524
TEST 2
Accuracy of 10 models with 0.00 vari: 0.633
Accuracy of 50 models with 0.00 vari: 0.609
Accuracy of 100 models with 0.00 vari: 0.623
Accuracy of 500 models with 0.00 vari: 0.683
Accuracy of 1000 models with 0.00 vari: 0.769
TEST 3
Accuracy of one model: 0.749
Accuracy of 10 models with +-0.00 vari: 0.982
Accuracy of 10 models with +-0.05 vari: 0.972
Accuracy of 10 models with +-0.10 vari: 0.985
Accuracy of 10 models with +-0.20 vari: 0.998
Như vậy rõ ràng là stacking càng nhiều model độc lập với nhau lại thì kết quả càng tốt, và variance lớn cũng không ảnh hưởng đến kết quả nhiều lắm.
Code ở đây: https://gitlab.com/andy143/ensemble-test/-/tree/master
Giải thích của mình nằm ở comment do viết nữa thì thành wall-of-text","Chào mọi người, Hôm nọ có bài hỏi về ensemble khá hot và có nhiều ý kiến tranh luận trên 2 topic chính: - Có nên gộp các model có variance lớn lại với nhau hay không - Quy luật của số lớn có ảnh hưởng thế nào đến kết quả của ensemble Trước khi trả lời thì mình có trình bày test của mình trên các tập như sau. - Cho một model với accuracy là x = 0.51 (gọi là weak model) chạy 1000 lần, tính accuracy trung bình (TEST 1) - Cho một tập các weak model ở trên với các size khác nhau (10, 50, 100, 500, 1000), tính major vote và tính accuracy trung bình của 1000 run. Test này để xem số lượng model ảnh hưởng thế nào đến accuracy của model (TEST 2) - Cho một tập các model với size là 10, chạy với các model có acc vari khác nhau xoay quanh ngưỡng kì vọng 0.75 với các vari (0, 0.05, 0.1, 0.2), i.e, 10 models với vari từ -0.2 đến 0.2; để xem kết quả ra sao. (TEST 3) Sau đây là kết quả: TEST 1 Accuracy of one model: 0.524 TEST 2 Accuracy of 10 models with 0.00 vari: 0.633 Accuracy of 50 models with 0.00 vari: 0.609 Accuracy of 100 models with 0.00 vari: 0.623 Accuracy of 500 models with 0.00 vari: 0.683 Accuracy of 1000 models with 0.00 vari: 0.769 TEST 3 Accuracy of one model: 0.749 Accuracy of 10 models with +-0.00 vari: 0.982 Accuracy of 10 models with +-0.05 vari: 0.972 Accuracy of 10 models with +-0.10 vari: 0.985 Accuracy of 10 models with +-0.20 vari: 0.998 Như vậy rõ ràng là stacking càng nhiều model độc lập với nhau lại thì kết quả càng tốt, và variance lớn cũng không ảnh hưởng đến kết quả nhiều lắm. Code ở đây: https://gitlab.com/andy143/ensemble-test/-/tree/master Giải thích của mình nằm ở comment do viết nữa thì thành wall-of-text",,,,,
"Mình train yolov4 , trong lúc đang train thì colab thông báo như này
/bin/bash: line 1: 269 Killed ./darknet detector train obj.data cfg/yolo4/custom-4corner.cfg backup/custom-4corner_last.weights -dont_show -map &> backup/training_4_corner.log
Mình có search thử nhưng ko thấy ai gặp lỗi này cả
Có bạn nào gặp lỗi tương tự chỉ mình đc không ạ","Mình train yolov4 , trong lúc đang train thì colab thông báo như này /bin/bash: line 1: 269 Killed ./darknet detector train obj.data cfg/yolo4/custom-4corner.cfg backup/custom-4corner_last.weights -dont_show -map &> backup/training_4_corner.log Mình có search thử nhưng ko thấy ai gặp lỗi này cả Có bạn nào gặp lỗi tương tự chỉ mình đc không ạ",,,,,
"Em chào mọi người ạ. Mọi người cho em hỏi là hiện tại em đang cần một bộ dataset giá chứng khoán của các cty ở Việt Nam thì có thể tìm được ở đâu , nếu không có thì có cách nào để crawl được dữ liệu về chứng khoán không ạ?Em cảm ơn ạ.","Em chào mọi người ạ. Mọi người cho em hỏi là hiện tại em đang cần một bộ dataset giá chứng khoán của các cty ở Việt Nam thì có thể tìm được ở đâu , nếu không có thì có cách nào để crawl được dữ liệu về chứng khoán không ạ?Em cảm ơn ạ.",,,,,
"Cho mình hỏi ở đây có bạn nào chạy thử yolov4 Pytorch từ link https://github.com/Tianxiaomo/pytorch-YOLOv4 mà đạt dc KQ real-time trên 30fps ko ạ? Mình chạy ở 1 máy Win cài conda GPU 6GB và 1 máy Linux card 8GB thì ở máy chạy Win đạt dc khoảng 17fps (load video từ HDD) và trên máy Linux (dùng SSD) đạt dc 20fps với điều kiện ko imshow hình bằng opencv, mình đang xét trên môi trường Python dùng Pytorch thôi ạ, ko xét đến việc chạy C/C++. Có phải Pytorch tối ưu ko tốt để đạt real-time hay ko? code mình dùng của tác giả, chỉ chỉnh sửa để đọc video từ máy thay vì đọc 1 ảnh đơn và chưa xét đến độ chính xác. Cũng đã cố gắng tra google và đọc issue của nó nhưng ko tìm dc câu trả lời. XIn cảm ơn","Cho mình hỏi ở đây có bạn nào chạy thử yolov4 Pytorch từ link https://github.com/Tianxiaomo/pytorch-YOLOv4 mà đạt dc KQ real-time trên 30fps ko ạ? Mình chạy ở 1 máy Win cài conda GPU 6GB và 1 máy Linux card 8GB thì ở máy chạy Win đạt dc khoảng 17fps (load video từ HDD) và trên máy Linux (dùng SSD) đạt dc 20fps với điều kiện ko imshow hình bằng opencv, mình đang xét trên môi trường Python dùng Pytorch thôi ạ, ko xét đến việc chạy C/C++. Có phải Pytorch tối ưu ko tốt để đạt real-time hay ko? code mình dùng của tác giả, chỉ chỉnh sửa để đọc video từ máy thay vì đọc 1 ảnh đơn và chưa xét đến độ chính xác. Cũng đã cố gắng tra google và đọc issue của nó nhưng ko tìm dc câu trả lời. XIn cảm ơn",,,,,
"Em xin chào mọi người ạ. Ở trong công thức tính conditional distribution của multivariate gaussian em thấy có công thức như hình 1, sau một lúc tìm trên mạng thì em thấy có chứng minh như hình. Nhưng hiện tại em vẫn không hiểu được ý nghĩa của Conditional Part và Marginal Part ạ. Trong phần chứng minh khác em vẫn không hiểu ý nghĩa cụ thể của E2 is exactly the exponent of the distribution ạ. Mong mọi người giải thích giúp em, em xin cảm ơn ạ.
Link chứng minh: https://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution
https://github.com/carlhenrikek/COMS30007/blob/2018/Lectures/gaussianidentities.pdf","Em xin chào mọi người ạ. Ở trong công thức tính conditional distribution của multivariate gaussian em thấy có công thức như hình 1, sau một lúc tìm trên mạng thì em thấy có chứng minh như hình. Nhưng hiện tại em vẫn không hiểu được ý nghĩa của Conditional Part và Marginal Part ạ. Trong phần chứng minh khác em vẫn không hiểu ý nghĩa cụ thể của E2 is exactly the exponent of the distribution ạ. Mong mọi người giải thích giúp em, em xin cảm ơn ạ. Link chứng minh: https://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution https://github.com/carlhenrikek/COMS30007/blob/2018/Lectures/gaussianidentities.pdf",,"#math, #Q&A",,,
"Mấy khi có cơ hội thử sức, kiếm tiền made in VN, mời các bạn cùng tham gia :)","Mấy khi có cơ hội thử sức, kiếm tiền made in VN, mời các bạn cùng tham gia :)",,,,,
"Chào mọi người,
Mình vừa đi pv ở một công ty AI lớn ở HCM. Lúc pv, anh pv có hỏi mình về stacking và luật số lớn. Mình trả lời là giá trị trung bình của kì vọng (predicted value) của các model bằng giá trị kì vọng thực (ground truth value), điều kiện là các model có độ chính xác tương tự nhau và lớn hơn baseline model (không nên stack 2 model có độ chính xác 99% và 70% lại với nhau).
Anh pv cho rằng mình thiếu điều kiện variance nhỏ, anh dẫn chứng rằng nếu ground truth value bằng 0 thì stack 2 model có predicted value là 1 và -1 sẽ tốt hơn việc stack 2 model có predicted value là 8 và -8.
Theo mình thì nếu độ chính xác của 4 model không quá khác biệt và có độ chính xác lớn hơn baseline model thì vẫn có thể stack cả 4 model lại. Xin ý kiến của mọi người về vấn đề này.","Chào mọi người, Mình vừa đi pv ở một công ty AI lớn ở HCM. Lúc pv, anh pv có hỏi mình về stacking và luật số lớn. Mình trả lời là giá trị trung bình của kì vọng (predicted value) của các model bằng giá trị kì vọng thực (ground truth value), điều kiện là các model có độ chính xác tương tự nhau và lớn hơn baseline model (không nên stack 2 model có độ chính xác 99% và 70% lại với nhau). Anh pv cho rằng mình thiếu điều kiện variance nhỏ, anh dẫn chứng rằng nếu ground truth value bằng 0 thì stack 2 model có predicted value là 1 và -1 sẽ tốt hơn việc stack 2 model có predicted value là 8 và -8. Theo mình thì nếu độ chính xác của 4 model không quá khác biệt và có độ chính xác lớn hơn baseline model thì vẫn có thể stack cả 4 model lại. Xin ý kiến của mọi người về vấn đề này.",,,,,
"Chào mọi người,
Mình đang làm đề tài về nhận diện biển số xe Việt Nam, có ai có tập ảnh của xe oto ra vào bãi giữ xe không ạ. Cho mình xin với!
Cảm ơn !!","Chào mọi người, Mình đang làm đề tài về nhận diện biển số xe Việt Nam, có ai có tập ảnh của xe oto ra vào bãi giữ xe không ạ. Cho mình xin với! Cảm ơn !!",,,,,
"We're excited to share with you all a NLU dataset for Vietnamese intent detection and slot filling: https://github.com/VinAIResearch/JointIDSF
See details in our paper: https://arxiv.org/abs/2104.02021",We're excited to share with you all a NLU dataset for Vietnamese intent detection and slot filling: https://github.com/VinAIResearch/JointIDSF See details in our paper: https://arxiv.org/abs/2104.02021,,,,,
"Hi mọi người !!!
Mọi người trong groupe có ai có kinh nghiệm modelling với y không phải là normally distributed chưa nhỉ?
Mon y étant continue, et il y a beaucoup de point d'accumulation (centralisation autour d'une valeur). En gros, ça ressemble à y = 0 (90% des cas), ou y varie et peut être très extrême
Ví dụ trong trường hợp của  là y = -1 (90%) hoặc y liên tục, và có những giá trị rất lớn
Mình chỉ mới test thử linear regression trên y này, thì kết quả không ngạc nhiên là rất tệ, R2 gần như 0 :'(
Mình cũng đã thử log-transformation, nhưng mà kết quả cũng không khá hơn
Có ai từng gặp trường hợp này chưa nhỉ? Hoặc nếu mọi người có ideas gì ko?
Cám ơn cả nhà đã đọc bài !","Hi mọi người !!! Mọi người trong groupe có ai có kinh nghiệm modelling với y không phải là normally distributed chưa nhỉ? Mon y étant continue, et il y a beaucoup de point d'accumulation (centralisation autour d'une valeur). En gros, ça ressemble à y = 0 (90% des cas), ou y varie et peut être très extrême Ví dụ trong trường hợp của là y = -1 (90%) hoặc y liên tục, và có những giá trị rất lớn Mình chỉ mới test thử linear regression trên y này, thì kết quả không ngạc nhiên là rất tệ, R2 gần như 0 :'( Mình cũng đã thử log-transformation, nhưng mà kết quả cũng không khá hơn Có ai từng gặp trường hợp này chưa nhỉ? Hoặc nếu mọi người có ideas gì ko? Cám ơn cả nhà đã đọc bài !",,,,,
"Chào mọi người!
Hiện tại em có 1 dự án nhưng mà đang bí ở 1 số phần. Hi vọng mọi người có thể giúp em khai sáng.
Đề tài là về ""Password behavior"", app của tụi em có thể tạo 1 password dựa trên những thói quen của người dùng. Hiện tại thì có 2 hướng: 1 là xây dựng 1 cái bảng câu hỏi rồi dựa trên câu trả lời làm 1 password mới, 2 là dựa trên những gì người dùng nhập (trong trình duyệt etc,...) xong lấy mấy cái thói quen đó tạo thành password (cách này thì em chưa hình dung được làm thế nào).
Phần quan trọng là app phải áp dụng machine learning. Mà theo như em trình bày hướng 1 ở trên thì em chỉ nghĩ đó là hash câu trả lời rồi ngẫu nhiên tổng hợp lại thành 1 tổ hợp password mới thôi. Chứ nó không có áp dụng machinelearning hay cụ thể là training 1 cái dataset nào hết.
Mọi người giúp em chỉ ra với nếu như áp dụng machinelearning (sử dụng dataset) cho đề bài này dùng 2 hướng trên thì sử dụng như thế nào ạ!
Xin cảm ơn mọi người","Chào mọi người! Hiện tại em có 1 dự án nhưng mà đang bí ở 1 số phần. Hi vọng mọi người có thể giúp em khai sáng. Đề tài là về ""Password behavior"", app của tụi em có thể tạo 1 password dựa trên những thói quen của người dùng. Hiện tại thì có 2 hướng: 1 là xây dựng 1 cái bảng câu hỏi rồi dựa trên câu trả lời làm 1 password mới, 2 là dựa trên những gì người dùng nhập (trong trình duyệt etc,...) xong lấy mấy cái thói quen đó tạo thành password (cách này thì em chưa hình dung được làm thế nào). Phần quan trọng là app phải áp dụng machine learning. Mà theo như em trình bày hướng 1 ở trên thì em chỉ nghĩ đó là hash câu trả lời rồi ngẫu nhiên tổng hợp lại thành 1 tổ hợp password mới thôi. Chứ nó không có áp dụng machinelearning hay cụ thể là training 1 cái dataset nào hết. Mọi người giúp em chỉ ra với nếu như áp dụng machinelearning (sử dụng dataset) cho đề bài này dùng 2 hướng trên thì sử dụng như thế nào ạ! Xin cảm ơn mọi người",,,,,
"Mình đang làm project giống như Jarvis, kiểu Voice Bot Assistant, nhưng đang gặp khó khăn ở vấn đề frontend (theo kiểu real time chứ k cần click) Không biết bạn nào đã có solutions về frontend cho phần này chưa? Cho mình xin gợi ý, mình có tìm thấy một vài chỗ như Web Speech API, frontend of Rasa mà vẫn chưa thực hiện được vì có vẻ như cần phải dùng javasripts hay react js. Nếu có template, hoặc no-code thì tốt quá. Cảm ơn mọi người đã quan tâm.","Mình đang làm project giống như Jarvis, kiểu Voice Bot Assistant, nhưng đang gặp khó khăn ở vấn đề frontend (theo kiểu real time chứ k cần click) Không biết bạn nào đã có solutions về frontend cho phần này chưa? Cho mình xin gợi ý, mình có tìm thấy một vài chỗ như Web Speech API, frontend of Rasa mà vẫn chưa thực hiện được vì có vẻ như cần phải dùng javasripts hay react js. Nếu có template, hoặc no-code thì tốt quá. Cảm ơn mọi người đã quan tâm.",,,,,
Ngoài Kaggle là nơi có nhiều open dataset ra thì paperwiththecodes mới mở ra thêm phần lưu trữ dataset. Hiện trang này đang lưu gần 4000 datasets đủ các chủ đề tại đây,Ngoài Kaggle là nơi có nhiều open dataset ra thì paperwiththecodes mới mở ra thêm phần lưu trữ dataset. Hiện trang này đang lưu gần 4000 datasets đủ các chủ đề tại đây,,,,,
"Classification on very skewed data
Mọi người ơi cho em hỏi nếu dataset của e bị lệch ntn thì augmentation có giúp dc nhiều k ạ
Dataset
~175,000 entry
52 classes","Classification on very skewed data Mọi người ơi cho em hỏi nếu dataset của e bị lệch ntn thì augmentation có giúp dc nhiều k ạ Dataset ~175,000 entry 52 classes",,,,,
Mọi người ơi cho em hỏi có ai dùng timm cho classification task chưa ạ cho em hỏi là nếu muốn tự build model mới custom dataset thì phải làm ntn ạ,Mọi người ơi cho em hỏi có ai dùng timm cho classification task chưa ạ cho em hỏi là nếu muốn tự build model mới custom dataset thì phải làm ntn ạ,,,,,
"Chúc mọi người cuối tuần vui vẻ 😁.
Mọi người cho e hỏi với ạ:
Mọi người đã ứng dụng các bài toán NLP cho doanh nghiệp của mình như thế nào ạ?",Chúc mọi người cuối tuần vui vẻ . Mọi người cho e hỏi với ạ: Mọi người đã ứng dụng các bài toán NLP cho doanh nghiệp của mình như thế nào ạ?,,,,,
"Hi mọi người!
Hiện tại em đang làm 1 project về nhận diện, điều khiển bằng giọng nói (Speech recognition and voice control). sử dụng ngôn ngữ C++ C#
M.n có tài liệu, đồ án ... về đề tài này có thể cho em xin tham khảo được không ạ!
Có tài liệu về nhận dạng và điều khiển bằng tiếng việt thì càng tốt ạ!
Thanks m.n !","Hi mọi người! Hiện tại em đang làm 1 project về nhận diện, điều khiển bằng giọng nói (Speech recognition and voice control). sử dụng ngôn ngữ C++ C# M.n có tài liệu, đồ án ... về đề tài này có thể cho em xin tham khảo được không ạ! Có tài liệu về nhận dạng và điều khiển bằng tiếng việt thì càng tốt ạ! Thanks m.n !",,,,,
"Chào mọi người. Hiện tại em đang làm bài tập về xây dựng một hệ thống nhận dạng và tìm kiếm hình ảnh với đầu vào là 1 file ảnh sữa tươi mới, đầu ra là một vài file ảnh trong CSDL có nội dung giống nhất hoặc chứa nội dung của ảnh đầu vào. Mọi người có thể cho em xin một vài ý tưởng với ạ. Em cảm ơn!","Chào mọi người. Hiện tại em đang làm bài tập về xây dựng một hệ thống nhận dạng và tìm kiếm hình ảnh với đầu vào là 1 file ảnh sữa tươi mới, đầu ra là một vài file ảnh trong CSDL có nội dung giống nhất hoặc chứa nội dung của ảnh đầu vào. Mọi người có thể cho em xin một vài ý tưởng với ạ. Em cảm ơn!",,,,,
Ở đây có ai đã sử dụng Datarobot chưa ạ? Theo như cái video demo này thì nó giúp mình tự động hoá data cleaning và parameter tuning. Mình đánh giá là nó sẽ giúp mình hoàn thành công việc nhanh hơn 90% so với mình tự làm (thực sự mình đang làm data analyst có áp dụng machine learning chứ không dám nhận là data scientist). Ai dùng rồi thì cho mình xin review và subscription fee với.,Ở đây có ai đã sử dụng Datarobot chưa ạ? Theo như cái video demo này thì nó giúp mình tự động hoá data cleaning và parameter tuning. Mình đánh giá là nó sẽ giúp mình hoàn thành công việc nhanh hơn 90% so với mình tự làm (thực sự mình đang làm data analyst có áp dụng machine learning chứ không dám nhận là data scientist). Ai dùng rồi thì cho mình xin review và subscription fee với.,,,,,
,nan,,,,,
"[Object Detection][Faster RCNN]
Chào mọi người, hiện tại mình đang làm 1 project liên quan đến object detection, cụ thể là về steel defect detection và thông qua tập NEU-DET dataset của Northeastern University. Cụ thể thì đây là tập data set về steel defection đã có đủ label và boxes cụ thể cho từng ảnh trên data thông qua 1 file xml. Hiện tại thì mình có đọc qua 1 vài papers để xử lý cho việc detect defection, mình có đọc qua 1 paper (https://ieeexplore.ieee.org/document/8709818) này và có dùng cách lấy các feature của ảnh thông qua các stage outputs của resnet và đưa nó qua 1 ROIPooling để train, song song với việc đó thì có áp dụng cả RPN. Vì thế theo ý hiểu của mình thì nó khá giống với cách tiếp cận của Faster RCNN. Mình đã đi thử implement Faster RCNN cho data set kia trước thông qua việc code lại theo repo này (https://github.com/kentaroy47/frcnn-from-scratch-with-keras) để thử xem solution này có hiệu quả không nhưng mà mình đang bị gặp 1 số vấn đề liên quan đến RPN, cụ thể là ở bước training, sau khi train xong RPN model và predict thì mình thấy số number of rois sau khi predict sẽ là dynamic (ví dụ: [(1, 1, 7), (1, 1, 48)] hoặc [(1, 2, 7), (1, 2, 48)] ), vì kết quả của RPN model prediction sẽ được dùng làm output data cho classifier model sau đó để đưa ra kết quả cuối cùng, nhưng vì output layer của classifier model được fix shape (ví dụ: 6 classes - classify layer: (1, 4, 7), locate layer: (1, 4, 24)). Vì y_train truyền vào có shape là dynamic nên sẽ xảy ra lỗi không matching với output layer của model. Vì 1 phần mình cũng là người mới về lĩnh vực này nên chưa có kinh nghiệm xử lý vấn đề này. Vậy có ai trong group đã từng implement theo repo trên hoặc xử lý 1 vấn đề liên quan đến vấn đề mình đang gặp thì có thể giúp mình được không ạ?
Mình xin cảm ơn mọi người!","[Object Detection][Faster RCNN] Chào mọi người, hiện tại mình đang làm 1 project liên quan đến object detection, cụ thể là về steel defect detection và thông qua tập NEU-DET dataset của Northeastern University. Cụ thể thì đây là tập data set về steel defection đã có đủ label và boxes cụ thể cho từng ảnh trên data thông qua 1 file xml. Hiện tại thì mình có đọc qua 1 vài papers để xử lý cho việc detect defection, mình có đọc qua 1 paper (https://ieeexplore.ieee.org/document/8709818) này và có dùng cách lấy các feature của ảnh thông qua các stage outputs của resnet và đưa nó qua 1 ROIPooling để train, song song với việc đó thì có áp dụng cả RPN. Vì thế theo ý hiểu của mình thì nó khá giống với cách tiếp cận của Faster RCNN. Mình đã đi thử implement Faster RCNN cho data set kia trước thông qua việc code lại theo repo này (https://github.com/kentaroy47/frcnn-from-scratch-with-keras) để thử xem solution này có hiệu quả không nhưng mà mình đang bị gặp 1 số vấn đề liên quan đến RPN, cụ thể là ở bước training, sau khi train xong RPN model và predict thì mình thấy số number of rois sau khi predict sẽ là dynamic (ví dụ: [(1, 1, 7), (1, 1, 48)] hoặc [(1, 2, 7), (1, 2, 48)] ), vì kết quả của RPN model prediction sẽ được dùng làm output data cho classifier model sau đó để đưa ra kết quả cuối cùng, nhưng vì output layer của classifier model được fix shape (ví dụ: 6 classes - classify layer: (1, 4, 7), locate layer: (1, 4, 24)). Vì y_train truyền vào có shape là dynamic nên sẽ xảy ra lỗi không matching với output layer của model. Vì 1 phần mình cũng là người mới về lĩnh vực này nên chưa có kinh nghiệm xử lý vấn đề này. Vậy có ai trong group đã từng implement theo repo trên hoặc xử lý 1 vấn đề liên quan đến vấn đề mình đang gặp thì có thể giúp mình được không ạ? Mình xin cảm ơn mọi người!",,,,,
"ĐIỀU GÌ ĐÃ XẢY RA VỚI KHUÔN MẶT Ở GẦN CAM HƠN?
Chào m.n, gần đây em có làm một bìa toán phân loại Mặt có đeo khẩu trang hay không. Có một issue làm em rất khó giải thích như này:
Em sử dụng một model với vài lớp conv như ảnh dưới, loss và acc trên val đều cho kết quả khá khả quan. Kết quả test trực tiếp trên webcam khá tốt, trừ việc khi đưa mặt sát vào webcam thì nó phân loại sai (hình dưới). Trong khi để ở khoảng cách vừa phải, cách khoảng 30cm so với cam, thì lại phân loại đúng. Vẫn dữ liệu đấy, t transfer sang MobileNet thì lại ko có hiện tượng này nữa.
Ảnh trong cửa sổ 'face' là cái mặt đã đc detect xong crop, resize trước khi cho vào mô hình phân loại để predict, các bước tiền xử lý đều giống với dữ liệu tập training.
Điều gì xảy ra với cái mặt ở gần cam vậy ạ?","ĐIỀU GÌ ĐÃ XẢY RA VỚI KHUÔN MẶT Ở GẦN CAM HƠN? Chào m.n, gần đây em có làm một bìa toán phân loại Mặt có đeo khẩu trang hay không. Có một issue làm em rất khó giải thích như này: Em sử dụng một model với vài lớp conv như ảnh dưới, loss và acc trên val đều cho kết quả khá khả quan. Kết quả test trực tiếp trên webcam khá tốt, trừ việc khi đưa mặt sát vào webcam thì nó phân loại sai (hình dưới). Trong khi để ở khoảng cách vừa phải, cách khoảng 30cm so với cam, thì lại phân loại đúng. Vẫn dữ liệu đấy, t transfer sang MobileNet thì lại ko có hiện tượng này nữa. Ảnh trong cửa sổ 'face' là cái mặt đã đc detect xong crop, resize trước khi cho vào mô hình phân loại để predict, các bước tiền xử lý đều giống với dữ liệu tập training. Điều gì xảy ra với cái mặt ở gần cam vậy ạ?",,,,,
"Chào mọi người, em có tạo một model làm ví dụ như hình dưới.
Sau khi chạy loss.backward() thì em có in ra grad của model.w thì nó hiện là None trong khi em đang cần update model.w trong quá trình train. Mong mọi người giúp đỡ","Chào mọi người, em có tạo một model làm ví dụ như hình dưới. Sau khi chạy loss.backward() thì em có in ra grad của model.w thì nó hiện là None trong khi em đang cần update model.w trong quá trình train. Mong mọi người giúp đỡ",,,,,
"Scaled-YOLOv4
https://arxiv.org/abs/2011.08036",Scaled-YOLOv4 https://arxiv.org/abs/2011.08036,,,,,
"Chào mọi người, em có một vấn đề mong được mọi người giải đáp.
Cụ thể như sau:
Em có một mạng sử dụng các Convolution Layer. Sau khi train xong thu được các kernel tensor Z. Sau khi prune model thì thu được các weight_mask G ( gồm các giá trị nhị phân 0,1 --> 1 biểu thị giá trị tại index đó tương ứng trong Z được giữ lại ).
Lúc này kernel tensor trở thành Z*G. Trong paper em đọc được thì người có bảo nguyên văn như thế này : 'Due to the existance of 𝑮, only important parameters are optimized, while pruned parameters keep unchanged because no gradients are created for them' 
Tức là:  vì sự tồn tại của G, chỉ các tham số quan trọng được tối ưu hóa, trong khi các tham số bị cắt tỉa được giữ nguyên vì không có gradient được tạo ra bởi chúng ""
Em ngồi suy nghĩ vẫn chưa làm sao chứng mình được luận điểm này của paper. Tức là chứng minh việc một parameters có giá trị bằng 0 thì sẽ không có gradient được tạo ra cho nó.  Trong paper người ta dùng activation function là ReLu
 Mong được mọi người giúp đỡ, em xin cảm ơn
Nguồn paper: https://arxiv.org/abs/2009.13724 ( Hình ảnh ở dưới nằm ở Stage 3 trang 5 /11 )","Chào mọi người, em có một vấn đề mong được mọi người giải đáp. Cụ thể như sau: Em có một mạng sử dụng các Convolution Layer. Sau khi train xong thu được các kernel tensor Z. Sau khi prune model thì thu được các weight_mask G ( gồm các giá trị nhị phân 0,1 --> 1 biểu thị giá trị tại index đó tương ứng trong Z được giữ lại ). Lúc này kernel tensor trở thành Z*G. Trong paper em đọc được thì người có bảo nguyên văn như thế này : 'Due to the existance of , only important parameters are optimized, while pruned parameters keep unchanged because no gradients are created for them' Tức là: vì sự tồn tại của G, chỉ các tham số quan trọng được tối ưu hóa, trong khi các tham số bị cắt tỉa được giữ nguyên vì không có gradient được tạo ra bởi chúng "" Em ngồi suy nghĩ vẫn chưa làm sao chứng mình được luận điểm này của paper. Tức là chứng minh việc một parameters có giá trị bằng 0 thì sẽ không có gradient được tạo ra cho nó. Trong paper người ta dùng activation function là ReLu Mong được mọi người giúp đỡ, em xin cảm ơn Nguồn paper: https://arxiv.org/abs/2009.13724 ( Hình ảnh ở dưới nằm ở Stage 3 trang 5 /11 )",,,,,
"Mọi người cho mình hỏi một chút về thư viện vaex. Mình có thử tìm hiểu thì thấy vaex xử lí data cực nhanh so với pandas, nhưng khi làm trên Colab thì không biết sai ở đâu mà thấy nó vẫn chậm ngang với pandas. Có ai đã gặp trường hợp này thì có thể giải thích cho mình được không ạ.","Mọi người cho mình hỏi một chút về thư viện vaex. Mình có thử tìm hiểu thì thấy vaex xử lí data cực nhanh so với pandas, nhưng khi làm trên Colab thì không biết sai ở đâu mà thấy nó vẫn chậm ngang với pandas. Có ai đã gặp trường hợp này thì có thể giải thích cho mình được không ạ.",,,,,
"mọi người cho em hỏi với đã có ai gặp trường hợp như thế này chưa ạ
trước em xài anaconda 3.8.5 em cài tensorflow nó không nhận bắt buộc hạ về bản 3.7.x mới nhận tensorflow
về 3.7.x thì em install lại cv2 thì nó tự động install opencv 3.4.1 như hình
sau đó em cài MTCNN thì MTCNN lại update opencv từ 3.4.1 lên 4.1.1
nhưng khi update opencv lên 4.1.1 thì opencv lại bị lỗi và em uninstall đi cài lại 3.4.1 thì bình thường nhưng lại mất MTCNN
ai có kinh nghiệm xử lý cái này cho em xin hướng đi ạ",mọi người cho em hỏi với đã có ai gặp trường hợp như thế này chưa ạ trước em xài anaconda 3.8.5 em cài tensorflow nó không nhận bắt buộc hạ về bản 3.7.x mới nhận tensorflow về 3.7.x thì em install lại cv2 thì nó tự động install opencv 3.4.1 như hình sau đó em cài MTCNN thì MTCNN lại update opencv từ 3.4.1 lên 4.1.1 nhưng khi update opencv lên 4.1.1 thì opencv lại bị lỗi và em uninstall đi cài lại 3.4.1 thì bình thường nhưng lại mất MTCNN ai có kinh nghiệm xử lý cái này cho em xin hướng đi ạ,,,,,
Có ai biết dòng y tại sao lại như vậy không!. Với lại mình đang bị lỗi ở dòng đó,Có ai biết dòng y tại sao lại như vậy không!. Với lại mình đang bị lỗi ở dòng đó,,,,,
"Chào mọi người. Mọi người cho em hỏi pro/con của azure machine learning với google cloud AI cho enterprise với ạ?
Anh/chị có biết thông tin gì hay đã sử dụng qua thì em mong mọi người chia sẻ cho em được hiểu thêm. Em cảm ơn!",Chào mọi người. Mọi người cho em hỏi pro/con của azure machine learning với google cloud AI cho enterprise với ạ? Anh/chị có biết thông tin gì hay đã sử dụng qua thì em mong mọi người chia sẻ cho em được hiểu thêm. Em cảm ơn!,,,,,
"GIGO (Garbage In, Garbage Out) is a popular concept in Computer Science expressing the idea that the product created from materials of low quality will also be low quality. Specifically for Data Science, bad data input can generate inaccurate insights leading to wrong decision making. For this reason, it is critical to monitor and maintain Data Quality to make sure the data meets certain standards for specific business use-cases.
To the best of my knowledge, however, there is no widely-used, industry-standard and generically built Data Quality framework from which organisations can adopt for any of their specific applications and therefore the big questions are always ""How should we do it?"", ""How to define high-quality data?"".
In this article, I will dictate the definition of Data Quality and its dimensions. I will explain the high-level implementation and the process of Data Quality Management as well as teams' responsibilities for managing Data Quality.
#datascience #data #dataquality #dataqualitymanagement #garbageingarbageout #bigdata #dataanalytics #insight #agile #ai #business #machinelearning #artificialintelligence #analytics","GIGO (Garbage In, Garbage Out) is a popular concept in Computer Science expressing the idea that the product created from materials of low quality will also be low quality. Specifically for Data Science, bad data input can generate inaccurate insights leading to wrong decision making. For this reason, it is critical to monitor and maintain Data Quality to make sure the data meets certain standards for specific business use-cases. To the best of my knowledge, however, there is no widely-used, industry-standard and generically built Data Quality framework from which organisations can adopt for any of their specific applications and therefore the big questions are always ""How should we do it?"", ""How to define high-quality data?"". In this article, I will dictate the definition of Data Quality and its dimensions. I will explain the high-level implementation and the process of Data Quality Management as well as teams' responsibilities for managing Data Quality.",#datascience	#data	#dataquality	#dataqualitymanagement	#garbageingarbageout	#bigdata	#dataanalytics	#insight	#agile	#ai	#business	#machinelearning	#artificialintelligence	#analytics,,,,
"Mặc dù tình trạng khan hiếm GPU đang diễn ra rất trầm trọng. Mấy tháng trước mình đã bán đi 2 con 1080Ti sau khi có 1 con 3090, sau lại bán tiếp đi 1 con 1060 6G thì mới phát hiện ra (1) anh Elon Musk (có lẽ qua phát biểu) và (2) tình trạng bất ổn kinh tế --> bitcoin lên ngôi, cùng với tình trạng khan hiếm linh kiện điện tử nói chung do (3) đại dịch Covid-19 và các xung đột chính trị. Ngược lại với câu chuyện trên thì các SOTA models ngày càng yêu cầu năng lực tính toán rất cao. Hôm qua, trên diễn đàn mình thấy một số guru có bàn luận về cách họ build hệ thống máy tính để làm việc. Trước đây, có thể chúng ta đã biết tới blog của anh Tim Dettmers (tại đây https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/), nhưng hướng dẫn này khá out-of-date. Do vậy mình thấy cách anh Emil Wallner có viết bài khá hay hướng dẫn build máy cho 3 nhóm đối tượng, là người dùng đơn lẻ (consumer), người dùng ""chuyên nghiệp"" (proconsumer), và doanh nghiệp (enterprise) tại đây: https://www.emilwallner.com/p/ml-rig.
Hi vọng nó hữu ích với mọi người.
Còn cá nhân mình có cách khác rất rẻ với việc chỉ dành tiền cho GPUs mà thôi!!! Tất nhiên, mình không đo benchmark như họ, nhưng qua trải nghiệm thực tế từ những lần nâng cấp thì thấy khá hài lòng với hệ thống hiện tại!","Mặc dù tình trạng khan hiếm GPU đang diễn ra rất trầm trọng. Mấy tháng trước mình đã bán đi 2 con 1080Ti sau khi có 1 con 3090, sau lại bán tiếp đi 1 con 1060 6G thì mới phát hiện ra (1) anh Elon Musk (có lẽ qua phát biểu) và (2) tình trạng bất ổn kinh tế --> bitcoin lên ngôi, cùng với tình trạng khan hiếm linh kiện điện tử nói chung do (3) đại dịch Covid-19 và các xung đột chính trị. Ngược lại với câu chuyện trên thì các SOTA models ngày càng yêu cầu năng lực tính toán rất cao. Hôm qua, trên diễn đàn mình thấy một số guru có bàn luận về cách họ build hệ thống máy tính để làm việc. Trước đây, có thể chúng ta đã biết tới blog của anh Tim Dettmers (tại đây https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/), nhưng hướng dẫn này khá out-of-date. Do vậy mình thấy cách anh Emil Wallner có viết bài khá hay hướng dẫn build máy cho 3 nhóm đối tượng, là người dùng đơn lẻ (consumer), người dùng ""chuyên nghiệp"" (proconsumer), và doanh nghiệp (enterprise) tại đây: https://www.emilwallner.com/p/ml-rig. Hi vọng nó hữu ích với mọi người. Còn cá nhân mình có cách khác rất rẻ với việc chỉ dành tiền cho GPUs mà thôi!!! Tất nhiên, mình không đo benchmark như họ, nhưng qua trải nghiệm thực tế từ những lần nâng cấp thì thấy khá hài lòng với hệ thống hiện tại!",,,,,
"chào mng, mk mới học nhứng đến phần maximum likelihood ko hiểu rõ, ko biết likelihood khác vs xác suất như thế nào v ạ,","chào mng, mk mới học nhứng đến phần maximum likelihood ko hiểu rõ, ko biết likelihood khác vs xác suất như thế nào v ạ,",,"#Q&A, #math",,,
"Kính chào các bác, do điều kiện máy móc không có GPU nên em tìm hiểu cách sử dụng luôn GPU này để inference nhằm tăng tốc độ nhận diện realtime qua webcam.
Hôm nay tranh thủ làm clip share cùng các bạn mới học. Mong ad duyệt bài!
Chúc anh em thành công!","Kính chào các bác, do điều kiện máy móc không có GPU nên em tìm hiểu cách sử dụng luôn GPU này để inference nhằm tăng tốc độ nhận diện realtime qua webcam. Hôm nay tranh thủ làm clip share cùng các bạn mới học. Mong ad duyệt bài! Chúc anh em thành công!",,,,,
"Chào mọi người, mình là newbie, muốn tìm hiểu và học để làm một số ứng dụng về reinforcement learning, mong mọi người chia sẻ giúp mình một số course vừa dạy lý thuyết vừa code theo để mình thực hành luôn ạ, mình cám ơn mọi người nhiều nhiều ạ","Chào mọi người, mình là newbie, muốn tìm hiểu và học để làm một số ứng dụng về reinforcement learning, mong mọi người chia sẻ giúp mình một số course vừa dạy lý thuyết vừa code theo để mình thực hành luôn ạ, mình cám ơn mọi người nhiều nhiều ạ",,,,,
"Chào mọi người, em có một vấn đề liên quan đến xác suất mong mọi người giúp đỡ.
Một nhà đầu tư đi tham vấn hai chuyên gia tài chính về việc tăng giảm giá cổ phiếu của công ty S( chỉ cần biết tăng hay giảm). Chuyên gia thứ nhất với xác suất dự đoán đúng là 60%, cho một quyết định nhưng không cho người đọc biết. Lại tiếp tục gọi giáo sư, giáo sư có khả năng dự đoán đúng lên đến 90% trong những lần trước đó, và lần này cho kết quả giống anh chuyên gia. Nhà đầu tư thắc mắc không biết khả năng 2 người cùng dự đoán đúng là bao nhiêu phần trăm?
Theo suy nghĩ của em thì đây là bài toán xác suất có điều kiện vì đề bài là 2 người đã có cùng 1 kết quả dự đoán trước đó, vậy nên hai người cùng đúng hoặc cùng sai, không có th người 1 đúng mà người 2 sai. Vậy nên xác suất 2 người cùng dự đoán đúng:
(xác suất 2 người cùng dự đoán đúng) / (xác suất 2 người cùng dự đoán đúng + xác suất 2 người cùng dự đoán sai)
= (0.9*0.6)/(0.9*0.6 + 0.1*0.4)
= 93.1%
Không biết là cách lí giải như vậy có hợp lí không? mong mọi người cho ý kiến. Em cảm ơn mn.","Chào mọi người, em có một vấn đề liên quan đến xác suất mong mọi người giúp đỡ. Một nhà đầu tư đi tham vấn hai chuyên gia tài chính về việc tăng giảm giá cổ phiếu của công ty S( chỉ cần biết tăng hay giảm). Chuyên gia thứ nhất với xác suất dự đoán đúng là 60%, cho một quyết định nhưng không cho người đọc biết. Lại tiếp tục gọi giáo sư, giáo sư có khả năng dự đoán đúng lên đến 90% trong những lần trước đó, và lần này cho kết quả giống anh chuyên gia. Nhà đầu tư thắc mắc không biết khả năng 2 người cùng dự đoán đúng là bao nhiêu phần trăm? Theo suy nghĩ của em thì đây là bài toán xác suất có điều kiện vì đề bài là 2 người đã có cùng 1 kết quả dự đoán trước đó, vậy nên hai người cùng đúng hoặc cùng sai, không có th người 1 đúng mà người 2 sai. Vậy nên xác suất 2 người cùng dự đoán đúng: (xác suất 2 người cùng dự đoán đúng) / (xác suất 2 người cùng dự đoán đúng + xác suất 2 người cùng dự đoán sai) = (0.9*0.6)/(0.9*0.6 + 0.1*0.4) = 93.1% Không biết là cách lí giải như vậy có hợp lí không? mong mọi người cho ý kiến. Em cảm ơn mn.",,,"#Q&A, #math",,
"Nội quy Forum:
1. Ngôn ngữ trong Forum BẮT BUỘC là tiếng Việt có dấu hoặc tiếng Anh.
2. Tìm kiếm trước khi đặt câu hỏi. Có thể tìm kiếm từ các nguồn khác hoặc trong Forum, cột bên trái có box ""Search this group"", cột bên phải có các topics khác nhau.
3. Suy nghĩ kỹ trước khi hỏi. Cố gắng diễn đạt câu hỏi một cách mạch lạc để người đọc có thể hiểu bạn hỏi gì.
4. Khi post bài, bạn được khuyến khích sử dụng hashtags. Một số hashtags thông dụng: #python, #tensorfow, #caffe, #objectdetection, #nlp, #jupyter_notebook,... Bạn cũng có thể add topics cho mỗi câu hỏi.
5. Với các câu hỏi liên quan đến code, bạn được khuyến khích debug trước khi hỏi (nếu bạn không biết debug là gì, vui lòng xem lại Điều 2). Khi hỏi cần đưa code, error và dòng lệnh xảy ra error đó. Kèm theo đó là việc bạn đã thử các giải pháp nào và các lỗi đi kèm.
6. Code nên được đưa lên các trang cho phép xem code online với highlight cho từng loại ngôn ngữ. Một vài ví dụ: github, ideone, ...
7. Các thành viên trong Forum cần tôn trọng lẫn nhau, sử dụng ngôn ngữ lịch sự, chuẩn mực. Thành viên sử dụng ngôn ngữ bất lịch sự, thiếu tôn trọng người khác sẽ bị xoá vĩnh viễn khỏi Forum.
8. Các comment spam sẽ bị xoá và thành viên sẽ bị xoá khỏi group. Comment chỉ có các dấu chấm (.) sẽ bị coi là spam. Nếu bạn muốn follow một post, bạn có thể chọn chế độ ""Turn on notifications for this post"".
9. Các post không được approve có thể do một trong các lý do:
* spam,
* câu hỏi không liên quan đến Forum,
* diễn đạt khó hiểu (kể cả việc viết tiếng Việt không dấu)
* câu hỏi lặp
* các nội dung vi phạm bản quyền.
10. Admins có thể xoá bất cứ bài hoặc bình luận nào mà không cần giải thích. Các bài/bình luận bị 3 report trở lên cũng sẽ bị xoá.
-------------------------
Tiếp tục cập nhật. Mời các bạn bổ sung.","Nội quy Forum: 1. Ngôn ngữ trong Forum BẮT BUỘC là tiếng Việt có dấu hoặc tiếng Anh. 2. Tìm kiếm trước khi đặt câu hỏi. Có thể tìm kiếm từ các nguồn khác hoặc trong Forum, cột bên trái có box ""Search this group"", cột bên phải có các topics khác nhau. 3. Suy nghĩ kỹ trước khi hỏi. Cố gắng diễn đạt câu hỏi một cách mạch lạc để người đọc có thể hiểu bạn hỏi gì. 4. Khi post bài, bạn được khuyến khích sử dụng hashtags. Một số hashtags thông dụng: Bạn cũng có thể add topics cho mỗi câu hỏi. 5. Với các câu hỏi liên quan đến code, bạn được khuyến khích debug trước khi hỏi (nếu bạn không biết debug là gì, vui lòng xem lại Điều 2). Khi hỏi cần đưa code, error và dòng lệnh xảy ra error đó. Kèm theo đó là việc bạn đã thử các giải pháp nào và các lỗi đi kèm. 6. Code nên được đưa lên các trang cho phép xem code online với highlight cho từng loại ngôn ngữ. Một vài ví dụ: github, ideone, ... 7. Các thành viên trong Forum cần tôn trọng lẫn nhau, sử dụng ngôn ngữ lịch sự, chuẩn mực. Thành viên sử dụng ngôn ngữ bất lịch sự, thiếu tôn trọng người khác sẽ bị xoá vĩnh viễn khỏi Forum. 8. Các comment spam sẽ bị xoá và thành viên sẽ bị xoá khỏi group. Comment chỉ có các dấu chấm (.) sẽ bị coi là spam. Nếu bạn muốn follow một post, bạn có thể chọn chế độ ""Turn on notifications for this post"". 9. Các post không được approve có thể do một trong các lý do: * spam, * câu hỏi không liên quan đến Forum, * diễn đạt khó hiểu (kể cả việc viết tiếng Việt không dấu) * câu hỏi lặp * các nội dung vi phạm bản quyền. 10. Admins có thể xoá bất cứ bài hoặc bình luận nào mà không cần giải thích. Các bài/bình luận bị 3 report trở lên cũng sẽ bị xoá. ------------------------- Tiếp tục cập nhật. Mời các bạn bổ sung.","#python,	#tensorfow,	#caffe,	#objectdetection,	#nlp,	#jupyter_notebook,...",,,,
"[Thống kê - CheatSheet CS106]
Thống kê là một bộ môn khoa học lâu đời và có tính ứng dụng cao. Dựa trên thống kê chúng ta có thể đưa ra dự báo về biên độ giao động của nhiệt độ, độ ẩm, lượng mưa, giá cả của các hàng hoá, giá chứng khoán, … kèm theo độ tin cậy xác định.
Đồng thời dựa vào thống kê chúng ta cũng có cơ sở để đưa ra quyết định chấp nhận hay bác bỏ những giả thuyết rất thiết thực trong cuộc sống như: Trung bình chiều cao của người Việt Nam là 1m65? Giá chung cư tại Hà Nội cao hơn so với HCM? Thu nhập bình quân đầu người của Việt Nam sẽ trên 2500$ vào năm sau?
Hiểu được các qui luật thống kê và biết cách áp dụng chúng sẽ giúp bạn đưa ra được nhiều kết luận thú vị và làm củng cố thêm sự chắc chắn cho việc ra quyết định.
Đây là cheat sheet của khoá học cs106 cung cấp một số kiến thức nền quan trọng về thống kê:
- Khái niệm về biến ngẫu nhiên, ước lượng tham số, độ chệch ước lượng.
- Ước lượng trung bình, phương sai,
- Ước lượng khoảng tin cậy: chưa biết và đã biết phương sai tổng thể.
- Hai loại mắc sai lầm: loại 1 (false alarm) và loại 2 (miss alarm)
- P-value và kiểm định phi tham số.
- Các kiểm định một phía, hai phía.
- Kiểm định sai số giữa hai trung bình mẫu
- Kiểm định độ phù hợp phân phối.
- Kiểm định dấu và kiểm định xu hướng.
- Ước lượng OLS và khoảng tin cậy của các tham số ước lượng.
Link:
https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics","[Thống kê - CheatSheet CS106] Thống kê là một bộ môn khoa học lâu đời và có tính ứng dụng cao. Dựa trên thống kê chúng ta có thể đưa ra dự báo về biên độ giao động của nhiệt độ, độ ẩm, lượng mưa, giá cả của các hàng hoá, giá chứng khoán, … kèm theo độ tin cậy xác định. Đồng thời dựa vào thống kê chúng ta cũng có cơ sở để đưa ra quyết định chấp nhận hay bác bỏ những giả thuyết rất thiết thực trong cuộc sống như: Trung bình chiều cao của người Việt Nam là 1m65? Giá chung cư tại Hà Nội cao hơn so với HCM? Thu nhập bình quân đầu người của Việt Nam sẽ trên 2500$ vào năm sau? Hiểu được các qui luật thống kê và biết cách áp dụng chúng sẽ giúp bạn đưa ra được nhiều kết luận thú vị và làm củng cố thêm sự chắc chắn cho việc ra quyết định. Đây là cheat sheet của khoá học cs106 cung cấp một số kiến thức nền quan trọng về thống kê: - Khái niệm về biến ngẫu nhiên, ước lượng tham số, độ chệch ước lượng. - Ước lượng trung bình, phương sai, - Ước lượng khoảng tin cậy: chưa biết và đã biết phương sai tổng thể. - Hai loại mắc sai lầm: loại 1 (false alarm) và loại 2 (miss alarm) - P-value và kiểm định phi tham số. - Các kiểm định một phía, hai phía. - Kiểm định sai số giữa hai trung bình mẫu - Kiểm định độ phù hợp phân phối. - Kiểm định dấu và kiểm định xu hướng. - Ước lượng OLS và khoảng tin cậy của các tham số ước lượng. Link: https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics",,,"#sharing, #math",,
"[AI Share]
Tổng hợp một số tool để thiết kế và visualize kiến trúc mạng Neural Network
1. 𝑷𝒍𝒐𝒕𝑵𝒆𝒖𝒓𝒂𝒍𝑵𝒆𝒕 : sử dụng latex để vẽ mạng neural network cho paper, slide...
Github: https://github.com/HarisIqbal88/PlotNeuralNet
Demo FCN-32 : https://www.overleaf.com/project/5ee51458e88684000165e8be
Demo Holistically-Nested Edge Detection: https://www.overleaf.com/project/5ee51033e88684000165e37c
2. 𝒏𝒆𝒕2𝒗𝒊𝒔 : visualize kiến trúc neural network từ model Keras
Github: https://github.com/viscom-ulm/Net2Vis
Demo ở đây: https://viscom.net2vis.uni-ulm.de/
3. 𝒗𝒊𝒔𝒖𝒂𝒍𝒌𝒆𝒓𝒂𝒔 là một package của Python để visualize kiến trúc mạng neural network từ model Keras.
Github: https://github.com/paulgavrikov/visualkeras/
4. 𝒅𝒓𝒂𝒘_𝒄𝒐𝒏𝒗𝒏𝒆𝒕 : visualize mạng Convolution Neural Network
Github: https://github.com/gwding/draw_convnet
5. 𝑵𝑵-𝑺𝑽𝑮 : tạo các kiến trúc neural network theo các tham số như số lớp ẩn, số nơ-ron trong một lớp, khoảng cách giữa các lớp, màu sắc,...
Github: https://github.com/alexlenail/NN-SVG
Demo : http://alexlenail.me/NN-SVG/AlexNet.html
6. 𝑻𝒆𝒏𝒔𝒐𝒓𝒃𝒐𝒂𝒓𝒅 : visualize kiến trúc mạng neural network từ Tensorflow graph.
Github: https://github.com/tensorflow/tensorboard
Demo trên colab: https://colab.research.google.com/.../docs/graphs.ipynb
7. 𝑵𝒆𝒕𝒓𝒐𝒏 : Netron giúp visualize các kiến trúc mạng phức tạp trong Deep Learning , tạo ra các hình ảnh đẹp để truyền đạt rõ ràng kiến trúc của mạng và có thể khám phá các mô hình một cách chi tiết.","[AI Share] Tổng hợp một số tool để thiết kế và visualize kiến trúc mạng Neural Network 1. : sử dụng latex để vẽ mạng neural network cho paper, slide... Github: https://github.com/HarisIqbal88/PlotNeuralNet Demo FCN-32 : https://www.overleaf.com/project/5ee51458e88684000165e8be Demo Holistically-Nested Edge Detection: https://www.overleaf.com/project/5ee51033e88684000165e37c 2. 2 : visualize kiến trúc neural network từ model Keras Github: https://github.com/viscom-ulm/Net2Vis Demo ở đây: https://viscom.net2vis.uni-ulm.de/ 3. là một package của Python để visualize kiến trúc mạng neural network từ model Keras. Github: https://github.com/paulgavrikov/visualkeras/ 4. _ : visualize mạng Convolution Neural Network Github: https://github.com/gwding/draw_convnet 5. - : tạo các kiến trúc neural network theo các tham số như số lớp ẩn, số nơ-ron trong một lớp, khoảng cách giữa các lớp, màu sắc,... Github: https://github.com/alexlenail/NN-SVG Demo : http://alexlenail.me/NN-SVG/AlexNet.html 6. : visualize kiến trúc mạng neural network từ Tensorflow graph. Github: https://github.com/tensorflow/tensorboard Demo trên colab: https://colab.research.google.com/.../docs/graphs.ipynb 7. : Netron giúp visualize các kiến trúc mạng phức tạp trong Deep Learning , tạo ra các hình ảnh đẹp để truyền đạt rõ ràng kiến trúc của mạng và có thể khám phá các mô hình một cách chi tiết.",,,,,
"Xin chào mn,
Mn cho mình hỏi, mình có 1 đoạn âm thanh đọc các số từ 1 đến 9, khi đọc xong 1 số mình sẽ nghỉ khoảng 1s. Mình muốn dùng Matlab để cắt từ đoạn âm thanh này ra từng file âm thanh nhỏ của từng số. Hiện tại mình đã thử dùng so sánh ngưỡng năng lượng nhưng vẫn chưa cắt được, nó chỉ plot được vùng nào có voice vùng nào không.
Mn cho mình hỏi giải thuật nào đơn giản để dùng cho bài toán này ạ
Xin cảm ơn mn","Xin chào mn, Mn cho mình hỏi, mình có 1 đoạn âm thanh đọc các số từ 1 đến 9, khi đọc xong 1 số mình sẽ nghỉ khoảng 1s. Mình muốn dùng Matlab để cắt từ đoạn âm thanh này ra từng file âm thanh nhỏ của từng số. Hiện tại mình đã thử dùng so sánh ngưỡng năng lượng nhưng vẫn chưa cắt được, nó chỉ plot được vùng nào có voice vùng nào không. Mn cho mình hỏi giải thuật nào đơn giản để dùng cho bài toán này ạ Xin cảm ơn mn",,,,,
"Mn cho em hỏi, tại sau Ma trận hiệp phương sai cần phải giống nhau và là ma trận đơn vị ạ. Và tại sau ""cov"" là ma trận đơn vị 2 hàng, 2 cột mà không phải 3 hàng, 3 cột hoặc hơn ạ. Em cảm ơn mn nhiều.","Mn cho em hỏi, tại sau Ma trận hiệp phương sai cần phải giống nhau và là ma trận đơn vị ạ. Và tại sau ""cov"" là ma trận đơn vị 2 hàng, 2 cột mà không phải 3 hàng, 3 cột hoặc hơn ạ. Em cảm ơn mn nhiều.",,,"#Q&A, #math",,
"Mọi người ơi cho em hỏi chút ạ.
Mọi người có biết dataset ảnh nào về thời tiết ko ạ? Kiểu foggy sunny snow cloudy rain etc ý ạ. Một cái ví dụ về ảnh snow như trong hình ạ. Ko quan tâm bối cảnh như thế nào lắm chỉ quan tâm thời tiết như thế nào thôi ạ.
Em đang làm final project của lớp ML về weather classification và em đang cố tìm thêm data ảnh để nhét vô model ạ.
Nếu có ai biết thêm dataset nào ổn thì chỉ em với ạ. Em cảm ơn mọi người!!",Mọi người ơi cho em hỏi chút ạ. Mọi người có biết dataset ảnh nào về thời tiết ko ạ? Kiểu foggy sunny snow cloudy rain etc ý ạ. Một cái ví dụ về ảnh snow như trong hình ạ. Ko quan tâm bối cảnh như thế nào lắm chỉ quan tâm thời tiết như thế nào thôi ạ. Em đang làm final project của lớp ML về weather classification và em đang cố tìm thêm data ảnh để nhét vô model ạ. Nếu có ai biết thêm dataset nào ổn thì chỉ em với ạ. Em cảm ơn mọi người!!,,,,,
"A/c cho e hỏi có tool nào annotate image segmentation mà có model hỗ trợ predict annotation sẵn, mình chỉ vào chỉnh lại thôi ko ạ?","A/c cho e hỏi có tool nào annotate image segmentation mà có model hỗ trợ predict annotation sẵn, mình chỉ vào chỉnh lại thôi ko ạ?",,,,,
"Mình đang có chút vấn đề với hàm np.linalg.svd, hàm này tính gía trị riêng của ma trận. Trên python thư viện numpy chạy hàm trên với full core của máy nhưng ở c++ hay java thì nó lại chỉ chạy trên 1 core, thành ra với ma trận 1000x1000 python hết có 0.456s thì trên c++ hay java đều mất đến 3s. MÌnh thử cả opencv, numcpp hay eigen đều chỉ chạy trên 1 core, có b nào biết library nào chạy full core để tối ưu performance hoặc giải thuật nào có thể implement lại được ko ạ?","Mình đang có chút vấn đề với hàm np.linalg.svd, hàm này tính gía trị riêng của ma trận. Trên python thư viện numpy chạy hàm trên với full core của máy nhưng ở c++ hay java thì nó lại chỉ chạy trên 1 core, thành ra với ma trận 1000x1000 python hết có 0.456s thì trên c++ hay java đều mất đến 3s. MÌnh thử cả opencv, numcpp hay eigen đều chỉ chạy trên 1 core, có b nào biết library nào chạy full core để tối ưu performance hoặc giải thuật nào có thể implement lại được ko ạ?",,,,,
"Google Research đang mở đơn PhD Fellowship cho nhiều khu vực, trong đó có các trường tại Đông Nam Á. Các bạn được nhận vào chương trình sẽ được hỗ trợ tài chính tối đa 3 năm, có mentor từ Google Research, và có cơ hội thực tập tại Google Research nếu phỏng vấn tốt. Hạn nộp đơn: 11:59 PM UTC on Thursday, 22nd April 2021.
Thông tin về chương trình: https://research.google/outreach/phd-fellowship/
Đơn dành cho trường tại Đông Nam Á (có thể truy cập từ link bên trên): https://cseduapplication.withgoogle.com/applications/phdfellowshipsea2021/create-application/edit","Google Research đang mở đơn PhD Fellowship cho nhiều khu vực, trong đó có các trường tại Đông Nam Á. Các bạn được nhận vào chương trình sẽ được hỗ trợ tài chính tối đa 3 năm, có mentor từ Google Research, và có cơ hội thực tập tại Google Research nếu phỏng vấn tốt. Hạn nộp đơn: 11:59 PM UTC on Thursday, 22nd April 2021. Thông tin về chương trình: https://research.google/outreach/phd-fellowship/ Đơn dành cho trường tại Đông Nam Á (có thể truy cập từ link bên trên): https://cseduapplication.withgoogle.com/applications/phdfellowshipsea2021/create-application/edit",,,,,
Mn cho em hỏi đạo hàm như thế nào để ra được như vậy ạ. Em đã xem những đạo hàm thường gặp của anh Tiệp nhưng vẫn ko hiểu. Em cảm ơn mn nhiều.,Mn cho em hỏi đạo hàm như thế nào để ra được như vậy ạ. Em đã xem những đạo hàm thường gặp của anh Tiệp nhưng vẫn ko hiểu. Em cảm ơn mn nhiều.,,,,,
"Kính chào các bác, dạo này em đang nghiên cứu Retinanet (ở mức độ ứng dụng) nhằm khắc phục việc YOLO detect small object kém quá và tiện nghiên cứu luôn bài toán nhận diện biển báo giao thông để học tập.
Em mạnh dạn viết bài chia sẻ để giúp các bạn newbie mới học tham khảo cùng và giao lưu cùng! Mong các bác chỉ giáo và mong admin duyệt bài ạ.","Kính chào các bác, dạo này em đang nghiên cứu Retinanet (ở mức độ ứng dụng) nhằm khắc phục việc YOLO detect small object kém quá và tiện nghiên cứu luôn bài toán nhận diện biển báo giao thông để học tập. Em mạnh dạn viết bài chia sẻ để giúp các bạn newbie mới học tham khảo cùng và giao lưu cùng! Mong các bác chỉ giáo và mong admin duyệt bài ạ.",,,,,
Mình có thông tin : Vinuni đang tuyển vị trí research assistant gửi các bạn quan tâm .,Mình có thông tin : Vinuni đang tuyển vị trí research assistant gửi các bạn quan tâm .,,,,,
"[Series Pytorch]
Bài 4: Train Neural Network
Bài này mình hướng dẫn mọi người train model Neural Network (NN), Convolutional Neural Network (CNN) bằng Pytorch.","[Series Pytorch] Bài 4: Train Neural Network Bài này mình hướng dẫn mọi người train model Neural Network (NN), Convolutional Neural Network (CNN) bằng Pytorch.",,,,,
"Hi mng, mình đạng học về môn statistic cơ bản và có câu hỏi này cần sự trợ giúp đắc lực từ mng :)","Hi mng, mình đạng học về môn statistic cơ bản và có câu hỏi này cần sự trợ giúp đắc lực từ mng :)",,,"#Q&A, #math",,
"Các anh chị cho em hỏi là tại sao các perceptrons (neurons) trong cùng 1 lớp Fully Connected Layer không kết nối với nhau mà chỉ kết nối với lớp trước hoặc sau đó?

Em cám ơn.",Các anh chị cho em hỏi là tại sao các perceptrons (neurons) trong cùng 1 lớp Fully Connected Layer không kết nối với nhau mà chỉ kết nối với lớp trước hoặc sau đó? Em cám ơn.,,,,,
"Mng ơi, mình đang học về classification predictive model và cần sự trợ giúp mng để giải quyết bài toán này!!","Mng ơi, mình đang học về classification predictive model và cần sự trợ giúp mng để giải quyết bài toán này!!",,,,,
"[VinDr Lab] [Open-source data annotation for Medical AI]
Github: https://github.com/vinbigdata-medical/vindr-lab/
⭐️ Sau cuộc thi VinBigData Chest X-ray Abnormalities Detection trên Kaggle (https://www.kaggle.com/.../vinbigdata-chest-xray.../), phòng Xử lý ảnh y tế - Viện Nghiên cứu Dữ liệu lớn Vingroup quyết định mở mã nguồn của phần mềm VinDr Lab.
⭐️ VinDr Lab là một công cụ gán nhãn hình ảnh DICOM đang được team sử dụng. Các tính năng gán nhãn, quản lý được định nghĩa, phát triển từ những vấn đề thực tế gặp phải, cải thiện dần qua quá trình làm việc với nhiều bộ dữ liệu y tế.
⭐️ Tiếp sau việc mở bộ dữ liệu y tế quy mô lớn VinDr-CXR, đây là đóng góp tiếp theo của VinBigdata trong việc chia sẻ dữ liệu và công cụ phát triển AI. Qua đây, chúng mình khuyến khích cộng đồng tăng cường việc chia sẻ dữ liệu để thúc đẩy nghiên cứu và phát triển AI tại Việt Nam.
Các bạn xem và để lại feedback cho team nhé.
Chúc mọi người cuối tuần vui vẻ !","[VinDr Lab] [Open-source data annotation for Medical AI] Github: https://github.com/vinbigdata-medical/vindr-lab/ Sau cuộc thi VinBigData Chest X-ray Abnormalities Detection trên Kaggle (https://www.kaggle.com/.../vinbigdata-chest-xray.../), phòng Xử lý ảnh y tế - Viện Nghiên cứu Dữ liệu lớn Vingroup quyết định mở mã nguồn của phần mềm VinDr Lab. VinDr Lab là một công cụ gán nhãn hình ảnh DICOM đang được team sử dụng. Các tính năng gán nhãn, quản lý được định nghĩa, phát triển từ những vấn đề thực tế gặp phải, cải thiện dần qua quá trình làm việc với nhiều bộ dữ liệu y tế. Tiếp sau việc mở bộ dữ liệu y tế quy mô lớn VinDr-CXR, đây là đóng góp tiếp theo của VinBigdata trong việc chia sẻ dữ liệu và công cụ phát triển AI. Qua đây, chúng mình khuyến khích cộng đồng tăng cường việc chia sẻ dữ liệu để thúc đẩy nghiên cứu và phát triển AI tại Việt Nam. Các bạn xem và để lại feedback cho team nhé. Chúc mọi người cuối tuần vui vẻ !",,,,,
"Chào mọi người! Cho mình hỏi mức lương ở VN cho data scientist (2 năm kn) khoảng bn nhỉ? Cụ thể với fintech thì có thể deal ở mức bao nhiêu ?
Cảm ơn mọi người ạ!",Chào mọi người! Cho mình hỏi mức lương ở VN cho data scientist (2 năm kn) khoảng bn nhỉ? Cụ thể với fintech thì có thể deal ở mức bao nhiêu ? Cảm ơn mọi người ạ!,,,,,
Mn cho em hỏi chỗ tô đậm sau lại ra được như vậy ạ. Em cảm ơn mn nhiều. Mn thông cảm nếu câu hỏi này có hơi ngốc ạ.,Mn cho em hỏi chỗ tô đậm sau lại ra được như vậy ạ. Em cảm ơn mn nhiều. Mn thông cảm nếu câu hỏi này có hơi ngốc ạ.,,,,,
"Chào anh chị ạ,em có một câu hỏi là liệu mình có thể kết hợp SURF và mạng CNN để làm object recognition được ko ạ?Nếu được thì làm sao để mình input được keypoint mà mình đã lấy ra ở SURF để đưa vào CNN ạ.Em cảm ơn ạ","Chào anh chị ạ,em có một câu hỏi là liệu mình có thể kết hợp SURF và mạng CNN để làm object recognition được ko ạ?Nếu được thì làm sao để mình input được keypoint mà mình đã lấy ra ở SURF để đưa vào CNN ạ.Em cảm ơn ạ",,,,,
"Chào a/c, e đang làm project về autonomous navigation. Anh chị cho em xin một vài model về cái này để tham khảo ạ
Regards,","Chào a/c, e đang làm project về autonomous navigation. Anh chị cho em xin một vài model về cái này để tham khảo ạ Regards,",,,,,
Anh Chị cho em hỏi em làm về LSTM và GRU. optimizer em chọn là Adam. khi chạy thực nghiệm model em có để số epochs khác nhau. Theo em được biết thì số epochs là số lần mô hình duyệt qua toàn bộ điểm dữ liệu. Trường hợp của em là với epochs 100 lần 250 lần và 500 lần. thì 100 lần sẽ có kết quả tốt nhất. như vậy chưa chắc số lần epochs cao thì kết quả cho càng tốt phải không ạ. (Nếu được nhờ Anh Chị giải thích giúp em tại sao lại như vậy ạ. trước kia e cứ tưởng epochs càng nhiều thì càng tốt nhưng thời gian chạy sẽ lâu.),Anh Chị cho em hỏi em làm về LSTM và GRU. optimizer em chọn là Adam. khi chạy thực nghiệm model em có để số epochs khác nhau. Theo em được biết thì số epochs là số lần mô hình duyệt qua toàn bộ điểm dữ liệu. Trường hợp của em là với epochs 100 lần 250 lần và 500 lần. thì 100 lần sẽ có kết quả tốt nhất. như vậy chưa chắc số lần epochs cao thì kết quả cho càng tốt phải không ạ. (Nếu được nhờ Anh Chị giải thích giúp em tại sao lại như vậy ạ. trước kia e cứ tưởng epochs càng nhiều thì càng tốt nhưng thời gian chạy sẽ lâu.),,,,,
Mn cho em hỏi đoạn đánh dấu sau ra dc như vậy ạ. Em cảm ơn mn nhiều.,Mn cho em hỏi đoạn đánh dấu sau ra dc như vậy ạ. Em cảm ơn mn nhiều.,,,,,
"Chào các Anh Chị.
Em thắc mắc một xíu về mạng LSTM GRU keras. Mn giúp em với.
hiện tại code em như hình gửi kèm.
Số Layer trong code này chọn là bao nhiêu layer? mình có tính dropout là một layer không? layer Dense(1) có được tính không ạ?
theo em nghỉ là 2 layer vì không tính dropout. Nhờ mọi người giúp em với. Em cảm ơn nhiều ạ.",Chào các Anh Chị. Em thắc mắc một xíu về mạng LSTM GRU keras. Mn giúp em với. hiện tại code em như hình gửi kèm. Số Layer trong code này chọn là bao nhiêu layer? mình có tính dropout là một layer không? layer Dense(1) có được tính không ạ? theo em nghỉ là 2 layer vì không tính dropout. Nhờ mọi người giúp em với. Em cảm ơn nhiều ạ.,,,,,
"Một số models dựa trên kiến trúc transformers công bố gần đây (tên model ~ ngày xuất bản theo tháng/ngày):
CaiT (3/31)
PiT (3/30)
ViViT (3/29)
ViL (3/29)
CvT (3/29)
CrossViT (3/27)
Swin Transformer (3/25)
STAM (3/25)
DPT (3/24)
DeepViT (3/22)
CeiT (3/22)
HVT (3/19)
ConViT (3/19)
PVT (2/24)
CPVT (2/22)
T2T-ViT (1/22)
….",Một số models dựa trên kiến trúc transformers công bố gần đây (tên model ~ ngày xuất bản theo tháng/ngày): CaiT (3/31) PiT (3/30) ViViT (3/29) ViL (3/29) CvT (3/29) CrossViT (3/27) Swin Transformer (3/25) STAM (3/25) DPT (3/24) DeepViT (3/22) CeiT (3/22) HVT (3/19) ConViT (3/19) PVT (2/24) CPVT (2/22) T2T-ViT (1/22) ….,,,,,
"Trong mấy tháng đầu năm 2021, với sự nổi lên của kiến trúc Transformer trong việc giải quyết các bài toán về computer vision, đặc biệt trong tháng 3/2021 trên trang paperwiththecodes.com có tới hơn nữa 10 bài về chủ đề này. Nhưng tới 1/4 Google Brain đã công bố mạng EfficientNetV2 tại đây https://arxiv.org/pdf/2104.00298.pdf. Chứng tỏ kiến trúc convolution sẽ còn tồn tại trong thời gian tới. Nếu bạn nào quan tâm tới các cuộc thi trên Kaggle thì các đội có thành tích cao rất thường sử dụng combo mạng EfficientNetV1 để giành kết quả triển vọng chung cuộc.","Trong mấy tháng đầu năm 2021, với sự nổi lên của kiến trúc Transformer trong việc giải quyết các bài toán về computer vision, đặc biệt trong tháng 3/2021 trên trang paperwiththecodes.com có tới hơn nữa 10 bài về chủ đề này. Nhưng tới 1/4 Google Brain đã công bố mạng EfficientNetV2 tại đây https://arxiv.org/pdf/2104.00298.pdf. Chứng tỏ kiến trúc convolution sẽ còn tồn tại trong thời gian tới. Nếu bạn nào quan tâm tới các cuộc thi trên Kaggle thì các đội có thành tích cao rất thường sử dụng combo mạng EfficientNetV1 để giành kết quả triển vọng chung cuộc.",,,,,
"Gần đây có một số bạn hỏi về việc nhận diện hình ảnh, ví dụ như xác nhận khuôn mặt, xác nhận chữ kí số, hay xác nhận vân tay. Đây là chủ đề liên quan tới verification chứ không phải là classification (phân loại chó ~ mèo), object detection (nhận diện xe cộ), hay segmentation (khoanh vùng lại vết nứt trên bê tông chẳng hạn). Vậy phương pháp giải bài toán này như thế nào? Rất may đã có giải pháp gọi chung là Siamese Networks.
Sau đây mình đưa 1 số tutorials cho các bạn:
1/ PyImageSearch với code trên Keras/TensorFlow tham khảo tại đây: https://www.pyimagesearch.com/.../siamese-networks-with.../
2/ FastAI với code trên FastAI/PyTorch tham khảo tại đây: https://docs.fast.ai/tutorial.siamese.html
3/ Ngoài ra còn có các nguồn trên Medium.com, nhưng giờ mở bằng medium0.com cũng đã bị khoá, chắc các bạn cần dùng dịch vụ VPN khác. Mình dùng duyệt trình Tor để vượt qua tường lửa!
Chúc các bạn vui!
https://www.pyimagesearch.com/.../keras_siamese_networks...
Ps. Nếu các bạn muốn hiểu rõ hơn với cách giải thích rất thú vị, các bạn có thể follow twitter của anh Santiago, @@svpino; blog của anh ấy http://digest.underfitted.io. Thậm trí, các bạn có thể mua tài khoản NewLetter từ anh ấy. Anh này có rất nhiều giải thích trực quan về Gradient Descent, Batch size, overfitting~underfitting,.... cho tới lĩnh vực triển khai models trong môi trường công nghiệp/doanh nghiệp","Gần đây có một số bạn hỏi về việc nhận diện hình ảnh, ví dụ như xác nhận khuôn mặt, xác nhận chữ kí số, hay xác nhận vân tay. Đây là chủ đề liên quan tới verification chứ không phải là classification (phân loại chó ~ mèo), object detection (nhận diện xe cộ), hay segmentation (khoanh vùng lại vết nứt trên bê tông chẳng hạn). Vậy phương pháp giải bài toán này như thế nào? Rất may đã có giải pháp gọi chung là Siamese Networks. Sau đây mình đưa 1 số tutorials cho các bạn: 1/ PyImageSearch với code trên Keras/TensorFlow tham khảo tại đây: https://www.pyimagesearch.com/.../siamese-networks-with.../ 2/ FastAI với code trên FastAI/PyTorch tham khảo tại đây: https://docs.fast.ai/tutorial.siamese.html 3/ Ngoài ra còn có các nguồn trên Medium.com, nhưng giờ mở bằng medium0.com cũng đã bị khoá, chắc các bạn cần dùng dịch vụ VPN khác. Mình dùng duyệt trình Tor để vượt qua tường lửa! Chúc các bạn vui! https://www.pyimagesearch.com/.../keras_siamese_networks... Ps. Nếu các bạn muốn hiểu rõ hơn với cách giải thích rất thú vị, các bạn có thể follow twitter của anh Santiago, @@svpino; blog của anh ấy http://digest.underfitted.io. Thậm trí, các bạn có thể mua tài khoản NewLetter từ anh ấy. Anh này có rất nhiều giải thích trực quan về Gradient Descent, Batch size, overfitting~underfitting,.... cho tới lĩnh vực triển khai models trong môi trường công nghiệp/doanh nghiệp",,,,,
ANN lợi hơn so với cái giải thuật xử lý ảnh để extract ra đặc trưng ở điểm nào ạ ?,ANN lợi hơn so với cái giải thuật xử lý ảnh để extract ra đặc trưng ở điểm nào ạ ?,,,,,
Mọi người ơi cho em hỏi câu rất newbie luôn ạ. Trong dataset khách hàng cung cấp cho em để classify động vật thì cái annotation là màu đỏ hình tròn như hình ý . Em sửa lại thành cái màu xanh nước biển thì khi training có dễ hơn k ạ. Em xin cảm ơn,Mọi người ơi cho em hỏi câu rất newbie luôn ạ. Trong dataset khách hàng cung cấp cho em để classify động vật thì cái annotation là màu đỏ hình tròn như hình ý . Em sửa lại thành cái màu xanh nước biển thì khi training có dễ hơn k ạ. Em xin cảm ơn,,,,,
"Em có một bài tiểu luận về nhận dạng vân tay mà không biết bắt đầu từ đâu. Thầy có đưa ra 1 số yêu cầu là
Phải có sơ đồ thuật toán và biện luận được nó
Giải thích được thuật toán và cho biết Input và output
Mong mọi người cho em hướng tìm hiểu ạ. Em cảm ơn!",Em có một bài tiểu luận về nhận dạng vân tay mà không biết bắt đầu từ đâu. Thầy có đưa ra 1 số yêu cầu là Phải có sơ đồ thuật toán và biện luận được nó Giải thích được thuật toán và cho biết Input và output Mong mọi người cho em hướng tìm hiểu ạ. Em cảm ơn!,,,,,
"Năm 2021 thì làm gì?
Mình có tổng hợp lại 1 số các cuộc thi / competition đang diễn ra, hi vọng hữu ích với mn :D
1. Shopee - Price Match Guarantee / Determine if two products are the same by their images https://www.kaggle.com/c/shopee-product-matching/overview
2. Graph Learning - KDD2021 https://ogb.stanford.edu/kddcup2021/
3. https://compete.hexagon-ml.com/practice/competition/39/#data
Multi-dataset Time Series Anomaly Detection - KDD2021
4. FGVC Workshop - CVPR2021 https://sites.google.com/view/fgvc8/competitions
5. City Brain Challenge - KDD2021 http://www.yunqiacademy.org
6. Recsys challenge 2021: https://recsys.acm.org/recsys21/challenge/
7. Discover how data is used for the public good https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/overview
8. Shared Task on Hateful Memes at WOAH 2021 - Multi-modal analysis
https://www.workshopononlineabuse.com/cfp/shared-task-on-hateful-memes
(C) https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/503511194389274/","Năm 2021 thì làm gì? Mình có tổng hợp lại 1 số các cuộc thi / competition đang diễn ra, hi vọng hữu ích với mn :D 1. Shopee - Price Match Guarantee / Determine if two products are the same by their images https://www.kaggle.com/c/shopee-product-matching/overview 2. Graph Learning - KDD2021 https://ogb.stanford.edu/kddcup2021/ 3. https://compete.hexagon-ml.com/practice/competition/39/#data Multi-dataset Time Series Anomaly Detection - KDD2021 4. FGVC Workshop - CVPR2021 https://sites.google.com/view/fgvc8/competitions 5. City Brain Challenge - KDD2021 http://www.yunqiacademy.org 6. Recsys challenge 2021: https://recsys.acm.org/recsys21/challenge/ 7. Discover how data is used for the public good https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/overview 8. Shared Task on Hateful Memes at WOAH 2021 - Multi-modal analysis https://www.workshopononlineabuse.com/cfp/shared-task-on-hateful-memes (C) https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/503511194389274/",,,,,
"Xin chào mọi người, sau vài tháng mày mò thì e đã nắm bắt đc kiến thức về DL cơ bản rồi. E có rải CV 1 vài công ty và họ có yêu cầu về dự án đã làm, e không biết dự án họ cần ở mức độ như thế nào?? Nhân tiện mọi người có thể cho e 1 vài ý tưởng dự án về Computer vision hay NLP đc không ạ??","Xin chào mọi người, sau vài tháng mày mò thì e đã nắm bắt đc kiến thức về DL cơ bản rồi. E có rải CV 1 vài công ty và họ có yêu cầu về dự án đã làm, e không biết dự án họ cần ở mức độ như thế nào?? Nhân tiện mọi người có thể cho e 1 vài ý tưởng dự án về Computer vision hay NLP đc không ạ??",,,,,
Mọi người cho e hỏi là bây giờ làm như nào để cái x axis cái graph trên giống của graph dưới v ạ. Thanks mọi người,Mọi người cho e hỏi là bây giờ làm như nào để cái x axis cái graph trên giống của graph dưới v ạ. Thanks mọi người,,,,,
"Gần đây mình có tìm hiểu một chút về cơ chế ""Self Attention"" và mong muốn chia sẻ tới mọi người. Hy vọng sẽ bài đọc sẽ giúp ích ít nhiều cho mọi người tìm hiểu kiến thức này. Nếu có điều gì sai sót trong bài, mọi người có thể góp ý giúp mình dưới bài viết này a. Cảm ơn mọi người đã theo dõi ạ. :))","Gần đây mình có tìm hiểu một chút về cơ chế ""Self Attention"" và mong muốn chia sẻ tới mọi người. Hy vọng sẽ bài đọc sẽ giúp ích ít nhiều cho mọi người tìm hiểu kiến thức này. Nếu có điều gì sai sót trong bài, mọi người có thể góp ý giúp mình dưới bài viết này a. Cảm ơn mọi người đã theo dõi ạ. :))",,,,,
"Chào các anh chị ạ! Em thấy hiện nay AI ở Việt Nam đang rất phát triển
Bình thường mọi người có dùng ứng dụng AI nào vào công việc hay cuộc sống không ạ? Trừ kiểu Siri hay Cortana ra... Em khá tò mò về điều này, hy vọng anh chị có thể chia sẻ ạ","Chào các anh chị ạ! Em thấy hiện nay AI ở Việt Nam đang rất phát triển Bình thường mọi người có dùng ứng dụng AI nào vào công việc hay cuộc sống không ạ? Trừ kiểu Siri hay Cortana ra... Em khá tò mò về điều này, hy vọng anh chị có thể chia sẻ ạ",,,,,
"Chào mọi người!
Mình đang kế hoạch đọc lại kiến thức Machine Learning cơ bản vì hiện tại hầu như chỉ sử dụng thư viện và google nên nhiều phần chưa nắm chắc. Mình đã có kiến thức nền về toán, thống kê và lập trình Python. Mình thấy sách Machine Learning cơ bản của anh Tiệp rất hay vì tổng hợp kiến thức có hệ thống và phù hợp với tư duy học của người Việt (nếu anh Tiệp đọc được thì cảm ơn anh nhiều ạ) nên muốn ôn lại kiến thức theo sách anh Tiệp.
Tuy nhiên, mình đã hơn 10 năm không đọc sách giáo khoa tiếng Việt và các kiến thức về đại số tuyến tính và thống kê mình đang quen khái niệm tiếng Anh nên đọc sách của anh Tiệp mình phải liên tục dịch sang tiếng Anh để hiểu được về mặt ngôn ngữ. Mình dự định sẽ theo lộ trình trong sách anh Tiệp và tham khảo thêm nguồn tiếng Anh để hiểu nhanh hơn. Mọi người có thể chia sẻ các khoá học bằng tiếng Anh hoặc sách nào bằng Tiếng Anh để mình sử dụng thêm song song với sách tiếng Việt của anh Tiệp không ạ? Cảm ơn mọi người nhiều.","Chào mọi người! Mình đang kế hoạch đọc lại kiến thức Machine Learning cơ bản vì hiện tại hầu như chỉ sử dụng thư viện và google nên nhiều phần chưa nắm chắc. Mình đã có kiến thức nền về toán, thống kê và lập trình Python. Mình thấy sách Machine Learning cơ bản của anh Tiệp rất hay vì tổng hợp kiến thức có hệ thống và phù hợp với tư duy học của người Việt (nếu anh Tiệp đọc được thì cảm ơn anh nhiều ạ) nên muốn ôn lại kiến thức theo sách anh Tiệp. Tuy nhiên, mình đã hơn 10 năm không đọc sách giáo khoa tiếng Việt và các kiến thức về đại số tuyến tính và thống kê mình đang quen khái niệm tiếng Anh nên đọc sách của anh Tiệp mình phải liên tục dịch sang tiếng Anh để hiểu được về mặt ngôn ngữ. Mình dự định sẽ theo lộ trình trong sách anh Tiệp và tham khảo thêm nguồn tiếng Anh để hiểu nhanh hơn. Mọi người có thể chia sẻ các khoá học bằng tiếng Anh hoặc sách nào bằng Tiếng Anh để mình sử dụng thêm song song với sách tiếng Việt của anh Tiệp không ạ? Cảm ơn mọi người nhiều.",,,,,
"Mọi người ơi có thể cho em hỏi, khi em train YOLOv4 thì 2 chỉ số total_bbox: 4411 và rewritten_bbox: 0.00000% có nghĩa là gì vậy ạ? Và có phải chỉ số rewritten_bbox của em bằng 0% thì có nghĩa là model đang học không tốt không ạ? Em cảm ơn.","Mọi người ơi có thể cho em hỏi, khi em train YOLOv4 thì 2 chỉ số total_bbox: 4411 và rewritten_bbox: 0.00000% có nghĩa là gì vậy ạ? Và có phải chỉ số rewritten_bbox của em bằng 0% thì có nghĩa là model đang học không tốt không ạ? Em cảm ơn.",,,,,
"Chào mọi người, em có thắc mắc như sau mong mọi người giúp đỡ:
Trong không gian n chiều, có phương pháp nào để tạo ra m điểm (m > n+1) và thỏa mãn điểu kiện là khoảng cách (c) giữa các điểm là bằng nhau không ạ.
Ví dụ 
d(x0,x1) = d(x0,x2) = ... = d(x0,xm) = c
d(x1,x0) = d(x1,x2) = ... = d(x1,xm) = c
....
Trong trường hợp không thể xác định m điểm với điều kiện như trên, có phương pháp nào khác đảm bảo khoảng cách giữa các điểm m không có sự chênh lệch nhiều về mặt khoảng cách không? Em xin cảm ơn ạ.","Chào mọi người, em có thắc mắc như sau mong mọi người giúp đỡ: Trong không gian n chiều, có phương pháp nào để tạo ra m điểm (m > n+1) và thỏa mãn điểu kiện là khoảng cách (c) giữa các điểm là bằng nhau không ạ. Ví dụ d(x0,x1) = d(x0,x2) = ... = d(x0,xm) = c d(x1,x0) = d(x1,x2) = ... = d(x1,xm) = c .... Trong trường hợp không thể xác định m điểm với điều kiện như trên, có phương pháp nào khác đảm bảo khoảng cách giữa các điểm m không có sự chênh lệch nhiều về mặt khoảng cách không? Em xin cảm ơn ạ.",,,"#Q&A, #math",,
"Chào mọi người, hiện em đang gặp vấn đề về việc Pruning Parameters bằng Pytorch, mong được mọi người giải đáp.
Vấn đề của em như sau:
-, Giả sử em có 1 weight tensor đơn giản cần pruning : Z = [ a b c ]
-, Sau khi pruning 66%, thu về được 1 mask G = [ 1 0 0]
-, Lúc này sẽ có weight mới là Z(1) = Z*G = [ a 0 0]
-, Sau đó em vẫn dùng Z để retrain lại model, nhưng yêu cầu là tham số a sẽ cố định ( tức kiểu stop_gradient (a), mình sẽ không học tham số a nữa ). Và sau khi retrain thu được Z"" = [ a d e]. Lúc này em chỉ muốn áp dụng prunning lên d và e thôi. Tức những tham số nào đã được prunning lần 1 và áp dụng stop_gradient thì sẽ bị prunning nữa.
Em đang gặp vấn đề trong việc triển khai code. Cụ thể ở phần làm sao lấy ra được tất cả các tham số được giữ lại sau lần prunning thứ nhất và áp dụng stop_gradient lên chúng. Kế đó là áp dụng prunning lần 2 lên những index tham số chưa được prunning ở lần 1.
Em xin cảm ơn.","Chào mọi người, hiện em đang gặp vấn đề về việc Pruning Parameters bằng Pytorch, mong được mọi người giải đáp. Vấn đề của em như sau: -, Giả sử em có 1 weight tensor đơn giản cần pruning : Z = [ a b c ] -, Sau khi pruning 66%, thu về được 1 mask G = [ 1 0 0] -, Lúc này sẽ có weight mới là Z(1) = Z*G = [ a 0 0] -, Sau đó em vẫn dùng Z để retrain lại model, nhưng yêu cầu là tham số a sẽ cố định ( tức kiểu stop_gradient (a), mình sẽ không học tham số a nữa ). Và sau khi retrain thu được Z"" = [ a d e]. Lúc này em chỉ muốn áp dụng prunning lên d và e thôi. Tức những tham số nào đã được prunning lần 1 và áp dụng stop_gradient thì sẽ bị prunning nữa. Em đang gặp vấn đề trong việc triển khai code. Cụ thể ở phần làm sao lấy ra được tất cả các tham số được giữ lại sau lần prunning thứ nhất và áp dụng stop_gradient lên chúng. Kế đó là áp dụng prunning lần 2 lên những index tham số chưa được prunning ở lần 1. Em xin cảm ơn.",,,,,
"#SVM
Chào mọi người em đang học SVM thông qua bài viết của anh Tiep.
Link blog: https://machinelearningcoban.com/2017/04/09/smv/#-xay-dung-bai-toan-toi-uu-cho-svm
Em có 1 số thắc mắc như sau ạ :
1. ""Nhận xét quan trọng nhất là nếu ta thay vector hệ số w bởi kw và b bởi kb trong đó k là một hằng số dương thì mặt phân chia không thay đổi, tức khoảng cách từ từng điểm đến mặt phân chia không đổi,"" --> Tại sao lại khoảng cách không đổi ạ ? theo em nghĩ thì khoảng cách đó vẫn đổi khi ta nhân thêm 1 số k vào chứ ạ ?
2.
""Dựa trên tính chất này, ta có thể giả sử: yn(wTxn+b)=1"". Tại sao mình lai giả sử giá trị này bằng 1. Mà sao không là 0.1 hoặc là 0.5 ạ ? Hay giá trị nào cũng được miễn là cho triệt tiêu đi biểu thức yn(wTxn+b) ?
Em cám ơn mọi người.","Chào mọi người em đang học SVM thông qua bài viết của anh Tiep. Link blog: https://machinelearningcoban.com/2017/04/09/smv/#-xay-dung-bai-toan-toi-uu-cho-svm Em có 1 số thắc mắc như sau ạ : 1. ""Nhận xét quan trọng nhất là nếu ta thay vector hệ số w bởi kw và b bởi kb trong đó k là một hằng số dương thì mặt phân chia không thay đổi, tức khoảng cách từ từng điểm đến mặt phân chia không đổi,"" --> Tại sao lại khoảng cách không đổi ạ ? theo em nghĩ thì khoảng cách đó vẫn đổi khi ta nhân thêm 1 số k vào chứ ạ ? 2. ""Dựa trên tính chất này, ta có thể giả sử: yn(wTxn+b)=1"". Tại sao mình lai giả sử giá trị này bằng 1. Mà sao không là 0.1 hoặc là 0.5 ạ ? Hay giá trị nào cũng được miễn là cho triệt tiêu đi biểu thức yn(wTxn+b) ? Em cám ơn mọi người.",#SVM,,,,
"Chào mọi người,
Mình đang có một vấn đề cần giải quyết như sau, một khách hàng khi apply vào ngân hàng của mình, sẽ được phân vào một loại campaign để có các promotion kích thích chi tiêu tương ứng.
Nhưng theo thời gian, campaign đó có thể sẽ không còn chính xác nữa. Mình cần quan sát behaviour của khách hàng, để có thể phân loại lại khách hàng vào các campaign phù hợp hơn, hoặc nghĩ ra các campaign mới cho một nhóm khách hàng nào đó.
Mình muốn làm một bài toán segmentation khách hàng, dựa trên behaviour với các segment cụ thể. Mô hình Kmeans có vẻ được ưa chuộng, nhưng các bạn đã thực hành Kmeans rổi có thể thấy rằng, các phân cụm nhiễu khá nhiều, mình không thể đưa ra định nghĩa gì cụ thể cho nhóm khách hàng đó. Ngoài ra mình còn kỳ vọng sẽ build được score cho từng khách hàng, để đo độ fixed của khách hàng với segment, canpaign.
Bạn nào có kinh nghiệm về bài toán này có thể gợi ý một chút về mô hình mình nên dùng, hướng giải quyết cho bài toán trên được không?
Cảm ơn mọi người.","Chào mọi người, Mình đang có một vấn đề cần giải quyết như sau, một khách hàng khi apply vào ngân hàng của mình, sẽ được phân vào một loại campaign để có các promotion kích thích chi tiêu tương ứng. Nhưng theo thời gian, campaign đó có thể sẽ không còn chính xác nữa. Mình cần quan sát behaviour của khách hàng, để có thể phân loại lại khách hàng vào các campaign phù hợp hơn, hoặc nghĩ ra các campaign mới cho một nhóm khách hàng nào đó. Mình muốn làm một bài toán segmentation khách hàng, dựa trên behaviour với các segment cụ thể. Mô hình Kmeans có vẻ được ưa chuộng, nhưng các bạn đã thực hành Kmeans rổi có thể thấy rằng, các phân cụm nhiễu khá nhiều, mình không thể đưa ra định nghĩa gì cụ thể cho nhóm khách hàng đó. Ngoài ra mình còn kỳ vọng sẽ build được score cho từng khách hàng, để đo độ fixed của khách hàng với segment, canpaign. Bạn nào có kinh nghiệm về bài toán này có thể gợi ý một chút về mô hình mình nên dùng, hướng giải quyết cho bài toán trên được không? Cảm ơn mọi người.",,,,,
"Gần đây, AMD mới hỗ trợ cho cả TensorFLow và PyTorch qua package có tên là ROCm. Không biết có ai cài thử và làm benchmark khi trên models trên 2 frameworks phổ biến này chưa nhỉ? Nếu có, cho xin thông tin review nhé. Cảm ơn cả nhà","Gần đây, AMD mới hỗ trợ cho cả TensorFLow và PyTorch qua package có tên là ROCm. Không biết có ai cài thử và làm benchmark khi trên models trên 2 frameworks phổ biến này chưa nhỉ? Nếu có, cho xin thông tin review nhé. Cảm ơn cả nhà",,,,,
"trước giờ em cũng có làm 1 vài dự án về data science mà chủ yếu là trên python ạ, giờ em muốn làm trên java nhưng k biết cài thư viện thế nào do hơi yếu java, mn có ai dùng deep4j và ndarray rồi có thể chỉ em không ạ.","trước giờ em cũng có làm 1 vài dự án về data science mà chủ yếu là trên python ạ, giờ em muốn làm trên java nhưng k biết cài thư viện thế nào do hơi yếu java, mn có ai dùng deep4j và ndarray rồi có thể chỉ em không ạ.",,,,,
Có ai dùng ML Model Binding của Android Studio cho mình hỏi là có option nào tương tự như GpuDelegate.setPrecisionLossAllowed không ạ?,Có ai dùng ML Model Binding của Android Studio cho mình hỏi là có option nào tương tự như GpuDelegate.setPrecisionLossAllowed không ạ?,,,,,
"Hello mọi người 😃. Mình vừa tạo group về MLOps để mọi người có thể cùng nhau chia sẻ kiến thức và bàn luận xung quanh vấn đề ""bring ML models into production"". Ai quan tâm đến vấn đề này thì join cho vui ạ. Xin cảm ơn mọi người 😃
https://www.facebook.com/groups/240875227766690/","Hello mọi người . Mình vừa tạo group về MLOps để mọi người có thể cùng nhau chia sẻ kiến thức và bàn luận xung quanh vấn đề ""bring ML models into production"". Ai quan tâm đến vấn đề này thì join cho vui ạ. Xin cảm ơn mọi người https://www.facebook.com/groups/240875227766690/",,,,,
"Mọi người cho em hỏi công thức O(epsilon) ở đây được tính theo công thức nào và tại sao ở công thức (4) ta lại có O(epsilon^2) ạ.
Em xin cảm ơn ạ.",Mọi người cho em hỏi công thức O(epsilon) ở đây được tính theo công thức nào và tại sao ở công thức (4) ta lại có O(epsilon^2) ạ. Em xin cảm ơn ạ.,,,"#Q&A, #math",,
"[Project khóa Python for Data Science]
Mình đăng nhiều project cuối khóa Data Science ứng dụng trong tài chính, ngân hàng, thương mại điện tử rồi, nay mình đăng một số project ứng dụng trong tin sinh.
1. Ứng dụng trong phân loại mô ung thư vú dựa trên các đặc điểm hình thái.
2. Ứng dụng chẩn đoán bệnh suy giảm trí nhớ Alzheimer.","[Project khóa Python for Data Science] Mình đăng nhiều project cuối khóa Data Science ứng dụng trong tài chính, ngân hàng, thương mại điện tử rồi, nay mình đăng một số project ứng dụng trong tin sinh. 1. Ứng dụng trong phân loại mô ung thư vú dựa trên các đặc điểm hình thái. 2. Ứng dụng chẩn đoán bệnh suy giảm trí nhớ Alzheimer.",,,,,
Em đang làm đồ án về xác định lượng tưới nước cho khu vườn. Các bác có thể gợi ý cho em sử dụng dataset nào với?,Em đang làm đồ án về xác định lượng tưới nước cho khu vườn. Các bác có thể gợi ý cho em sử dụng dataset nào với?,,,,,
"Mọi người cho em hỏi chút: Em đang test thử self supervised learning sử dụng cifar10 dataset. Em định dùng resnet34 bỏ đi layer cuối để lấy features vector r dùng clustering (kmeans, kmedoids) phân cụm thành các class. Thế nhưng có việc rất lạ là khi em test trên pytorch thì kmeans + resnet34 chạy khá ổn, ít nhất nó tạo được 10 cụm cho 10 class trên cifar10 mà đa số sample mỗi cluster có cùng class, nhưng khi em chuyển qua tensorflow keras thì kmeans không phân cụm tốt được như vậy, và hơn nữa em thấy resnet trên keras và pytorch có vẻ khác biệt vài layer, VD như bên pytorch có AdaptAver/Pool layer nhưng bên keras thì không. Có ai từng test thử 1 thí nghiệm tương tự có thể giúp em giải đáp mấy vấn đề này không?","Mọi người cho em hỏi chút: Em đang test thử self supervised learning sử dụng cifar10 dataset. Em định dùng resnet34 bỏ đi layer cuối để lấy features vector r dùng clustering (kmeans, kmedoids) phân cụm thành các class. Thế nhưng có việc rất lạ là khi em test trên pytorch thì kmeans + resnet34 chạy khá ổn, ít nhất nó tạo được 10 cụm cho 10 class trên cifar10 mà đa số sample mỗi cluster có cùng class, nhưng khi em chuyển qua tensorflow keras thì kmeans không phân cụm tốt được như vậy, và hơn nữa em thấy resnet trên keras và pytorch có vẻ khác biệt vài layer, VD như bên pytorch có AdaptAver/Pool layer nhưng bên keras thì không. Có ai từng test thử 1 thí nghiệm tương tự có thể giúp em giải đáp mấy vấn đề này không?",,,,,
"#selfsupervised_learning
Xin chào tất cả các tiền bối. Mình là người mới. Mình đang gặp vấn đề về việc build 1 model bằng self supervised learning.
Mình có thể build được các model supervised or unsupervised. Nhưng mình vẫn không biết được nên bắt đầu từ đâu để làm được 1 model self-supervised. Mình được biết là nó tự gắn nhãn cho 1 phần data chưa dán nhãn. Thật sự không tìm được bất cứ một hướng dẫn nào cụ thể. Mong được mọi người chỉ giáo?",Xin chào tất cả các tiền bối. Mình là người mới. Mình đang gặp vấn đề về việc build 1 model bằng self supervised learning. Mình có thể build được các model supervised or unsupervised. Nhưng mình vẫn không biết được nên bắt đầu từ đâu để làm được 1 model self-supervised. Mình được biết là nó tự gắn nhãn cho 1 phần data chưa dán nhãn. Thật sự không tìm được bất cứ một hướng dẫn nào cụ thể. Mong được mọi người chỉ giáo?,#selfsupervised_learning,,,,
"em chào anh/ chị ạ?
em có 1 câu hỏi mong ac giải đáp.
Chẳng hạn e có một mạng segmentation và một bộ dữ liệu. giờ e muốn đưa toàn bộ dữ liệu qua mạng để segment, output là những mask(ảnh) của phần tử cần segment thì làm thế nào để em thực hiện việc đưa toàn bộ ảnh vào và lưu lại những ảnh output đó ra 1 file ạ? em cảm ơn","em chào anh/ chị ạ? em có 1 câu hỏi mong ac giải đáp. Chẳng hạn e có một mạng segmentation và một bộ dữ liệu. giờ e muốn đưa toàn bộ dữ liệu qua mạng để segment, output là những mask(ảnh) của phần tử cần segment thì làm thế nào để em thực hiện việc đưa toàn bộ ảnh vào và lưu lại những ảnh output đó ra 1 file ạ? em cảm ơn",,,,,
Ở Mỹ thì thông thường 1 loại thuốc mới sẽ mất hơn 10 năm để đi từ ý tưởng cho đến khi được phép sản xuất đại trà. Hi vọng là AI có thể giúp đẩy nhanh quá trình này trong tương lai không xa :),Ở Mỹ thì thông thường 1 loại thuốc mới sẽ mất hơn 10 năm để đi từ ý tưởng cho đến khi được phép sản xuất đại trà. Hi vọng là AI có thể giúp đẩy nhanh quá trình này trong tương lai không xa :),,,,,
"Sử dụng cross validation/nested cross validation và early stopping cùng lúc để tuning các siêu tham số và lựa chọn mô hình cho neural network?
Chào mọi người, mình/em có đọc một bài trên machinelearningmastery có nói về việc không nên dùng early stopping khi sử dụng cross validation, không biết quan điểm của mọi người như thế nào về vấn đề này ạ?","Sử dụng cross validation/nested cross validation và early stopping cùng lúc để tuning các siêu tham số và lựa chọn mô hình cho neural network? Chào mọi người, mình/em có đọc một bài trên machinelearningmastery có nói về việc không nên dùng early stopping khi sử dụng cross validation, không biết quan điểm của mọi người như thế nào về vấn đề này ạ?",,,,,
"[AI Share]
Tổng hợp các câu hỏi về một số topic về AI:
1. Top 100 câu hỏi về NLP :
https://drive.google.com/file/d/1L_9FKt10dWnzTnM0DJdQrU3Esf1f5_c5/view?fbclid=IwAR0uefXEPQWoZv0YG3YDZh4b8uH7g_OVNKKV2Vrk3ypJtjVvlcLR4cV_9wc
2. Top 100 câu hỏi về ML:
https://drive.google.com/file/d/1tIijzF1-OsRJciklC9XAg-Iyo__d2gaK/view?fbclid=IwAR3rszMons6v9da5dNlIf0VmC7v0KnEDgmN4lWYoX0xYQwr5FfVsGRIhMYM
3. Câu hỏi về Data Science, Statistic, Data Analysis, Machine Learning, Deep Learning:
https://drive.google.com/file/d/1AxwUKd4onP4ZYFctzYcAju_JfU9l8wfD/view?usp=drivesdk
4. Câu hỏi về Python:
https://drive.google.com/file/d/1yLnNMrxArqkQJIl_NpgN7NSOG4bfedv8/view?fbclid=IwAR0WVSPUqsTyC4cLSdX1I8_qT5q0W9eQtt0buPkc003NA4Nk-lmSnImid_4
Nguồn: https://www.linkedin.com/public-profile/in/stevenouri","[AI Share] Tổng hợp các câu hỏi về một số topic về AI: 1. Top 100 câu hỏi về NLP : https://drive.google.com/file/d/1L_9FKt10dWnzTnM0DJdQrU3Esf1f5_c5/view?fbclid=IwAR0uefXEPQWoZv0YG3YDZh4b8uH7g_OVNKKV2Vrk3ypJtjVvlcLR4cV_9wc 2. Top 100 câu hỏi về ML: https://drive.google.com/file/d/1tIijzF1-OsRJciklC9XAg-Iyo__d2gaK/view?fbclid=IwAR3rszMons6v9da5dNlIf0VmC7v0KnEDgmN4lWYoX0xYQwr5FfVsGRIhMYM 3. Câu hỏi về Data Science, Statistic, Data Analysis, Machine Learning, Deep Learning: https://drive.google.com/file/d/1AxwUKd4onP4ZYFctzYcAju_JfU9l8wfD/view?usp=drivesdk 4. Câu hỏi về Python: https://drive.google.com/file/d/1yLnNMrxArqkQJIl_NpgN7NSOG4bfedv8/view?fbclid=IwAR0WVSPUqsTyC4cLSdX1I8_qT5q0W9eQtt0buPkc003NA4Nk-lmSnImid_4 Nguồn: https://www.linkedin.com/public-profile/in/stevenouri",,,,,
Mời các bạn join seminar bây giờ: https://fpt-software.webex.com/fpt-software/onstage/g.php?MTID=ea41cf8ef8a7703f59c66de12fa71e795,Mời các bạn join seminar bây giờ: https://fpt-software.webex.com/fpt-software/onstage/g.php?MTID=ea41cf8ef8a7703f59c66de12fa71e795,,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 2 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 2 vào comment của post này.",,,,,
"Chào Anh, Chị, Em có thắc mắc về mô hình arima cụ thể như sau nhờ anh chị giúp em với ạ.
Em có tập train gồm 2000 điểm dữ liệu và test 544 điểm dữ liệu
em có sử dụng thư viện statsmodels để làm như hình. các hệ số p,d,q em chọn là 3,1,2 ( đây cũng là mô hình tối ưu).
Hiện tại em chưa hiểu được cách vận hành của đoạn code này khi train với 2000 điểm và đoán 544 điểm dữ liệu mới thì thư viện statsmodel sẽ tính toán ntn ạ.
Nếu có Anh, Chị nào rành về phần này giải thích giúp em với ạ. E cảm ơn rất nhiều.","Chào Anh, Chị, Em có thắc mắc về mô hình arima cụ thể như sau nhờ anh chị giúp em với ạ. Em có tập train gồm 2000 điểm dữ liệu và test 544 điểm dữ liệu em có sử dụng thư viện statsmodels để làm như hình. các hệ số p,d,q em chọn là 3,1,2 ( đây cũng là mô hình tối ưu). Hiện tại em chưa hiểu được cách vận hành của đoạn code này khi train với 2000 điểm và đoán 544 điểm dữ liệu mới thì thư viện statsmodel sẽ tính toán ntn ạ. Nếu có Anh, Chị nào rành về phần này giải thích giúp em với ạ. E cảm ơn rất nhiều.",,,,,
"Lộ diện khách mời đầu tiên sẽ tham gia 𝐖𝐞𝐛𝐢𝐧𝐚𝐫 ""𝐀𝐈 𝐈𝐍 𝐀𝐂𝐓𝐈𝐎𝐍: 𝐈𝐧𝐝𝐮𝐬𝐭𝐫𝐢𝐚𝐥 𝐮𝐬𝐞 𝐜𝐚𝐬𝐞𝐬""
🌟Moderator: Nguyễn Xuân Phong
AI Scientist at Hitachi & Mila - Quebec AI Institute
__________________
Anh Phong tốt nghiệp Carnegie Mellon University (CMU) - một trong những trường nằm top đầu thế giới về công nghệ thông tin.
Năm 2013, sau thời gian thực tập ở tập đoàn Hitachi của Nhật Bản, anh Phong được mời làm việc tại Trung tâm Nghiên cứu và Phát triển, nơi được coi là bộ não của tập đoàn. Tại tập đoàn Hitachi anh Phong trở thành một trong những chuyên gia hàng đầu về AI. Năm 24 tuổi, anh Phong trở thành một trong 50 nhà khoa học hàng đầu của tập đoàn Hitachi.
8 năm làm việc tại Nhật, anh Phong tiếp tục nghiên cứu sâu về AI và lấy bằng Tiến sĩ tại Đại học Tokyo ngành Trí tuệ Nhân tạo ứng dụng.
Năm 30 tuổi, anh Phong được tập đoàn cử sang Mila (Canada), làm việc tại viện nghiên cứu Trí tuệ Nhân tạo hàng đầu thế giới. Tại đây, anh được nghiên cứu và trao đổi trực tiếp với giáo sư Yoshua Bengio, người từng đoạt giải ACM A.M. Turing Award - giải thưởng được ví như giải Nobel trong ngành khoa học máy tính.
Tham gia Webinar với vai trò Moderator, anh Phong sẽ kết nối câu chuyện của các diễn giả, giúp người tham gia có những góc nhìn tổng quan hơn về chương trình.
___________________________
Thông tin về event:
⏰ Thời gian diễn ra sự kiện: 14:00 JST, thứ 7, ngày 27 tháng 03 năm 2021
💻 Hình thức tổ chức: Online (Qua Webex)
📥 Đăng ký tại: https://www.surveymonkey.com/r/AiInAction
📧 Mọi thắc mắc vui lòng liên hệ: Fage FPT Japan Holdings","Lộ diện khách mời đầu tiên sẽ tham gia "" : "" Moderator: Nguyễn Xuân Phong AI Scientist at Hitachi & Mila - Quebec AI Institute __________________ Anh Phong tốt nghiệp Carnegie Mellon University (CMU) - một trong những trường nằm top đầu thế giới về công nghệ thông tin. Năm 2013, sau thời gian thực tập ở tập đoàn Hitachi của Nhật Bản, anh Phong được mời làm việc tại Trung tâm Nghiên cứu và Phát triển, nơi được coi là bộ não của tập đoàn. Tại tập đoàn Hitachi anh Phong trở thành một trong những chuyên gia hàng đầu về AI. Năm 24 tuổi, anh Phong trở thành một trong 50 nhà khoa học hàng đầu của tập đoàn Hitachi. 8 năm làm việc tại Nhật, anh Phong tiếp tục nghiên cứu sâu về AI và lấy bằng Tiến sĩ tại Đại học Tokyo ngành Trí tuệ Nhân tạo ứng dụng. Năm 30 tuổi, anh Phong được tập đoàn cử sang Mila (Canada), làm việc tại viện nghiên cứu Trí tuệ Nhân tạo hàng đầu thế giới. Tại đây, anh được nghiên cứu và trao đổi trực tiếp với giáo sư Yoshua Bengio, người từng đoạt giải ACM A.M. Turing Award - giải thưởng được ví như giải Nobel trong ngành khoa học máy tính. Tham gia Webinar với vai trò Moderator, anh Phong sẽ kết nối câu chuyện của các diễn giả, giúp người tham gia có những góc nhìn tổng quan hơn về chương trình. ___________________________ Thông tin về event: ⏰ Thời gian diễn ra sự kiện: 14:00 JST, thứ 7, ngày 27 tháng 03 năm 2021 Hình thức tổ chức: Online (Qua Webex) Đăng ký tại: https://www.surveymonkey.com/r/AiInAction Mọi thắc mắc vui lòng liên hệ: Fage FPT Japan Holdings",,,,,
"chào mọi người
hiện máy mình gặp vấn đề là : code thì chạy được nhưng khi gõ from tensforflow. thì nó không hiện suggestion, báo gạch chân code như này, mình tìm cách sửa không được, Bạn nào biết nguyên nhân ko ạ?
chỉ có lệnh from tensforflow. là nó không hiện suggestion còn các lệnh khác thì bình thường.","chào mọi người hiện máy mình gặp vấn đề là : code thì chạy được nhưng khi gõ from tensforflow. thì nó không hiện suggestion, báo gạch chân code như này, mình tìm cách sửa không được, Bạn nào biết nguyên nhân ko ạ? chỉ có lệnh from tensforflow. là nó không hiện suggestion còn các lệnh khác thì bình thường.",,,,,
"Em chào anh chị ạ, hiện tại em đang làm đề án về machine learning, anh chị có thể giải đáp cho em 1 số câu hỏi không ạ?
1. Thuật toán phân lớp có nhãn là thuật toán như thế nào ạ?
2. Sử dụng 70% để learning và 30% để quan sát nghĩa là sao ạ!
Em cảm ơn anh chị rất nhiều ạ!","Em chào anh chị ạ, hiện tại em đang làm đề án về machine learning, anh chị có thể giải đáp cho em 1 số câu hỏi không ạ? 1. Thuật toán phân lớp có nhãn là thuật toán như thế nào ạ? 2. Sử dụng 70% để learning và 30% để quan sát nghĩa là sao ạ! Em cảm ơn anh chị rất nhiều ạ!",,,,,
"Chào mọi người, mình đang code lại cái Linformer, theo paper này https://arxiv.org/pdf/2006.04768.pdf. Mọi thứ có vẻ ổn đến khi mình thiết kế mask. Nếu trong Transformer thì attention score sẽ có dimension là TxT với T là độ dài của chuỗi, thì giờ với Linformer, attention score là Txk với k là một số chọn trước. Mình đang thắc mắc không biết làm cách nào để thiết kế mask cho cái này, nhất là memory mask. Mong gọi ý từ mọi người.
Cảm ơn mọi người.","Chào mọi người, mình đang code lại cái Linformer, theo paper này https://arxiv.org/pdf/2006.04768.pdf. Mọi thứ có vẻ ổn đến khi mình thiết kế mask. Nếu trong Transformer thì attention score sẽ có dimension là TxT với T là độ dài của chuỗi, thì giờ với Linformer, attention score là Txk với k là một số chọn trước. Mình đang thắc mắc không biết làm cách nào để thiết kế mask cho cái này, nhất là memory mask. Mong gọi ý từ mọi người. Cảm ơn mọi người.",,,,,
"Kính chào các bác. Em đang học về TFLite nên mạnh dạn làm clip chia sẻ cùng anh em tiếp series về triển khai model lên các thiết bị đầu cuối. Hôm nay sẽ là triển khai model lên mobile bằng TFLite nhé! Mong giúp được các bạn mới học.
Cảm ơn các bác và mong admin duyệt bài!",Kính chào các bác. Em đang học về TFLite nên mạnh dạn làm clip chia sẻ cùng anh em tiếp series về triển khai model lên các thiết bị đầu cuối. Hôm nay sẽ là triển khai model lên mobile bằng TFLite nhé! Mong giúp được các bạn mới học. Cảm ơn các bác và mong admin duyệt bài!,,,,,
"Chào các anh chị và các bạn. Em có một bài toán thế này: dự đoán giá trị của bất động sản tại tp HCM trong tương lai gần. Hiện tại bọn em đã crawl được data từ 09.03.2021, có khoảng 700k samples. Tuy nhiên khi đưa vào bài toán time-series thì em đang bị cụt đường. Cụ thể hướng giải quyết của em là thế này: em sẽ cluster data ra thành 5 clusters tùy theo features. Trong mỗi cluster, em lấy giá trị trung bình, cận trên và cận dưới để tạo thành time-series data. Bọn em không có được lịch sử giá nhà bán. Em muốn hỏi mọi người các giải quyết ạ.
Em còn suy nghĩ khác là dựa vào những thông tin có lịch sử: GDP, số lượng bất động sản bán/mua, số lượng công ty bất động sản thành lập/giải thế để hỗ trợ. Em có tìm đọc nhưng tối đường. Em cám ơn ạ.","Chào các anh chị và các bạn. Em có một bài toán thế này: dự đoán giá trị của bất động sản tại tp HCM trong tương lai gần. Hiện tại bọn em đã crawl được data từ 09.03.2021, có khoảng 700k samples. Tuy nhiên khi đưa vào bài toán time-series thì em đang bị cụt đường. Cụ thể hướng giải quyết của em là thế này: em sẽ cluster data ra thành 5 clusters tùy theo features. Trong mỗi cluster, em lấy giá trị trung bình, cận trên và cận dưới để tạo thành time-series data. Bọn em không có được lịch sử giá nhà bán. Em muốn hỏi mọi người các giải quyết ạ. Em còn suy nghĩ khác là dựa vào những thông tin có lịch sử: GDP, số lượng bất động sản bán/mua, số lượng công ty bất động sản thành lập/giải thế để hỗ trợ. Em có tìm đọc nhưng tối đường. Em cám ơn ạ.",,,,,
"Em là sinh viên chuyên ngành về điện tử - viễn thông. Mới nhập môn về ML, DL trong 2 tháng. Định hướng của em là sẽ đi theo hướng Computer Vision engineer vì em nghĩ nó liên quan đến các ứng dụng bên chuyên ngành em nhiều nhất.
Hiện em đã đọc hiểu hầu hết các thuật toán cơ bản của Machine Learning, hiểu được các cấu trúc cơ bản của DL, CNN. Nhưng vấn đề là khi đưa ra 1 bài toán và bắt lựa chọn phương pháp giải sao cho đúng thì em thấy đang hoàn toàn bị động.
Nên nhờ các anh chị tư vấn cho em phương pháp học tiếp theo làm sao để có thể hình thành được tư duy giải quyết các bài toán về Computer Vision cho hiệu quả, áp dụng được những kiến thức mình đã học vào các dự án thực tế thuần thục hơn. (Như nên theo tiếp các khóa học nào, tham khảo những tài liệu chuyên ngành nào,... ) Mục tiêu ngắn hạn nhất là trong 3 tháng tới có thể pass được 1 vị trí Fresh tại 1 công ty nào đó ạ.","Em là sinh viên chuyên ngành về điện tử - viễn thông. Mới nhập môn về ML, DL trong 2 tháng. Định hướng của em là sẽ đi theo hướng Computer Vision engineer vì em nghĩ nó liên quan đến các ứng dụng bên chuyên ngành em nhiều nhất. Hiện em đã đọc hiểu hầu hết các thuật toán cơ bản của Machine Learning, hiểu được các cấu trúc cơ bản của DL, CNN. Nhưng vấn đề là khi đưa ra 1 bài toán và bắt lựa chọn phương pháp giải sao cho đúng thì em thấy đang hoàn toàn bị động. Nên nhờ các anh chị tư vấn cho em phương pháp học tiếp theo làm sao để có thể hình thành được tư duy giải quyết các bài toán về Computer Vision cho hiệu quả, áp dụng được những kiến thức mình đã học vào các dự án thực tế thuần thục hơn. (Như nên theo tiếp các khóa học nào, tham khảo những tài liệu chuyên ngành nào,... ) Mục tiêu ngắn hạn nhất là trong 3 tháng tới có thể pass được 1 vị trí Fresh tại 1 công ty nào đó ạ.",,,,,
"[Xác suất - cs106 Cheatsheet]
Xác suất là một trong những kiến thức rất quan trọng trong AI. Một số phân phối xác suất được sử dụng để khởi tạo trọng số của mạng neural network như phân phối chuẩn, phân phối đồng nhất. Một số khác được sử dụng trong các models liên quan tới xác suất như LDA (dùng để xác định topics), Naive Bayes models (áp dụng trong các bài toán phân loại văn bản). Xác suất có điều kiện thường được áp dụng trong các mô hình dịch máy và graphical model. Batch normalization cũng được xây dựng dựa trên công thức chuẩn hoá của phân phối chuẩn. Khi khảo sát dữ liệu thì bạn cần nhìn vào đặc trưng của dữ liệu như trung bình, phương sai để xem dữ liệu có phù hợp hay không. Và vô số những kiến thức khác trong AI cần sử dụng xác suất. Cheatsheet này sẽ giúp bạn hệ thống lại những kiến thức cơ bản nhất về xác suất để việc học AI trở nên dễ dàng hơn. Nội dung bao gồm:
- Khái niệm về sự kiện và không gian mẫu.
- Công thức tổ hợp, chỉnh hợp.
- Xác suất có điều kiện, công thức Bayes và Bayes mở rộng.
- Hàm mật độ xác suất pdf, phân phối xác suất pmf và phân phối xác suất tích luỹ cdf
- Các phân phối xác suất phổ biến bao gồm: Phân phối rời rạc, phân phối liên tục.
- Các đặc trưng kỳ vọng, phương sai, moment xác suất của các phân phối xác suất tiêu biểu.
- Hiệp phương sai và hệ số tương quan.
- Xác suất chung của nhiều biến ngẫu nhiên.
Nguồn:
https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability
--------------------------------------------------------------------------------------
Note: Bạn có thể comment ở bên dưới để cùng thảo luận kĩ hơn.","[Xác suất - cs106 Cheatsheet] Xác suất là một trong những kiến thức rất quan trọng trong AI. Một số phân phối xác suất được sử dụng để khởi tạo trọng số của mạng neural network như phân phối chuẩn, phân phối đồng nhất. Một số khác được sử dụng trong các models liên quan tới xác suất như LDA (dùng để xác định topics), Naive Bayes models (áp dụng trong các bài toán phân loại văn bản). Xác suất có điều kiện thường được áp dụng trong các mô hình dịch máy và graphical model. Batch normalization cũng được xây dựng dựa trên công thức chuẩn hoá của phân phối chuẩn. Khi khảo sát dữ liệu thì bạn cần nhìn vào đặc trưng của dữ liệu như trung bình, phương sai để xem dữ liệu có phù hợp hay không. Và vô số những kiến thức khác trong AI cần sử dụng xác suất. Cheatsheet này sẽ giúp bạn hệ thống lại những kiến thức cơ bản nhất về xác suất để việc học AI trở nên dễ dàng hơn. Nội dung bao gồm: - Khái niệm về sự kiện và không gian mẫu. - Công thức tổ hợp, chỉnh hợp. - Xác suất có điều kiện, công thức Bayes và Bayes mở rộng. - Hàm mật độ xác suất pdf, phân phối xác suất pmf và phân phối xác suất tích luỹ cdf - Các phân phối xác suất phổ biến bao gồm: Phân phối rời rạc, phân phối liên tục. - Các đặc trưng kỳ vọng, phương sai, moment xác suất của các phân phối xác suất tiêu biểu. - Hiệp phương sai và hệ số tương quan. - Xác suất chung của nhiều biến ngẫu nhiên. Nguồn: https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability -------------------------------------------------------------------------------------- Note: Bạn có thể comment ở bên dưới để cùng thảo luận kĩ hơn.",,,,,
"Chào mọi người, em đang giải quyết bài toán regression, và gặp vấn đề như sau, em mong nhận được ý kiến đóng góp của mọi người.
""In a dataset, there is a feature named ""Server/Machine Type"", how would you transform/prepare this feature so that it can be used in a regression model (one only accepting float/bool as value)?
Some example of values on this feature:
Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 2950 MHz, 385570 MB RAM, 12079 MB swap
Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz (x86_64), 2500 MHz, 95717 MB RAM, 149012 MB swap
Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz (x86_64), 1300 MHz, 257868 MB RAM, 12027 MB swap
Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 3138 MHz, 772642 MB RAM, 9042 MB swap
Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 2214 MHz, 385570 MB RAM, 12090 MB swap
Core(TM) i7-6700HQ CPU @ 2.60GHz (x86_64), 2600 MHz, 40078 MB RAM, 75183 MB swap
Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz (x86_64), 1199 MHz, 257868 MB RAM, 12247 MB swap
Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 3246 MHz, 514658 MB RAM, 10770 MB swap
Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 2483 MHz, 772642 MB RAM, 8266 MB swap""","Chào mọi người, em đang giải quyết bài toán regression, và gặp vấn đề như sau, em mong nhận được ý kiến đóng góp của mọi người. ""In a dataset, there is a feature named ""Server/Machine Type"", how would you transform/prepare this feature so that it can be used in a regression model (one only accepting float/bool as value)? Some example of values on this feature: Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 2950 MHz, 385570 MB RAM, 12079 MB swap Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz (x86_64), 2500 MHz, 95717 MB RAM, 149012 MB swap Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz (x86_64), 1300 MHz, 257868 MB RAM, 12027 MB swap Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 3138 MHz, 772642 MB RAM, 9042 MB swap Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 2214 MHz, 385570 MB RAM, 12090 MB swap Core(TM) i7-6700HQ CPU @ 2.60GHz (x86_64), 2600 MHz, 40078 MB RAM, 75183 MB swap Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz (x86_64), 1199 MHz, 257868 MB RAM, 12247 MB swap Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 3246 MHz, 514658 MB RAM, 10770 MB swap Intel(R) Xeon(R) Gold 6142 CPU @ 2.60GHz (x86_64), 2483 MHz, 772642 MB RAM, 8266 MB swap""",,,,,
"chào mọi người, sắp tới em sẽ viết bài tốt nghiệp, đề bài là nhận diện xem bàn tay có đặt trên vô lăng không và có đặt đúng theo mẫu không, data đầu vào kiểu tương tự như ảnh dưới đây (góc ảnh từ trên xuống). em đang nghĩ đến việc đầu tiên tách được vô lăng và bàn tay trên vô lăng rồi tiếp đó phân loại ra xem có đặt tay đúng yêu cầu không. em có tìm hiểu qua thấy có phương pháp instance segmentation có vẻ phù hợp, các anh chị có kinh nghiệm có thể gợi ý cho em phương pháp hoặc keywords để em tìm hiểu thêm. em cảm ơn ạ","chào mọi người, sắp tới em sẽ viết bài tốt nghiệp, đề bài là nhận diện xem bàn tay có đặt trên vô lăng không và có đặt đúng theo mẫu không, data đầu vào kiểu tương tự như ảnh dưới đây (góc ảnh từ trên xuống). em đang nghĩ đến việc đầu tiên tách được vô lăng và bàn tay trên vô lăng rồi tiếp đó phân loại ra xem có đặt tay đúng yêu cầu không. em có tìm hiểu qua thấy có phương pháp instance segmentation có vẻ phù hợp, các anh chị có kinh nghiệm có thể gợi ý cho em phương pháp hoặc keywords để em tìm hiểu thêm. em cảm ơn ạ",,,,,
"Chào  tất cả  các  anh (chị).
Hiện tại em có 1 bài :  "" Xác định cộng đồng bằng cách phân tích cấu trúc đồ thị của Wikipedia (Detect Communities by Analyzing Graph Structure of Wikipedia)"".
Vấn đề của em: 
Em chưa xác định rõ đề bài yêu cầu làm  gì, mong anh chị giải  thích (cho ví dụ nếu được).
Các kiến  thức  cần có đề  làm bài này ?
Anh (chị) có tài liệu (link, bài báo...) về vấn đề này thì cho em xin để  tham khảo  với ạ.
Em cám ơn anh (chị) đã xem, chúc anh(chị) nhiều sức khỏe.","Chào tất cả các anh (chị). Hiện tại em có 1 bài : "" Xác định cộng đồng bằng cách phân tích cấu trúc đồ thị của Wikipedia (Detect Communities by Analyzing Graph Structure of Wikipedia)"". Vấn đề của em: Em chưa xác định rõ đề bài yêu cầu làm gì, mong anh chị giải thích (cho ví dụ nếu được). Các kiến thức cần có đề làm bài này ? Anh (chị) có tài liệu (link, bài báo...) về vấn đề này thì cho em xin để tham khảo với ạ. Em cám ơn anh (chị) đã xem, chúc anh(chị) nhiều sức khỏe.",,,,,
"Em mới nhập môn về ML. Hôm nay em đọc tới bài Logistic Regeression thì có 2 chỗ code ở thuật toán mà chưa thể hiểu được ý nghĩa. Nhờ anh chị giải đáp giúp em với ạ.
1. (Khung màu cam): Tại sao phải dùng method reshape(d,1) cho X[ :, 1 ]. Theo như em thấy thì vốn dĩ X[ :, 1 ] đã có shape là (d, 1) rồi. Vậy mình phải reshape nó lại 1 lần với shape tương tự nữa để làm gì.
2. (Khung hồng): Em vẫn chưa hiểu tại sao phải chờ count đếm tới mức của check_w_after (tức là 20) mới kiểm tra điều kiện tiếp theo. Và tại sao check_w_after lại set bằng 20 ạ.
Link trích dẫn bài viết và code: https://machinelearningcoban.com/2017/01/27/logisticregression/","Em mới nhập môn về ML. Hôm nay em đọc tới bài Logistic Regeression thì có 2 chỗ code ở thuật toán mà chưa thể hiểu được ý nghĩa. Nhờ anh chị giải đáp giúp em với ạ. 1. (Khung màu cam): Tại sao phải dùng method reshape(d,1) cho X[ :, 1 ]. Theo như em thấy thì vốn dĩ X[ :, 1 ] đã có shape là (d, 1) rồi. Vậy mình phải reshape nó lại 1 lần với shape tương tự nữa để làm gì. 2. (Khung hồng): Em vẫn chưa hiểu tại sao phải chờ count đếm tới mức của check_w_after (tức là 20) mới kiểm tra điều kiện tiếp theo. Và tại sao check_w_after lại set bằng 20 ạ. Link trích dẫn bài viết và code: https://machinelearningcoban.com/2017/01/27/logisticregression/",,,,,
"Em chào mọi người ạ.
Hiện tại em đang muốn học lên thạc sĩ ở khu vực phía HCM, em muốn nhờ mọi người tư vấn trường có nhiều đề tài nghiên cứu khoa học được công bố trên conference và có nhiều thầy hướng dẫn tốt ạ. Em làm nhiều bên Computer Vision trong Deep Learning nên nếu được em muốn biết trường nào mạnh về mảng xử lý ảnh với ạ.
Mong mọi người tư vấn giúp em, em xin cảm ơn ạ.","Em chào mọi người ạ. Hiện tại em đang muốn học lên thạc sĩ ở khu vực phía HCM, em muốn nhờ mọi người tư vấn trường có nhiều đề tài nghiên cứu khoa học được công bố trên conference và có nhiều thầy hướng dẫn tốt ạ. Em làm nhiều bên Computer Vision trong Deep Learning nên nếu được em muốn biết trường nào mạnh về mảng xử lý ảnh với ạ. Mong mọi người tư vấn giúp em, em xin cảm ơn ạ.",,,,,
"Xin phép ad,
Chào a/c, mn cho em hỏi làm cách nào để mình có thể xem được hàm loss mà mạng CNN dùng là hàm gì (L1, L2, CrossEntropy...) hoặc là optimizer mình dùng là cái nào (SGD, Adam...) bằng cách nhìn vào cấu trúc của model chứ không nhìn vào code ạ. E có dùng model.summary() hay model.state_dict() mà vẫn không thấy được, em cảm ơn a/c.","Xin phép ad, Chào a/c, mn cho em hỏi làm cách nào để mình có thể xem được hàm loss mà mạng CNN dùng là hàm gì (L1, L2, CrossEntropy...) hoặc là optimizer mình dùng là cái nào (SGD, Adam...) bằng cách nhìn vào cấu trúc của model chứ không nhìn vào code ạ. E có dùng model.summary() hay model.state_dict() mà vẫn không thấy được, em cảm ơn a/c.",,,,,
"Mọi người giải thích giúp em với ạ!
Tại sao lại coi lượng thay đổi giữa theta_t và theta_(t+1) là vận tốc ạ? Mặc dù, viên bi được vận tốc thúc đẩy để xuống đáy núi nhanh hơn (ví dụ trong bài 8 của anh Tiệp) nhưng em không thấy sự liên quan trong công thức ạ. Em cảm ơn ạ.","Mọi người giải thích giúp em với ạ! Tại sao lại coi lượng thay đổi giữa theta_t và theta_(t+1) là vận tốc ạ? Mặc dù, viên bi được vận tốc thúc đẩy để xuống đáy núi nhanh hơn (ví dụ trong bài 8 của anh Tiệp) nhưng em không thấy sự liên quan trong công thức ạ. Em cảm ơn ạ.",,,"#Q&A, #math",,
"[Logistic Classification - Xử lý khi có quá nhiều biến thế nào ]
Chào các bạn, mình đang xây model phân loại nợ xấu, hiện tại mình có rất nhiều biến đầu vào (2000 biến), việc binning để tính IV rất mất thời gian. Có bạn nào có cách nào xử lý nhanh công đọan này có thể chia sẻ giúp mình không. Mình cám ơn.","[Logistic Classification - Xử lý khi có quá nhiều biến thế nào ] Chào các bạn, mình đang xây model phân loại nợ xấu, hiện tại mình có rất nhiều biến đầu vào (2000 biến), việc binning để tính IV rất mất thời gian. Có bạn nào có cách nào xử lý nhanh công đọan này có thể chia sẻ giúp mình không. Mình cám ơn.",,,,,
"[Pytorch series]
Bài 3: Neural Network
Bài này mình hướng dẫn xây dựng model Neural Network, CNN bằng Pytorch. Thêm vào đó, mình nói về hook function trong Pytorch cũng như so sánh giữa __call__ và forward, nn.ReLU và F.relu.","[Pytorch series] Bài 3: Neural Network Bài này mình hướng dẫn xây dựng model Neural Network, CNN bằng Pytorch. Thêm vào đó, mình nói về hook function trong Pytorch cũng như so sánh giữa __call__ và forward, nn.ReLU và F.relu.",,,,,
"Mn cho e hỏi, e mới học về NN và code thử đoạn nhỏ. nhưng nó sai ở model fit. Lỗi đó là sao thế ạ?","Mn cho e hỏi, e mới học về NN và code thử đoạn nhỏ. nhưng nó sai ở model fit. Lỗi đó là sao thế ạ?",,,,,
Mình đã ấp ủ việc viết về Dữ liệu dạng bảng từ lâu nhưng chưa bao giờ bắt đầu được. Hôm nay mình quyết định công bố để lấy động lực viết từ từ. Hy vọng dự án nhận được sự ủng hộ của cộng đồng.,Mình đã ấp ủ việc viết về Dữ liệu dạng bảng từ lâu nhưng chưa bao giờ bắt đầu được. Hôm nay mình quyết định công bố để lấy động lực viết từ từ. Hy vọng dự án nhận được sự ủng hộ của cộng đồng.,,,,,
"Chào mọi người, gần đây mình có tham dự webinar MATH & AI conference https://www.math-ia.fr/. Mình muốn chia sẻ 3 bài nói mình thấy hữu ích: 
Self-supervised learning and uncertainty representation, by Yann LeCun (Facebook) : https://vimeo.com/522390193 
Perspectives, by Michael Jordan (Berkeley) : https://vimeo.com/522733917
Convex and non-convex optimization for machine learning, by Francis Bach (Inria) : https://vimeo.com/522392083
Cảm ơn mọi người đã đọc tin.","Chào mọi người, gần đây mình có tham dự webinar MATH & AI conference https://www.math-ia.fr/. Mình muốn chia sẻ 3 bài nói mình thấy hữu ích: Self-supervised learning and uncertainty representation, by Yann LeCun (Facebook) : https://vimeo.com/522390193 Perspectives, by Michael Jordan (Berkeley) : https://vimeo.com/522733917 Convex and non-convex optimization for machine learning, by Francis Bach (Inria) : https://vimeo.com/522392083 Cảm ơn mọi người đã đọc tin.",,,,,
"em chào mọi người, em đang làm project cần tìm 1 dataset để phân tích. dưới đây là 1 ví dụ cô đưa cho. Em sẽ dùng R để chạy và trả lời những câu hỏi tự nghĩ ra. e có đang nghĩ tới vaccine và add thêm các yếu tố cân nặng, chiều cao, giới tính, học vấn,... và sau khi phân tích sẽ trả lời những câu hỏi như là các yếu tố đó ảnh hưởng ra sao, yếu tố nào là quan trọng... nhưng bộ data như vậy thì ko tìm dc
mng đã ai làm project tương tự, có ý tưởng gì cho 1 bộ data khác ko thì hdan e với ạ. em tìm mãi mà bí quá.
thank you mng","em chào mọi người, em đang làm project cần tìm 1 dataset để phân tích. dưới đây là 1 ví dụ cô đưa cho. Em sẽ dùng R để chạy và trả lời những câu hỏi tự nghĩ ra. e có đang nghĩ tới vaccine và add thêm các yếu tố cân nặng, chiều cao, giới tính, học vấn,... và sau khi phân tích sẽ trả lời những câu hỏi như là các yếu tố đó ảnh hưởng ra sao, yếu tố nào là quan trọng... nhưng bộ data như vậy thì ko tìm dc mng đã ai làm project tương tự, có ý tưởng gì cho 1 bộ data khác ko thì hdan e với ạ. em tìm mãi mà bí quá. thank you mng",,,,,
"[Bài giảng đại chúng] GS. Vũ Hà Văn với CHUYỆN CỦA PAUL
Một trong những nhân vật nổi tiếng nhất của World Cup 2010 là anh Paul bạch tuộc. Bạch tuộc đây không phải tên hiệu xã hội đen của Paul, như Hải rồng xanh hay Hoàng sư tử. Paul là một bạch tuộc chính cống, tức là nó có 8 vòi, và bơi trong một bể nước.
Paul nổi tiếng vì nó đã đoán trúng kết quả cả 7 trận đấu của tuyển Đức. (Paul mang quốc tịch Đức, hay chính xác hơn, nó cư trú trong bể cá của một quán ăn tại Đức.) Phải nói rằng đoán trúng kết quả 3,4 trận đấu liên tiếp là điều rất ít nhà bình luận thể thao nào làm được. Duy nhất có xác xuất gần 100 phần trăm ngang với Paul, không ai khác ngoài Pele vĩ đại, người thường xuyên đoán sai hầu như tất cả các trận đấu.
Cùng nhìn lại chuyện của Paul với con mắt toán học, và thảo luận một số vấn đề liên quan qua phần trình bày của GS. Vũ Hà Văn (Giám đốc Khoa học Viện Nghiên cứu Dữ liệu lớn VinBigdata, Giáo sư Toán học ĐH Yale) tại đây.
youtube.com/watch?v=FEMJpwkizAU","[Bài giảng đại chúng] GS. Vũ Hà Văn với CHUYỆN CỦA PAUL Một trong những nhân vật nổi tiếng nhất của World Cup 2010 là anh Paul bạch tuộc. Bạch tuộc đây không phải tên hiệu xã hội đen của Paul, như Hải rồng xanh hay Hoàng sư tử. Paul là một bạch tuộc chính cống, tức là nó có 8 vòi, và bơi trong một bể nước. Paul nổi tiếng vì nó đã đoán trúng kết quả cả 7 trận đấu của tuyển Đức. (Paul mang quốc tịch Đức, hay chính xác hơn, nó cư trú trong bể cá của một quán ăn tại Đức.) Phải nói rằng đoán trúng kết quả 3,4 trận đấu liên tiếp là điều rất ít nhà bình luận thể thao nào làm được. Duy nhất có xác xuất gần 100 phần trăm ngang với Paul, không ai khác ngoài Pele vĩ đại, người thường xuyên đoán sai hầu như tất cả các trận đấu. Cùng nhìn lại chuyện của Paul với con mắt toán học, và thảo luận một số vấn đề liên quan qua phần trình bày của GS. Vũ Hà Văn (Giám đốc Khoa học Viện Nghiên cứu Dữ liệu lớn VinBigdata, Giáo sư Toán học ĐH Yale) tại đây. youtube.com/watch?v=FEMJpwkizAU",,,,,
"#Cheatsheet #RNN
[Recurrent Neural Network - Cheatsheet]
Mạng truy hồi (Recurrent Neural Network) là một kiến trúc quan trọng và được sử dụng rất nhiều trong đa dạng các lớp bài toán seq2seq, classification, prediction. Chúng ta có thể kể tới một vài ứng dụng nổi bật của nó như: Dịch máy (machine learning translation), phân tích tin tích cực/tiêu cực (sentiment analysis), dự báo chuỗi thời gian (time series), hệ thống gợi ý (recommender system), dự báo thời tiết, dự báo phiên tăng/giảm cổ phiếu,... Hiểu được RNN sẽ giúp bạn có thêm công cụ để giải quyết các vấn đề có tính ứng dụng thực tiễn cao. Vì vậy nó đóng một vai trò quan trọng và cần thiết cho data scientist. Xin giới thiệu tới các bạn một cheatsheet đáng tin cậy của khoá học cs230 tổng hợp các kiến thức về RNN một cách ngắn gọn, dễ hiểu. Qua đó bạn sẽ được ôn tập các kiến thức:
- Cấu trúc của mạng RNN
- Các dạng bài toán mà RNN có thể giải quyết: One2One, One2Many, Many2One, Many2Many
- Cơ chế lan truyền ngược trong RNN và sự phụ thuộc dài hạn.
- Các kiến trúc RNN tiêu biểu: LSTM, GRU.
- Các biến thể kiến trúc của RNN: DeepRNN, Bidirectional RNN
- Phương pháp biểu diễn từ: Word2Vec, GloVe, biểu diễn t-SNE
- Bài toán dịch máy: kiến trúc encoder-decoder, beam search, BLEU score
- Cơ chế attention
Source:
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks","[Recurrent Neural Network - Cheatsheet] Mạng truy hồi (Recurrent Neural Network) là một kiến trúc quan trọng và được sử dụng rất nhiều trong đa dạng các lớp bài toán seq2seq, classification, prediction. Chúng ta có thể kể tới một vài ứng dụng nổi bật của nó như: Dịch máy (machine learning translation), phân tích tin tích cực/tiêu cực (sentiment analysis), dự báo chuỗi thời gian (time series), hệ thống gợi ý (recommender system), dự báo thời tiết, dự báo phiên tăng/giảm cổ phiếu,... Hiểu được RNN sẽ giúp bạn có thêm công cụ để giải quyết các vấn đề có tính ứng dụng thực tiễn cao. Vì vậy nó đóng một vai trò quan trọng và cần thiết cho data scientist. Xin giới thiệu tới các bạn một cheatsheet đáng tin cậy của khoá học cs230 tổng hợp các kiến thức về RNN một cách ngắn gọn, dễ hiểu. Qua đó bạn sẽ được ôn tập các kiến thức: - Cấu trúc của mạng RNN - Các dạng bài toán mà RNN có thể giải quyết: One2One, One2Many, Many2One, Many2Many - Cơ chế lan truyền ngược trong RNN và sự phụ thuộc dài hạn. - Các kiến trúc RNN tiêu biểu: LSTM, GRU. - Các biến thể kiến trúc của RNN: DeepRNN, Bidirectional RNN - Phương pháp biểu diễn từ: Word2Vec, GloVe, biểu diễn t-SNE - Bài toán dịch máy: kiến trúc encoder-decoder, beam search, BLEU score - Cơ chế attention Source: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks",#Cheatsheet	#RNN,,,,
">>> Năm mới thì làm gì?! - Phần 2
Tiếp nối phần 1, từ 1 bài chia sẻ mình đã đăng cách đây hơn 1 tháng trước, các cuộc thi / competition về ML/AI đang được diễn ra:
Link phần 1: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/474062150667512
6. https://ogb.stanford.edu/kddcup2021/
- Large-Scale Graph ML Competition, là 1 cuộc thi về dữ liệu lớn dạng đồ thị (large-scale dataset) với 3 task con: Node classification, Link Prediction và Graph Regression, tổ chức bởi KDD2021.
- Topic: Graph Neural Network, Knowledge Graph Embedding
----------
7. https://compete.hexagon-ml.com/practice/competition/39/
- Multi-dataset Time Series Anomaly Detection, cuộc thi về xác định điểm bất thường với dữ liệu theo thời gian (time-series), tổ chức bởi KDD2021
- Topic: Time-series, Anomaly Detection
----------
8. http://www.yunqiacademy.org/
- City Brain Challenge - How many vehicles can you serve at most with a city-scale road network, cuộc thi về điều phối lưu lượng giao thông tại các điểm nút với các điều kiện cho trước, tổ chức bởi KDD2021.
----------
9. https://sites.google.com/view/fgvc8/competitions
- FGVC8 Competitions Workshop, 1 danh sách các cuộc thi về Computer Vision với nhiều chủ đề khác nhau, đồng tổ chức bởi Kaggle và CVPR2021
----------
Danh sách các bài chia sẻ liên quan khác:
- Năm mới thì làm gì?! - Phần 1: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/474062150667512
- Must-read papers on graph neural networks (GNN) https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/492800285460365/
- ml-contests: Categorical list of ML and DL related challenges/contests https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/486196196120774/
- Ebook về GNN: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/478900516850342/
- Bài toán trích rút thông tin hóa đơn với GCN: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/458796488860745/
- MC-OCR competition: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/427792028627858/
- MC-OCR solution, tác giả Nguyễn Việt Hoài: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/496813281725732/
>>> Nếu share bài viết tới các group liên quan, các bạn vui lòng dẫn link tới bài viết gốc của Vietnam AI Link Sharing Community
#Q12021 #machine_learning #deep_learning #HappyNewYear #competition #challenge #kaggle #KDD2021",">>> Năm mới thì làm gì?! - Phần 2 Tiếp nối phần 1, từ 1 bài chia sẻ mình đã đăng cách đây hơn 1 tháng trước, các cuộc thi / competition về ML/AI đang được diễn ra: Link phần 1: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/474062150667512 6. https://ogb.stanford.edu/kddcup2021/ - Large-Scale Graph ML Competition, là 1 cuộc thi về dữ liệu lớn dạng đồ thị (large-scale dataset) với 3 task con: Node classification, Link Prediction và Graph Regression, tổ chức bởi KDD2021. - Topic: Graph Neural Network, Knowledge Graph Embedding ---------- 7. https://compete.hexagon-ml.com/practice/competition/39/ - Multi-dataset Time Series Anomaly Detection, cuộc thi về xác định điểm bất thường với dữ liệu theo thời gian (time-series), tổ chức bởi KDD2021 - Topic: Time-series, Anomaly Detection ---------- 8. http://www.yunqiacademy.org/ - City Brain Challenge - How many vehicles can you serve at most with a city-scale road network, cuộc thi về điều phối lưu lượng giao thông tại các điểm nút với các điều kiện cho trước, tổ chức bởi KDD2021. ---------- 9. https://sites.google.com/view/fgvc8/competitions - FGVC8 Competitions Workshop, 1 danh sách các cuộc thi về Computer Vision với nhiều chủ đề khác nhau, đồng tổ chức bởi Kaggle và CVPR2021 ---------- Danh sách các bài chia sẻ liên quan khác: - Năm mới thì làm gì?! - Phần 1: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/474062150667512 - Must-read papers on graph neural networks (GNN) https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/492800285460365/ - ml-contests: Categorical list of ML and DL related challenges/contests https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/486196196120774/ - Ebook về GNN: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/478900516850342/ - Bài toán trích rút thông tin hóa đơn với GCN: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/458796488860745/ - MC-OCR competition: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/427792028627858/ - MC-OCR solution, tác giả Nguyễn Việt Hoài: https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/496813281725732/ >>> Nếu share bài viết tới các group liên quan, các bạn vui lòng dẫn link tới bài viết gốc của Vietnam AI Link Sharing Community",#Q12021	#machine_learning	#deep_learning	#HappyNewYear	#competition	#challenge	#kaggle	#KDD2021,,,,
"Chào cả nhà ạ
Cả nhà cho e hỏi : e đang nghiên cứu thuật toán K - Mean ứng dụng vào phân cụm khách hàng dựa trên dữ liệu thực tế .Dữ liệu gồm các trường (khách hàng, tên sản phẩm, địa chỉ). Em thực hiện phân cụm khách hàng dựa vào 2 trường tên sản phẩm và địa chỉ
Vậy Giá trị tâm cụm có ý nghĩa gì trong việc xác định tên sản phẩm và địa chỉ.
Em cảm ơn ạ.","Chào cả nhà ạ Cả nhà cho e hỏi : e đang nghiên cứu thuật toán K - Mean ứng dụng vào phân cụm khách hàng dựa trên dữ liệu thực tế .Dữ liệu gồm các trường (khách hàng, tên sản phẩm, địa chỉ). Em thực hiện phân cụm khách hàng dựa vào 2 trường tên sản phẩm và địa chỉ Vậy Giá trị tâm cụm có ý nghĩa gì trong việc xác định tên sản phẩm và địa chỉ. Em cảm ơn ạ.",,,,,
"Chào mn, em đang tìm hiểu và thử nghiệm thuật toán random forest. Hiện tại em đã cài đặt và chạy thành công trên PC bằng thư viện có sẵn trong sklearn, lưu save file bằng pickle hoặc joblib như trong ảnh
Em muốn hỏi có cách nào để load file save trên điện thoại android và thực hiện predict ko ạ ?","Chào mn, em đang tìm hiểu và thử nghiệm thuật toán random forest. Hiện tại em đã cài đặt và chạy thành công trên PC bằng thư viện có sẵn trong sklearn, lưu save file bằng pickle hoặc joblib như trong ảnh Em muốn hỏi có cách nào để load file save trên điện thoại android và thực hiện predict ko ạ ?",,,,,
"[URGENT] PhD studentship opportunity in theoretical machine learning for international students
I am looking for a strong candidate for the EUTOPIA PhD studentship opportunity. Should the candidate receive the funding, they will be hosted by my research lab at the University of Warwick, UK, and co-hosted for max 12 months at one of my project partners (most likely at Vrije Universiteit Brussel (VUB), Belgium). The research topics I am interested in are:
- algebraic topology/geometry + machine learning for explainable AI
- combinatorial optimisation + online machine learning
- theoretical RL with very large MDPs
- game theory + machine learning for strategic multi-agent learning
Since these topics require mathematical/theoretical computer science background, the candidate is expected to have good skills in at least one of the following topics: optimisation, control theory (theory side), theoretical computer science (algorithmic graph theory, complexity theory, etc). Knowledge in topological data analysis is an advantage.
*IMPORTANT*:
Eligibility requirement: The candidate should have a MSc degree by the start of the PhD.
Application deadline: 28th April 2021 (so if you are interested, please contact me asap at: long.tran-thanh@warwick.ac.uk)
More details can be found here:
https://warwick.ac.uk/global/partnerships/europe/eutopia/fundingop/eutopia_phd_call_description_2021_fv_1.pdf
A few words about Warwick: Its Computer Science department is one of the top ones in the UK. It has one of the strongest theoretical computer science research labs in Europe. Its discrete Maths centre (DIMAP) is also word-leading. My lab focuses on the theoretical side of human-agent learning (e.g., multiagent learning, strategic learning, explainability, etc.), which usually requires combinations of game theory, graph theory, algebraic topology etc. with machine learning.","[URGENT] PhD studentship opportunity in theoretical machine learning for international students I am looking for a strong candidate for the EUTOPIA PhD studentship opportunity. Should the candidate receive the funding, they will be hosted by my research lab at the University of Warwick, UK, and co-hosted for max 12 months at one of my project partners (most likely at Vrije Universiteit Brussel (VUB), Belgium). The research topics I am interested in are: - algebraic topology/geometry + machine learning for explainable AI - combinatorial optimisation + online machine learning - theoretical RL with very large MDPs - game theory + machine learning for strategic multi-agent learning Since these topics require mathematical/theoretical computer science background, the candidate is expected to have good skills in at least one of the following topics: optimisation, control theory (theory side), theoretical computer science (algorithmic graph theory, complexity theory, etc). Knowledge in topological data analysis is an advantage. *IMPORTANT*: Eligibility requirement: The candidate should have a MSc degree by the start of the PhD. Application deadline: 28th April 2021 (so if you are interested, please contact me asap at: long.tran-thanh@warwick.ac.uk) More details can be found here: https://warwick.ac.uk/global/partnerships/europe/eutopia/fundingop/eutopia_phd_call_description_2021_fv_1.pdf A few words about Warwick: Its Computer Science department is one of the top ones in the UK. It has one of the strongest theoretical computer science research labs in Europe. Its discrete Maths centre (DIMAP) is also word-leading. My lab focuses on the theoretical side of human-agent learning (e.g., multiagent learning, strategic learning, explainability, etc.), which usually requires combinations of game theory, graph theory, algebraic topology etc. with machine learning.",,,,,
"Mng ơi cho em hỏi là khi làm mấy dự án computer vision detection thì mng hay chọn algorithm nào. Có yolov4 vs faster Rcnn là mình nghĩ đến nhanh nhất
Yolov4 khi xét về ảnh thì hiệu quả kém hơn faster rcnn. Mà yolov5 lại kém hơn v4 và là hàng nhái đạo tên
Faster RCNN thì ra từ 2015 rồi, chắc bayh phải có algorithm nào đấy tốt hơn chứ nhỉ?
Edit: classification -> detection
Pic for reference","Mng ơi cho em hỏi là khi làm mấy dự án computer vision detection thì mng hay chọn algorithm nào. Có yolov4 vs faster Rcnn là mình nghĩ đến nhanh nhất Yolov4 khi xét về ảnh thì hiệu quả kém hơn faster rcnn. Mà yolov5 lại kém hơn v4 và là hàng nhái đạo tên Faster RCNN thì ra từ 2015 rồi, chắc bayh phải có algorithm nào đấy tốt hơn chứ nhỉ? Edit: classification -> detection Pic for reference",,,,,
FYI!,FYI!,,,,,
Chip Huyen is Live Now at the #FOSSASIA Summit 2021 speaking on #Machine learning for scalable applications,Chip Huyen is Live Now at the Summit 2021 speaking on learning for scalable applications,#FOSSASIA	#Machine,,,,
"Chào mọi người, mới đây thì nhóm nghiên cứu của em vừa submit một bài báo về thuật toán Newton mở rộng (GDNM, xem [2]), dùng để giải các bài toán tối ưu dạng tổng của hàm toàn phương và một hàm lồi.
Nhóm cũng đã thực hiện các thí nghiệm số trên bài toán Lasso (Lasso regression, l1 regression) và kết quả cho thấy cũng khả quan khi trong các trường hợp mà GDNM có thể giải được thì tốc độ giải đều cơ bản nhanh hơn các thuật toán nổi tiếng khác như FISTA, APG, ADMM, và một thuật toán bậc 2 mới nổi là SSNAL (xem hình về so sánh tốc độ và thuật toán chi tiết). Thuật toán đặc biệt hữu hiệu khi cần đạt độ chính xác cao.
Ở độ chính xác trung bình, thuật toán ADMM vẫn cho thấy tốc độ tốt hơn, một phần vì phần code của em cũng chưa tối giản, một phần vì đó cũng là hiện tượng trong cổ điển (ADMM vẫn nhanh hơn Newton cổ điển ở độ chính xác trung bình).
Em cũng mong muốn làm thử image processing bằng Lasso như bài báo [1] đối với FISTA (xem hình). Vì là người mới trong lĩnh vực này nên em mong muốn được trao đổi một số câu hỏi như sau ạ.
1. Có tài liệu nào mô tả chi tiết về bài toán linear inverse problems (deblurring images by Gaussian blur) không ạ? Em có đọc trong bài báo [trang 184, 1], nhưng người ta không chỉ cụ thể cách làm sao để lấy blur matrix, và inverse wavelet transform matrix.
2. Để làm thí nghiệm image processing (cụ thể là deblurring image) cho ảnh (256x256) thì cần máy cấu hình như thế nào ạ? Hiện tại máy em 16GB RAM thì máy báo tràn bộ nhớ, cần khoảng 50GB RAM. Mà em cũng không biết khi lắp ram 64GB vô chạy đối với máy CPU tốc độ 4.1 Ghz (khoảng 500 bước lặp đối với FISTA) có bị lâu (cỡ khoảng vài ngày) không ạ?
3. Thuật toán GDNM chưa thể xử lí trường hợp ma trận được cho là singular (hoặc sparse, gần singular). Mọi người cho em hỏi là các ma trận gần singular như thế này có gặp nhiều trong các bài toán Lasso thực tế không ạ?
Ngoài ra, nếu mọi người có thể xem qua và cho em nhận xét về phần code của em ở https://github.com/he9180/GDNM thì em cũng rất cảm ơn ạ!
[1] A. Beck, M. Teboulle: A fast iterative shrinkage-thresholding algorithm for linear inverse problems, SIAM journal on imaging sciences 2 (1), 183-202
link http://www.cs.cmu.edu/afs/cs/Web/People/airg/readings/2012_02_21_a_fast_iterative_shrinkage-thresholding.pdf
[2] PD. Khanh, B. Mordukhovich, VT. Phat, DB. Tran: Generalized Damped Newton Algorithms in Nonsmooth Optimization with Applications to Lasso Problems, arXiv preprint https://arxiv.org/abs/2101.10555
 — với Võ Thành Phát.","Chào mọi người, mới đây thì nhóm nghiên cứu của em vừa submit một bài báo về thuật toán Newton mở rộng (GDNM, xem [2]), dùng để giải các bài toán tối ưu dạng tổng của hàm toàn phương và một hàm lồi. Nhóm cũng đã thực hiện các thí nghiệm số trên bài toán Lasso (Lasso regression, l1 regression) và kết quả cho thấy cũng khả quan khi trong các trường hợp mà GDNM có thể giải được thì tốc độ giải đều cơ bản nhanh hơn các thuật toán nổi tiếng khác như FISTA, APG, ADMM, và một thuật toán bậc 2 mới nổi là SSNAL (xem hình về so sánh tốc độ và thuật toán chi tiết). Thuật toán đặc biệt hữu hiệu khi cần đạt độ chính xác cao. Ở độ chính xác trung bình, thuật toán ADMM vẫn cho thấy tốc độ tốt hơn, một phần vì phần code của em cũng chưa tối giản, một phần vì đó cũng là hiện tượng trong cổ điển (ADMM vẫn nhanh hơn Newton cổ điển ở độ chính xác trung bình). Em cũng mong muốn làm thử image processing bằng Lasso như bài báo [1] đối với FISTA (xem hình). Vì là người mới trong lĩnh vực này nên em mong muốn được trao đổi một số câu hỏi như sau ạ. 1. Có tài liệu nào mô tả chi tiết về bài toán linear inverse problems (deblurring images by Gaussian blur) không ạ? Em có đọc trong bài báo [trang 184, 1], nhưng người ta không chỉ cụ thể cách làm sao để lấy blur matrix, và inverse wavelet transform matrix. 2. Để làm thí nghiệm image processing (cụ thể là deblurring image) cho ảnh (256x256) thì cần máy cấu hình như thế nào ạ? Hiện tại máy em 16GB RAM thì máy báo tràn bộ nhớ, cần khoảng 50GB RAM. Mà em cũng không biết khi lắp ram 64GB vô chạy đối với máy CPU tốc độ 4.1 Ghz (khoảng 500 bước lặp đối với FISTA) có bị lâu (cỡ khoảng vài ngày) không ạ? 3. Thuật toán GDNM chưa thể xử lí trường hợp ma trận được cho là singular (hoặc sparse, gần singular). Mọi người cho em hỏi là các ma trận gần singular như thế này có gặp nhiều trong các bài toán Lasso thực tế không ạ? Ngoài ra, nếu mọi người có thể xem qua và cho em nhận xét về phần code của em ở https://github.com/he9180/GDNM thì em cũng rất cảm ơn ạ! [1] A. Beck, M. Teboulle: A fast iterative shrinkage-thresholding algorithm for linear inverse problems, SIAM journal on imaging sciences 2 (1), 183-202 link http://www.cs.cmu.edu/afs/cs/Web/People/airg/readings/2012_02_21_a_fast_iterative_shrinkage-thresholding.pdf [2] PD. Khanh, B. Mordukhovich, VT. Phat, DB. Tran: Generalized Damped Newton Algorithms in Nonsmooth Optimization with Applications to Lasso Problems, arXiv preprint https://arxiv.org/abs/2101.10555 — với Võ Thành Phát.",,,,,
"Mn cho em hỏi có cần thiết phải code đc thuật toán SVM from scratch ko? chứ em đọc bài cơ sở toán học từ bài Convex của anh Tiệp thấy khó hiểu quá à.
Thanks!",Mn cho em hỏi có cần thiết phải code đc thuật toán SVM from scratch ko? chứ em đọc bài cơ sở toán học từ bài Convex của anh Tiệp thấy khó hiểu quá à. Thanks!,,,,,
"Hey, I've created an introduction to different types of probability distributions using R programming: https://statisticsglobe.com/probability-distributions-in-r
#datascienceeducation #RStats","Hey, I've created an introduction to different types of probability distributions using R programming: https://statisticsglobe.com/probability-distributions-in-r",#datascienceeducation	#RStats,,,,
"CÁCH MÌNH ĐÃ TẠO RA PLUGIN ĐẦU TIÊN TRÊN THẾ GIỚI GIÚP YOUTUBE NHẬN DIỆN... TIẾNG KHOAI TÂY CHIÊN SAU 178 GIỜ ĂN
Hi các bạn, mình là Minh, hiện đang là một kĩ sư AI tại Sài Gòn
Đợt dịch vừa rồi, vì ở nhà rảnh rỗi và cũng ăn nhiều khoai tây chiến quá đà, nên mình đã nhận ra một vấn đề VÔ CÙNG NGHIÊM TRỌNG: tiếng khoai tây chiên... át mất tiếng video youtube của mình.
Chính vì vậy, mình đã phát triển một plugin có khả năng tự động nhận diện tiếng khoai tây chiên để kích hoạt phụ đề, giúp bạn vừa nhai khoai tây chiên, vừa không lo bỏ lỡ nội dung video
Mọi người có thể tham khảo link plugin tại đây nha: http://bit.ly/3eDI0t8

BƯỚC 01: Thu thập dữ liệu
Để dạy âm thanh giòn cho máy học, bộ sưu tập gồm 17.512 mẫu âm thanh “giòn rụm"" khác nhau đã được thử nghiệm, tương đương với 178 giờ ăn khoai tây chiên.
Điều này cũng đồng nghĩa rất nhiều Snack Khoai Tây đã “hao tốn"" trong quá trình phát triển plugin.

BƯỚC 02: Tiền xử lý
Lúc này, các âm thanh được chuẩn bị sẵn sàng cho việc xử lý.
Để đảm bảo plugin chạy được hoàn toàn ở phía người dùng trên trình duyệt sử dụng Tensorflow JS, các mẫu sử dụng phải tuân thủ các điều kiện và quy trình khắt khe:
Âm thanh đơn âm 22.050 KHz, có thể chuyển đổi nếu cần.
Sử dụng librosa trong Python để trích xuất các mẫu âm thanh để xác thực hoặc loại bỏ.
Tiếp đến, các hệ tần số Mel được tạo ra để chuyển dữ liệu thô thành thông tin nhận thức và giảm kích thước dữ liệu. Cuối cùng, dùng Meyda để trích xuất dữ liệu.
Sau các bước kể trên, plugin Crispy Subtitles có được thông tin quang phổ như hình

BƯỚC 03: Hình thành mô hình
Tiếp theo, dùng Keras và Tensorflow để thiết lập mô hình máy học phân loại âm thanh.
Trước tiên, tập dữ liệu được phân loại theo nhãn tích cực và tiêu cực, tạo nên tập hợp thử nghiệm và đào tạo bằng sklearn.
Sau rất nhiều thử nghiệm, mô hình phân lớp bao gồm 2 lớp Conv2D + Maxpooling2D, một lớp Dense (128) với kích hoạt relu và một lớp Dense (2) với kích hoạt softmax được lựa chọn.
Những âm thanh bị loại được thêm vào giữa mỗi lớp để tránh bị quá khớp.

BƯỚC 04: Huấn luyện
Cross entropy phân loại để tính toán tổn thất và sử dụng trình tối ưu hóa Adam để đào tạo mô hình với kích thước lô là 64 trong 75 epochs. Sau cùng, mô hình có độ không chính xác là 0,0831 và độ chính xác là 0,986.

BƯỚC 05: Thiết lập plugin và tích hợp mô hình
Quá trình này được thực hiện trong hệ sinh thái Python.
Sau đó, chúng được chuyển đổi bằng Tensorflow JS và cuối cùng chỉ còn kích thước dưới 5MB và được tích hợp trong plugin. Khi kích hoạt plugin, phụ đề sẽ xuất hiện.
Trong trường hợp video có phụ đề nhưng bị tắt thì máy học cũng sẽ nhận diện và tự động bật phụ đề lên sau 10s.","CÁCH MÌNH ĐÃ TẠO RA PLUGIN ĐẦU TIÊN TRÊN THẾ GIỚI GIÚP YOUTUBE NHẬN DIỆN... TIẾNG KHOAI TÂY CHIÊN SAU 178 GIỜ ĂN Hi các bạn, mình là Minh, hiện đang là một kĩ sư AI tại Sài Gòn Đợt dịch vừa rồi, vì ở nhà rảnh rỗi và cũng ăn nhiều khoai tây chiến quá đà, nên mình đã nhận ra một vấn đề VÔ CÙNG NGHIÊM TRỌNG: tiếng khoai tây chiên... át mất tiếng video youtube của mình. Chính vì vậy, mình đã phát triển một plugin có khả năng tự động nhận diện tiếng khoai tây chiên để kích hoạt phụ đề, giúp bạn vừa nhai khoai tây chiên, vừa không lo bỏ lỡ nội dung video Mọi người có thể tham khảo link plugin tại đây nha: http://bit.ly/3eDI0t8 BƯỚC 01: Thu thập dữ liệu Để dạy âm thanh giòn cho máy học, bộ sưu tập gồm 17.512 mẫu âm thanh “giòn rụm"" khác nhau đã được thử nghiệm, tương đương với 178 giờ ăn khoai tây chiên. Điều này cũng đồng nghĩa rất nhiều Snack Khoai Tây đã “hao tốn"" trong quá trình phát triển plugin. BƯỚC 02: Tiền xử lý Lúc này, các âm thanh được chuẩn bị sẵn sàng cho việc xử lý. Để đảm bảo plugin chạy được hoàn toàn ở phía người dùng trên trình duyệt sử dụng Tensorflow JS, các mẫu sử dụng phải tuân thủ các điều kiện và quy trình khắt khe: Âm thanh đơn âm 22.050 KHz, có thể chuyển đổi nếu cần. Sử dụng librosa trong Python để trích xuất các mẫu âm thanh để xác thực hoặc loại bỏ. Tiếp đến, các hệ tần số Mel được tạo ra để chuyển dữ liệu thô thành thông tin nhận thức và giảm kích thước dữ liệu. Cuối cùng, dùng Meyda để trích xuất dữ liệu. Sau các bước kể trên, plugin Crispy Subtitles có được thông tin quang phổ như hình BƯỚC 03: Hình thành mô hình Tiếp theo, dùng Keras và Tensorflow để thiết lập mô hình máy học phân loại âm thanh. Trước tiên, tập dữ liệu được phân loại theo nhãn tích cực và tiêu cực, tạo nên tập hợp thử nghiệm và đào tạo bằng sklearn. Sau rất nhiều thử nghiệm, mô hình phân lớp bao gồm 2 lớp Conv2D + Maxpooling2D, một lớp Dense (128) với kích hoạt relu và một lớp Dense (2) với kích hoạt softmax được lựa chọn. Những âm thanh bị loại được thêm vào giữa mỗi lớp để tránh bị quá khớp. BƯỚC 04: Huấn luyện Cross entropy phân loại để tính toán tổn thất và sử dụng trình tối ưu hóa Adam để đào tạo mô hình với kích thước lô là 64 trong 75 epochs. Sau cùng, mô hình có độ không chính xác là 0,0831 và độ chính xác là 0,986. BƯỚC 05: Thiết lập plugin và tích hợp mô hình Quá trình này được thực hiện trong hệ sinh thái Python. Sau đó, chúng được chuyển đổi bằng Tensorflow JS và cuối cùng chỉ còn kích thước dưới 5MB và được tích hợp trong plugin. Khi kích hoạt plugin, phụ đề sẽ xuất hiện. Trong trường hợp video có phụ đề nhưng bị tắt thì máy học cũng sẽ nhận diện và tự động bật phụ đề lên sau 10s.",,,,,
"Chào các bạn, mình có vài thắc mắc về mean, medium, mode.
Mình biết cách tính 3 giá trị đó. Tuy nhiên khi nào sử dụng thì vẫn còn mơ hồ. Các số này thường sử dụng nhất là khi điền vào các giá trị bị thiếu.
Trong đầu mình mặc định: Nếu dữ liệu cân bằng. Auto dùng mean. Nếu dữ liệu mất cân bằng, phải sử dụng median. Câu hỏi: Tại sao người ta không luôn luôn sử dụng median? Cứ xem như dữ liệu luôn mất cân bằng.
Mode sử dụng khi nào?
Nếu mình sự dụng một đại lượng mới: (mean+median)/2 làm giá trị trung bình thì có ổn không?

Cảm ơn nhiều.","Chào các bạn, mình có vài thắc mắc về mean, medium, mode. Mình biết cách tính 3 giá trị đó. Tuy nhiên khi nào sử dụng thì vẫn còn mơ hồ. Các số này thường sử dụng nhất là khi điền vào các giá trị bị thiếu. Trong đầu mình mặc định: Nếu dữ liệu cân bằng. Auto dùng mean. Nếu dữ liệu mất cân bằng, phải sử dụng median. Câu hỏi: Tại sao người ta không luôn luôn sử dụng median? Cứ xem như dữ liệu luôn mất cân bằng. Mode sử dụng khi nào? Nếu mình sự dụng một đại lượng mới: (mean+median)/2 làm giá trị trung bình thì có ổn không? Cảm ơn nhiều.",,,,,
"Chào mọi người, mình đang làm nghiên cứu sinh tiến sĩ ở DH Khoa học Công Nghệ Quốc gia Seoul (SEOULTECH). Mình đang làm nghiên cứu về hệ thống thử ảo quần áo và cần mọi người làm bài đánh giá chất lượng hệ thống.
Nhân tiện cũng xin hỏi mọi người về cách đánh giá chất lượng ảnh được sinh ra ạ. Mình đang dùng IS, FID, LPIPS, SSIM. Nhưng có vẻ khi ảnh được sinh ra một cách tự nhiên và như thật nhưng không aligned với ảnh groundtruth thì các độ đo này không đúng lắm.
Còn về ảnh tạo ra mà không có groundtruth thì ngoài cách nhờ người dùng đánh giá thì còn các nào khác không ạ?
Mong mọi người hỗ trợ đánh giá và góp ý.
Xin chân thành cảm ơn.
Link đánh giá bên dưới.","Chào mọi người, mình đang làm nghiên cứu sinh tiến sĩ ở DH Khoa học Công Nghệ Quốc gia Seoul (SEOULTECH). Mình đang làm nghiên cứu về hệ thống thử ảo quần áo và cần mọi người làm bài đánh giá chất lượng hệ thống. Nhân tiện cũng xin hỏi mọi người về cách đánh giá chất lượng ảnh được sinh ra ạ. Mình đang dùng IS, FID, LPIPS, SSIM. Nhưng có vẻ khi ảnh được sinh ra một cách tự nhiên và như thật nhưng không aligned với ảnh groundtruth thì các độ đo này không đúng lắm. Còn về ảnh tạo ra mà không có groundtruth thì ngoài cách nhờ người dùng đánh giá thì còn các nào khác không ạ? Mong mọi người hỗ trợ đánh giá và góp ý. Xin chân thành cảm ơn. Link đánh giá bên dưới.",,,,,
Hiện tại mình đang làm về gợi ý các sản phẩm thương mại điện tử. Tức là khi khách hàng đặt click chọn vào các sản phẩm thì sẽ gợi ý ra các sản phẩm tương tự. Nhưng nếu thực hiện theo phương pháp sau đây thì thấy không khả thi. Với trường hợp này mình nên dùng mô hình nào để thực hiện yêu cầu bài toán này hả mn?,Hiện tại mình đang làm về gợi ý các sản phẩm thương mại điện tử. Tức là khi khách hàng đặt click chọn vào các sản phẩm thì sẽ gợi ý ra các sản phẩm tương tự. Nhưng nếu thực hiện theo phương pháp sau đây thì thấy không khả thi. Với trường hợp này mình nên dùng mô hình nào để thực hiện yêu cầu bài toán này hả mn?,,,,,
"Xin phép Admin
Xin chào mọi người, Seri Hướng dẫn về Deeplearning đã ra được upload hoàn tất, ae nào mới tìm hiểu về Deepleaning thì vào xem nhé. Mong sẽ giúp được nhiều bạn hiểu rõ bản chất. Để lại comment về các video để chúng tôi cải thiện nhé!","Xin phép Admin Xin chào mọi người, Seri Hướng dẫn về Deeplearning đã ra được upload hoàn tất, ae nào mới tìm hiểu về Deepleaning thì vào xem nhé. Mong sẽ giúp được nhiều bạn hiểu rõ bản chất. Để lại comment về các video để chúng tôi cải thiện nhé!",,,,,
"Em chào mọi người. VinAI đang tổ chức một chuỗi research seminar với speakers là giáo sư từ các trường đại học hàng đầu trên thế giới như Stanford, UC Berkeley, UT Austin, etc. Seminar diễn ra online và mọi người chỉ cần có link Youtube là có thể theo dõi ở bất kì đâu.
Seminar gần nhất sẽ diễn ra vào 10h sáng thứ sáu tuần này, speaker là giáo sư 𝐀𝐧𝐢𝐦𝐞𝐬𝐡 𝐆𝐚𝐫𝐠 từ University of Toronto, với chủ đề ""Generalization in Reinforcement Learning and Robotics"". 
Thông tin chi tiết: https://www.facebook.com/2279355382307133/posts/2822994024609930/?d=n
Link youtube: https://www.youtube.com/watch?v=iRSh3FIs5lU
Thông tin về các seminar đã và sẽ diễn ra: https://sites.google.com/view/vinairesearchseminarseries/","Em chào mọi người. VinAI đang tổ chức một chuỗi research seminar với speakers là giáo sư từ các trường đại học hàng đầu trên thế giới như Stanford, UC Berkeley, UT Austin, etc. Seminar diễn ra online và mọi người chỉ cần có link Youtube là có thể theo dõi ở bất kì đâu. Seminar gần nhất sẽ diễn ra vào 10h sáng thứ sáu tuần này, speaker là giáo sư từ University of Toronto, với chủ đề ""Generalization in Reinforcement Learning and Robotics"". Thông tin chi tiết: https://www.facebook.com/2279355382307133/posts/2822994024609930/?d=n Link youtube: https://www.youtube.com/watch?v=iRSh3FIs5lU Thông tin về các seminar đã và sẽ diễn ra: https://sites.google.com/view/vinairesearchseminarseries/",,,,,
"Chào Anh, Chị,
Cho em hỏi về Arima model một xíu ạ.
Nhờ các Anh, Chị chuyên về phần này giúp em với ạ.
E có đọc một bài viết của anh Khánh về cách kiểm định chuỗi dừng
Em thắc mắc một số vấn đề như:
Hệ số chặn là gì
P-value là gì, công thức toán học của nó ntn, tại sao p-value <0.05 thì bác bỏ giả thuyết H0 và chấp nhận H1 và kết luận chuỗi dừng.
Hệ số L của phương trình đặc trưng nghĩa là gì.
Phi ^ và SE trong công thức DF em gửi kèm theo trong hình là gì ạ.
Rất mong được các anh chị giải đáp giúp. E rất cảm ơn, mong admin duyệt bài giúp em ạ.","Chào Anh, Chị, Cho em hỏi về Arima model một xíu ạ. Nhờ các Anh, Chị chuyên về phần này giúp em với ạ. E có đọc một bài viết của anh Khánh về cách kiểm định chuỗi dừng Em thắc mắc một số vấn đề như: Hệ số chặn là gì P-value là gì, công thức toán học của nó ntn, tại sao p-value <0.05 thì bác bỏ giả thuyết H0 và chấp nhận H1 và kết luận chuỗi dừng. Hệ số L của phương trình đặc trưng nghĩa là gì. Phi ^ và SE trong công thức DF em gửi kèm theo trong hình là gì ạ. Rất mong được các anh chị giải đáp giúp. E rất cảm ơn, mong admin duyệt bài giúp em ạ.",,,"#Q&A, #math",,
"Các anh chị cho em hỏi. Trainset và testset của em được tạo bằng MATLAB với kiểu dữ liệu double (theo em tìm hiểu thì nó tương đương với float64). Tuy nhiên model của em đã predict tập testset thành kiểu single (tương đương với float32). Vậy có cách nào để keras luôn trả về float64 không, do em nghĩ điều này có ảnh hưởng đến chất lượng mô hình.","Các anh chị cho em hỏi. Trainset và testset của em được tạo bằng MATLAB với kiểu dữ liệu double (theo em tìm hiểu thì nó tương đương với float64). Tuy nhiên model của em đã predict tập testset thành kiểu single (tương đương với float32). Vậy có cách nào để keras luôn trả về float64 không, do em nghĩ điều này có ảnh hưởng đến chất lượng mô hình.",,,,,
"Chào mọi người, hiện tại thì em tốt nghiệp bách khoa tphcm cũng được gần 3 năm, ngành điện - điện tử, gpa lúc trước k khá lắm do lúc học có nhiều chuyện tác động. Giờ em muốn học lên cao học ngành Data science thì nên học chương trình thạc sỹ DS của KHTN hay học tiep tại BK HCM ạ","Chào mọi người, hiện tại thì em tốt nghiệp bách khoa tphcm cũng được gần 3 năm, ngành điện - điện tử, gpa lúc trước k khá lắm do lúc học có nhiều chuyện tác động. Giờ em muốn học lên cao học ngành Data science thì nên học chương trình thạc sỹ DS của KHTN hay học tiep tại BK HCM ạ",,,,,
"#Cheatsheet #CNN
[Convolutional Neural Network - Cheatsheet]
- Kể từ khi được Yan Lecun ứng dụng vào năm 1998 trong LeNet. Chúng ta không thể phủ nhận rằng CNN đã thay thế hoàn toàn các mô hình học có giám sát truyền thống như SVM, kNN, Logistic,.... trong các tác vụ học có giám sát của thị giác máy tính.
- Cheatsheet này sẽ giúp các bạn hệ thống lại các khái niệm chính của tích chập bao gồm: Bộ lọc, bước nhảy, padding, hàm kích hoạt, Vùng nhận thức,.... các layer Convol2D, Maxpooling, Fully Connected và cách chúng kết hợp trong một mạng CNN điển hình.
- Phân biệt và hiểu sâu các lớp bài toán trong xử lý ảnh như: Image Classification, Object Localization, Object Detection; các bài toán liên quan tới face như Face Verification, Face Recognition.
- Lớp các mô hình cơ bản trong Object Detection: YOLO, Faster R-CNN. Các bước thực thi và sự khác biệt giữa chúng.
- Lớp các bài toán sinh ảnh nghệ thuật như Neural Style Transfer và GAN kèm theo những diễn giải ngắn gọn và dễ hiểu.
Link tham khảo:
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks","[Convolutional Neural Network - Cheatsheet] - Kể từ khi được Yan Lecun ứng dụng vào năm 1998 trong LeNet. Chúng ta không thể phủ nhận rằng CNN đã thay thế hoàn toàn các mô hình học có giám sát truyền thống như SVM, kNN, Logistic,.... trong các tác vụ học có giám sát của thị giác máy tính. - Cheatsheet này sẽ giúp các bạn hệ thống lại các khái niệm chính của tích chập bao gồm: Bộ lọc, bước nhảy, padding, hàm kích hoạt, Vùng nhận thức,.... các layer Convol2D, Maxpooling, Fully Connected và cách chúng kết hợp trong một mạng CNN điển hình. - Phân biệt và hiểu sâu các lớp bài toán trong xử lý ảnh như: Image Classification, Object Localization, Object Detection; các bài toán liên quan tới face như Face Verification, Face Recognition. - Lớp các mô hình cơ bản trong Object Detection: YOLO, Faster R-CNN. Các bước thực thi và sự khác biệt giữa chúng. - Lớp các bài toán sinh ảnh nghệ thuật như Neural Style Transfer và GAN kèm theo những diễn giải ngắn gọn và dễ hiểu. Link tham khảo: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks",#Cheatsheet	#CNN,,,,
"Chào các anh chị,
Hiện tại em đang nghiên cứu về LSTM trong time-series database. A chị cho em hỏi là làm sao để mình lựa chọn được số layer phù hợp cho mô hình mạng của mình được ạ. Cụ thể dựa trên tiêu chí gì ạ. Em có đọc các bài báo về LSTM nhưng phần này các tác giả ít đề cập đến ạ. E rất mong tìm được anh chị nào có hướng nghiên cứu cùng với em để giúp em thêm kiến thức phần này với ạ.","Chào các anh chị, Hiện tại em đang nghiên cứu về LSTM trong time-series database. A chị cho em hỏi là làm sao để mình lựa chọn được số layer phù hợp cho mô hình mạng của mình được ạ. Cụ thể dựa trên tiêu chí gì ạ. Em có đọc các bài báo về LSTM nhưng phần này các tác giả ít đề cập đến ạ. E rất mong tìm được anh chị nào có hướng nghiên cứu cùng với em để giúp em thêm kiến thức phần này với ạ.",,,,,
"Super Fast and Accurate 3D Object Detection based on LiDAR
Fast training, Fast inference
An Anchor-free approach
No Non-Max-Suppression
Model:
ResNet-based Keypoint Feature Pyramid Network (KFPN)
Inputs: Bird-eye-view (BEV) maps that are encoded by height, intensity, and density of 3D LiDAR point clouds.
Outputs: 7 degrees of freedom (7-DOF) of objects: 3D center (cx, cy, cz), 3D dimension (l, w, h), and heading angle (θ) in radians of bounding boxes.
Objects: Cars, Pedestrians, Cyclists.
The pre-trained model has been released in the repo.

Source code: https://github.com/maudzung/Super-Fast-Accurate-3D-Object-Detection

------------------------------------------------------
Hướng tiếp cận ở trên có thể mở rộng với bài toán xác định vị trí, kích thước, và phân loại các hộp xoay (rorated boxes) chứa vật thể trong hình ảnh 2D RGB thông thường. Dùng YOLO cũng có thể giải quyết được bài toán trên nhưng có một số hạn chế như: Việc tìm anchors phù hợp không đơn giản (vì phải quan tâm tới cả góc xoay), việc tính toán IoU giữa các hộp xoay cũng rất ""tốn kém"", chậm vì không vectorize được. Điều đó dẫn tới hệ quả là training model lâu hơn, tốc độ inference chậm hơn rất nhiều so với hướng tiếp cận anchor-free. Ngoài ra, model sử dụng ở trong hướng tiếp cận anchor-free ở trên cũng nhỏ (ít parameters) hơn nhiều so với YOLO models, điều đó giúp tránh overfitting khi tập dữ liệu không quá lớn.","Super Fast and Accurate 3D Object Detection based on LiDAR Fast training, Fast inference An Anchor-free approach No Non-Max-Suppression Model: ResNet-based Keypoint Feature Pyramid Network (KFPN) Inputs: Bird-eye-view (BEV) maps that are encoded by height, intensity, and density of 3D LiDAR point clouds. Outputs: 7 degrees of freedom (7-DOF) of objects: 3D center (cx, cy, cz), 3D dimension (l, w, h), and heading angle (θ) in radians of bounding boxes. Objects: Cars, Pedestrians, Cyclists. The pre-trained model has been released in the repo. Source code: https://github.com/maudzung/Super-Fast-Accurate-3D-Object-Detection ------------------------------------------------------ Hướng tiếp cận ở trên có thể mở rộng với bài toán xác định vị trí, kích thước, và phân loại các hộp xoay (rorated boxes) chứa vật thể trong hình ảnh 2D RGB thông thường. Dùng YOLO cũng có thể giải quyết được bài toán trên nhưng có một số hạn chế như: Việc tìm anchors phù hợp không đơn giản (vì phải quan tâm tới cả góc xoay), việc tính toán IoU giữa các hộp xoay cũng rất ""tốn kém"", chậm vì không vectorize được. Điều đó dẫn tới hệ quả là training model lâu hơn, tốc độ inference chậm hơn rất nhiều so với hướng tiếp cận anchor-free. Ngoài ra, model sử dụng ở trong hướng tiếp cận anchor-free ở trên cũng nhỏ (ít parameters) hơn nhiều so với YOLO models, điều đó giúp tránh overfitting khi tập dữ liệu không quá lớn.",,,,,
"Link chi tiết: https://www.reddit.com/r/MachineLearning/comments/m3li7t/p_pytorch_lightning_hydra_tensorboard_project/
Chào mọi người,
chán việc phải mở project cũ lên để copy code qua project mới, mình có viết một cái Github template để nhanh chóng bắt đầu một dự án Deep Learning cỡ nhỏ sử dụng Pytorch Lightning, Hydra và TensorBoard. Các bạn vui lòng xem thêm chi tiết và góp ý tại link reddit ở trên.
Hi vọng sẽ có ích cho các bạn.","Link chi tiết: https://www.reddit.com/r/MachineLearning/comments/m3li7t/p_pytorch_lightning_hydra_tensorboard_project/ Chào mọi người, chán việc phải mở project cũ lên để copy code qua project mới, mình có viết một cái Github template để nhanh chóng bắt đầu một dự án Deep Learning cỡ nhỏ sử dụng Pytorch Lightning, Hydra và TensorBoard. Các bạn vui lòng xem thêm chi tiết và góp ý tại link reddit ở trên. Hi vọng sẽ có ích cho các bạn.",,,,,
"Chào mọi người. DeepMind trao học bổng Thạc Sỹ và Tiến Sỹ tại một số trường đại học (bao gồm tại Anh, Mĩ, Canada, Pháp) cho học sinh từ các nhóm ít đại diện trong AI. Trong định nghĩa ""ít đại diện"" có bao gồm các khu vực lãnh thổ chưa có nhiều người làm việc trong AI, và thường thì Đông Nam Á, bao gồm Việt Nam, được tính vào khu vực này.
Nếu các bạn muốn tìm hiểu thêm thông tin, thứ 4 ngày 24/3 lúc 19:00 - 20:00 giờ Việt Nam, DeepMind có một buổi giới thiệu trực tuyến để các bạn có thể nghe từ các sinh viên được nhận học bổng và DeepMinders về việc theo đuổi cao học, quá trình phát triển nghề nghiệp và trải nghiệm chương trình học bổng.
Thông tin về học bổng: https://deepmind.com/scholarships
Đăng ký tham gia buổi giới thiệu học bổng: https://eventsonair.withgoogle.com/events/deepmind-hiai","Chào mọi người. DeepMind trao học bổng Thạc Sỹ và Tiến Sỹ tại một số trường đại học (bao gồm tại Anh, Mĩ, Canada, Pháp) cho học sinh từ các nhóm ít đại diện trong AI. Trong định nghĩa ""ít đại diện"" có bao gồm các khu vực lãnh thổ chưa có nhiều người làm việc trong AI, và thường thì Đông Nam Á, bao gồm Việt Nam, được tính vào khu vực này. Nếu các bạn muốn tìm hiểu thêm thông tin, thứ 4 ngày 24/3 lúc 19:00 - 20:00 giờ Việt Nam, DeepMind có một buổi giới thiệu trực tuyến để các bạn có thể nghe từ các sinh viên được nhận học bổng và DeepMinders về việc theo đuổi cao học, quá trình phát triển nghề nghiệp và trải nghiệm chương trình học bổng. Thông tin về học bổng: https://deepmind.com/scholarships Đăng ký tham gia buổi giới thiệu học bổng: https://eventsonair.withgoogle.com/events/deepmind-hiai",,,,,
"Xin chào mợi người, chuyện là em đang học môn thay thế tốt nghiệp về machine learning cụ thể hơn là Azure machine learning, anh chị nào là master thì có thể hướng dẫn em được không ạ. Em cảm ơn.","Xin chào mợi người, chuyện là em đang học môn thay thế tốt nghiệp về machine learning cụ thể hơn là Azure machine learning, anh chị nào là master thì có thể hướng dẫn em được không ạ. Em cảm ơn.",,,,,
"Chào mọi người, mình có thắc mắc về 2 job data scientist và ML engineer. Mong mọi người giải đáp!
Khi mình còn học trên giảng đường, mình thường xuyên ước mong trở thành data scientist với lý do: ít code, chủ yếu phân tích dựa vào data (có thể dùng tool Weka hoặc scikit learn ... kết hợp với toán xác suất, visualize ra đoán), từ đó mà các nhà chiến lược trong công ty sẽ thay đổi cách làm bla bla.
Vài tháng sau, định nghĩa đó lại thay đổi. Data science giờ đây chính là việc crawl dữ liệu, tiền xử lý, xây dựng sẵn pipeline, dùng thư viện có sẵn apply model.
Vài tháng kế, mình lại thấy sẽ có 1 nhóm người làm công việc crawl, pre-process, phân tích sơ bộ dữ liệu về nhiễu, loại bỏ feature bị dư thừa, nhóm này mình gọi là data mining/data analysis. Và 1 nhóm người sẽ xây dựng pipepline (từ lúc input vào đến output, xây luôn inference), nhóm này mình gọi là ML engineer. Và cuối cùng, những người được gọi là data scientist sẽ làm những việc như: R&D model dựa vào paper, tunning param, thử một số mô hình mới.

Và khi mình theo dõi một số tuyển dụng của Việt Nam và nước ngoài, mình nhận thấy rất nhiều công ty xem Data scientist và ML engineer là như nhau. Mình muốn hỏi những ai đã làm ML engineer và Data scientist rằng, liệu mình đã định nghĩa công việc phải làm của DS và ML engineer đúng chưa? 

.ps: Hiện tại mình đang làm fresher ML và vô cùng thắc mắc thật sự DS làm những gì. Mình đang có ý định switch qua DS làm thử.
Thank you.","Chào mọi người, mình có thắc mắc về 2 job data scientist và ML engineer. Mong mọi người giải đáp! Khi mình còn học trên giảng đường, mình thường xuyên ước mong trở thành data scientist với lý do: ít code, chủ yếu phân tích dựa vào data (có thể dùng tool Weka hoặc scikit learn ... kết hợp với toán xác suất, visualize ra đoán), từ đó mà các nhà chiến lược trong công ty sẽ thay đổi cách làm bla bla. Vài tháng sau, định nghĩa đó lại thay đổi. Data science giờ đây chính là việc crawl dữ liệu, tiền xử lý, xây dựng sẵn pipeline, dùng thư viện có sẵn apply model. Vài tháng kế, mình lại thấy sẽ có 1 nhóm người làm công việc crawl, pre-process, phân tích sơ bộ dữ liệu về nhiễu, loại bỏ feature bị dư thừa, nhóm này mình gọi là data mining/data analysis. Và 1 nhóm người sẽ xây dựng pipepline (từ lúc input vào đến output, xây luôn inference), nhóm này mình gọi là ML engineer. Và cuối cùng, những người được gọi là data scientist sẽ làm những việc như: R&D model dựa vào paper, tunning param, thử một số mô hình mới. Và khi mình theo dõi một số tuyển dụng của Việt Nam và nước ngoài, mình nhận thấy rất nhiều công ty xem Data scientist và ML engineer là như nhau. Mình muốn hỏi những ai đã làm ML engineer và Data scientist rằng, liệu mình đã định nghĩa công việc phải làm của DS và ML engineer đúng chưa? .ps: Hiện tại mình đang làm fresher ML và vô cùng thắc mắc thật sự DS làm những gì. Mình đang có ý định switch qua DS làm thử. Thank you.",,,,,
"Mn đã đọc qua code của anh tiệp phần K-means Clustering: Simple Applications. Mình không hiểu code chỗ này như nào. Mn giúp mình với
link: https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/kmeans/display_network.py",Mn đã đọc qua code của anh tiệp phần K-means Clustering: Simple Applications. Mình không hiểu code chỗ này như nào. Mn giúp mình với link: https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/kmeans/display_network.py,,,,,
"[Pytorch series]
Bài 2: Autograd
Bài này ở phần đầu mình có giải thích Machine Learning là gì? Mình viết dễ hiểu theo ngôn ngữ đời thường, mọi người ai chưa biết có thể vào đọc 🤩
Phần sau mình nói kĩ về cơ chế backpropagation trong Pytorch cũng như các kĩ thuật, tips liên quan.
Phần cuối mình áp dụng autograd vào bài toán Linear Regression.
Ps: Mọi người thấy mình vẫn ra bài đều không, tại mình đang dùng 1000% để viết. Mọi người thấy bài hay cho mình xin 1 like, 1 share thôi 😍😍😍","[Pytorch series] Bài 2: Autograd Bài này ở phần đầu mình có giải thích Machine Learning là gì? Mình viết dễ hiểu theo ngôn ngữ đời thường, mọi người ai chưa biết có thể vào đọc Phần sau mình nói kĩ về cơ chế backpropagation trong Pytorch cũng như các kĩ thuật, tips liên quan. Phần cuối mình áp dụng autograd vào bài toán Linear Regression. Ps: Mọi người thấy mình vẫn ra bài đều không, tại mình đang dùng 1000% để viết. Mọi người thấy bài hay cho mình xin 1 like, 1 share thôi",,,,,
"Xin phép Admin
Xin chào mọi người, để giúp những NEWBIE về Deeplearning, hôm nay mình ra tiếp video Hiểu và Làm Deeplearning P2, mong rằng sẽ giúp được nhiều anh em hiểu được bản chất và tự xây dựng được các mô hình Deep của riêng mình. Hãy để lại phản hồi cho chúng tôi để chỉnh sửa trong các video tiếp theo nhé, cảm ơn Mọi người!","Xin phép Admin Xin chào mọi người, để giúp những NEWBIE về Deeplearning, hôm nay mình ra tiếp video Hiểu và Làm Deeplearning P2, mong rằng sẽ giúp được nhiều anh em hiểu được bản chất và tự xây dựng được các mô hình Deep của riêng mình. Hãy để lại phản hồi cho chúng tôi để chỉnh sửa trong các video tiếp theo nhé, cảm ơn Mọi người!",,,,,
"[Knowledge Distillation]
Là ý tưởng được sáng tạo bởi nhà khoa học nổi tiếng Geoffrey Hinton. Knowledge Distillation cho phép mô hình có kích thước lớn hơn (teacher model) có thể chuyển giao tri thức lại mô hình có kích thước nhỏ hơn (student model).
Phương pháp đã giúp cải thiện phần lớn độ chính xác của các mô hình trong computer vision và NLP. Cùng tìm hiểu nội dung một cách dễ hiểu, ngắn gọn và trực quan qua bài viết kỳ này.
https://phamdinhkhanh.github.io/2021/03/13/KnownledgeDistillation.html","[Knowledge Distillation] Là ý tưởng được sáng tạo bởi nhà khoa học nổi tiếng Geoffrey Hinton. Knowledge Distillation cho phép mô hình có kích thước lớn hơn (teacher model) có thể chuyển giao tri thức lại mô hình có kích thước nhỏ hơn (student model). Phương pháp đã giúp cải thiện phần lớn độ chính xác của các mô hình trong computer vision và NLP. Cùng tìm hiểu nội dung một cách dễ hiểu, ngắn gọn và trực quan qua bài viết kỳ này. https://phamdinhkhanh.github.io/2021/03/13/KnownledgeDistillation.html",,,,,
"Chào mọi người, có thể chỉ em về Extreme gradient boost được không ạ, ví dụ như kiểu code XGBoost cho Iris flower ạ.","Chào mọi người, có thể chỉ em về Extreme gradient boost được không ạ, ví dụ như kiểu code XGBoost cho Iris flower ạ.",,,,,
"xin phép ad ah.
anh chị cho hỏi em định mua con card này. dùng để test thôi.
thì có được ko ạ",xin phép ad ah. anh chị cho hỏi em định mua con card này. dùng để test thôi. thì có được ko ạ,,,,,
"Chào mọi người, em đang học ml, và đây là một bài tập của em về giá sản phẩm.
https://www.kaggle.com/c/mercari-price-suggestion-challenge
Cho em hỏi là việc dự báo giá sản phẩm có dùng nhiều thuộc tính như là brand, category, conditon, .... Vậy thì với mỗi thuộc tính mình sẽ dùng một model, rồi kết hợp lại với nhau ạ? Nếu thế thì kết hợp như thế nào, còn nếu không thì hướng giải quyết ra sao ạ?","Chào mọi người, em đang học ml, và đây là một bài tập của em về giá sản phẩm. https://www.kaggle.com/c/mercari-price-suggestion-challenge Cho em hỏi là việc dự báo giá sản phẩm có dùng nhiều thuộc tính như là brand, category, conditon, .... Vậy thì với mỗi thuộc tính mình sẽ dùng một model, rồi kết hợp lại với nhau ạ? Nếu thế thì kết hợp như thế nào, còn nếu không thì hướng giải quyết ra sao ạ?",,,,,
"Kính chào các bác, hôm nay em đang học về TensorflowJS để triển khai model lên trình duyệt nên làm clip chia sẻ cho các bạn mới học.
Mong ad duyệt bài và mong các bác chỉ giáo thêm! Em xin cảm ơn!","Kính chào các bác, hôm nay em đang học về TensorflowJS để triển khai model lên trình duyệt nên làm clip chia sẻ cho các bạn mới học. Mong ad duyệt bài và mong các bác chỉ giáo thêm! Em xin cảm ơn!",,,,,
"Chào mọi người, mình đang nghiên cứu về depth image. Khi mình train thì gặp vấn đề như sau: các disparity map luôn xuất hiện vết đen sọc trắng tương ứng với vị trị bầu trời hoặc khu vực xa xôi. Mình không biết có thuật ngữ nào dành cho vấn đề này hay không? Mong xin được ý kiến mọi người.
Cảm ơn.","Chào mọi người, mình đang nghiên cứu về depth image. Khi mình train thì gặp vấn đề như sau: các disparity map luôn xuất hiện vết đen sọc trắng tương ứng với vị trị bầu trời hoặc khu vực xa xôi. Mình không biết có thuật ngữ nào dành cho vấn đề này hay không? Mong xin được ý kiến mọi người. Cảm ơn.",,,,,
"Dữ liệu là cực kỳ quan trọng đối với các bạn làm về học máy, phân tích dữ liệu. Do đó, thu thập dữ liệu từ internet là một kỹ năng cần thiết đối với các bạn.
Nhân dịp nghỉ lễ, mình có dành thời gian viết 1 bài chia sẻ nho nhỏ về cách thu thập dữ liệu tin tức cực kỳ đơn giản. Hi vọng bài viết này giúp ích được mọi người trong quá trình học tập & làm việc.
https://nguyenvanhieu.vn/thu-thap-du-lieu-cua-trang-tin-tuc-bat-ky/","Dữ liệu là cực kỳ quan trọng đối với các bạn làm về học máy, phân tích dữ liệu. Do đó, thu thập dữ liệu từ internet là một kỹ năng cần thiết đối với các bạn. Nhân dịp nghỉ lễ, mình có dành thời gian viết 1 bài chia sẻ nho nhỏ về cách thu thập dữ liệu tin tức cực kỳ đơn giản. Hi vọng bài viết này giúp ích được mọi người trong quá trình học tập & làm việc. https://nguyenvanhieu.vn/thu-thap-du-lieu-cua-trang-tin-tuc-bat-ky/",,,,,
"Xin phép Admin, Chào các bạn!
21/3 này bọn mình có mở 1 dự án Sử dụng Python để phát hiện tư thế người có trong ảnh hoặc video.
Dự án sẽ được chia thành 03 giai đoạn tương đương với 3 phần công việc chính:
Giới thiệu về cách làm việc với Pytorch và Openpose
Xây dựng ứng dụng nhận biết tư thế với đầu vào là 1 ảnh
Mở rộng ứng dụng nhận biết tư thế với 1 video
Để tham gia dự án yêu cầu đã biết lập trình Python cơ bản.
Thời gian diễn ra trong 3 tuần (21/3 start)
Mentor hướng dẫn dự án: Anh Nguyễn Công Thành - CEO HachiX Nhật Bản (Được tạp chí Nikkei Asia vinh danh là người Việt Nam có đóng góp lớn trong sự phát triển AI tại Nhật Bản)
Dự án là môi trường học tập rất tốt cho các bạn đang học Python, sử dụng Python như là công cụ chính để làm việc trong các lĩnh vực như AI, ML...
Dự án có hạn chế số lượng thành viên và có chính sách tài trợ 100% chi phí tham gia.
Nếu có hứng thú hãy inbox, giới thiệu bản thân để đăng ký với mình nhé.","Xin phép Admin, Chào các bạn! 21/3 này bọn mình có mở 1 dự án Sử dụng Python để phát hiện tư thế người có trong ảnh hoặc video. Dự án sẽ được chia thành 03 giai đoạn tương đương với 3 phần công việc chính: Giới thiệu về cách làm việc với Pytorch và Openpose Xây dựng ứng dụng nhận biết tư thế với đầu vào là 1 ảnh Mở rộng ứng dụng nhận biết tư thế với 1 video Để tham gia dự án yêu cầu đã biết lập trình Python cơ bản. Thời gian diễn ra trong 3 tuần (21/3 start) Mentor hướng dẫn dự án: Anh Nguyễn Công Thành - CEO HachiX Nhật Bản (Được tạp chí Nikkei Asia vinh danh là người Việt Nam có đóng góp lớn trong sự phát triển AI tại Nhật Bản) Dự án là môi trường học tập rất tốt cho các bạn đang học Python, sử dụng Python như là công cụ chính để làm việc trong các lĩnh vực như AI, ML... Dự án có hạn chế số lượng thành viên và có chính sách tài trợ 100% chi phí tham gia. Nếu có hứng thú hãy inbox, giới thiệu bản thân để đăng ký với mình nhé.",,,,,
"Chào mọi người,
Mình đang thắc mắc một chút về word embedding. Khi mình muốn emdedding input thì mình sẽ thường sử dụng một model pre-trained vì theo mình nghĩ thì nó có nhiều dữ liệu học hơn nên sẽ biểu diễn tốt hơn, tuy nhiên nếu như model pre-trained không biểu diễn được hết input (có một số word input mà model pre-trained không biểu diễn được) thì phải làm như nào? (nên tự build model embdding hay phải làm cách nào?)
Liệu mình có thể sử dụng kết hợp được model mình tự build với một model pre-trained không ?","Chào mọi người, Mình đang thắc mắc một chút về word embedding. Khi mình muốn emdedding input thì mình sẽ thường sử dụng một model pre-trained vì theo mình nghĩ thì nó có nhiều dữ liệu học hơn nên sẽ biểu diễn tốt hơn, tuy nhiên nếu như model pre-trained không biểu diễn được hết input (có một số word input mà model pre-trained không biểu diễn được) thì phải làm như nào? (nên tự build model embdding hay phải làm cách nào?) Liệu mình có thể sử dụng kết hợp được model mình tự build với một model pre-trained không ?",,,,,
"Chào mọi người, e mới bắt đầu nghiên cứu nhân diện text trong ảnh. Nếu ảnh rỏ nét thì bình thường, còn nếu ảnh bị mờ thì không đọc được hoặc lỗi. Thử Rescaling thì kết quả chỉ cải thiện 1 chút.
Có giải pháp nào khác phục không ạ
demo bằng: pytesseract, opencv-python","Chào mọi người, e mới bắt đầu nghiên cứu nhân diện text trong ảnh. Nếu ảnh rỏ nét thì bình thường, còn nếu ảnh bị mờ thì không đọc được hoặc lỗi. Thử Rescaling thì kết quả chỉ cải thiện 1 chút. Có giải pháp nào khác phục không ạ demo bằng: pytesseract, opencv-python",,,,,
"Chào cả nhà
Mình cần tìm giải pháp Camera AI gì đó cho các nhu cầu sau cho chuỗi nhà Hàng
1. Tính bàn trống và biết tần suất sử dụng, thời gian dùng bàn, khu vực
2. Nhận diện khách vào CH, khách cũ, mới, tỷ lệ chuyển đổi
Ngoài ra bạn có giải pháp hay đề xuất giúp nha
Cảm ơn cả nhà","Chào cả nhà Mình cần tìm giải pháp Camera AI gì đó cho các nhu cầu sau cho chuỗi nhà Hàng 1. Tính bàn trống và biết tần suất sử dụng, thời gian dùng bàn, khu vực 2. Nhận diện khách vào CH, khách cũ, mới, tỷ lệ chuyển đổi Ngoài ra bạn có giải pháp hay đề xuất giúp nha Cảm ơn cả nhà",,,,,
"Chào mọi người,
Mình đang có thắc mắc một chút về data normalization. Khi sử dụng MinMaxScaler thì ta sử dụng min, max của tập train để fit sau đó tranform cho tập val/test, hay mỗi tập mình sử dụng min, max riêng để rescale cho từng tập đó ?","Chào mọi người, Mình đang có thắc mắc một chút về data normalization. Khi sử dụng MinMaxScaler thì ta sử dụng min, max của tập train để fit sau đó tranform cho tập val/test, hay mỗi tập mình sử dụng min, max riêng để rescale cho từng tập đó ?",,,,,
,nan,,,,,
">>> Must-read papers on graph neural networks (GNN)

(C) https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/492800285460365/ 
Trong năm 2020, cùng với Transformer, GNN hay Graph Neural Network nhận được nhiều sự chú ý và quan tâm hơn từ cộng đồng. 1 số lớp bài toán điển hình của GNN bao gồm: Node Classification, Graph Classification, Link Prediction, Graph Clustering,...

Trong thực tế, GNN có thể được áp dụng vào nhiều kiểu dữ liệu và bài toán khác nhau. 1 số ứng dụng như:
- https://arxiv.org/abs/2102.07753 dùng CNN với GNN phía sau cho metric learning để học embedding, có so sánh với rất nhiều các pp khác từ triplet loss, angular loss,.. và là phương pháp SOTA hiện tại
- https://arxiv.org/abs/2003.07493 : paper này thì dùng graph với bài toán scene text detection
- https://arxiv.org/abs/2002.01276 : paper này thì dùng GCN với CTC loss cho bài toán scene text recognition
- hay ví dụ 1 bài toán đang khá nổi bật trong thời gian gần đây như: Scene Graph Generation. 1 số bài toán khác như keypoint matching (SuperGlue) https://arxiv.org/abs/1911.11763 , hand-pose estimation (HOPE): https://arxiv.org/abs/2004.00060 , information extraction từ ảnh (PICK): https://arxiv.org/abs/2004.07464 . 
- task về Image-Text Matching, phục vụ cho bài toán như Visual Semantic Search Engine: https://arxiv.org/abs/1909.02701
- task về session-based recommend system: https://arxiv.org/abs/1811.00855
- task về text classification (TextGCN): https://arxiv.org/abs/1809.05679 , 1 paper khác áp dụng GCN cho bài toán phân loại văn bản tiếng Việt: https://www.researchgate.net/publication/344433020_Vietnamese_Document_Classification_Using_Graph_Convolutional_Network
- task về table structure recognition, lưu giữ định dạng bảng biểu: (Rethinking TSN) https://arxiv.org/abs/1905.13391 , (Complicated TSN) https://arxiv.org/abs/1908.04729
- GCN áp dụng trong bài toán Multi-Object Tracking: https://arxiv.org/abs/2006.13164
- GCN còn được sử dụng trong y sinh, biểu diễn thuốc, DNA dưới dạng graph, 1 paper kèm theo của tác giả Tuan Nguyen (tác giả ebook Deep Learning cơ bản)    https://ieeexplore.ieee.org/document/9359501
- và còn rất nhiều các ứng dụng khác nữa...

Tham khảo tại: https://github.com/thunlp/GNNPapers
(C) https://mobile.twitter.com/omarsar0/status/1368167852763717641

1 số bài chia sẻ khác trên Viblo về GNN
- Tổng quan về GNN: https://viblo.asia/p/6J3ZgP0qlmB
- GNN cho bài toán trích rút thông tin từ ảnh hóa đơn: https://viblo.asia/p/djeZ1yPGZWz

#Q12021 #viblo #machine_learning #deep_learning #gnn #gcn #graph_neural_network #graph_convolutional_network",">>> Must-read papers on graph neural networks (GNN) (C) https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/492800285460365/ Trong năm 2020, cùng với Transformer, GNN hay Graph Neural Network nhận được nhiều sự chú ý và quan tâm hơn từ cộng đồng. 1 số lớp bài toán điển hình của GNN bao gồm: Node Classification, Graph Classification, Link Prediction, Graph Clustering,... Trong thực tế, GNN có thể được áp dụng vào nhiều kiểu dữ liệu và bài toán khác nhau. 1 số ứng dụng như: - https://arxiv.org/abs/2102.07753 dùng CNN với GNN phía sau cho metric learning để học embedding, có so sánh với rất nhiều các pp khác từ triplet loss, angular loss,.. và là phương pháp SOTA hiện tại - https://arxiv.org/abs/2003.07493 : paper này thì dùng graph với bài toán scene text detection - https://arxiv.org/abs/2002.01276 : paper này thì dùng GCN với CTC loss cho bài toán scene text recognition - hay ví dụ 1 bài toán đang khá nổi bật trong thời gian gần đây như: Scene Graph Generation. 1 số bài toán khác như keypoint matching (SuperGlue) https://arxiv.org/abs/1911.11763 , hand-pose estimation (HOPE): https://arxiv.org/abs/2004.00060 , information extraction từ ảnh (PICK): https://arxiv.org/abs/2004.07464 . - task về Image-Text Matching, phục vụ cho bài toán như Visual Semantic Search Engine: https://arxiv.org/abs/1909.02701 - task về session-based recommend system: https://arxiv.org/abs/1811.00855 - task về text classification (TextGCN): https://arxiv.org/abs/1809.05679 , 1 paper khác áp dụng GCN cho bài toán phân loại văn bản tiếng Việt: https://www.researchgate.net/publication/344433020_Vietnamese_Document_Classification_Using_Graph_Convolutional_Network - task về table structure recognition, lưu giữ định dạng bảng biểu: (Rethinking TSN) https://arxiv.org/abs/1905.13391 , (Complicated TSN) https://arxiv.org/abs/1908.04729 - GCN áp dụng trong bài toán Multi-Object Tracking: https://arxiv.org/abs/2006.13164 - GCN còn được sử dụng trong y sinh, biểu diễn thuốc, DNA dưới dạng graph, 1 paper kèm theo của tác giả Tuan Nguyen (tác giả ebook Deep Learning cơ bản) https://ieeexplore.ieee.org/document/9359501 - và còn rất nhiều các ứng dụng khác nữa... Tham khảo tại: https://github.com/thunlp/GNNPapers (C) https://mobile.twitter.com/omarsar0/status/1368167852763717641 1 số bài chia sẻ khác trên Viblo về GNN - Tổng quan về GNN: https://viblo.asia/p/6J3ZgP0qlmB - GNN cho bài toán trích rút thông tin từ ảnh hóa đơn: https://viblo.asia/p/djeZ1yPGZWz",#Q12021	#viblo	#machine_learning	#deep_learning	#gnn	#gcn	#graph_neural_network	#graph_convolutional_network,,,,
"Xin phép Admin
Xin chào mọi người, video đầu tiên giúp các bạn mới học về Deeplearning có thể HIỂU được BẢN CHẤT và TỰ XÂY DỰNG các mô hình Deep, mong rằng sẽ giúp được nhiều bạn. Mọi người xem hãy để lại phản hồi cho mình để chỉnh sửa trong các video tiếp theo nhé. Cảm ơn mn!","Xin phép Admin Xin chào mọi người, video đầu tiên giúp các bạn mới học về Deeplearning có thể HIỂU được BẢN CHẤT và TỰ XÂY DỰNG các mô hình Deep, mong rằng sẽ giúp được nhiều bạn. Mọi người xem hãy để lại phản hồi cho mình để chỉnh sửa trong các video tiếp theo nhé. Cảm ơn mn!",,,,,
mn cho em hỏi em tạo hàm active contour loss nhưng bị lỗi không đúng chiều như này. Em phải sửa ntn ạ?,mn cho em hỏi em tạo hàm active contour loss nhưng bị lỗi không đúng chiều như này. Em phải sửa ntn ạ?,,,,,
"Chào mọi người, không biết ở đây có ai học theo cuốn `Computer Vision: Models, Learning, and Inference` không ạ?
Mình gặp một vấn đề.
Bài toán là chứng minh, dựa vào bayes và Schur complement. Họ có cho một phần solution nhưng em đọc vẫn không hiểu. Mong được mọi người giúp đỡ giải đáp phần lý thuyết này với ạ. Mình xin cảm ơn","Chào mọi người, không biết ở đây có ai học theo cuốn `Computer Vision: Models, Learning, and Inference` không ạ? Mình gặp một vấn đề. Bài toán là chứng minh, dựa vào bayes và Schur complement. Họ có cho một phần solution nhưng em đọc vẫn không hiểu. Mong được mọi người giúp đỡ giải đáp phần lý thuyết này với ạ. Mình xin cảm ơn",,,,,
"Xin chào mọi người, em import sugartensor thì bị lỗi AttributeError: module 'sugartensor' has no attribute 'GraphKeys', giúp em fix với ạ, em cảm ơn.

Log đầy đủ:
2021-03-09 14:43:25.634133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2021-03-09 14:43:27.111468: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2021-03-09 14:43:27.112350: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-03-09 14:43:27.114195: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-88AGTME
2021-03-09 14:43:27.114346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-88AGTME
2021-03-09 14:43:27.114788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-09 14:43:27.120568: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17cb6bf58e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-03-09 14:43:27.120617: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""preprocess.py"", line 5, in <module>
   import data
  File ""D:\workspace\pycharm\vietnamese-speech-to-text-wavenet\data_processing\data.py"", line 6, in <module>
    import sugartensor as tf
  File ""C:\Program Files\python\lib\site-packages\sugartensor\__init__.py"", line 5, in <module>
    from .sg_main import *
  File ""C:\Program Files\python\lib\site-packages\sugartensor\sg_main.py"", line 45, in <module>
    _phase = tf.Variable(False, name='phase', trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])
AttributeError: module 'sugartensor' has no attribute 'GraphKeys'","Xin chào mọi người, em import sugartensor thì bị lỗi AttributeError: module 'sugartensor' has no attribute 'GraphKeys', giúp em fix với ạ, em cảm ơn. Log đầy đủ: 2021-03-09 14:43:25.634133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2021-03-09 14:43:27.111468: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll 2021-03-09 14:43:27.112350: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error 2021-03-09 14:43:27.114195: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-88AGTME 2021-03-09 14:43:27.114346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-88AGTME 2021-03-09 14:43:27.114788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-03-09 14:43:27.120568: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17cb6bf58e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-03-09 14:43:27.120617: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version Traceback (most recent call last): File ""preprocess.py"", line 5, in <module> import data File ""D:\workspace\pycharm\vietnamese-speech-to-text-wavenet\data_processing\data.py"", line 6, in <module> import sugartensor as tf File ""C:\Program Files\python\lib\site-packages\sugartensor\__init__.py"", line 5, in <module> from .sg_main import * File ""C:\Program Files\python\lib\site-packages\sugartensor\sg_main.py"", line 45, in <module> _phase = tf.Variable(False, name='phase', trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES]) AttributeError: module 'sugartensor' has no attribute 'GraphKeys'",,,,,
"Chào cả nhà
Đây là bài giới thiệu tóm tắt kèm demo của team mình về Dall-E, nghiên cứu mới nhất của OpenAI về xử lý ảnh.
Mong là nó có ích cho các bạn :D","Chào cả nhà Đây là bài giới thiệu tóm tắt kèm demo của team mình về Dall-E, nghiên cứu mới nhất của OpenAI về xử lý ảnh. Mong là nó có ích cho các bạn :D",,,,,
"Nhận dạng ảnh trong computer vision đòi hỏi phải training với các ảnh được dán nhãn, việc này tốn nhiều công sức và tiền bạc.
Facebook đã phát triển thuật toán SEER (SElf supERvised) và SwAV (SWapping Assignments between multiple Views) có thể nhận dạng tập ảnh hàng tỷ thông số từ Instagram mà không cần phải dán nhãn.
Thuật toán dựa vào khả năng nhận dạng của não em bé.","Nhận dạng ảnh trong computer vision đòi hỏi phải training với các ảnh được dán nhãn, việc này tốn nhiều công sức và tiền bạc. Facebook đã phát triển thuật toán SEER (SElf supERvised) và SwAV (SWapping Assignments between multiple Views) có thể nhận dạng tập ảnh hàng tỷ thông số từ Instagram mà không cần phải dán nhãn. Thuật toán dựa vào khả năng nhận dạng của não em bé.",,,,,
"A e đang implement: Giải pháp thống kê an toàn giao thông thông minh qua máy học.
Gồm các vấn đề
- Check mật độ giao thông
- Check không đội mũ bảo hiểm
- Check vứt rác bừa bãi
- Check đám đông tụ tập
Mời pà con vào chỉ đạo, chém gió để ứng dụng được tiến bộ hơn nhé.
Cám ơn bà con!
#koolj_deepimage
Mời bạn thử:","A e đang implement: Giải pháp thống kê an toàn giao thông thông minh qua máy học. Gồm các vấn đề - Check mật độ giao thông - Check không đội mũ bảo hiểm - Check vứt rác bừa bãi - Check đám đông tụ tập Mời pà con vào chỉ đạo, chém gió để ứng dụng được tiến bộ hơn nhé. Cám ơn bà con! Mời bạn thử:",#koolj_deepimage,,,,
"[Help][Làm sao crawl toàn bộ dữ liệu trò chuyện trên Facebook fanpage]
Em chào anh/chị, hiện tại em đang làm dự án thì cần toàn bộ dữ liệu cuộc trò chuyện giữa admin và user trên Facebook fanpage. Không biết có anh/chị nào mình có kinh nghiệm để mình crawl về bằng code không ạ? Em search google thì chỉ toàn là cách tải dữ liệu trò chuyện của mỗi cá nhân thôi.
Em cảm ơn rất nhiều ạ.","[Help][Làm sao crawl toàn bộ dữ liệu trò chuyện trên Facebook fanpage] Em chào anh/chị, hiện tại em đang làm dự án thì cần toàn bộ dữ liệu cuộc trò chuyện giữa admin và user trên Facebook fanpage. Không biết có anh/chị nào mình có kinh nghiệm để mình crawl về bằng code không ạ? Em search google thì chỉ toàn là cách tải dữ liệu trò chuyện của mỗi cá nhân thôi. Em cảm ơn rất nhiều ạ.",,,,,
"Trong nhóm có bạn nào làm về Urban Growth Forecasting không nhỉ?
Các bạn có thể gợi ý cho mình biết thường nếu forecast growth của một city (at census tract) level thì có những model loại gì? Nếu muốn predict growth cho 1 năm, 3năm, 5 năm thì phải làm như thế nào? Nếu các bạn có reference nữa thì tốt quá.
Xin cảm ơn tất cả các gợi ý của các bạn.","Trong nhóm có bạn nào làm về Urban Growth Forecasting không nhỉ? Các bạn có thể gợi ý cho mình biết thường nếu forecast growth của một city (at census tract) level thì có những model loại gì? Nếu muốn predict growth cho 1 năm, 3năm, 5 năm thì phải làm như thế nào? Nếu các bạn có reference nữa thì tốt quá. Xin cảm ơn tất cả các gợi ý của các bạn.",,,,,
"[PyTorch]
Chào mọi người, có bạn nào rành về Pytorch cho mình xin chút kinh nghiệm.
Mình có 1 model và list weight của model theo từng iterations. Mình muốn tính gradient của model w.r.t từng weights trong cái list mình có. Hiện giờ mình chỉ attribute weight vào model rồi call loss.backward để tìm gradient của từng thằng nhưng kết quả không hợp lý so với một version khác mà mình code gradient function bằng tay. Có bạn nào biết cách khác tính gradient trên pytorch thì cho mình ít cao kiến với.
Cảm ơn các bạn","[PyTorch] Chào mọi người, có bạn nào rành về Pytorch cho mình xin chút kinh nghiệm. Mình có 1 model và list weight của model theo từng iterations. Mình muốn tính gradient của model w.r.t từng weights trong cái list mình có. Hiện giờ mình chỉ attribute weight vào model rồi call loss.backward để tìm gradient của từng thằng nhưng kết quả không hợp lý so với một version khác mà mình code gradient function bằng tay. Có bạn nào biết cách khác tính gradient trên pytorch thì cho mình ít cao kiến với. Cảm ơn các bạn",,,,,
"Hi mọi người,
E/Mình muốn làm bài toán Time series Classification cho ECG HeartBeat nhưng không tìm được Dataset, có bạn nào biết chỉ mình với ,
Mình có tìm được 1 dataset trên kaggle https://www.kaggle.com/shayanfazeli/heartbeat
nhưng mỗi cái segmentation nó làm có 1 heartbeat nên dữ liệu không còn là time-serie nữa rồi
E/mình Xin cảm ơn,","Hi mọi người, E/Mình muốn làm bài toán Time series Classification cho ECG HeartBeat nhưng không tìm được Dataset, có bạn nào biết chỉ mình với , Mình có tìm được 1 dataset trên kaggle https://www.kaggle.com/shayanfazeli/heartbeat nhưng mỗi cái segmentation nó làm có 1 heartbeat nên dữ liệu không còn là time-serie nữa rồi E/mình Xin cảm ơn,",,,,,
"Chào mn, em dùng ubuntu 20.04 import tensorflow thì bị lỗi này, giúp em với ạ. Tks.
could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
Ignore above cudart dlerror if you do not have a GPU set up on your machine.","Chào mn, em dùng ubuntu 20.04 import tensorflow thì bị lỗi này, giúp em với ạ. Tks. could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory Ignore above cudart dlerror if you do not have a GPU set up on your machine.",,,,,
"Giới thiệu với các bạn một phần nhỏ nữa của cuốn ""Machine Learning cho dữ liệu dạng bảng""
https://machinelearningcoban.com/tabml_book/ch_model/decision_tree.html
Phần này chưa hoàn thành và chưa khớp với nội dung đã viết của cuốn sách; tuy nhiên mình đăng lên đây để giới thiệu với các bạn về việc phần này được bạn Tuấn Nguyễn (DL cơ bản) đóng góp. Rất cảm ơn bạn đã tham gia viết về phần mô hình này.
Nội dung cuốn sách sẽ thường xuyên được chỉnh sửa cho phù hợp với mạch viết chung. Mình sẽ chỉnh sửa nội dung của phần này cho nhất quán với các phần khác của cuốn sách khi viết tới đây.
 — với Tuan Nguyen.","Giới thiệu với các bạn một phần nhỏ nữa của cuốn ""Machine Learning cho dữ liệu dạng bảng"" https://machinelearningcoban.com/tabml_book/ch_model/decision_tree.html Phần này chưa hoàn thành và chưa khớp với nội dung đã viết của cuốn sách; tuy nhiên mình đăng lên đây để giới thiệu với các bạn về việc phần này được bạn Tuấn Nguyễn (DL cơ bản) đóng góp. Rất cảm ơn bạn đã tham gia viết về phần mô hình này. Nội dung cuốn sách sẽ thường xuyên được chỉnh sửa cho phù hợp với mạch viết chung. Mình sẽ chỉnh sửa nội dung của phần này cho nhất quán với các phần khác của cuốn sách khi viết tới đây. — với Tuan Nguyen.",,,,,
"Nhập môn Machine Learning cơ bản bằng TensorFlow
Chào mọi người! Mình vừa mới upload 4 video nhập môn Machine Learning bằng tiếng Việt lên kênh YouTube của TensorFlow. 4 video này hướng đến đối tượng những bạn muốn tìm hiểu về machine learning theo hướng thực hành trên code (mỳ ăn liền 😀) thay vì việc học các công thức toán. Hy vọng các bạn thấy nội dung này hữu ích, và cho mình feedback nhé 🙂

https://youtu.be/NVsw-JrXv9I?list=PLQY2H8rRoyvxNqk9EV5VP5fS0cWEXW5QQ","Nhập môn Machine Learning cơ bản bằng TensorFlow Chào mọi người! Mình vừa mới upload 4 video nhập môn Machine Learning bằng tiếng Việt lên kênh YouTube của TensorFlow. 4 video này hướng đến đối tượng những bạn muốn tìm hiểu về machine learning theo hướng thực hành trên code (mỳ ăn liền ) thay vì việc học các công thức toán. Hy vọng các bạn thấy nội dung này hữu ích, và cho mình feedback nhé https://youtu.be/NVsw-JrXv9I?list=PLQY2H8rRoyvxNqk9EV5VP5fS0cWEXW5QQ",,,,,
Chào các anh chị! Em đang tìm hiểu về cách dùng yolo v5 cho face recognition nhưng chưa tìm được dataset. Các anh chị có biết bộ dataset nào dùng cho face recognition có label rồi ko ạ.,Chào các anh chị! Em đang tìm hiểu về cách dùng yolo v5 cho face recognition nhưng chưa tìm được dataset. Các anh chị có biết bộ dataset nào dùng cho face recognition có label rồi ko ạ.,,,,,
"xin phép AE cho hỏi: kinh nghiệm mua card màn hình để xử lý ảnh vơis ạh
hiện công ty em đang làm cái hệ thống nhận diện khuôn mặt. em đang dùng python để làm.
hiên đang cần xử lý khoảng 20-50 cái ảnh chứa mặt ngừoi trong cùng 1 thời điểm.
thì nên chọn loại card nvidia nào ạ.
em cám ơn ạ.",xin phép AE cho hỏi: kinh nghiệm mua card màn hình để xử lý ảnh vơis ạh hiện công ty em đang làm cái hệ thống nhận diện khuôn mặt. em đang dùng python để làm. hiên đang cần xử lý khoảng 20-50 cái ảnh chứa mặt ngừoi trong cùng 1 thời điểm. thì nên chọn loại card nvidia nào ạ. em cám ơn ạ.,,,,,
"Có ae nào đã làm về bot ghi lại các tương tác người dùng và tự động phân tích hành vi ko Cho mình xin 1 vài info nhé
Thanks !",Có ae nào đã làm về bot ghi lại các tương tác người dùng và tự động phân tích hành vi ko Cho mình xin 1 vài info nhé Thanks !,,,,,
"Xin chào mọi người, mình vừa up 4 video về hướng dẫn xây dựng Mô Hình Nhận Dạng Giọng nói do nhóm mình nghiên cứu thời gian qua, rất mong sẽ giúp ích được cho mọi người và nhận được phản hồi, xin cảm ơn!","Xin chào mọi người, mình vừa up 4 video về hướng dẫn xây dựng Mô Hình Nhận Dạng Giọng nói do nhóm mình nghiên cứu thời gian qua, rất mong sẽ giúp ích được cho mọi người và nhận được phản hồi, xin cảm ơn!",,,,,
"Em chào mọi người ạ.
Em có đọc qua statistic từ cuốn Dive In to Deep Learning và coi sơ qua trên mạng nhưng em vẫn không hiểu được statistic có tác dụng gì ạ. Mong mọi người có thể cho em xin link nói tổng quát và vài tên sách về statistic trong Machine/Deep Learning ạ, em có đọc 1 cuốn nhưng nó khá nhiều thuật ngữ về bên thống kê thuần nên em không hiểu hết được ý nghĩa của nó, mong mọi người gợi ý giúp em vài cuốn thiên về ML nhiều hơn.
Em xin cảm ơn mọi người ạ.","Em chào mọi người ạ. Em có đọc qua statistic từ cuốn Dive In to Deep Learning và coi sơ qua trên mạng nhưng em vẫn không hiểu được statistic có tác dụng gì ạ. Mong mọi người có thể cho em xin link nói tổng quát và vài tên sách về statistic trong Machine/Deep Learning ạ, em có đọc 1 cuốn nhưng nó khá nhiều thuật ngữ về bên thống kê thuần nên em không hiểu hết được ý nghĩa của nó, mong mọi người gợi ý giúp em vài cuốn thiên về ML nhiều hơn. Em xin cảm ơn mọi người ạ.",,,,,
"Chào mọi người,mọi người cho em hỏi là học Tự động hóa sau này có thể làm về Machine learning không ạ? Vì em có đọc về hướng nghiên cứu của ngành thì vẫn có nhận dạng hình ảnh , giọng nói...","Chào mọi người,mọi người cho em hỏi là học Tự động hóa sau này có thể làm về Machine learning không ạ? Vì em có đọc về hướng nghiên cứu của ngành thì vẫn có nhận dạng hình ảnh , giọng nói...",,,,,
"[Pytorch Series]
Bài 1: Tensor
Bài này mình hướng dẫn các kiến thức nền tảng về torch tensor: vector, ma trận, tensor.
Từ phần cơ bản như index, slicing đến những phần phức tạp hơn như storage, size, offset, stride, hay contiguous tensor rồi đến tensor GPU, numpy sang tensor.
https://nttuan8.com/bai-1-tensor/
Ps: Hay thì cho mình xin 1 like, 1 share ☺","[Pytorch Series] Bài 1: Tensor Bài này mình hướng dẫn các kiến thức nền tảng về torch tensor: vector, ma trận, tensor. Từ phần cơ bản như index, slicing đến những phần phức tạp hơn như storage, size, offset, stride, hay contiguous tensor rồi đến tensor GPU, numpy sang tensor. https://nttuan8.com/bai-1-tensor/ Ps: Hay thì cho mình xin 1 like, 1 share",,,,,
"Các Bác cho em hỏi với ạ.
Em đang Train Yolov4 -AlexDarknet, em đọc cũng như tìm hiểu thì khi trainning thì các ảnh trong dataset của mình thì yolo sẽ resize ảnh về weight và height được khai báo trong file config và không giữ aspect ratio vì vậy đa số object trong ảnh bị méo . Và em có set parameter Random = 1 ở layer cuối thì sẽ resize network thì tương ứng với việc resize input resolution đúng k ạ ?
1 - với ảnh trong train dataset mà có size khác nhau thì việc k giữ aspect ratio khi train có ảnh hưởng gi đến accuracy không ạ ? .
2 - Nếu padding cho các ảnh để giữ aspect ratio dùng letterbox thì có cải thiện được kết quả trainning không ạ ?","Các Bác cho em hỏi với ạ. Em đang Train Yolov4 -AlexDarknet, em đọc cũng như tìm hiểu thì khi trainning thì các ảnh trong dataset của mình thì yolo sẽ resize ảnh về weight và height được khai báo trong file config và không giữ aspect ratio vì vậy đa số object trong ảnh bị méo . Và em có set parameter Random = 1 ở layer cuối thì sẽ resize network thì tương ứng với việc resize input resolution đúng k ạ ? 1 - với ảnh trong train dataset mà có size khác nhau thì việc k giữ aspect ratio khi train có ảnh hưởng gi đến accuracy không ạ ? . 2 - Nếu padding cho các ảnh để giữ aspect ratio dùng letterbox thì có cải thiện được kết quả trainning không ạ ?",,,,,
"My minimal implementation of SSD: Single Shot MultiBox Detector
Source code: https://github.com/uvipen/SSD-pytorch
Full demo: https://youtu.be/Br0QiEuBMzU",My minimal implementation of SSD: Single Shot MultiBox Detector Source code: https://github.com/uvipen/SSD-pytorch Full demo: https://youtu.be/Br0QiEuBMzU,,,,,
"Một phần nhỏ tiếp theo của cuốn ""Machine Learning cho dữ liệu dạng bảng""
https://machinelearningcoban.com/tabml_book/ch_tabular_data/datasets.html.
Phần này giới thiệu một vài (không phải toàn bộ) bộ dữ liệu được dùng trong sách và giới thiệu về EDA -- bước đầu tiên khi làm một bài toán với dữ liệu dạng bảng.","Một phần nhỏ tiếp theo của cuốn ""Machine Learning cho dữ liệu dạng bảng"" https://machinelearningcoban.com/tabml_book/ch_tabular_data/datasets.html. Phần này giới thiệu một vài (không phải toàn bộ) bộ dữ liệu được dùng trong sách và giới thiệu về EDA -- bước đầu tiên khi làm một bài toán với dữ liệu dạng bảng.",,,,,
"Bản dịch của cuốn sách Học máy khả diễn giải đã được hoàn thành, các bạn có thể tải về bản pdf ở đây!
Github: https://github.com/giangnguyen2412/InterpretableMLBook-Vietnamese
Tweet của tác giả về bản dịch: https://twitter.com/ChristophMolnar/status/1366383437645574145","Bản dịch của cuốn sách Học máy khả diễn giải đã được hoàn thành, các bạn có thể tải về bản pdf ở đây! Github: https://github.com/giangnguyen2412/InterpretableMLBook-Vietnamese Tweet của tác giả về bản dịch: https://twitter.com/ChristophMolnar/status/1366383437645574145",,,,,
"Em chào các anh chị, hiện em đang tìm hiểu cây quyết định thì trong kết quả có hiển thị value. 
Ví dụ value = [37, 34, 41] tại node.
Em không biết là giá trị value đó biểu thị và mang ý nghĩa gì ạ.
Mong mọi người giải đáp ạ.

Nguồn: https://ichi.pro/vi/hinh-dung-cay-quyet-dinh-bang-python-scikit-learning-graphviz-matplotlib-31132954028252","Em chào các anh chị, hiện em đang tìm hiểu cây quyết định thì trong kết quả có hiển thị value. Ví dụ value = [37, 34, 41] tại node. Em không biết là giá trị value đó biểu thị và mang ý nghĩa gì ạ. Mong mọi người giải đáp ạ. Nguồn: https://ichi.pro/vi/hinh-dung-cay-quyet-dinh-bang-python-scikit-learning-graphviz-matplotlib-31132954028252",,,,,
FastICA - phân tích thành phần độc lập là một thuật toán Unsupervised Learning rất hữu ích trong các bài toán xử lý đặc trưng. Đặc biệt là những tín hiệu độc lập hòa trộn vào nhau.,FastICA - phân tích thành phần độc lập là một thuật toán Unsupervised Learning rất hữu ích trong các bài toán xử lý đặc trưng. Đặc biệt là những tín hiệu độc lập hòa trộn vào nhau.,,,,,
"Lọc ý tưởng cởi nghiệp AI/ML cho Healthcare, mà nhìn quanh thấy Tây Tàu có nhiều thứ hay ho quá, nào là AI doctors, remote patient monitoring, virtual scribes, precision medicine, ... hơi hoang mang. Cần tìm người an ủi hoặc đánh đập.","Lọc ý tưởng cởi nghiệp AI/ML cho Healthcare, mà nhìn quanh thấy Tây Tàu có nhiều thứ hay ho quá, nào là AI doctors, remote patient monitoring, virtual scribes, precision medicine, ... hơi hoang mang. Cần tìm người an ủi hoặc đánh đập.",,,,,
"Chào mn, em muốn học về mảng âm thanh nói chung, bao gồm phân loại, nhận diện âm thanh, tổng hợp, nhận dạng tiếng nói, ... của ML, DL thì lộ trình học như nào ạ?
Hiện tại em đã code tốt python, đã từng làm theo các project có sẵn trên git và train được 1 số model âm thanh; nhưng em cảm thấy chưa hiểu sâu
em muốn học cơ bản từ đầu, tinh chỉnh để hiểu rõ từng param của model, từ đọc input, tính mfcc, fft, ....
mong mọi người chỉ dẫn ạ, em cảm ơn","Chào mn, em muốn học về mảng âm thanh nói chung, bao gồm phân loại, nhận diện âm thanh, tổng hợp, nhận dạng tiếng nói, ... của ML, DL thì lộ trình học như nào ạ? Hiện tại em đã code tốt python, đã từng làm theo các project có sẵn trên git và train được 1 số model âm thanh; nhưng em cảm thấy chưa hiểu sâu em muốn học cơ bản từ đầu, tinh chỉnh để hiểu rõ từng param của model, từ đọc input, tính mfcc, fft, .... mong mọi người chỉ dẫn ạ, em cảm ơn",,,,,
"Kính chào các bác.
Nhiều anh em đang học sốt ruột nên đêm qua em đã ra luôn Phần 2 cho anh em. Hi vọng sẽ giúp được anh em có cái nhìn tổng quan về triển khai một model như nào.
Kiến thức trong bài này hoàn toàn có thể áp dụng cho nhiều bài khác!
Chú ý: Em có note vài ý trong bài rồi, các bạn chú ý để xử lý thêm khi đưa vào thực tế nhé.","Kính chào các bác. Nhiều anh em đang học sốt ruột nên đêm qua em đã ra luôn Phần 2 cho anh em. Hi vọng sẽ giúp được anh em có cái nhìn tổng quan về triển khai một model như nào. Kiến thức trong bài này hoàn toàn có thể áp dụng cho nhiều bài khác! Chú ý: Em có note vài ý trong bài rồi, các bạn chú ý để xử lý thêm khi đưa vào thực tế nhé.",,,,,
"Em chào mọi người ạ. Em đang học trên trang machinelearningcoban của anh Tiệp. Em có thắc mắc rằng tại sao ở hàm loss bài Logistic Regression tác giả lại dùng log cơ số e mà không dùng cơ số khác, nó có ý nghĩa gì về mặt toán học không ạ. Em xin cảm ơn","Em chào mọi người ạ. Em đang học trên trang machinelearningcoban của anh Tiệp. Em có thắc mắc rằng tại sao ở hàm loss bài Logistic Regression tác giả lại dùng log cơ số e mà không dùng cơ số khác, nó có ý nghĩa gì về mặt toán học không ạ. Em xin cảm ơn",,,,,
"Kính chào các bác, em đang làm bài toán phát hiện tiêu đề mang tính giật gân trên báo mạng. Mục tiêu là phân tích định tính và phân loại các tiêu đề mang tính giật gân trên báo (ví dụ: 'Toang' thực sự, 150 tỷ USD biến mất, dân chơi hoảng sợ). Giúp cho người đọc hiểu thêm về cách đặt các tiêu đề giật gân trên các báo trên khía cạnh ngôn ngữ. Các cao nhân có nhiều kinh nghiệm có thể cho em vài gợi ý với ạ.","Kính chào các bác, em đang làm bài toán phát hiện tiêu đề mang tính giật gân trên báo mạng. Mục tiêu là phân tích định tính và phân loại các tiêu đề mang tính giật gân trên báo (ví dụ: 'Toang' thực sự, 150 tỷ USD biến mất, dân chơi hoảng sợ). Giúp cho người đọc hiểu thêm về cách đặt các tiêu đề giật gân trên các báo trên khía cạnh ngôn ngữ. Các cao nhân có nhiều kinh nghiệm có thể cho em vài gợi ý với ạ.",,,,,
"Chào các anh chị,
Hiện tại em đang gặp bế tắc khi làm việc với ARIMA model dùng để dự đoán dữ liệu Time series, cụ thể là dữ liệu stock, nếu có anh chị nào giúp được em, em xin cảm ơn và hậu tạ, em ở tp hcm ạ.","Chào các anh chị, Hiện tại em đang gặp bế tắc khi làm việc với ARIMA model dùng để dự đoán dữ liệu Time series, cụ thể là dữ liệu stock, nếu có anh chị nào giúp được em, em xin cảm ơn và hậu tạ, em ở tp hcm ạ.",,,,,
"[Xin giúp đỡ về training Unet3D trên colab]
Em xin chào mọi người,
Em đang gặp vấn đề về huấn luyện mạng Unet3D trên Colab. Cụ thể là việc hết VRAM trên GPU.
Em sử dụng gói Colab Pro, kích thước hình ảnh đầu vào là (4,128,128,128), kích thước mô hình Unet3D (ảnh 1) . Khi huấn luyện thì được báo là ""CUDA out of memory: (ảnh 2), do em dùng Colab Pro nên còn thừa rất nhiều RAM chỉ sử dụng 3.5/25,5GB.
Không biết đã có ai huấn luyện một mô hình tương tự như vậy chưa ạ, em đã thử nhiều cách tuy nhiên không được, mong mọi người góp ý cho em. Em xin cám ơn!
P/S: Em sử dụng pytorch","[Xin giúp đỡ về training Unet3D trên colab] Em xin chào mọi người, Em đang gặp vấn đề về huấn luyện mạng Unet3D trên Colab. Cụ thể là việc hết VRAM trên GPU. Em sử dụng gói Colab Pro, kích thước hình ảnh đầu vào là (4,128,128,128), kích thước mô hình Unet3D (ảnh 1) . Khi huấn luyện thì được báo là ""CUDA out of memory: (ảnh 2), do em dùng Colab Pro nên còn thừa rất nhiều RAM chỉ sử dụng 3.5/25,5GB. Không biết đã có ai huấn luyện một mô hình tương tự như vậy chưa ạ, em đã thử nhiều cách tuy nhiên không được, mong mọi người góp ý cho em. Em xin cám ơn! P/S: Em sử dụng pytorch",,,,,
"Kính chào các bác, hôm nay em mới học về YOLO v5 nên xem xin mạnh dạn viết bài gửi đến anh em Phần 1 của series làm việc với YOLO v5. Hi vọng giúp được gì đó cho anh em newbie mới học.
Các cao thủ xin được chỉ giáo thêm ah.
Ps: Các bài trên Mì AI chỉ mang tính chất học tập, mong các bác không comment như một sản phẩm thương mại!","Kính chào các bác, hôm nay em mới học về YOLO v5 nên xem xin mạnh dạn viết bài gửi đến anh em Phần 1 của series làm việc với YOLO v5. Hi vọng giúp được gì đó cho anh em newbie mới học. Các cao thủ xin được chỉ giáo thêm ah. Ps: Các bài trên Mì AI chỉ mang tính chất học tập, mong các bác không comment như một sản phẩm thương mại!",,,,,
"Dạ em chào mọi người, cho em hỏi là có thư viện nào hổ trợ mình tạo ra 1 chương trình để nhập input vào trong mô hình neural network không ạ. Em cảm ơn mọi người.","Dạ em chào mọi người, cho em hỏi là có thư viện nào hổ trợ mình tạo ra 1 chương trình để nhập input vào trong mô hình neural network không ạ. Em cảm ơn mọi người.",,,,,
"[Series Pytorch]
Gần đây mình có làm một vài project về Pytorch nên mình muốn chia sẻ kiến thức tới mọi người. Ở series Pytorch này, mình sẽ viết chi tiết từ những phần cơ bản nhất của Pytorch như tensor cho đến phần triển khai model đưa ra sử dụng thực tế cho mọi người (deployment).
Để cho những người mới chưa sử dụng Pytorch bao giờ cũng có thể hiểu các kiền thức nền tảng của Pytorch, sau đó xây dựng và phát triển ứng dụng Deep Learning với Pytorch.
Khác với series Deep Learning cơ bản, ở series Pytorch lần này, mình tập trung vào giải thích chi tiết cơ chế hoạt động của Pytorch và cách triển khai model Deep Learning ra ngoài thực tế dùng Pytorch.
Nội dung series ở đây: https://nttuan8.com/gioi-thieu-series-pytorch/
Tần suất ra bài : 1-2 tuần/1 bài vào cuối tuần.
Ps: Như thường lệ mình xin 1 like, 1 share để thêm tinh thần viết 😍","[Series Pytorch] Gần đây mình có làm một vài project về Pytorch nên mình muốn chia sẻ kiến thức tới mọi người. Ở series Pytorch này, mình sẽ viết chi tiết từ những phần cơ bản nhất của Pytorch như tensor cho đến phần triển khai model đưa ra sử dụng thực tế cho mọi người (deployment). Để cho những người mới chưa sử dụng Pytorch bao giờ cũng có thể hiểu các kiền thức nền tảng của Pytorch, sau đó xây dựng và phát triển ứng dụng Deep Learning với Pytorch. Khác với series Deep Learning cơ bản, ở series Pytorch lần này, mình tập trung vào giải thích chi tiết cơ chế hoạt động của Pytorch và cách triển khai model Deep Learning ra ngoài thực tế dùng Pytorch. Nội dung series ở đây: https://nttuan8.com/gioi-thieu-series-pytorch/ Tần suất ra bài : 1-2 tuần/1 bài vào cuối tuần. Ps: Như thường lệ mình xin 1 like, 1 share để thêm tinh thần viết",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 3/2020 vào comment của post này.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 3/2020 vào comment của post này.",,,,,
"[AI application] AI agent plays Contra
Source code: https://github.com/uvipen/Contra-PPO-pytorch
Full demo: https://youtu.be/YBd4l806Di8
#Python #AI #ReinforcementLearning",[AI application] AI agent plays Contra Source code: https://github.com/uvipen/Contra-PPO-pytorch Full demo: https://youtu.be/YBd4l806Di8,#Python	#AI	#ReinforcementLearning,,,,
"[Deep learning - Hỏi về trật tự các inputs của NN]
Xin chào mọi người. Mình đang làm bài toán về Virtual try on trên multiple pose.
Model mình dùng là Unet like (pix2pixHD).
Input của mô hình mình sử dụng là sự concat giữa các đặc trưng như là: source image, pose heat map, segmentation,...
Câu hỏi của mình là: Việc sắp xếp thứ tự các input cho model theo hai trường hợp sau thì có khác nhau gì không?
output1 = pix2pixHD(concat(inputA, inputB, inputC))
output2 = pix2pixHD(concat(inputB, inputC, inputA))
Mình muốn biết là sự sắp xếp thứ tự các input có ý nghĩa như thế nào đến chất lượng đầu ra ouput1 vs output2.
Thân chào.","[Deep learning - Hỏi về trật tự các inputs của NN] Xin chào mọi người. Mình đang làm bài toán về Virtual try on trên multiple pose. Model mình dùng là Unet like (pix2pixHD). Input của mô hình mình sử dụng là sự concat giữa các đặc trưng như là: source image, pose heat map, segmentation,... Câu hỏi của mình là: Việc sắp xếp thứ tự các input cho model theo hai trường hợp sau thì có khác nhau gì không? output1 = pix2pixHD(concat(inputA, inputB, inputC)) output2 = pix2pixHD(concat(inputB, inputC, inputA)) Mình muốn biết là sự sắp xếp thứ tự các input có ý nghĩa như thế nào đến chất lượng đầu ra ouput1 vs output2. Thân chào.",,,,,
,nan,,,,,
"Hành trình AI của một sinh viên tồi
https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/485380192869041/
1 bài viết khá dài về hành trình bước chân và theo đuổi AI (trí tuệ nhân tạo) của ... 1 bạn sinh viên tồi. Mình đọc thấy khá thú vị và khá vui :D Trong bài tác giả có đề cập các dấu mốc trên con đường học và làm AI của mình. Từ những bước đầu đọc blog ML cơ bản của a Tiệp, blog DL cơ bản của a Tuấn, lần thực tập đầu tiên, lên lab nghiên cứu và paper đầu tiên,....
""Blog trình bày rất mạch lạc, văn phong và các biến đổi cũng dễ hiểu, và cái hay ho đó là từ lúc tiếp cận kiến thức đến kết luận vẫn rất logic. Mình vui nhất là lúc mình train và chạy được ví dụ xe tự lái trong bài CNN. Trong blog bài nào cũng có code ví dụ, và mỗi lần chạy các ví dụ trực quan làm mình vô cùng phấn khích. Ví dụ, bài image captioning, mình chạy và lấy ra caption cho cái ảnh thằng bạn mình đang ngồi đánh điện tử, sau đó mình gửi ngay cho mẹ mình để khoe, mẹ mình gửi cái ảnh kèm caption cho mẹ thằng bạn, mẹ nó đánh nó một trận, nó cũng cầm cái ảnh có caption cho mình xem và không quên kèm theo quả đấm cảm ơn. Mặc dù miệng mình khá đau sau quả đấm nhưng mình vẫn khá vui và cố nhoẻn miệng cười"" =)))
Author: Học Hiếu
Viblo: https://viblo.asia/p/hanh-trinh-ai-cua-mot-sinh-vien-toi-m68Z0AVzlkG
1 bài chia sẻ khác của tác giả Nguyễn Thành Trung về con đường theo đuổi AI: https://viblo.asia/p/con-duong-ai-cua-toi-XL6lADwpZek
Mình đã tự học AI như thế nào? https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/479432823463778/
#viblo #Q12021 #machine_learning #deep_learning","Hành trình AI của một sinh viên tồi https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/485380192869041/ 1 bài viết khá dài về hành trình bước chân và theo đuổi AI (trí tuệ nhân tạo) của ... 1 bạn sinh viên tồi. Mình đọc thấy khá thú vị và khá vui :D Trong bài tác giả có đề cập các dấu mốc trên con đường học và làm AI của mình. Từ những bước đầu đọc blog ML cơ bản của a Tiệp, blog DL cơ bản của a Tuấn, lần thực tập đầu tiên, lên lab nghiên cứu và paper đầu tiên,.... ""Blog trình bày rất mạch lạc, văn phong và các biến đổi cũng dễ hiểu, và cái hay ho đó là từ lúc tiếp cận kiến thức đến kết luận vẫn rất logic. Mình vui nhất là lúc mình train và chạy được ví dụ xe tự lái trong bài CNN. Trong blog bài nào cũng có code ví dụ, và mỗi lần chạy các ví dụ trực quan làm mình vô cùng phấn khích. Ví dụ, bài image captioning, mình chạy và lấy ra caption cho cái ảnh thằng bạn mình đang ngồi đánh điện tử, sau đó mình gửi ngay cho mẹ mình để khoe, mẹ mình gửi cái ảnh kèm caption cho mẹ thằng bạn, mẹ nó đánh nó một trận, nó cũng cầm cái ảnh có caption cho mình xem và không quên kèm theo quả đấm cảm ơn. Mặc dù miệng mình khá đau sau quả đấm nhưng mình vẫn khá vui và cố nhoẻn miệng cười"" =))) Author: Học Hiếu Viblo: https://viblo.asia/p/hanh-trinh-ai-cua-mot-sinh-vien-toi-m68Z0AVzlkG 1 bài chia sẻ khác của tác giả Nguyễn Thành Trung về con đường theo đuổi AI: https://viblo.asia/p/con-duong-ai-cua-toi-XL6lADwpZek Mình đã tự học AI như thế nào? https://www.facebook.com/groups/VietnamAiLlinkSharing/permalink/479432823463778/",#viblo	#Q12021	#machine_learning	#deep_learning,,,,
"Halo anh chị <3
Hiện e đang tìm hiểu sử dụng Detectron2 của FaceBook.
Nhưng đang bị stuck chổ labelImage. Em sử dụng công cụ labelme.
Định dạng không giông bộ dataset của theo file Detectron2 Beginner's Tutorial.
Anh/chi nào đã từng làm cho em xin tips chuyển data qua đúng định dạng với ạ.
Link :
github: https://github.com/facebookresearch/detectron2
Detectron2 Beginner's Tutorial : https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5
Data mẫu mà FB sử dụng : https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip
Tool Labelme : https://github.com/wkentaro/labelme",Halo anh chị <3 Hiện e đang tìm hiểu sử dụng Detectron2 của FaceBook. Nhưng đang bị stuck chổ labelImage. Em sử dụng công cụ labelme. Định dạng không giông bộ dataset của theo file Detectron2 Beginner's Tutorial. Anh/chi nào đã từng làm cho em xin tips chuyển data qua đúng định dạng với ạ. Link : github: https://github.com/facebookresearch/detectron2 Detectron2 Beginner's Tutorial : https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5 Data mẫu mà FB sử dụng : https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip Tool Labelme : https://github.com/wkentaro/labelme,,,,,
"Mọi người cho mình hỏi là nếu mình muốn đem một model ML lên app thì nên viết API rồi request khi cần hay xuất model ra tflite rồi incorperate nó vào app luôn ạ
Mình cảm ơn",Mọi người cho mình hỏi là nếu mình muốn đem một model ML lên app thì nên viết API rồi request khi cần hay xuất model ra tflite rồi incorperate nó vào app luôn ạ Mình cảm ơn,,,,,
Onward we match.,Onward we match.,,,,,
"Chào cả nhà
Mình đang làm bài toán về chống giả mạo face. Mình có sử dụng 1 model được train dataset casia-surf (được tạo từ camera intel realsense sr300). Về bộ dữ liệu này gồm 3 loại dữ liệu: rgb, depth , ir và ảnh depth đã được convert thành ảnh rgb 3 kênh màu. Về mô hình mình sử dụng chỉ được training nguyên trên ảnh depth . Sau đó mình test với camera intel realsense D435 thì thu được kết quả rất tệ. Mình đang thắc mắc vấn đề có thể vấn đề như sau:
1, Việc convert ảnh depth 16bit từ D435 thành color chưa đúng ==> dẫn đến phân phối depth khác với casia-surf dataset. Về convert ảnh depth sang rgb mình dùng thư viện pyrealsense2. Mình có thử một số cách khác nhưng vẫn không hiệu quả.
2, Có thể ảnh depth thu từ camera d435 không thể áp dụng được với mô hình được training với casia-surf dataset.
Rất mong nhận được góp ý từ cả nhà :d :d :d
Mình cám ơn","Chào cả nhà Mình đang làm bài toán về chống giả mạo face. Mình có sử dụng 1 model được train dataset casia-surf (được tạo từ camera intel realsense sr300). Về bộ dữ liệu này gồm 3 loại dữ liệu: rgb, depth , ir và ảnh depth đã được convert thành ảnh rgb 3 kênh màu. Về mô hình mình sử dụng chỉ được training nguyên trên ảnh depth . Sau đó mình test với camera intel realsense D435 thì thu được kết quả rất tệ. Mình đang thắc mắc vấn đề có thể vấn đề như sau: 1, Việc convert ảnh depth 16bit từ D435 thành color chưa đúng ==> dẫn đến phân phối depth khác với casia-surf dataset. Về convert ảnh depth sang rgb mình dùng thư viện pyrealsense2. Mình có thử một số cách khác nhưng vẫn không hiệu quả. 2, Có thể ảnh depth thu từ camera d435 không thể áp dụng được với mô hình được training với casia-surf dataset. Rất mong nhận được góp ý từ cả nhà :d :d :d Mình cám ơn",,,,,
,nan,,,,,
Curve fitting in action :),Curve fitting in action :),,,,,
"Kính chào các bác. Em xem trên Group có nhiều bạn khi cần train model không có chỗ để train: train PC thì chậm, train COLAB thì giới hạn, máy chủ thì không có....
Em thấy dịch vụ thuê GPU này hay nên làm clip chia sẻ cùng anh em newbie! Mong ad duyệt bài!","Kính chào các bác. Em xem trên Group có nhiều bạn khi cần train model không có chỗ để train: train PC thì chậm, train COLAB thì giới hạn, máy chủ thì không có.... Em thấy dịch vụ thuê GPU này hay nên làm clip chia sẻ cùng anh em newbie! Mong ad duyệt bài!",,,,,
"Mọi người cho em hỏi:
Em đang convert 1 model từ tf1 sang tf2, nhưng có 1 module tf.contrib.slim em ko tìm ra cách chuyển sang tf2 đc.
Mọi người ai có cách nào convert sang ko ạ?","Mọi người cho em hỏi: Em đang convert 1 model từ tf1 sang tf2, nhưng có 1 module tf.contrib.slim em ko tìm ra cách chuyển sang tf2 đc. Mọi người ai có cách nào convert sang ko ạ?",,,,,
"Chào các bác!
Em đang làm bài toán face anti-spoofing mà gặp khó khăn không kiếm được dữ liệu.
Em có nghiên cứu và thấy bộ MSU-USSA và CASIA-SURF khá tốt nhưng để tải về cần có chữ ký của giáo sư, cái này em không có.
Bác nào trong group có các bộ dữ liệu này hoặc bộ dữ liệu khác cho em xin với ạ.
Đội ơn các bác nhiều!","Chào các bác! Em đang làm bài toán face anti-spoofing mà gặp khó khăn không kiếm được dữ liệu. Em có nghiên cứu và thấy bộ MSU-USSA và CASIA-SURF khá tốt nhưng để tải về cần có chữ ký của giáo sư, cái này em không có. Bác nào trong group có các bộ dữ liệu này hoặc bộ dữ liệu khác cho em xin với ạ. Đội ơn các bác nhiều!",,,,,
"Mình xin phép hỏi một câu hơi ngoài lề. Mình muốn viết một phần mềm test tiếng Anh, dữ liệu đầu vào sẽ là text, hình ảnh, file nghe mp3 ( đề của bài test). Đầu ra sẽ là kết quả làm bài dạng multiple choices, file viết của thí sinh và file nói của thí sinh. Mình muốn hỏi là hiện tại dùng ngôn ngữ nào sẽ xử lý những công việc như vậy tốt nhất? Mình cảm ơn.","Mình xin phép hỏi một câu hơi ngoài lề. Mình muốn viết một phần mềm test tiếng Anh, dữ liệu đầu vào sẽ là text, hình ảnh, file nghe mp3 ( đề của bài test). Đầu ra sẽ là kết quả làm bài dạng multiple choices, file viết của thí sinh và file nói của thí sinh. Mình muốn hỏi là hiện tại dùng ngôn ngữ nào sẽ xử lý những công việc như vậy tốt nhất? Mình cảm ơn.",,,,,
"Em chưa có kiến thức về ML và DL, em chỉ có kiến thức bên điện tử. Đề tài đa của em có liên quan đến anomaly detection. Em đang muốn tìm một vài sản phẩm sử dụng thuật toán này trong các máy công nghiệp. Em cảm ơn","Em chưa có kiến thức về ML và DL, em chỉ có kiến thức bên điện tử. Đề tài đa của em có liên quan đến anomaly detection. Em đang muốn tìm một vài sản phẩm sử dụng thuật toán này trong các máy công nghiệp. Em cảm ơn",,,,,
"e đang làm app scan dữ liệu từ a4 cần đồ chính xác cao rồi dùng iron ocr xuất ra excel, mọi người cho e xin ít thông tin loại camera chuyên dụng ạh ?? giá tầm 5tr đổ lại","e đang làm app scan dữ liệu từ a4 cần đồ chính xác cao rồi dùng iron ocr xuất ra excel, mọi người cho e xin ít thông tin loại camera chuyên dụng ạh ?? giá tầm 5tr đổ lại",,,,,
"Xin chào mọi người, em đang làm bài toán binary classification, có 36 features nhưng đều là binary feature và phần lớn các giá trị là False. em đã thử các classifier khác nhau, nhưng kết quả đạt khá thấp, tầm 0.5641. Xin các Anh/Chị/Em cho em lời khuyên cho bài toán này ạ. Em cảm ơn","Xin chào mọi người, em đang làm bài toán binary classification, có 36 features nhưng đều là binary feature và phần lớn các giá trị là False. em đã thử các classifier khác nhau, nhưng kết quả đạt khá thấp, tầm 0.5641. Xin các Anh/Chị/Em cho em lời khuyên cho bài toán này ạ. Em cảm ơn",,,,,
"Có ace nào về làm đồng nghiệp với em không ạ?
Viện Toán ứng dụng và Tin học tuyển giảng viên trong các lĩnh vực:
Toán, Toán ứng dụng (Toán Công nghiệp, Toán Kinh tế, Toán Tài Chính, ...)
Xác suất, Thống kê, Khoa học dữ liệu, Máy học, Trí tuệ nhân tạo
Tối ưu, Vận trù học (Operation Research), Kỹ thuật công nghiệp (Industrial Engineering)
Khoa học Máy tính, Kỹ thuật Máy tính, Tin học
Và các lĩnh vực liên quan khác ...
Môi trường làm việc vui vẻ, hòa đồng, lương tháng khoảng 20tr ++
Ưu tiên tuyển dụng các cán bộ trình độ tiến sĩ, có kết quả nghiên cứu khoa học tốt, có kinh nghiệm giảng dạy, thành thạo ngoại ngữ, Ngoài ra ưu tiên các ace có sở thích thể dục thể thao như đá bóng, chạy bộ, ham thích văn nghệ như hát karaoke, ...
---------
PS: Em có lương tháng khoảng 40tr ++","Có ace nào về làm đồng nghiệp với em không ạ? Viện Toán ứng dụng và Tin học tuyển giảng viên trong các lĩnh vực: Toán, Toán ứng dụng (Toán Công nghiệp, Toán Kinh tế, Toán Tài Chính, ...) Xác suất, Thống kê, Khoa học dữ liệu, Máy học, Trí tuệ nhân tạo Tối ưu, Vận trù học (Operation Research), Kỹ thuật công nghiệp (Industrial Engineering) Khoa học Máy tính, Kỹ thuật Máy tính, Tin học Và các lĩnh vực liên quan khác ... Môi trường làm việc vui vẻ, hòa đồng, lương tháng khoảng 20tr ++ Ưu tiên tuyển dụng các cán bộ trình độ tiến sĩ, có kết quả nghiên cứu khoa học tốt, có kinh nghiệm giảng dạy, thành thạo ngoại ngữ, Ngoài ra ưu tiên các ace có sở thích thể dục thể thao như đá bóng, chạy bộ, ham thích văn nghệ như hát karaoke, ... --------- PS: Em có lương tháng khoảng 40tr ++",,,,,
"[Help][Dữ liệu Python đọc DB vào RAM quá nhiều] Câu hỏi mới của em đây ạ, câu hỏi cũ em đã cập nhật đưa xuống dưới.
Trong lúc làm việc với python, em nhận thấy là do Python load vào RAM quá nhiều. Các bước em làm như sau:
Bước 1: Query dữ liệu từ MongoDB (Bước này em xem thì dữ liệu cả bảng chỉ chiến 170MB)
Bước 2: Chuyển câu query thành list (Lúc này RAM bắt đầu tăng lên rất nhiều do Python đọc dữ liệu vào RAM)
Bước 3: Chuyển sang pandas dataframe
Hiện tại em đã tìm ra được 2 solution là:
1. Query từ MongoDB trên Python để đạt được kết quả mong muốn (Như vậy em không cần qua Bước 2 và 3 như ở trên em trình bày)
2. Sử dụng thư viện vaex thay cho pandas
Không biết anh chị và các bạn có thể tư vấn giúp em solution nào tối ưu hơn hay có solution khác tối ưu hơn không ạ?
Em cảm ơn mọi người rất nhiều
===========================================
Câu hỏi cũ: [Help][Đọc dữ liệu từ MongoDB vào Python chiếm quá nhiều RAM]
Em chào mọi người,
Hiện tại em đang làm việc với dữ liệu trên python. Và em gặp một vấn đề là khi đọc dữ liệu một bảng trong MongoDB vào Pandas DataFrame thì thấy bị chiếm quá nhiều RAM, dữ liệu của em có khoảng 10000 dòng thì chiếm gần 3GB RAM.
Không biết có anh/chị hay bạn nào gặp trường hợp này chưa? Có gì cho em xin chút kinh nghiệm với ạ, em cảm ơn mọi người rất nhiều.","[Help][Dữ liệu Python đọc DB vào RAM quá nhiều] Câu hỏi mới của em đây ạ, câu hỏi cũ em đã cập nhật đưa xuống dưới. Trong lúc làm việc với python, em nhận thấy là do Python load vào RAM quá nhiều. Các bước em làm như sau: Bước 1: Query dữ liệu từ MongoDB (Bước này em xem thì dữ liệu cả bảng chỉ chiến 170MB) Bước 2: Chuyển câu query thành list (Lúc này RAM bắt đầu tăng lên rất nhiều do Python đọc dữ liệu vào RAM) Bước 3: Chuyển sang pandas dataframe Hiện tại em đã tìm ra được 2 solution là: 1. Query từ MongoDB trên Python để đạt được kết quả mong muốn (Như vậy em không cần qua Bước 2 và 3 như ở trên em trình bày) 2. Sử dụng thư viện vaex thay cho pandas Không biết anh chị và các bạn có thể tư vấn giúp em solution nào tối ưu hơn hay có solution khác tối ưu hơn không ạ? Em cảm ơn mọi người rất nhiều =========================================== Câu hỏi cũ: [Help][Đọc dữ liệu từ MongoDB vào Python chiếm quá nhiều RAM] Em chào mọi người, Hiện tại em đang làm việc với dữ liệu trên python. Và em gặp một vấn đề là khi đọc dữ liệu một bảng trong MongoDB vào Pandas DataFrame thì thấy bị chiếm quá nhiều RAM, dữ liệu của em có khoảng 10000 dòng thì chiếm gần 3GB RAM. Không biết có anh/chị hay bạn nào gặp trường hợp này chưa? Có gì cho em xin chút kinh nghiệm với ạ, em cảm ơn mọi người rất nhiều.",,,,,
"Xin chào mọi người, mình đang tìm hiểu về tensorflow. Có một chỗ hơi khó hiểu muốn nhờ mọi người giải thích hộ. Tại sao cái parameter của hàm classifier.train() lại là function, mà không phải là giá trị mà function đó trả về? (dùng lambda có nghĩa là trả về function object)","Xin chào mọi người, mình đang tìm hiểu về tensorflow. Có một chỗ hơi khó hiểu muốn nhờ mọi người giải thích hộ. Tại sao cái parameter của hàm classifier.train() lại là function, mà không phải là giá trị mà function đó trả về? (dùng lambda có nghĩa là trả về function object)",,,,,
"Hi cả nhà! Em đang muốn tìm hiểu về Logistic Regression (theory thôi ạ)
Do em cần hiểu và present cho người khác nên em muốn hỏi nên làm thế nào?",Hi cả nhà! Em đang muốn tìm hiểu về Logistic Regression (theory thôi ạ) Do em cần hiểu và present cho người khác nên em muốn hỏi nên làm thế nào?,,,,,
"Kính chào các bác, em đang học về Vaex nên đăng bài để mong được chia sẻ lại bài note của em về Vaex cho các bạn newbie. Mong nhận được các góp ý của các bác và mong ad duyệt bài!","Kính chào các bác, em đang học về Vaex nên đăng bài để mong được chia sẻ lại bài note của em về Vaex cho các bạn newbie. Mong nhận được các góp ý của các bác và mong ad duyệt bài!",,,,,
"Eddited:
Làm ơn cho em hỏi
Em có 4 nghiệm: R,G,B (lấy từ Instagram image), và Instagram like
Em muốn chạy để xem mầu nào có khả năng nhận được nhiều like nhất
Em có đọc qua paper này và định dùng K-Mean clustering cho 3 mầu RGB và sau đó chạy Poisson regression với clusters và like.
https://www.researchgate.net/profile/Robertas-Damasevicius/publication/318697867_Brand_communication_in_social_media_The_use_of_image_colours_in_popular_posts/links/59b048360f7e9b374346614d/Brand-communication-in-social-media-The-use-of-image-colours-in-popular-posts.pdf
Em không rõ việc em làm có nghĩa không, em không rõ làm sao để tính probabilities của từng mầu như trong bài báo được?
Em cảm ơn","Eddited: Làm ơn cho em hỏi Em có 4 nghiệm: R,G,B (lấy từ Instagram image), và Instagram like Em muốn chạy để xem mầu nào có khả năng nhận được nhiều like nhất Em có đọc qua paper này và định dùng K-Mean clustering cho 3 mầu RGB và sau đó chạy Poisson regression với clusters và like. https://www.researchgate.net/profile/Robertas-Damasevicius/publication/318697867_Brand_communication_in_social_media_The_use_of_image_colours_in_popular_posts/links/59b048360f7e9b374346614d/Brand-communication-in-social-media-The-use-of-image-colours-in-popular-posts.pdf Em không rõ việc em làm có nghĩa không, em không rõ làm sao để tính probabilities của từng mầu như trong bài báo được? Em cảm ơn",,,,,
"Chào mọi người,
Phoenix Team xin chia sẻ lại source code và solution chi tiết của nhóm ở challenge Cassava Leaf Disease Classification trên Kaggle. Hy vọng bài viết và phần source code hữu ích đối với các bạn đang tìm hiểu về ML/DL.
Link chia sẻ về solution: https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/221113
Link source code: https://github.com/freedom1810/kaggle-cassava
1. Thông tin về challenge:
- Dạng bài: Image Classification with strongly noisy labels.
- Kết quả của nhóm: Top 16/3900 trên private leaderboard, gold medal.
2. Một số điểm nhấn trong solution của nhóm:
- EMA.
- Splitting strategies.
- Bi-tempered loss.
- Snapmix augmentation.
- SAM optimizer.
3. Các thành viên trong team: Nguyễn Hải Nam - xSeries - FUNiX, Nguyen Hai Son - SV năm cuối ĐHBKHN - Asilla.
Phoenix Team cũng đang phụ trách đào tạo toàn bộ nhóm học viên nhận học bổng AI Engineer từ QAI - FPT SOFTWARE của FUNiX cùng với các mentor khác của FUNiX. Bạn nào có hứng thú vào học và tham gia các challenge với mình thì đăng ký ở link này nhé:
https://docs.google.com/forms/d/e/1FAIpQLScScbmRhUbsRiBdAY_MX6BTFudjI8vJMY6RFb2Xv3jYZctJYg/viewform
Cảm ơn các bạn đã đọc bài và xem code, hy vọng trong thời gian tới sẽ gặp được nhiều các bạn AI Engineer của Việt Nam trên các challenge của thế giới :)","Chào mọi người, Phoenix Team xin chia sẻ lại source code và solution chi tiết của nhóm ở challenge Cassava Leaf Disease Classification trên Kaggle. Hy vọng bài viết và phần source code hữu ích đối với các bạn đang tìm hiểu về ML/DL. Link chia sẻ về solution: https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/221113 Link source code: https://github.com/freedom1810/kaggle-cassava 1. Thông tin về challenge: - Dạng bài: Image Classification with strongly noisy labels. - Kết quả của nhóm: Top 16/3900 trên private leaderboard, gold medal. 2. Một số điểm nhấn trong solution của nhóm: - EMA. - Splitting strategies. - Bi-tempered loss. - Snapmix augmentation. - SAM optimizer. 3. Các thành viên trong team: Nguyễn Hải Nam - xSeries - FUNiX, Nguyen Hai Son - SV năm cuối ĐHBKHN - Asilla. Phoenix Team cũng đang phụ trách đào tạo toàn bộ nhóm học viên nhận học bổng AI Engineer từ QAI - FPT SOFTWARE của FUNiX cùng với các mentor khác của FUNiX. Bạn nào có hứng thú vào học và tham gia các challenge với mình thì đăng ký ở link này nhé: https://docs.google.com/forms/d/e/1FAIpQLScScbmRhUbsRiBdAY_MX6BTFudjI8vJMY6RFb2Xv3jYZctJYg/viewform Cảm ơn các bạn đã đọc bài và xem code, hy vọng trong thời gian tới sẽ gặp được nhiều các bạn AI Engineer của Việt Nam trên các challenge của thế giới :)",,,,,
"Em đang làm thử 1 bài toán về Time series. Input là n arrays, mỗi array là 1000 steps. Output mong muốn là 1 array với 1000 steps. Mọi người có thể gợi ý dùng phương pháp nào để làm bài toán trên được không ạ. Em xin cảm ơn.","Em đang làm thử 1 bài toán về Time series. Input là n arrays, mỗi array là 1000 steps. Output mong muốn là 1 array với 1000 steps. Mọi người có thể gợi ý dùng phương pháp nào để làm bài toán trên được không ạ. Em xin cảm ơn.",,,,,
Mn cho e hỏi. E update sklearn và test code nhưng nó ra đoạn này là sao ạ?,Mn cho e hỏi. E update sklearn và test code nhưng nó ra đoạn này là sao ạ?,,,,,
"Hi mọi người mình đang tìm hiểu bài báo khoa học về CVPR, có phần sau không hiểu tại sao ra vậy 
Chỉ số Recall@50, Recall@100, mình chưa hiểu nó là gì & cách đánh giá của nó để ra được bảng kết quả như vậy
Predicate Recognition/Union Box Detection/Two Boxes Detection - ý nghĩa và cách dùng
Mong mọi người giải thích giúp mình những phần này
Cám ơn mọi người!

https://drive.google.com/file/d/1R2TpNmWT9PEOKchkZ_FzTHR0-FzSfaeT/view?usp=sharing","Hi mọi người mình đang tìm hiểu bài báo khoa học về CVPR, có phần sau không hiểu tại sao ra vậy Chỉ số Recall@50, Recall@100, mình chưa hiểu nó là gì & cách đánh giá của nó để ra được bảng kết quả như vậy Predicate Recognition/Union Box Detection/Two Boxes Detection - ý nghĩa và cách dùng Mong mọi người giải thích giúp mình những phần này Cám ơn mọi người! https://drive.google.com/file/d/1R2TpNmWT9PEOKchkZ_FzTHR0-FzSfaeT/view?usp=sharing",,,,,
"Bạn Chip Huyền sẽ có một bài thuyết trình trực tuyến về ""Machine learning for scalable applications"" tại #Online FOSSASIA Summit tháng 3, 13-21. Ngoài ra còn có Product Lead của PyTorch và rất nhiều nội dung hấp dẫn về #AI và #MachineLearning. Mời các bạn đăng ký tham gia miễn phí tại đây https://eventyay.com/e/fa96ae2c","Bạn Chip Huyền sẽ có một bài thuyết trình trực tuyến về ""Machine learning for scalable applications"" tại FOSSASIA Summit tháng 3, 13-21. Ngoài ra còn có Product Lead của PyTorch và rất nhiều nội dung hấp dẫn về và Mời các bạn đăng ký tham gia miễn phí tại đây https://eventyay.com/e/fa96ae2c",#Online	#AI	#MachineLearning.,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2020 vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 9/2020 vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
"Chào mọi người, trước đây khi đụng phải dataset không cân bằng thì ngoài augment thêm hình, em cũng áp dụng ROC curve để tìm threshold tối ưu nhất và luôn cho r kết quả rất khả quan.
Nhưng lần này kết quả có vẻ tệ hơn ban đầu một chút. Mọi người cho em hỏi đây có phải là do model vẫn chưa đủ tốt, hay hướng tiếp cận ROC curve cho dataset này là sai ạ? Với dataset mất cân bằng thì còn cách nào khác để giải quyết không ạ?
p/s: hình đầu tiên threshold mặc định 0.5","Chào mọi người, trước đây khi đụng phải dataset không cân bằng thì ngoài augment thêm hình, em cũng áp dụng ROC curve để tìm threshold tối ưu nhất và luôn cho r kết quả rất khả quan. Nhưng lần này kết quả có vẻ tệ hơn ban đầu một chút. Mọi người cho em hỏi đây có phải là do model vẫn chưa đủ tốt, hay hướng tiếp cận ROC curve cho dataset này là sai ạ? Với dataset mất cân bằng thì còn cách nào khác để giải quyết không ạ? p/s: hình đầu tiên threshold mặc định 0.5",,,,,
,nan,,,,,
"Xin chào anh/chị, em là sinh viên đại học đang làm đồ án tốt nghiệp về Ms Azure Kinect. Một mình tự tìm hiểu hơi khó nên ai có hứng thú & am hiểu về nó có thể kết bạn, cùng tìm hiểu & chỉ thêm cho em ạ. Em cảm ơn!","Xin chào anh/chị, em là sinh viên đại học đang làm đồ án tốt nghiệp về Ms Azure Kinect. Một mình tự tìm hiểu hơi khó nên ai có hứng thú & am hiểu về nó có thể kết bạn, cùng tìm hiểu & chỉ thêm cho em ạ. Em cảm ơn!",,,,,
Mọi người cho mình hỏi cần học những gì để có được 1 công việc ở vị trí Data Engineer vậy ?,Mọi người cho mình hỏi cần học những gì để có được 1 công việc ở vị trí Data Engineer vậy ?,,,,,
"#MLNET
Chào mọi người.
Hiện tại mình đang tìm hiểu thằng ML.NET mình Detect được Object rồi, nhưng làm sao để nó dùng GPU để Detect nhỉ? Mình Search từ hôm qua đến giờ vẫn không có bài viết nào nói vấn đề này. Các bác có ai làm qua rồi thì Support mình với.","Chào mọi người. Hiện tại mình đang tìm hiểu thằng ML.NET mình Detect được Object rồi, nhưng làm sao để nó dùng GPU để Detect nhỉ? Mình Search từ hôm qua đến giờ vẫn không có bài viết nào nói vấn đề này. Các bác có ai làm qua rồi thì Support mình với.",#MLNET,,,,
"Chào các bạn :) mình mới bắt đầu tìm hiểu AI ML DL … bạn nào đã tìm hiểu qua có thể hướng dẫn và kèm cặp mình cách học, hướng học với được không ạ ^_^ cám ơn các bạn quan tâm.","Chào các bạn :) mình mới bắt đầu tìm hiểu AI ML DL … bạn nào đã tìm hiểu qua có thể hướng dẫn và kèm cặp mình cách học, hướng học với được không ạ ^_^ cám ơn các bạn quan tâm.",,,,,
"#deeplearning #efficientnet
Em bị khúc mắc khi train model này với tập data nhỏ ( tầm 1300 ảnh) thì hàm loss tối ưu giảm dần.
Nhưng khi lên data to ( 33 000 ảnh) thì loss lại khựng lại.
Mọi người chỉ ra vấn đề giúp em với ạ.
Cảm ơn group",Em bị khúc mắc khi train model này với tập data nhỏ ( tầm 1300 ảnh) thì hàm loss tối ưu giảm dần. Nhưng khi lên data to ( 33 000 ảnh) thì loss lại khựng lại. Mọi người chỉ ra vấn đề giúp em với ạ. Cảm ơn group,#deeplearning	#efficientnet,,,,
"Xin chào cả nhà, em là thành viên mới của nhóm, em đang nghiên cứu về khả năng ứng dụng ML trong lĩnh vực định vị dẫn đường thiết bị chuyển động trong thời gian thực. Em có một câu hỏi rất mong các ace trong nhóm có thể trả lời giúp em ạ:
Em đang sử dụng mạng Extreme learning mahchine (ELM) để huấn luyện một mạng thời gian thực, Em dùng hàm sigmoid là hàm kích hoạt cho khâu cuối cùng của mạng, do vậy trước khi đưa vào huấn luyện mạng thì phần dữ liệu của tập target em cần phải scale về miền giá trị từ 0-1, do đó tập giá trị trọng số W thu được sau khi luyện sẽ ứng với target là trong khoảng 0-1. Vậy khi chuyển qua quá trình dự đoán nếu ta sử dụng bộ trọng số W thu được ở trên để dự đoán đầu ra thì giá trị đầu ra cũng sẽ nằm trong miền giá trị 0-1, vậy lúc này làm cách nào để mình trả về giá trị đúng của giá trị đầu ra? ace nào biết cách có thể giúp em câu hỏi này được không ạ, em cám ơn rất nhiều ạ","Xin chào cả nhà, em là thành viên mới của nhóm, em đang nghiên cứu về khả năng ứng dụng ML trong lĩnh vực định vị dẫn đường thiết bị chuyển động trong thời gian thực. Em có một câu hỏi rất mong các ace trong nhóm có thể trả lời giúp em ạ: Em đang sử dụng mạng Extreme learning mahchine (ELM) để huấn luyện một mạng thời gian thực, Em dùng hàm sigmoid là hàm kích hoạt cho khâu cuối cùng của mạng, do vậy trước khi đưa vào huấn luyện mạng thì phần dữ liệu của tập target em cần phải scale về miền giá trị từ 0-1, do đó tập giá trị trọng số W thu được sau khi luyện sẽ ứng với target là trong khoảng 0-1. Vậy khi chuyển qua quá trình dự đoán nếu ta sử dụng bộ trọng số W thu được ở trên để dự đoán đầu ra thì giá trị đầu ra cũng sẽ nằm trong miền giá trị 0-1, vậy lúc này làm cách nào để mình trả về giá trị đúng của giá trị đầu ra? ace nào biết cách có thể giúp em câu hỏi này được không ạ, em cám ơn rất nhiều ạ",,,,,
"Hello cả nhà.
Mọi người đã nghe nói tới SOTA mới NFNets do DeepMind giới thiệu về image classification chưa? (Link paper: https://arxiv.org/abs/2102.06171). Nghiên cứu mới này đã chỉ ra những điểm tốt và hạn chế của BatchNorm, từ đó tiến tới loại bỏ nó. DeepMind có đề xuất một họ các models NFNets đã đạt được tới 86,5% acc top1 trên imagenet, đánh bại EfficientNets về cả độ chính xác và thời gian.
Mình có implement những models này bằng Tensorflow2 theo link dưới đây. Bạn nào có máy cấu hình mạnh có thể thử train thử xem kết quả cuối cùng ra sao, nhất là mấy models nặng nhất. Bạn nào có hứng thú có thể ib thảo luận thêm hoặc trực tiếp pull requests/create issues, you’re welcome.
https://github.com/hoangthang1607/nfnets-Tensorflow-2
Update: Thêm pre-trained weights converted từ bên DeepMind.","Hello cả nhà. Mọi người đã nghe nói tới SOTA mới NFNets do DeepMind giới thiệu về image classification chưa? (Link paper: https://arxiv.org/abs/2102.06171). Nghiên cứu mới này đã chỉ ra những điểm tốt và hạn chế của BatchNorm, từ đó tiến tới loại bỏ nó. DeepMind có đề xuất một họ các models NFNets đã đạt được tới 86,5% acc top1 trên imagenet, đánh bại EfficientNets về cả độ chính xác và thời gian. Mình có implement những models này bằng Tensorflow2 theo link dưới đây. Bạn nào có máy cấu hình mạnh có thể thử train thử xem kết quả cuối cùng ra sao, nhất là mấy models nặng nhất. Bạn nào có hứng thú có thể ib thảo luận thêm hoặc trực tiếp pull requests/create issues, you’re welcome. https://github.com/hoangthang1607/nfnets-Tensorflow-2 Update: Thêm pre-trained weights converted từ bên DeepMind.",,,,,
"Xin chào mọi người. Mình đang tìm hiểu về DDPG trong reinforcement learning nhưng khó hiểu quá. Có anh chị nào có tai liêu dễ hiểu xin giúp em với.
Xin cám ơn",Xin chào mọi người. Mình đang tìm hiểu về DDPG trong reinforcement learning nhưng khó hiểu quá. Có anh chị nào có tai liêu dễ hiểu xin giúp em với. Xin cám ơn,,,,,
"Mình có dịch nội dung phần #matrixfactorization dành cho #Recommendationsystem. Có nhiều chỗ mình hiểu và dịch (và đọc lại thì vẫn hiểu vì đã hiểu từ nghĩa tiếng Việt) nhưng ko chắc là người khác đọc có hiểu liền ko. Do đó mình post lên sẵn nhờ mọi người review và đóng góp ý kiến.
Cám ơn anh Vũ Hữu Tiệp đã cho phép em translate nội dung.",Mình có dịch nội dung phần dành cho Có nhiều chỗ mình hiểu và dịch (và đọc lại thì vẫn hiểu vì đã hiểu từ nghĩa tiếng Việt) nhưng ko chắc là người khác đọc có hiểu liền ko. Do đó mình post lên sẵn nhờ mọi người review và đóng góp ý kiến. Cám ơn anh Vũ Hữu Tiệp đã cho phép em translate nội dung.,#matrixfactorization	#Recommendationsystem.,,,,
"em chào mọi người ạ!
Anh/ chị và các bạn cho em hỏi là google có open source / giải pháp nào trong Data assessment ạ ?
Em cảm ơn!",em chào mọi người ạ! Anh/ chị và các bạn cho em hỏi là google có open source / giải pháp nào trong Data assessment ạ ? Em cảm ơn!,,,,,
"Giới thiệu VN AIDr (https://dr.vnopenai.org/) - Nền tảng xử lý ảnh y tế nguồn mở xây dựng bởi các kĩ sư Việt Nam!
Chi tiết xin mọi người xem ở link dưới.",Giới thiệu VN AIDr (https://dr.vnopenai.org/) - Nền tảng xử lý ảnh y tế nguồn mở xây dựng bởi các kĩ sư Việt Nam! Chi tiết xin mọi người xem ở link dưới.,,,,,
"I'd like to post this opening we have (if the group admin permits).
Our company Yaraku Inc (developing adaptive machine translation systems) is looking for DL/NLP researchers/engineers passionate about languages and interested in working on machine translation. We have an opening at Yaraku Inc's office in Tokyo.
https://www.linkedin.com/jobs/view/2380307850/
Languages we are interested in translating to/from include English, Vietnamese, Korean, Thai and Chinese.
I have been looking for Vietnamese NLP/DL professionals for a long time and have not been able to find them, so I was hoping to find referrals to suitable candidates here. Please note: the job requires a good knowledge of English and excellent Python programming and algorithms skills in addition to the ability to build DL models from scratch and a good NLP/MT academic background. The candidate will be working remotely from Vietnam till Covid is over but is then expected to join the team in Tokyo.
Please apply directly using the LinkedIn link if interested.","I'd like to post this opening we have (if the group admin permits). Our company Yaraku Inc (developing adaptive machine translation systems) is looking for DL/NLP researchers/engineers passionate about languages and interested in working on machine translation. We have an opening at Yaraku Inc's office in Tokyo. https://www.linkedin.com/jobs/view/2380307850/ Languages we are interested in translating to/from include English, Vietnamese, Korean, Thai and Chinese. I have been looking for Vietnamese NLP/DL professionals for a long time and have not been able to find them, so I was hoping to find referrals to suitable candidates here. Please note: the job requires a good knowledge of English and excellent Python programming and algorithms skills in addition to the ability to build DL models from scratch and a good NLP/MT academic background. The candidate will be working remotely from Vietnam till Covid is over but is then expected to join the team in Tokyo. Please apply directly using the LinkedIn link if interested.",,,,,
"Xin chào mọi người, em là người mới học machine learning, em hiện đang làm một dự án về xử lý ngôn ngữ tự nhiên cho tiếng Việt, em làm bước word embedding dùng fastText và word2vec trên tập dữ liệu mà em crawl từ facebook, nhưng mà kết quả của em không được tốt, em có xem các bài hướng dẫn khác thì họ bảo xây dựng trên dataset của bác Lê Hồng Phương thì khá tốt mà em không tìm để tải được, có bác nào có thông tin về bộ dataset nào thì cho em xin được không ạ, em xin cảm ơn nhiều ạ.","Xin chào mọi người, em là người mới học machine learning, em hiện đang làm một dự án về xử lý ngôn ngữ tự nhiên cho tiếng Việt, em làm bước word embedding dùng fastText và word2vec trên tập dữ liệu mà em crawl từ facebook, nhưng mà kết quả của em không được tốt, em có xem các bài hướng dẫn khác thì họ bảo xây dựng trên dataset của bác Lê Hồng Phương thì khá tốt mà em không tìm để tải được, có bác nào có thông tin về bộ dataset nào thì cho em xin được không ạ, em xin cảm ơn nhiều ạ.",,,,,
"Trong lúc đại dịch Covid-19 chưa buông tha nhân loại dù đã có vaccine. Ở Việt Nam, dịch COVID-19 cũng gây ra nhiều tổn thất về người, kinh tế, đảo lộn cuộc sống vốn đã khó khăn của nhiều người. Cùng chung tay, góp sức đẩy lùi đại dịch, nhóm phát triển phần mềm của Đại học Duy Tân đã phát triển công cụ chẩn đoán dựa trên X Quang và CT phổi các tác nhân gây viêm phổi là COVID-19, Lao phổi, viêm phổi do tác nhân khác và ảnh phổi bình thường tại đây","Trong lúc đại dịch Covid-19 chưa buông tha nhân loại dù đã có vaccine. Ở Việt Nam, dịch COVID-19 cũng gây ra nhiều tổn thất về người, kinh tế, đảo lộn cuộc sống vốn đã khó khăn của nhiều người. Cùng chung tay, góp sức đẩy lùi đại dịch, nhóm phát triển phần mềm của Đại học Duy Tân đã phát triển công cụ chẩn đoán dựa trên X Quang và CT phổi các tác nhân gây viêm phổi là COVID-19, Lao phổi, viêm phổi do tác nhân khác và ảnh phổi bình thường tại đây",,,,,
"Em đang tìm hiểu về bài toán Invalid traffic classification.
Ví dụ trong một chiến dịch quảng cáo có những người click vào, Invalid traffic sẽ bao gồm vô tình bấm và cố tình bầm(click farm)
Dữ liệu bao gồm như hình.
Các bác có ý tưởng hay từng làm bài toán này cho em xin tham khảo ý tưởng ạ. Vì nhìn vào dữ liệu, em hiện tại chưa có ý tưởng :(","Em đang tìm hiểu về bài toán Invalid traffic classification. Ví dụ trong một chiến dịch quảng cáo có những người click vào, Invalid traffic sẽ bao gồm vô tình bấm và cố tình bầm(click farm) Dữ liệu bao gồm như hình. Các bác có ý tưởng hay từng làm bài toán này cho em xin tham khảo ý tưởng ạ. Vì nhìn vào dữ liệu, em hiện tại chưa có ý tưởng :(",,,,,
"mng cho e hỏi sao w lại là dấu cộng mà không phải dấu trừ v ạ,
link:https://machinelearningcoban.com/2017/01/27/logisticregression/","mng cho e hỏi sao w lại là dấu cộng mà không phải dấu trừ v ạ, link:https://machinelearningcoban.com/2017/01/27/logisticregression/",,,,,
"Marketing ứng dụng Trí Tuệ Nhân Tạo (AI) như thế nào?
#AI #Marketing #MachineLearning #DeepLearning",Marketing ứng dụng Trí Tuệ Nhân Tạo (AI) như thế nào?,#AI	#Marketing	#MachineLearning	#DeepLearning,,,,
"Tổng hợp giọng nói đã xuất hiện lâu rồi, nhưng nếu bạn muốn mày mò viết chương trình python đọc văn bản tiếng Việt dựa trên các thư viện đã có?
Tài liệu lượm lặt!","Tổng hợp giọng nói đã xuất hiện lâu rồi, nhưng nếu bạn muốn mày mò viết chương trình python đọc văn bản tiếng Việt dựa trên các thư viện đã có? Tài liệu lượm lặt!",,,,,
,nan,,,,,
"Em xin phép chia sẻ một dự án AI cho sức khoẻ trong mùa dịch COVID: Đếm số hít đất trong luyện tập thể thao.
Dự án này mới ở mức sơ khai, chưa hoàn thiện. Nhóm em hi vọng có thể chia sẻ ý tưởng và baseline để mọi người cùng nhau hoàn thiện nó thành một sản phẩm hữu ích cho người dùng.
https://www.facebook.com/aicurious/posts/180473117203551","Em xin phép chia sẻ một dự án AI cho sức khoẻ trong mùa dịch COVID: Đếm số hít đất trong luyện tập thể thao. Dự án này mới ở mức sơ khai, chưa hoàn thiện. Nhóm em hi vọng có thể chia sẻ ý tưởng và baseline để mọi người cùng nhau hoàn thiện nó thành một sản phẩm hữu ích cho người dùng. https://www.facebook.com/aicurious/posts/180473117203551",,,,,
"Mọi nguời cho em hỏi với ạ
Em đã train xong model và truớc đó em có dùng pd.get_dummies() cho tập X_test, giờ em muốn model em predict một single record đã xử lý qua pd.get_dummies() nhưng nó cho ra ít column hơn so với X_test, vậy làm thế nào để khắc phục ạ, hoặc có những kĩ thuật nào khác để đưa một single record vào để predict ạ. Em cảm ơn các bác !","Mọi nguời cho em hỏi với ạ Em đã train xong model và truớc đó em có dùng pd.get_dummies() cho tập X_test, giờ em muốn model em predict một single record đã xử lý qua pd.get_dummies() nhưng nó cho ra ít column hơn so với X_test, vậy làm thế nào để khắc phục ạ, hoặc có những kĩ thuật nào khác để đưa một single record vào để predict ạ. Em cảm ơn các bác !",,,,,
"Hi mọi người, em đang có ý định build một ML model bằng C++ nhưng không biết có thư viện nào của C++ chuyên dùng cho data visualization và algebra không. Mong các anh chị recommend giúp em ạ. Em cám ơn mọi người.","Hi mọi người, em đang có ý định build một ML model bằng C++ nhưng không biết có thư viện nào của C++ chuyên dùng cho data visualization và algebra không. Mong các anh chị recommend giúp em ạ. Em cám ơn mọi người.",,,,,
"Dataset: Oulu-CASIA NIR&VIS.
Mình xin chia sẽ bộ dataset về cảm xúc khuôn mặt Oulu-CASIA NIR&VIS.
Dataset bao gồm 80 chủ thể (people) )được quay bởi 2 loại camera (Near Infrared và Camera thường), mỗi camera quay dưới 3 điều kiện ánh sáng khác nhau: ánh sáng thường, ánh sáng yếu (chỉ mở mỗi màn hình PC) và trường hợp tối (tắt hết đèn).
Mỗi chủ thể có 6 cảm xúc.
Link download: https://drive.google.com/file/d/1UvAFqc6GIgpMd7wkEvFqNHGvwG0jA2Y4/view?usp=sharing
Source: download từ Baidu.","Dataset: Oulu-CASIA NIR&VIS. Mình xin chia sẽ bộ dataset về cảm xúc khuôn mặt Oulu-CASIA NIR&VIS. Dataset bao gồm 80 chủ thể (people) )được quay bởi 2 loại camera (Near Infrared và Camera thường), mỗi camera quay dưới 3 điều kiện ánh sáng khác nhau: ánh sáng thường, ánh sáng yếu (chỉ mở mỗi màn hình PC) và trường hợp tối (tắt hết đèn). Mỗi chủ thể có 6 cảm xúc. Link download: https://drive.google.com/file/d/1UvAFqc6GIgpMd7wkEvFqNHGvwG0jA2Y4/view?usp=sharing Source: download từ Baidu.",,,,,
"Em chào các anh chị,
Em đang làm một số câu quiz về confusion matrix. Em có 1 số câu hỏi sau:
1. Trước đây em hay chọn ngưỡng để phân predict label là 0.5, hôm nay em đọc 1 bài bảo nên chọn ngưỡng mà có giá trị F1 cao nhất. Điều này có đúng ko ạ?
2. Em đã đọc tài liệu nhưng ko hiểu rõ khi nào dùng f1score với average='macro' thì hợp lý. Em thường thử với average='binary'.
3. Với 2 hình như dưới, hình phải thì có F1 score cao hơn nhưng TPR thấp hơn, mình chọn có ổn ko ạ.
Đây là hình minh hoạ nên F1 score cả 2 hình đều thấp ạ.
Xin cám ơn các anh chị chỉ giáo.","Em chào các anh chị, Em đang làm một số câu quiz về confusion matrix. Em có 1 số câu hỏi sau: 1. Trước đây em hay chọn ngưỡng để phân predict label là 0.5, hôm nay em đọc 1 bài bảo nên chọn ngưỡng mà có giá trị F1 cao nhất. Điều này có đúng ko ạ? 2. Em đã đọc tài liệu nhưng ko hiểu rõ khi nào dùng f1score với average='macro' thì hợp lý. Em thường thử với average='binary'. 3. Với 2 hình như dưới, hình phải thì có F1 score cao hơn nhưng TPR thấp hơn, mình chọn có ổn ko ạ. Đây là hình minh hoạ nên F1 score cả 2 hình đều thấp ạ. Xin cám ơn các anh chị chỉ giáo.",,,,,
"[ĐẶT TRƯỚC] Bản dịch cuốn Interpretable Machine Learning (Học máy khả diễn giải)

Chào các bạn, mình đã từng kêu gọi xây dựng nhóm dịch cuốn Interpretable Machine Learning như hình dưới. Sau gần 8 tháng hoạt động liên tục, nhóm dịch gồm 2 dịch giả chính là mình (Giang Nguyen - KAIST) và Duy-Tung Nguyen (University of Missouri) dự kiến sẽ release bản dịch của cuốn sách vào giữa tháng 2/2021. Tại version đầu tiên của cuốn sách, nhóm sẽ chỉ gửi ra ĐÚNG 200 bản và hoàn toàn FREE. Nếu bạn muốn nhận bản dịch hãy để lại địa chỉ email dưới comment và nhóm sẽ gửi thông tin cũng như bản dịch của cuốn sách cho bạn vào giữa tháng sau.

Cám ơn và chúc các bạn học vui vẻ!
 — với Nguyễn Tùng.","[ĐẶT TRƯỚC] Bản dịch cuốn Interpretable Machine Learning (Học máy khả diễn giải) Chào các bạn, mình đã từng kêu gọi xây dựng nhóm dịch cuốn Interpretable Machine Learning như hình dưới. Sau gần 8 tháng hoạt động liên tục, nhóm dịch gồm 2 dịch giả chính là mình (Giang Nguyen - KAIST) và Duy-Tung Nguyen (University of Missouri) dự kiến sẽ release bản dịch của cuốn sách vào giữa tháng 2/2021. Tại version đầu tiên của cuốn sách, nhóm sẽ chỉ gửi ra ĐÚNG 200 bản và hoàn toàn FREE. Nếu bạn muốn nhận bản dịch hãy để lại địa chỉ email dưới comment và nhóm sẽ gửi thông tin cũng như bản dịch của cuốn sách cho bạn vào giữa tháng sau. Cám ơn và chúc các bạn học vui vẻ! — với Nguyễn Tùng.",,,,,
"Em chào mọi người ạ. Em đang học về machine learning và đang train một model về food detection. Em có sử dụng data set ở kaggle và sử dụng pre-trained model : EfficientNetB0. Code em để link tại đây: https://www.kaggle.com/quynhngo7075/efficientnetb0-training
Sau khi nhìn vào model thì model của em bị overfitting (val_accurency thấp nhưng train_accurency cao). Em đã thử áp dụng phương pháp data augmentation và add thêm dropout layers (với drop out rate = 0.2) nhưng accurency không được improve.
Không biết mọi người có cao kiến gì để em có thể tăng accurency lên không ạ ? Em cảm ơn mọi người nhiều
Edit lần 1:
Chào mọi người lần nữa ạ, lời đầu tiên cảm ơn mọi người đã góp ý kiến cũng như chỉ ra những lỗi sai trong model của em.
Sau vài ngày chỉnh sửa, accuracy đã tăng lên đến 80% và đưới đây là link code sau khi em đã chỉnh sửa để mọi người tham khảo ạ.
https://www.kaggle.com/quynhngo7075/efficientnetb0-training","Em chào mọi người ạ. Em đang học về machine learning và đang train một model về food detection. Em có sử dụng data set ở kaggle và sử dụng pre-trained model : EfficientNetB0. Code em để link tại đây: https://www.kaggle.com/quynhngo7075/efficientnetb0-training Sau khi nhìn vào model thì model của em bị overfitting (val_accurency thấp nhưng train_accurency cao). Em đã thử áp dụng phương pháp data augmentation và add thêm dropout layers (với drop out rate = 0.2) nhưng accurency không được improve. Không biết mọi người có cao kiến gì để em có thể tăng accurency lên không ạ ? Em cảm ơn mọi người nhiều Edit lần 1: Chào mọi người lần nữa ạ, lời đầu tiên cảm ơn mọi người đã góp ý kiến cũng như chỉ ra những lỗi sai trong model của em. Sau vài ngày chỉnh sửa, accuracy đã tăng lên đến 80% và đưới đây là link code sau khi em đã chỉnh sửa để mọi người tham khảo ạ. https://www.kaggle.com/quynhngo7075/efficientnetb0-training",,,,,
"Baidu Team Introduces ERNIE-M: A Multilingual Model that Learns 96 Languages from Monolingual Corpora
Paper Summary: https://www.marktechpost.com/2021/02/12/baidu-team-introduces-ernie-m-a-multilingual-model-that-learns-96-languages-from-monolingual-corpora/
Paper: https://arxiv.org/abs/2012.15674",Baidu Team Introduces ERNIE-M: A Multilingual Model that Learns 96 Languages from Monolingual Corpora Paper Summary: https://www.marktechpost.com/2021/02/12/baidu-team-introduces-ernie-m-a-multilingual-model-that-learns-96-languages-from-monolingual-corpora/ Paper: https://arxiv.org/abs/2012.15674,,,,,
"Nắng đẹp mùng 2 Tết Tân Sửu 2021....
#share
Chúc anh chị em đang đã và sẽ tham gia nghề phân tích dữ liệu, xử lý dữ liệu, tạo ứng dụng ra quyết định thông minh một năm 2021 sức khoẻ thành công!","Nắng đẹp mùng 2 Tết Tân Sửu 2021.... Chúc anh chị em đang đã và sẽ tham gia nghề phân tích dữ liệu, xử lý dữ liệu, tạo ứng dụng ra quyết định thông minh một năm 2021 sức khoẻ thành công!",#share,,,,
"Làm ơn cho em hỏi, em mơi học và đang cố làm mô hình Gan để dựng lại măt người cho image size 64 có mầu:
Input shape = (5299, 64, 64, 3) từ function load_read_samples() nhưng khi em đút vào model thì nó bị reshape thành (1, 5299, 64, 64) và bị throw lỗi như sau:
ValueError: Input 0 of layer dense_2 is incompatible with the layer: expected axis -1 of input shape to have value 32768 but received input with shape (1, 173604864)
WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='conv2d_3_input'), name='conv2d_3_input', description=""created by layer 'conv2d_3_input'""), but it was called on an input with incompatible shape (1, 5298, 64, 64, 3).
Em đã check lại upscale and downscale layer và search google nhưng vẫn không biết sai ở đâu. Làm ơn giúp em với.
Đây là model em đang lamf:
def define_discriminator(in_shape=(64,64,3)):
model = Sequential()
# downsample
model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=in_shape))
model.add(LeakyReLU(alpha=0.2))
# downsample
model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.2))
# classifier
model.add(Flatten())
model.add(Dropout(0.4))
model.add(Dense(1, activation='sigmoid'))
# compile model
opt = Adam(lr=0.0002, beta_1=0.5)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
return model
def define_generator(latent_dim):
model = Sequential()
# foundation for 7x7 image
n_nodes = 512 * 8 * 8
model.add(Dense(n_nodes, input_dim=latent_dim))
model.add(LeakyReLU(alpha=0.2))
model.add(Reshape((8, 8, 512)))
model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.2))
model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.2))
model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.2))
# generate
model.add(Conv2D(3, (8,8), activation='tanh', padding='same'))
return model
def define_gan(generator, discriminator):
# make weights in the discriminator not trainable
discriminator.trainable = False
# connect them
model = Sequential()
# add generator
model.add(generator)
# add the discriminator
model.add(discriminator)
# compile model
opt = Adam(lr=0.0002, beta_1=0.5)
model.compile(loss='binary_crossentropy', optimizer=opt)
return model
# load fashion mnist images
def load_real_samples(path, size=(64,64)):
data_list = list()
# enumerate filenames in directory, assume all are images
for filename in listdir(path):
pixels = load_img(path + filename, target_size=size)
pixels = img_to_array(pixels)
data_list.append(pixels)
return asarray(data_list)

# def load_real_samples():
# # load dataset
# (trainX, _), (_, _) = load_data()
# # expand to 3d, e.g. add channels
# X = expand_dims(trainX, axis=-1)
# # convert from ints to floats
# X = X.astype('float32')
# # scale from [0,255] to [-1,1]
# X = (X - 127.5) / 127.5
# return X
# select real samples
def generate_real_samples(dataset, n_samples):
# choose random instances
ix = randint(0, 5298, n_samples).any()
# select images
X = dataset[ix]
# generate class labels
y = ones((n_samples, 1))
return X, y
# generate points in latent space as input for the generator
def generate_latent_points(latent_dim, n_samples):
# generate points in the latent space
x_input = randn(latent_dim * n_samples)
# reshape into a batch of inputs for the network
x_input = x_input.reshape(n_samples, latent_dim)
return x_input
# use the generator to generate n fake examples, with class labels
def generate_fake_samples(generator, latent_dim, n_samples):
# generate points in latent space
x_input = generate_latent_points(latent_dim, n_samples)
# predict outputs
X = generator.predict(x_input)
# create class labels
y = zeros((n_samples, 1))
return X, y
# train the generator and discriminator
def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=2):
bat_per_epo = int(5298 // n_batch)
half_batch = int(n_batch / 2)
# manually enumerate epochs
for i in range(n_epochs):
# enumerate batches over the training set
for j in range(bat_per_epo):
# get randomly selected 'real' samples
X_real, y_real = generate_real_samples(dataset, half_batch)
# update discriminator model weights
d_loss1, _ = d_model.train_on_batch(X_real, y_real)
# generate 'fake' examples
X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
# update discriminator model weights
d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)
# prepare points in latent space as input for the generator
X_gan = generate_latent_points(latent_dim, n_batch)
# create inverted labels for the fake samples
y_gan = ones((n_batch, 1))
# update the generator via the discriminator's error
g_loss = gan_model.train_on_batch(X_gan, y_gan)
# summarize loss on this batch
print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %
(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))
# save the generator model
g_model.save('generator.h5')
# size of the latent space
latent_dim = 100
# create the discriminator
discriminator = define_discriminator()
# create the generator
generator = define_generator(latent_dim)
# create the gan
gan_model = define_gan(generator, discriminator)
# load image data
dataset = load_real_samples(dir)
# train model
train(generator, discriminator, gan_model, dataset, latent_dim)","Làm ơn cho em hỏi, em mơi học và đang cố làm mô hình Gan để dựng lại măt người cho image size 64 có mầu: Input shape = (5299, 64, 64, 3) từ function load_read_samples() nhưng khi em đút vào model thì nó bị reshape thành (1, 5299, 64, 64) và bị throw lỗi như sau: ValueError: Input 0 of layer dense_2 is incompatible with the layer: expected axis -1 of input shape to have value 32768 but received input with shape (1, 173604864) WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='conv2d_3_input'), name='conv2d_3_input', description=""created by layer 'conv2d_3_input'""), but it was called on an input with incompatible shape (1, 5298, 64, 64, 3). Em đã check lại upscale and downscale layer và search google nhưng vẫn không biết sai ở đâu. Làm ơn giúp em với. Đây là model em đang lamf: def define_discriminator(in_shape=(64,64,3)): model = Sequential() # downsample model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=in_shape)) model.add(LeakyReLU(alpha=0.2)) # downsample model.add(Conv2D(128, (3,3), strides=(2,2), padding='same')) model.add(LeakyReLU(alpha=0.2)) # classifier model.add(Flatten()) model.add(Dropout(0.4)) model.add(Dense(1, activation='sigmoid')) # compile model opt = Adam(lr=0.0002, beta_1=0.5) model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) return model def define_generator(latent_dim): model = Sequential() # foundation for 7x7 image n_nodes = 512 * 8 * 8 model.add(Dense(n_nodes, input_dim=latent_dim)) model.add(LeakyReLU(alpha=0.2)) model.add(Reshape((8, 8, 512))) model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same')) model.add(LeakyReLU(alpha=0.2)) model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same')) model.add(LeakyReLU(alpha=0.2)) model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same')) model.add(LeakyReLU(alpha=0.2)) # generate model.add(Conv2D(3, (8,8), activation='tanh', padding='same')) return model def define_gan(generator, discriminator): # make weights in the discriminator not trainable discriminator.trainable = False # connect them model = Sequential() # add generator model.add(generator) # add the discriminator model.add(discriminator) # compile model opt = Adam(lr=0.0002, beta_1=0.5) model.compile(loss='binary_crossentropy', optimizer=opt) return model # load fashion mnist images def load_real_samples(path, size=(64,64)): data_list = list() # enumerate filenames in directory, assume all are images for filename in listdir(path): pixels = load_img(path + filename, target_size=size) pixels = img_to_array(pixels) data_list.append(pixels) return asarray(data_list) # def load_real_samples(): # # load dataset # (trainX, _), (_, _) = load_data() # # expand to 3d, e.g. add channels # X = expand_dims(trainX, axis=-1) # # convert from ints to floats # X = X.astype('float32') # # scale from [0,255] to [-1,1] # X = (X - 127.5) / 127.5 # return X # select real samples def generate_real_samples(dataset, n_samples): # choose random instances ix = randint(0, 5298, n_samples).any() # select images X = dataset[ix] # generate class labels y = ones((n_samples, 1)) return X, y # generate points in latent space as input for the generator def generate_latent_points(latent_dim, n_samples): # generate points in the latent space x_input = randn(latent_dim * n_samples) # reshape into a batch of inputs for the network x_input = x_input.reshape(n_samples, latent_dim) return x_input # use the generator to generate n fake examples, with class labels def generate_fake_samples(generator, latent_dim, n_samples): # generate points in latent space x_input = generate_latent_points(latent_dim, n_samples) # predict outputs X = generator.predict(x_input) # create class labels y = zeros((n_samples, 1)) return X, y # train the generator and discriminator def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=2): bat_per_epo = int(5298 // n_batch) half_batch = int(n_batch / 2) # manually enumerate epochs for i in range(n_epochs): # enumerate batches over the training set for j in range(bat_per_epo): # get randomly selected 'real' samples X_real, y_real = generate_real_samples(dataset, half_batch) # update discriminator model weights d_loss1, _ = d_model.train_on_batch(X_real, y_real) # generate 'fake' examples X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch) # update discriminator model weights d_loss2, _ = d_model.train_on_batch(X_fake, y_fake) # prepare points in latent space as input for the generator X_gan = generate_latent_points(latent_dim, n_batch) # create inverted labels for the fake samples y_gan = ones((n_batch, 1)) # update the generator via the discriminator's error g_loss = gan_model.train_on_batch(X_gan, y_gan) # summarize loss on this batch print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss)) # save the generator model g_model.save('generator.h5') # size of the latent space latent_dim = 100 # create the discriminator discriminator = define_discriminator() # create the generator generator = define_generator(latent_dim) # create the gan gan_model = define_gan(generator, discriminator) # load image data dataset = load_real_samples(dir) # train model train(generator, discriminator, gan_model, dataset, latent_dim)",,,,,
"[Student scholarship for AAMAS 2021] Dear all, AAMAS is one of the top-tier annual conferences in artificial intelligence. This year it will be virtual and therefore anyone from the world can attend online. There's a student scholarship scheme that allows students to attend the whole conference for free!! See the attached link below for more details.
The deadline is tomorrow (12 Febr). So I strongly encourage you to apply. It's a very good opportunity to attend a top-tier conference for free. We have so much funding + sponsorship so basically we can approve any application - but you need to apply on time!! It's quite, simple, you just need to fill an online form.
University faculty members: Please forward this message to your students and ask them to apply asap (the form asks you to be a PhD student, but MSc + undergrad students can also apply).
Looking forward to seeing you at the conference.","[Student scholarship for AAMAS 2021] Dear all, AAMAS is one of the top-tier annual conferences in artificial intelligence. This year it will be virtual and therefore anyone from the world can attend online. There's a student scholarship scheme that allows students to attend the whole conference for free!! See the attached link below for more details. The deadline is tomorrow (12 Febr). So I strongly encourage you to apply. It's a very good opportunity to attend a top-tier conference for free. We have so much funding + sponsorship so basically we can approve any application - but you need to apply on time!! It's quite, simple, you just need to fill an online form. University faculty members: Please forward this message to your students and ask them to apply asap (the form asks you to be a PhD student, but MSc + undergrad students can also apply). Looking forward to seeing you at the conference.",,,,,
,nan,,,,,
"Mở đầu năm mới bằng 1 bài viết mới, chúc bạn đọc năm mới vạn sự như ý
#Crawler","Mở đầu năm mới bằng 1 bài viết mới, chúc bạn đọc năm mới vạn sự như ý",#Crawler,,,,
"Một bạn học PhD ở Canada đọc blog Machine Learning cơ bản thấy thú vị đã dịch một số bài ra tiếng Anh để chia sẻ với các bạn trong lab.
Mình xin chia sẻ lại ở đây. Bản này trình bày còn đẹp hơn bản gốc.
https://techsharing21.com/system-design/recommendation-system-basic-concepts
https://techsharing21.com/system-design/collaborative-filtering-an-introduction
Chúc mừng năm mới.",Một bạn học PhD ở Canada đọc blog Machine Learning cơ bản thấy thú vị đã dịch một số bài ra tiếng Anh để chia sẻ với các bạn trong lab. Mình xin chia sẻ lại ở đây. Bản này trình bày còn đẹp hơn bản gốc. https://techsharing21.com/system-design/recommendation-system-basic-concepts https://techsharing21.com/system-design/collaborative-filtering-an-introduction Chúc mừng năm mới.,,,,,
"Xin chào mọi người ạh.
Em mới học được hai tháng, xong supersived trên coursera. Lên hiểu biết còn hạn chế, nông cạn. Em có đọc hands on bản tiếng anh và vài bài báo về công nghệ này nhưng do mỗi bài mỗi thuật toán khác nhau. Em có câu hỏi mong muốn mọi người đóng góp ý kiến ạh. Em chân thành cảm ơn ạh ( hạn cuối của em nộp cho manager còn gần tháng nữa :(( )
* Bài toán: nhận diện cử chỉ ( gesture recognization) chuyện động vật lý của bàn tay để tương tác với màn hình mà không cần chạm
* Class: 4 ( quoét trái, quoét phải, vuốt lên, vuốt xuống)
* Tổng số mẫu: khoảng 4000
* feature: 5
Em nên dùng thuật toán toán nào cho bài toán trên ạh.
Em nên dùng phương pháp đánh giá mô hình nào cho bài toán trên ạh.","Xin chào mọi người ạh. Em mới học được hai tháng, xong supersived trên coursera. Lên hiểu biết còn hạn chế, nông cạn. Em có đọc hands on bản tiếng anh và vài bài báo về công nghệ này nhưng do mỗi bài mỗi thuật toán khác nhau. Em có câu hỏi mong muốn mọi người đóng góp ý kiến ạh. Em chân thành cảm ơn ạh ( hạn cuối của em nộp cho manager còn gần tháng nữa :(( ) * Bài toán: nhận diện cử chỉ ( gesture recognization) chuyện động vật lý của bàn tay để tương tác với màn hình mà không cần chạm * Class: 4 ( quoét trái, quoét phải, vuốt lên, vuốt xuống) * Tổng số mẫu: khoảng 4000 * feature: 5 Em nên dùng thuật toán toán nào cho bài toán trên ạh. Em nên dùng phương pháp đánh giá mô hình nào cho bài toán trên ạh.",,,,,
"mình đang làm 1 đề tài về NILM (phân tích điện sử dụng qua công tơ tổng). mình có nghiên cứu khá nhiều whitepaper mấy tháng nay về lý thuyết mà vẫn chưa biết phải chạy code thực trên 1 hệ thống thực tế (của mình phát triển) thì như thế nào. có cao nhân nào đã làm qua đề tài này cho mình thỉnh giáo với.
mình tóm tắt bài toán như sau:
- ví dụ đơn giản nhất: công tơ tổng đo điện nhà cho số 5000W, ML sẽ phân tích ra gồm: 70% máy lạnh, 20% microwave, 10% tủ lạnh
- supervised training (có giám sát), data training đầu vào là số đo điện tổng & số đo điện từng thiết bị riêng biệt
- data lúc chạy live thì chỉ có 1 con số tổng","mình đang làm 1 đề tài về NILM (phân tích điện sử dụng qua công tơ tổng). mình có nghiên cứu khá nhiều whitepaper mấy tháng nay về lý thuyết mà vẫn chưa biết phải chạy code thực trên 1 hệ thống thực tế (của mình phát triển) thì như thế nào. có cao nhân nào đã làm qua đề tài này cho mình thỉnh giáo với. mình tóm tắt bài toán như sau: - ví dụ đơn giản nhất: công tơ tổng đo điện nhà cho số 5000W, ML sẽ phân tích ra gồm: 70% máy lạnh, 20% microwave, 10% tủ lạnh - supervised training (có giám sát), data training đầu vào là số đo điện tổng & số đo điện từng thiết bị riêng biệt - data lúc chạy live thì chỉ có 1 con số tổng",,,,,
"Ngày càng nhiều công ty quan tâm đến kỹ năng Machine Learning System Design (MLSD). Các câu hỏi liên quan đến lĩnh vực này cũng ngày một phổ biến trong quá trình phỏng vấn. Có một thực tế là dù bạn có thuật toán rất tốt nhưng không có thiết kế hệ thống tốt thì thuật toán đó mãi chỉ ở trên notebook và trong các demo mà thôi. Việc log dữ liệu sao cho hiệu quả ở quy mô lớn và tối ưu độ trễ trong việc dự đoán có vai trò không kém độ chính xác của thuật toán.
Giới thiệu với các bạn một khoá học MLSD của một anh bạn người Việt thiết kế trên educative.
https://www.educative.io/courses/machine-learning-system-design
(Khoá học này có phí, có chính sách trả lại tiền.)","Ngày càng nhiều công ty quan tâm đến kỹ năng Machine Learning System Design (MLSD). Các câu hỏi liên quan đến lĩnh vực này cũng ngày một phổ biến trong quá trình phỏng vấn. Có một thực tế là dù bạn có thuật toán rất tốt nhưng không có thiết kế hệ thống tốt thì thuật toán đó mãi chỉ ở trên notebook và trong các demo mà thôi. Việc log dữ liệu sao cho hiệu quả ở quy mô lớn và tối ưu độ trễ trong việc dự đoán có vai trò không kém độ chính xác của thuật toán. Giới thiệu với các bạn một khoá học MLSD của một anh bạn người Việt thiết kế trên educative. https://www.educative.io/courses/machine-learning-system-design (Khoá học này có phí, có chính sách trả lại tiền.)",,,,,
"Em có thể xin chia sẻ từ các bác đang đi học đi làm về nghành nghề IT và cả trong lĩnh vực ML về các góc độ như cơ hội, môi trường làm việc, dự đoán tương lai phát triển của ML, kế hoạch đi làm được không ạ?
Em cám ơn các bác, em cũng xin tư vấn thêm + tài liệu và kế hoạch học tập ạ","Em có thể xin chia sẻ từ các bác đang đi học đi làm về nghành nghề IT và cả trong lĩnh vực ML về các góc độ như cơ hội, môi trường làm việc, dự đoán tương lai phát triển của ML, kế hoạch đi làm được không ạ? Em cám ơn các bác, em cũng xin tư vấn thêm + tài liệu và kế hoạch học tập ạ",,,,,
"[Chia Sẻ] Hướng Dẫn Tự Học Data Science cho Người Mới Bắt Đầu]
Hello mọi người,
Nhóm mình có làm Series về kiến thức Data Science cơ bản dành cho các bạn mới bắt đầu hoặc đang học ở trường. Tụi mình hiện đang là Software Engineer đang làm việc ở Sing với hi vọng chia sẻ kiến thức IT cho các bạn đam mê lập trình. Trong clip tụi mình tập trung vào:
Hướng Dẫn Cài Đặt và Làm Quen vs Anaconda và Jupyter Notebook
Làm quen với Pandas và DataFrame
Hướng dẫn các xử lý một dữ liệu cụ thể của một nhà hàng.
Hi vọng clip này giúp ích được mọi người !","[Chia Sẻ] Hướng Dẫn Tự Học Data Science cho Người Mới Bắt Đầu] Hello mọi người, Nhóm mình có làm Series về kiến thức Data Science cơ bản dành cho các bạn mới bắt đầu hoặc đang học ở trường. Tụi mình hiện đang là Software Engineer đang làm việc ở Sing với hi vọng chia sẻ kiến thức IT cho các bạn đam mê lập trình. Trong clip tụi mình tập trung vào: Hướng Dẫn Cài Đặt và Làm Quen vs Anaconda và Jupyter Notebook Làm quen với Pandas và DataFrame Hướng dẫn các xử lý một dữ liệu cụ thể của một nhà hàng. Hi vọng clip này giúp ích được mọi người !",,,,,
Xử lý dữ liệu text dùng python,Xử lý dữ liệu text dùng python,,,,,
"Chào anh chị ạ !!!
Em vừa bước vào lĩnh vực machibe learning này. Thầy có giao tìm hiểu về mô hình XGBoost. E đã đọc 1 số tài liệu tiếng việt lẫn tiếng anh. Nhưng không đến đâu, trình bày lại mô hình cho ng khác nghe, em không biết trình bày như thế nào luôn. Có anh chị nào trong group biết vê mô hình này thì cho em xin cách trình bày mô hinh này với.
Em cảm ơn mọi ng đã đọc bài.
Chúc mọi người buổi tối vui vẻ","Chào anh chị ạ !!! Em vừa bước vào lĩnh vực machibe learning này. Thầy có giao tìm hiểu về mô hình XGBoost. E đã đọc 1 số tài liệu tiếng việt lẫn tiếng anh. Nhưng không đến đâu, trình bày lại mô hình cho ng khác nghe, em không biết trình bày như thế nào luôn. Có anh chị nào trong group biết vê mô hình này thì cho em xin cách trình bày mô hinh này với. Em cảm ơn mọi ng đã đọc bài. Chúc mọi người buổi tối vui vẻ",,,,,
Bài viết đầu tay của mình về trade off bias-variance. Hy vọng giúp ích và được mọi người góp ý thêm ạ.,Bài viết đầu tay của mình về trade off bias-variance. Hy vọng giúp ích và được mọi người góp ý thêm ạ.,,,,,
"Chào mọi người
Cho mình hỏi chút về BinaryAccuracy trong Tensorflow""
Mình có 2 cái labels như trên:
[[1], [0], [1], [1]]
[[0.6], [0.51], [0], [0.6]]
Với threshold = 0.5:
=> 2 bộ true/pred labels sẽ thành:
1 0 1 1
và 1 1 0 1
với sample weight = 1,1,1,1
Binary Accuracy = 2/4 = 0.5(very clear!!)
Nhưng với sample weight = 1,1,1,0, tại sao Binary Accuracy lại là 0.333334 ?
Mình gu gồ mãi chưa ra chỗ này.
Rất mong các bạn thông não, đây là ảnh mình chụp khi run trên Colab","Chào mọi người Cho mình hỏi chút về BinaryAccuracy trong Tensorflow"" Mình có 2 cái labels như trên: [[1], [0], [1], [1]] [[0.6], [0.51], [0], [0.6]] Với threshold = 0.5: => 2 bộ true/pred labels sẽ thành: 1 0 1 1 và 1 1 0 1 với sample weight = 1,1,1,1 Binary Accuracy = 2/4 = 0.5(very clear!!) Nhưng với sample weight = 1,1,1,0, tại sao Binary Accuracy lại là 0.333334 ? Mình gu gồ mãi chưa ra chỗ này. Rất mong các bạn thông não, đây là ảnh mình chụp khi run trên Colab",,,,,
Công cụ xử lý văn bản tiếng Việt,Công cụ xử lý văn bản tiếng Việt,,,,,
"Lập trình mạng nơ ron tốt nhất là dùng các hàm trong python và thư viện, nhưng nếu bạn tò mò muốn lập trình từ định nghĩa toán học?","Lập trình mạng nơ ron tốt nhất là dùng các hàm trong python và thư viện, nhưng nếu bạn tò mò muốn lập trình từ định nghĩa toán học?",,,,,
"Chào mọi người ,thầy giao em bài tập tìm và lọc ít nhất 4000 ảnh về quả chôm chôm để train, em tìm được web flickr thì thấy ok , nhưng lại ko biết làm sao để tải vài trăm / ngàn ảnh về máy cùng 1 lúc được, ai chỉ cách cho em với ạ, em lên mạng kiếm mấy cái source code nhưng lại không biết cách dùng","Chào mọi người ,thầy giao em bài tập tìm và lọc ít nhất 4000 ảnh về quả chôm chôm để train, em tìm được web flickr thì thấy ok , nhưng lại ko biết làm sao để tải vài trăm / ngàn ảnh về máy cùng 1 lúc được, ai chỉ cách cho em với ạ, em lên mạng kiếm mấy cái source code nhưng lại không biết cách dùng",,,,,
"Khoá Machine Learning System Design của Stanford đang diễn ra, rất nhiều thông tin bổ ích cho cả các bạn sinh viên và các bạn làm việc với ML in production:
https://stanford-cs329s.github.io/?fbclid=IwAR0C23h0nlJjIJQPcDs6YUKxBi-trWoPY5fRpm1CzZRF4SvxhFpIFb5fu4E","Khoá Machine Learning System Design của Stanford đang diễn ra, rất nhiều thông tin bổ ích cho cả các bạn sinh viên và các bạn làm việc với ML in production: https://stanford-cs329s.github.io/?fbclid=IwAR0C23h0nlJjIJQPcDs6YUKxBi-trWoPY5fRpm1CzZRF4SvxhFpIFb5fu4E",,,,,
"Hi mọi người. Em đang làm về nhận diện tên thuốc, thành phần thuốc và sửa lỗi chính tả nếu có.
- Với phần nhận diện tên thuốc và thành phần thuốc: Em xây dựng một bộ phân lớp có 3 class: thuốc, thành phần và khác
- Với phần sửa lỗi chính tả em mong muốn tìm một model cho việc sửa lỗi
Anh chị có ai làm về mảng này góp ý giúp em với ạ.","Hi mọi người. Em đang làm về nhận diện tên thuốc, thành phần thuốc và sửa lỗi chính tả nếu có. - Với phần nhận diện tên thuốc và thành phần thuốc: Em xây dựng một bộ phân lớp có 3 class: thuốc, thành phần và khác - Với phần sửa lỗi chính tả em mong muốn tìm một model cho việc sửa lỗi Anh chị có ai làm về mảng này góp ý giúp em với ạ.",,,,,
"chào mng, e có 1 câu hỏi, e đọc tới giả nghich đảo này thì hơi lú , sinh ra giả nghịch đảo để cho dù nó ko có ma trận nghịch đảo thì mình vẫn xây dựng dc giả nghịch đảo v ko ạ ? v nếu lỡ trúng ma trận đó ko có nghichj đảo thì xây dựng pseudo invertible như thế nào ạ","chào mng, e có 1 câu hỏi, e đọc tới giả nghich đảo này thì hơi lú , sinh ra giả nghịch đảo để cho dù nó ko có ma trận nghịch đảo thì mình vẫn xây dựng dc giả nghịch đảo v ko ạ ? v nếu lỡ trúng ma trận đó ko có nghichj đảo thì xây dựng pseudo invertible như thế nào ạ",,,,,
"Kính chào các bác. Hôm nay em thực hiện nốt món quà website Xem bói mặt cho anh em newbie. Em mạnh dạn chia sẻ để các bạn nắm được cách thức xây dựng một hệ thống đơn giản, ở mức đồ án.
Chúc các bác năm mới bình an, hạnh phúc!
Mong ad duyệt bài!","Kính chào các bác. Hôm nay em thực hiện nốt món quà website Xem bói mặt cho anh em newbie. Em mạnh dạn chia sẻ để các bạn nắm được cách thức xây dựng một hệ thống đơn giản, ở mức đồ án. Chúc các bác năm mới bình an, hạnh phúc! Mong ad duyệt bài!",,,,,
"Một khoá học hay khác về ML, trong đây có hai phần rất hay về dữ liệu dạng bảng (tabular data) là Feature Crosses và Embeddings.","Một khoá học hay khác về ML, trong đây có hai phần rất hay về dữ liệu dạng bảng (tabular data) là Feature Crosses và Embeddings.",,,,,
Chào mọi người. mình là nguời mới về ML. mình có vấn đề về sử dụng deeplearning để phân tích nội dung ngữ nghĩa đoạn văn. đã có ai làm về này cho mình xin huớng dẫn với ạ. Cảm ơn Mn,Chào mọi người. mình là nguời mới về ML. mình có vấn đề về sử dụng deeplearning để phân tích nội dung ngữ nghĩa đoạn văn. đã có ai làm về này cho mình xin huớng dẫn với ạ. Cảm ơn Mn,,,,,
"Kính chào các bác. Nhân dịp Tết đến xuân sang, em có xây dựng website xem bói mặt, nhận vào một khuôn mặt và hiển thị lên kiểu mặt: dài, tròn, trái tim.... kèm theo các lời diễn giải về tướng số liên quan đến khuôn mặt đó.
Nay em chia sẻ cùng mọi người!
Mong giúp đỡ được các bạn mới học! Mong ad duyệt bài!","Kính chào các bác. Nhân dịp Tết đến xuân sang, em có xây dựng website xem bói mặt, nhận vào một khuôn mặt và hiển thị lên kiểu mặt: dài, tròn, trái tim.... kèm theo các lời diễn giải về tướng số liên quan đến khuôn mặt đó. Nay em chia sẻ cùng mọi người! Mong giúp đỡ được các bạn mới học! Mong ad duyệt bài!",,,,,
"các anh, chị cho em hỏi lỗi này là gì và cách khắc phục được không ạ? em cảm ơn nhiều😍","các anh, chị cho em hỏi lỗi này là gì và cách khắc phục được không ạ? em cảm ơn nhiều",,,,,
"Hi mọi người,
Hôm nay mình xin phép chia sẻ blog về thực nghiệm các mô hình thường hay được sử dụng trong bài toàn OCR, cụ thể AttentionOCR và TransformerOCR, và trả lời câu hỏi ""Liệu sử dụng kiến trúc mới như transformer có làm tăng độ chính xác của quá trình nhận dạng lên nhiều hay không?"",
Đồng thời, mình cũng cung cấp thư viện VietOCR được phát triển với mục tiêu dễ sài, dễ huấn luyện cho ngôn ngữ tiếng việt, cùng với pretrained model trên 10m ảnh.
Thư viện: https://github.com/pbcquoc/vietocr
Blog: https://pbcquoc.github.io/vietocr/
Notebook hướng dẫn sử dụng thư viện: https://github.com/pbcquoc/vietocr/blob/master/vietocr_gettingstart.ipynb","Hi mọi người, Hôm nay mình xin phép chia sẻ blog về thực nghiệm các mô hình thường hay được sử dụng trong bài toàn OCR, cụ thể AttentionOCR và TransformerOCR, và trả lời câu hỏi ""Liệu sử dụng kiến trúc mới như transformer có làm tăng độ chính xác của quá trình nhận dạng lên nhiều hay không?"", Đồng thời, mình cũng cung cấp thư viện VietOCR được phát triển với mục tiêu dễ sài, dễ huấn luyện cho ngôn ngữ tiếng việt, cùng với pretrained model trên 10m ảnh. Thư viện: https://github.com/pbcquoc/vietocr Blog: https://pbcquoc.github.io/vietocr/ Notebook hướng dẫn sử dụng thư viện: https://github.com/pbcquoc/vietocr/blob/master/vietocr_gettingstart.ipynb",,,,,
Ai muốn học Machine Learning miễn phí thì lưu lại nhé!,Ai muốn học Machine Learning miễn phí thì lưu lại nhé!,,,,,
"Chào các anh/chị và các bạn, em đang gặp vấn đề về định hướng, em rất mong nhận được tư vấn từ các anh/chị đi trước.
Em là người chuyển ngành, em muốn làm về công nghệ, nhưng em kiến thức và kinh nghiệm chưa có và chưa hiểu biết gì về ngành công nghệ thông tin, nên em quyết định xin thực tập lập trình phần mềm, em cảm thấy hơi mơ hồ với lựa chọn của mình. Vào công ty thì mình gần như không biết gì, mới toàn tập. Cty này có sản phẩm là phần mềm cho Doanh nghiệp.
Em đang suy nghĩ là em sẽ học các công cụ, kiến thức để trước mắt làm việc tại Cty, hiểu ngành - hiểu nghề rồi sau khoảng 1, 2 có kinh nghiệm rồi bắt đầu học thêm về dữ liệu, các thuật toán AI để ứng dụng nó vào sản phẩm.
Em có đọc một số con đường của những người học ngành không phải công nghệ thông tin hoặc KHMT, thì hầu hết câu trả lời là tham gia các khóa học trên mạng, thi trên Kaggle... Nhưng em chưa chọn vì chưa rõ mục đích mình sử dụng AI để làm gì.
Hiện tại em đang mơ hồ về con đường sự nghiệp, mong mọi người chia sẻ kinh nghiệm để em học hỏi ạ. Cảm ơn mọi người.","Chào các anh/chị và các bạn, em đang gặp vấn đề về định hướng, em rất mong nhận được tư vấn từ các anh/chị đi trước. Em là người chuyển ngành, em muốn làm về công nghệ, nhưng em kiến thức và kinh nghiệm chưa có và chưa hiểu biết gì về ngành công nghệ thông tin, nên em quyết định xin thực tập lập trình phần mềm, em cảm thấy hơi mơ hồ với lựa chọn của mình. Vào công ty thì mình gần như không biết gì, mới toàn tập. Cty này có sản phẩm là phần mềm cho Doanh nghiệp. Em đang suy nghĩ là em sẽ học các công cụ, kiến thức để trước mắt làm việc tại Cty, hiểu ngành - hiểu nghề rồi sau khoảng 1, 2 có kinh nghiệm rồi bắt đầu học thêm về dữ liệu, các thuật toán AI để ứng dụng nó vào sản phẩm. Em có đọc một số con đường của những người học ngành không phải công nghệ thông tin hoặc KHMT, thì hầu hết câu trả lời là tham gia các khóa học trên mạng, thi trên Kaggle... Nhưng em chưa chọn vì chưa rõ mục đích mình sử dụng AI để làm gì. Hiện tại em đang mơ hồ về con đường sự nghiệp, mong mọi người chia sẻ kinh nghiệm để em học hỏi ạ. Cảm ơn mọi người.",,,,,
"Chào mọi người, em là một newbie trong lĩnh vực Machine learning, vì vậy kỹ năng về thao tác và làm việc với numpy của em còn nhiều hạn chế. Các anh, chị có thể recommend giúp em một vài khoá học về numpy không ạ? Em cám ơn nhiều ạ.
Mọi người stay safe nhé.","Chào mọi người, em là một newbie trong lĩnh vực Machine learning, vì vậy kỹ năng về thao tác và làm việc với numpy của em còn nhiều hạn chế. Các anh, chị có thể recommend giúp em một vài khoá học về numpy không ạ? Em cám ơn nhiều ạ. Mọi người stay safe nhé.",,,,,
"Mình chào mọi người. Mình đang có ý định học Machine Learning ở TPHCM. Mình đang phân vân giữa học ở VTC Academy hoặc là ĐH Khoa Học Tự Nhiên. Mọi người ai có kinh nghiệm hướng dẫn mình với.
Cảm ơn mọi người.",Mình chào mọi người. Mình đang có ý định học Machine Learning ở TPHCM. Mình đang phân vân giữa học ở VTC Academy hoặc là ĐH Khoa Học Tự Nhiên. Mọi người ai có kinh nghiệm hướng dẫn mình với. Cảm ơn mọi người.,,,,,
"Agree, an ML blogger also said this many times.
""""""
One superstar ML engineer once said: “Start small. Smaller. Still too big.” Starting with a small model has three benefits:
It helps you validate that your data can do something useful.
It helps you validate your pipeline to make sure that your training pipeline and inference pipeline do the same things.
Simple models can act as baselines to which you can compare your more complex models. Your complex models should do significantly better than simple models to justify their complexity.
""""""","Agree, an ML blogger also said this many times. """""" One superstar ML engineer once said: “Start small. Smaller. Still too big.” Starting with a small model has three benefits: It helps you validate that your data can do something useful. It helps you validate your pipeline to make sure that your training pipeline and inference pipeline do the same things. Simple models can act as baselines to which you can compare your more complex models. Your complex models should do significantly better than simple models to justify their complexity. """"""",,,,,
"Vâng, anh em đang thử bài toán chẩn đoán bệnh qua phim X từ xa, dựa trên máy học, từ Vingroup
#koolj_deepimage
A em đã xử lý cơ bản xong.
GPU detect khá nhanh.
A e chuyển demo sang:
Dathoc.net/vx
Mời pà con vào đánh giá tiếp.
Mong góp ý từ anh em!","Vâng, anh em đang thử bài toán chẩn đoán bệnh qua phim X từ xa, dựa trên máy học, từ Vingroup A em đã xử lý cơ bản xong. GPU detect khá nhanh. A e chuyển demo sang: Dathoc.net/vx Mời pà con vào đánh giá tiếp. Mong góp ý từ anh em!",#koolj_deepimage,,,,
"Adas: state-of-the-art training performance
#Share",Adas: state-of-the-art training performance,#Share,,,,
"Chào cả nhà, mình muốn bắt đầu tìm hiểu về text to speech cho Tiếng Việt thì có bắt đầu từ đâu hoặc có code, data tham khảo ở đâu thì tốt nhỉ?
Mình tìm qua một lượt nhưng không thấy nhiều lắm và khá cũ.
Background: mình có một chút kinh nghiệm về xử lý ảnh/video và deep learning. Chưa làm về text và speech bao giờ.
Xin cảm ơn","Chào cả nhà, mình muốn bắt đầu tìm hiểu về text to speech cho Tiếng Việt thì có bắt đầu từ đâu hoặc có code, data tham khảo ở đâu thì tốt nhỉ? Mình tìm qua một lượt nhưng không thấy nhiều lắm và khá cũ. Background: mình có một chút kinh nghiệm về xử lý ảnh/video và deep learning. Chưa làm về text và speech bao giờ. Xin cảm ơn",,,,,
"Em chào mọi người!!!
Em có 1 câu hỏi là
Em có hàng dọc như trong ảnh, em muốn đổi nó thành hàng ngang, trong openGL thì làm ntn ạ, hoặc chỉ cần cho e xin cái keyword thôi là em cảm ơn lắm rồi","Em chào mọi người!!! Em có 1 câu hỏi là Em có hàng dọc như trong ảnh, em muốn đổi nó thành hàng ngang, trong openGL thì làm ntn ạ, hoặc chỉ cần cho e xin cái keyword thôi là em cảm ơn lắm rồi",,,,,
"Xin chào mọi người ạ. Cho em/mình hỏi là có cách nào trích và đánh nhãn luồng từ file pcap của các bộ dữ liệu như CIC IDS 2017 (https://www.unb.ca/cic/datasets/ids-2017.html) hay là
ToN-IoT (https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-ton-iot-Datasets/) không ạ?",Xin chào mọi người ạ. Cho em/mình hỏi là có cách nào trích và đánh nhãn luồng từ file pcap của các bộ dữ liệu như CIC IDS 2017 (https://www.unb.ca/cic/datasets/ids-2017.html) hay là ToN-IoT (https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-ton-iot-Datasets/) không ạ?,,,,,
"Mình cần viết 1 ứng dụng có thể giúp camera soi để xác định vật thể đó là gì? Ứng dung cho việc lọc sản phẩm của công ty mình.
Bạn nào có thể làm được inbox báo giá giúp cho mình được không?!
Cám ơn mọi người đã đọc tin.",Mình cần viết 1 ứng dụng có thể giúp camera soi để xác định vật thể đó là gì? Ứng dung cho việc lọc sản phẩm của công ty mình. Bạn nào có thể làm được inbox báo giá giúp cho mình được không?! Cám ơn mọi người đã đọc tin.,,,,,
"https://colab.research.google.com/drive/1grAiUD5hjlkxl5l0L1EaeaRCPPFR3NkL?usp=sharing
mọi người ơi cho em hỏi ngu hỏi dại cái. mục tiêu của em là train data object detector. dùng module của darknet. image = 6 labels = 13 , batch =  64, sub = 16 dùng yolo v3. cho em hỏi vậy tại sao khi train thì ở log nó ghi là 71552 images vậy. ^^ các thẻ yolo em đều đã chỉnh về 13 như hình bên dưới. cái này em cần sản phẩm chứ em không học hành kỹ lưỡng mọi người biết giải thích giúp em phát để em đỡ lo ngồi canh chạy hết 15 tiếng xong kết quả như cứt thì đúng là quải lắm á ^^","https://colab.research.google.com/drive/1grAiUD5hjlkxl5l0L1EaeaRCPPFR3NkL?usp=sharing mọi người ơi cho em hỏi ngu hỏi dại cái. mục tiêu của em là train data object detector. dùng module của darknet. image = 6 labels = 13 , batch = 64, sub = 16 dùng yolo v3. cho em hỏi vậy tại sao khi train thì ở log nó ghi là 71552 images vậy. ^^ các thẻ yolo em đều đã chỉnh về 13 như hình bên dưới. cái này em cần sản phẩm chứ em không học hành kỹ lưỡng mọi người biết giải thích giúp em phát để em đỡ lo ngồi canh chạy hết 15 tiếng xong kết quả như cứt thì đúng là quải lắm á ^^",,,,,
Chào mọi người hiện tại em mới tìm hiểu mảng xử lý ảnh số nhưng bước đầu cài đặt OpenCv bị lỗi như thế này không biết mọi người có ai biết cách sửa lỗi không!,Chào mọi người hiện tại em mới tìm hiểu mảng xử lý ảnh số nhưng bước đầu cài đặt OpenCv bị lỗi như thế này không biết mọi người có ai biết cách sửa lỗi không!,,,,,
Em đang làm đề tài này mọi người chỉ em hướng làm với ạ.Em xin cảm ơn,Em đang làm đề tài này mọi người chỉ em hướng làm với ạ.Em xin cảm ơn,,,,,
"Colored ASCII generator (image2image and video2video) written in Python
Source code: https://github.com/uvipen/ASCII-generator",Colored ASCII generator (image2image and video2video) written in Python Source code: https://github.com/uvipen/ASCII-generator,,,,,
"#plot the error function
Chào các anh chị,
Sau khi train, test với SVM đơn giản, thì em có mô hình, ypred, ytrue, AUC, AUPR... Em muốn hỏi là mình cần vẽ hàm lỗi trong Python thì có công cụ hỗ trợ ko ạ và cần truyền tham số nào ạ.
Em cám ơn mọi người!","the error function Chào các anh chị, Sau khi train, test với SVM đơn giản, thì em có mô hình, ypred, ytrue, AUC, AUPR... Em muốn hỏi là mình cần vẽ hàm lỗi trong Python thì có công cụ hỗ trợ ko ạ và cần truyền tham số nào ạ. Em cám ơn mọi người!",#plot,,,,
"Hello cả nhà,
Em giải quyết bài toán phát hiện chuyển động bất ngờ. ví dụ : xe đứng yên, đột ngột tăng tốc. Cả nhà cho mình xin hướng giải quyết, thuật ngữ với ạ.
Em cảm ơn ạ","Hello cả nhà, Em giải quyết bài toán phát hiện chuyển động bất ngờ. ví dụ : xe đứng yên, đột ngột tăng tốc. Cả nhà cho mình xin hướng giải quyết, thuật ngữ với ạ. Em cảm ơn ạ",,,,,
Các bác nào có biết nguồn lấy dataset các hội thoại việt nam cho em xin với ạ :D,Các bác nào có biết nguồn lấy dataset các hội thoại việt nam cho em xin với ạ :D,,,,,
Trong group mình có ai đang học ML mà ở Đà Nẵng không? Mình đang tìm bạn ở gần để lập team học,Trong group mình có ai đang học ML mà ở Đà Nẵng không? Mình đang tìm bạn ở gần để lập team học,,,,,
Anh em còn FA. Rảnh rổi thì zo chat con con chatbot crush của mình nó thông minh tí với.,Anh em còn FA. Rảnh rổi thì zo chat con con chatbot crush của mình nó thông minh tí với.,,,,,
"#kaggle #pytorch #cnn
Hi mn
- Chuyện là em đang tìm kết quả cột labels/target của tập dữ liệu TEST (test.csv) trong cuộc thi [siim-isic-melanoma-classification] trên kaggle Vì cuộc thi này đã kết thúc. Không biết là có anh chị nào tham gia và có target/labels đó cho em xin ạ.
Cảm ơn anh chị!
https://www.kaggle.com/c/siim-isic-melanoma-classification?fbclid=IwAR0UpQW3ftqcPyWmhigDy4rJvhDuvUFYpO8kRzSyqdIxQiRz5u8WYrVHtdE",Hi mn - Chuyện là em đang tìm kết quả cột labels/target của tập dữ liệu TEST (test.csv) trong cuộc thi [siim-isic-melanoma-classification] trên kaggle Vì cuộc thi này đã kết thúc. Không biết là có anh chị nào tham gia và có target/labels đó cho em xin ạ. Cảm ơn anh chị! https://www.kaggle.com/c/siim-isic-melanoma-classification?fbclid=IwAR0UpQW3ftqcPyWmhigDy4rJvhDuvUFYpO8kRzSyqdIxQiRz5u8WYrVHtdE,#kaggle	#pytorch	#cnn,,,,
"Hi mọi người,
Mình đang tìm hiểu cách chuẩn đoán bệnh dựa trên hình ảnh, kiểu như thế này: https://vingroup.net/tin-tuc-su-kien/bai-viet/2199/trien-khai-giai-phap-ai-vindr-trong-chan-doan-hinh-anh-y-te
Nhờ mọi người hướng dẫn gịúp mình một số tài liệu/video về để tài này ? Lĩnh vực này có tài liệu nào kinh điển vậy ạ? Mình search thấy nhiều quá mà không biết tài liệu nào ổn nhất ạ
Cảm ơn mọi người nhiều ạ. ","Hi mọi người, Mình đang tìm hiểu cách chuẩn đoán bệnh dựa trên hình ảnh, kiểu như thế này: https://vingroup.net/tin-tuc-su-kien/bai-viet/2199/trien-khai-giai-phap-ai-vindr-trong-chan-doan-hinh-anh-y-te Nhờ mọi người hướng dẫn gịúp mình một số tài liệu/video về để tài này ? Lĩnh vực này có tài liệu nào kinh điển vậy ạ? Mình search thấy nhiều quá mà không biết tài liệu nào ổn nhất ạ Cảm ơn mọi người nhiều ạ.",,,,,
"Chia sẻ với các bạn bài viết của mình trên medium.com. Trong bài viết này, mình phân tích những khó khăn, thách thức mà doanh nghiệp có thể gặp phải khi vận hành và triển khai mô hình ML ra môi trường sản phẩm, và tổng quan giải pháp MLOps hiện đang là xu hướng được nhiều doanh nghiệp và các cá nhân thực hành ML quan tâm.
https://nghiacd.medium.com/duy-tr%C3%AC-v%E1%BA%ADn-h%C3%A0nh-m%C3%B4-h%C3%ACnh-machine-learning-kh%C3%B3-kh%C4%83n-th%C3%A1ch-th%E1%BB%A9c-v%C3%A0-gi%E1%BA%A3i-ph%C3%A1p-6b7b42743d78","Chia sẻ với các bạn bài viết của mình trên medium.com. Trong bài viết này, mình phân tích những khó khăn, thách thức mà doanh nghiệp có thể gặp phải khi vận hành và triển khai mô hình ML ra môi trường sản phẩm, và tổng quan giải pháp MLOps hiện đang là xu hướng được nhiều doanh nghiệp và các cá nhân thực hành ML quan tâm. https://nghiacd.medium.com/duy-tr%C3%AC-v%E1%BA%ADn-h%C3%A0nh-m%C3%B4-h%C3%ACnh-machine-learning-kh%C3%B3-kh%C4%83n-th%C3%A1ch-th%E1%BB%A9c-v%C3%A0-gi%E1%BA%A3i-ph%C3%A1p-6b7b42743d78",,,,,
"Em đang đọc về mạng inception v3 ở trang
https://cloud.google.com/tpu/docs/inception-v3-advanced
Em nhận thấy là ở layer giữa có tạo ra một output (ảnh). Em không thấy trong tài liệu có nói về output này.
Vậy output này có tác dụng gì và sử dụng trong những trường hợp nào
Em cảm ơn ạ",Em đang đọc về mạng inception v3 ở trang https://cloud.google.com/tpu/docs/inception-v3-advanced Em nhận thấy là ở layer giữa có tạo ra một output (ảnh). Em không thấy trong tài liệu có nói về output này. Vậy output này có tác dụng gì và sử dụng trong những trường hợp nào Em cảm ơn ạ,,,,,
"Em đang xử lí một bài toán text classification tiếng việt, tương tự bài toán Spam Classification khi nó có 2 labels, tập dữ liệu khoảng 4000 rows. Em có dùng nhiều thuật toán ML và nhận thấy Naive Bayes cho kết quả tốt nhất, accuracy là 83%. Cho e hỏi các ac đã từng có kinh nghiệm xử lí bài toán text classification tiếng việt, thì ngoài những thuật toán ML thì có những phương pháp đem lại kết quả tốt nhất không ạ? Em xin cảm ơn.","Em đang xử lí một bài toán text classification tiếng việt, tương tự bài toán Spam Classification khi nó có 2 labels, tập dữ liệu khoảng 4000 rows. Em có dùng nhiều thuật toán ML và nhận thấy Naive Bayes cho kết quả tốt nhất, accuracy là 83%. Cho e hỏi các ac đã từng có kinh nghiệm xử lí bài toán text classification tiếng việt, thì ngoài những thuật toán ML thì có những phương pháp đem lại kết quả tốt nhất không ạ? Em xin cảm ơn.",,,,,
"Bài viết chia sẻ của mình về bài toán Key Information Extraction, trích rút thông tin từ hóa đơn với Graph Convolution Network 😁 hi vọng giúp ích cho các bạn ^^

#viblo","Bài viết chia sẻ của mình về bài toán Key Information Extraction, trích rút thông tin từ hóa đơn với Graph Convolution Network hi vọng giúp ích cho các bạn ^^",#viblo,,,,
"Chào các bạn, mình thấy có 1 số bạn trong nhóm hỏi về giải pháp chấm công, điểm danh bằng nhận diện khuôn mặt, mình xin giới thiệu với các bạn HANET Ai Camera.
Hiện tại bên mình đã đạt được tốc độ như trong video, nhận cùng lúc nhiều người, đeo khẩu trang và di chuyển nhanh. Nhận dạng offline, ngay cả khi mất kết nối internet. Và phát hiện chấm công bằng ảnh.
Bên mình hỗ trợ API cho các đối tác cùng phát triển giải pháp.
Tài liệu:
https://developers.hanet.ai/document
Camera giá: 3.5 triệu.
Ống kính 2K
Ram 1GB
Wifi 2.4&5.8Ghz
EMMC 8Gb
Lưu trữ: 50.000 khuôn mặt.
Tính phí cloud lưu trữ video và sử dụng FaceID
Chi tiết tham khảo:
https://hanet.com/hanet-ai-camera-m/
Rất mong được hợp tác cùng phát triển!","Chào các bạn, mình thấy có 1 số bạn trong nhóm hỏi về giải pháp chấm công, điểm danh bằng nhận diện khuôn mặt, mình xin giới thiệu với các bạn HANET Ai Camera. Hiện tại bên mình đã đạt được tốc độ như trong video, nhận cùng lúc nhiều người, đeo khẩu trang và di chuyển nhanh. Nhận dạng offline, ngay cả khi mất kết nối internet. Và phát hiện chấm công bằng ảnh. Bên mình hỗ trợ API cho các đối tác cùng phát triển giải pháp. Tài liệu: https://developers.hanet.ai/document Camera giá: 3.5 triệu. Ống kính 2K Ram 1GB Wifi 2.4&5.8Ghz EMMC 8Gb Lưu trữ: 50.000 khuôn mặt. Tính phí cloud lưu trữ video và sử dụng FaceID Chi tiết tham khảo: https://hanet.com/hanet-ai-camera-m/ Rất mong được hợp tác cùng phát triển!",,,,,
Mọi người ơi cho em hỏi làm cách nào để dùng gradient descent để tìm đc global minimum cho function ạ. E mới bắt đầu tự nghiên cứu ML nên chưa rõ lắm về loss function với các library hay dùng ạ. Em chỉ biết dùng basic python thôi ạ,Mọi người ơi cho em hỏi làm cách nào để dùng gradient descent để tìm đc global minimum cho function ạ. E mới bắt đầu tự nghiên cứu ML nên chưa rõ lắm về loss function với các library hay dùng ạ. Em chỉ biết dùng basic python thôi ạ,,,,,
"Chào các bác.
Chuyện là em đang học cấp 3 và muốn tìm hiểu về Machine Learning thì mình nên học những kiến thức nền tảng toán và lập trình nào ạ, tại em có đọc thử một cuốn về máy học của ad mà khựng ngay từ vài trang đầu ""ôn tập đại số tuyến tính"" vì hông hiểu gì :""( hic
Mong các cao nhân chỉ bảo ạ, e xin cảm ơn :>","Chào các bác. Chuyện là em đang học cấp 3 và muốn tìm hiểu về Machine Learning thì mình nên học những kiến thức nền tảng toán và lập trình nào ạ, tại em có đọc thử một cuốn về máy học của ad mà khựng ngay từ vài trang đầu ""ôn tập đại số tuyến tính"" vì hông hiểu gì :""( hic Mong các cao nhân chỉ bảo ạ, e xin cảm ơn :>",,,"#Q&A, #math, #machine_learning",,
"Em chào cả nhà ạ, hiện tại tập training data của e có khoảng 2000 text loại này (ở các trang thương mại điện tử, đếm ngược để mua hàng). Các anh chị cho e hỏi là e nên dùng gì để train đống data này, để khi vào một trang thương mại điện tử có đếm ngược thì e biết được là trang này có đếm vậy ạ?","Em chào cả nhà ạ, hiện tại tập training data của e có khoảng 2000 text loại này (ở các trang thương mại điện tử, đếm ngược để mua hàng). Các anh chị cho e hỏi là e nên dùng gì để train đống data này, để khi vào một trang thương mại điện tử có đếm ngược thì e biết được là trang này có đếm vậy ạ?",,,,,
,nan,,,,,
"[WiDS 2021 - updated]
Tiếp nối thông tin về chuỗi sự kiện trong khuôn khổ WiDS 2021, nhóm WiDS Vietnam gửi tới các bạn thông tin về webinar Fireside Chat 29/01/2021.
Nội dung Datathon 2021 là xây dựng model dự đoán xem một bệnh nhân được chuyển vào phòng cấp cứu có tiền sử mắc bệnh tiểu đường hay không, dựa trên dữ liệu thu thập trong 24h đầu tiên. Để giúp các đội thi bổ sung kiến thức về chuyên môn về y khoa, phục vụ cho việc xây dựng feature và tăng hiệu suất model, WiDS Stanford University tổ chức Webinar trực tuyến với chủ đề ""Medical history prediction: Predicting diabetes from ICU admission data"".
Các speaker sẽ đánh giá một mô hình mẫu phân loại (classification model) các loại bệnh tiểu đường từ bộ dữ liệu của datathon và cùng trao đổi với người có chuyên môn về việc tối ưu mô hình.
Webinar không chỉ dành cho các team đang dự thi mà mở rộng tới tất cả các bạn có quan tâm tới chẩn đoán, điều trị, và data science.
Speaker gồm:
Vani Mandava - Director of Data Science at Microsoft Research
Leo Anthony Celi MD MS MPH, MIT - Beth Israel Deaconess Medical Center
Roselyn Mateo MD MS - Rush University Medical Center
Sharada Kalanidhi - Stanford University
⏰ Thời gian: 23h00 Thứ 6, ngày 29/01/2021 (giờ Việt Nam).
📎 Link đăng ký: https://airtable.com/shrnv2FjiYnvXEmZQ
(Sau khi đăng ký, thông tin login sẽ được gửi trong vòng 24h trước Webinar)
Nếu có thắc mắc trong quá trình đăng ký và dự thi Datathon 2021, bạn có thể liên hệ với team WiDS Vietnam để được hỗ trợ.
#widsvietnam #wids
#datathon2021
#viettelgroup
__________________________
WOMEN IN DATA SCIENCE HANOI
Hội thảo định hướng và truyền cảm hứng cho phụ nữ Việt Nam có niềm đam mê với ngành Khoa học dữ liệu và giúp kết nối các đội thi đến với cuộc thi WiDS Datathon.
➤ Official fanpage:
https://www.facebook.com/Women-in-data-science-Vietnam-Wids-103440151734352/
➤ Fanpage group:
https://www.facebook.com/groups/3744623902266580/
➤ LinkedIn: https://www.linkedin.com/company/wids-hanoi/
➤ Email: widshanoi@gmail.com
➤ Hotline: 0345861051 (Thảo); 0973066876 (Hạnh)","[WiDS 2021 - updated] Tiếp nối thông tin về chuỗi sự kiện trong khuôn khổ WiDS 2021, nhóm WiDS Vietnam gửi tới các bạn thông tin về webinar Fireside Chat 29/01/2021. Nội dung Datathon 2021 là xây dựng model dự đoán xem một bệnh nhân được chuyển vào phòng cấp cứu có tiền sử mắc bệnh tiểu đường hay không, dựa trên dữ liệu thu thập trong 24h đầu tiên. Để giúp các đội thi bổ sung kiến thức về chuyên môn về y khoa, phục vụ cho việc xây dựng feature và tăng hiệu suất model, WiDS Stanford University tổ chức Webinar trực tuyến với chủ đề ""Medical history prediction: Predicting diabetes from ICU admission data"". Các speaker sẽ đánh giá một mô hình mẫu phân loại (classification model) các loại bệnh tiểu đường từ bộ dữ liệu của datathon và cùng trao đổi với người có chuyên môn về việc tối ưu mô hình. Webinar không chỉ dành cho các team đang dự thi mà mở rộng tới tất cả các bạn có quan tâm tới chẩn đoán, điều trị, và data science. Speaker gồm: Vani Mandava - Director of Data Science at Microsoft Research Leo Anthony Celi MD MS MPH, MIT - Beth Israel Deaconess Medical Center Roselyn Mateo MD MS - Rush University Medical Center Sharada Kalanidhi - Stanford University ⏰ Thời gian: 23h00 Thứ 6, ngày 29/01/2021 (giờ Việt Nam). Link đăng ký: https://airtable.com/shrnv2FjiYnvXEmZQ (Sau khi đăng ký, thông tin login sẽ được gửi trong vòng 24h trước Webinar) Nếu có thắc mắc trong quá trình đăng ký và dự thi Datathon 2021, bạn có thể liên hệ với team WiDS Vietnam để được hỗ trợ. __________________________ WOMEN IN DATA SCIENCE HANOI Hội thảo định hướng và truyền cảm hứng cho phụ nữ Việt Nam có niềm đam mê với ngành Khoa học dữ liệu và giúp kết nối các đội thi đến với cuộc thi WiDS Datathon. Official fanpage: https://www.facebook.com/Women-in-data-science-Vietnam-Wids-103440151734352/ Fanpage group: https://www.facebook.com/groups/3744623902266580/ LinkedIn: https://www.linkedin.com/company/wids-hanoi/ Email: widshanoi@gmail.com Hotline: 0345861051 (Thảo); 0973066876 (Hạnh)",#widsvietnam	#wids	#datathon2021	#viettelgroup,,,,
Mn cho e hỏi BPE là gì ạ và nó có chức năng giống như w2v không ạ? E cảm ơn,Mn cho e hỏi BPE là gì ạ và nó có chức năng giống như w2v không ạ? E cảm ơn,,,,,
,nan,,,,,
,nan,,,,,
"Em chào mọi người ạ, hiện tại em đang làm luận văn tốt nghiệp về camera Nhân diện hành động giám sát trong nhà bằng python đến giai đoạn pre-training model bằng ResNets PyTorch, cho em hỏi có ai từng làm về đề tài này cho em xin ít tài liệu về cách pre-training model với ạ, có tiếng việt thì càng tốt ạ (only cpu - non gpu)","Em chào mọi người ạ, hiện tại em đang làm luận văn tốt nghiệp về camera Nhân diện hành động giám sát trong nhà bằng python đến giai đoạn pre-training model bằng ResNets PyTorch, cho em hỏi có ai từng làm về đề tài này cho em xin ít tài liệu về cách pre-training model với ạ, có tiếng việt thì càng tốt ạ (only cpu - non gpu)",,,,,
Nhóm mình có cao nhân nào biết về lập trình plugin cho 3dsmax không nhỉ. Mình đang muốn hợp tác để lập 1 plugin trên 3dsmax cho mục đích dự toán nội thất trên nền tảng nhận diện hình ảnh và vật liệu.,Nhóm mình có cao nhân nào biết về lập trình plugin cho 3dsmax không nhỉ. Mình đang muốn hợp tác để lập 1 plugin trên 3dsmax cho mục đích dự toán nội thất trên nền tảng nhận diện hình ảnh và vật liệu.,,,,,
"Xin chào các bạn và các anh chị,
Mình xin phép chia sẻ với các bạn mới học Python một số video ngắn, cơ bản về lập trình Python cho Khoa học Dữ liệu. Ngôn ngữ: tiếng Việt. 
https://www.facebook.com/184314072371636/posts/881926252610411/
#python #vicohub #vietacademy","Xin chào các bạn và các anh chị, Mình xin phép chia sẻ với các bạn mới học Python một số video ngắn, cơ bản về lập trình Python cho Khoa học Dữ liệu. Ngôn ngữ: tiếng Việt. https://www.facebook.com/184314072371636/posts/881926252610411/",#python	#vicohub	#vietacademy,,,,
"Dạ em xin chia sẻ bài phân tích dữ liệu này về Phân loại bình luận (Comments Clustering) trên Youtube.
Bài viết sử dụng trường hợp cụ thể là comment của US Presidential Debates, nhưng cách tiếp cận thì có thể ứng dụng cho rất nhiều use cases khác nữa như: Influencer sử dụng để trích xuất được những idea chính trong số hàng ngàn comments nhận được và làm video để trả lời fan, hoặc cũng có thể được mở rộng cho việc chăm sóc khách hàng của doanh nghiệp, tổng hợp ý kiến từ survey, etc.","Dạ em xin chia sẻ bài phân tích dữ liệu này về Phân loại bình luận (Comments Clustering) trên Youtube. Bài viết sử dụng trường hợp cụ thể là comment của US Presidential Debates, nhưng cách tiếp cận thì có thể ứng dụng cho rất nhiều use cases khác nữa như: Influencer sử dụng để trích xuất được những idea chính trong số hàng ngàn comments nhận được và làm video để trả lời fan, hoặc cũng có thể được mở rộng cho việc chăm sóc khách hàng của doanh nghiệp, tổng hợp ý kiến từ survey, etc.",,,,,
"#pytorch #ml
hi group,
Không biết có anh chị nào trong group từng đọc qua cuốn này, và thư viện wtfml của tác giả không ạ.
Em tìm ebook /pdf này( được thì có thể share hoặc cho em xin :3)
Cũng như cần tìm hiểu document của thư viện ""wtfml""
Cảm ơn mn.","hi group, Không biết có anh chị nào trong group từng đọc qua cuốn này, và thư viện wtfml của tác giả không ạ. Em tìm ebook /pdf này( được thì có thể share hoặc cho em xin :3) Cũng như cần tìm hiểu document của thư viện ""wtfml"" Cảm ơn mn.",#pytorch	#ml,,,,
"Chào các bác!
Em đang tham khảo một bài báo dự đoán quỹ đạo chuyển động của người ""Social LSTM: Human Trajectory Prediction in Crowded Spaces"". Code của bài báo được cung cấp tại https://github.com/quancore/social-lstm
Em đã đọc và hiểu nội dung bài báo, nhưng đang khó khăn trong việc hiện thực hóa nó trong thời gian thực, có bác nào đã visualize nó trên video thì cho em xin code để tham khảo được không.
Cám ơn các bác nhiều!","Chào các bác! Em đang tham khảo một bài báo dự đoán quỹ đạo chuyển động của người ""Social LSTM: Human Trajectory Prediction in Crowded Spaces"". Code của bài báo được cung cấp tại https://github.com/quancore/social-lstm Em đã đọc và hiểu nội dung bài báo, nhưng đang khó khăn trong việc hiện thực hóa nó trong thời gian thực, có bác nào đã visualize nó trên video thì cho em xin code để tham khảo được không. Cám ơn các bác nhiều!",,,,,
"Cho mình hỏi, thầy mình giao bài phân loại về text thì class mình bị imbalance nặng như hình thì làm sao để balance được ạ ?","Cho mình hỏi, thầy mình giao bài phân loại về text thì class mình bị imbalance nặng như hình thì làm sao để balance được ạ ?",,,,,
"Chào mọi người, chả là mình tính mua Google Colab Pro, mà không biết nó có hoạt động được ở Việt Nam hay không. Có bạn nào đã mua có thể thông tin giúp mình với.","Chào mọi người, chả là mình tính mua Google Colab Pro, mà không biết nó có hoạt động được ở Việt Nam hay không. Có bạn nào đã mua có thể thông tin giúp mình với.",,,,,
"Kính chào các bác, em thấy ít dữ liệu quá hoặc dữ liệu phân phối không đều dẫn đến khi bạn chia train, val để đánh giá model sẽ không chuẩn, bị phiến diện (không dạy model mà vẫn bắt nó predict).
Do dó em nghiên cứu thử K-Fold Cross Validation và tiện luôn em viết chút chia sẻ cùng các bạn!
Hi vọng giúp được anh em chút gì đó. Mong admin duyệt bài!","Kính chào các bác, em thấy ít dữ liệu quá hoặc dữ liệu phân phối không đều dẫn đến khi bạn chia train, val để đánh giá model sẽ không chuẩn, bị phiến diện (không dạy model mà vẫn bắt nó predict). Do dó em nghiên cứu thử K-Fold Cross Validation và tiện luôn em viết chút chia sẻ cùng các bạn! Hi vọng giúp được anh em chút gì đó. Mong admin duyệt bài!",,,,,
"Xin phép admin cho share 1 sự kiện do cộng đồng những người làm về AI ở Nhật bản tổ chức.
Đây là chương trình chia sẻ về ứng dụng của AI trong công nghiệp do những người đang làm việc trực tiếp chia sẻ về bài toán cũng như khó khăn thách thức của nó. Mời mọi người cùng tham gia và học hỏi.",Xin phép admin cho share 1 sự kiện do cộng đồng những người làm về AI ở Nhật bản tổ chức. Đây là chương trình chia sẻ về ứng dụng của AI trong công nghiệp do những người đang làm việc trực tiếp chia sẻ về bài toán cũng như khó khăn thách thức của nó. Mời mọi người cùng tham gia và học hỏi.,,,,,
"Chào mọi người
Em muốn được hỏi liệu ML có thể ứng dụng để phân tích cho subcribes trên youtube, cụ thể mục đích chỉ là để tăng subcribes. Liệu với ML có tricks gì có thể được áp dụng để cải thiện điều này chưa ạ!
E cảm ơn mọi người ạ!","Chào mọi người Em muốn được hỏi liệu ML có thể ứng dụng để phân tích cho subcribes trên youtube, cụ thể mục đích chỉ là để tăng subcribes. Liệu với ML có tricks gì có thể được áp dụng để cải thiện điều này chưa ạ! E cảm ơn mọi người ạ!",,,,,
"Chào mọi người ạ, em có một bài toán về xử lý ảnh. Trong một ảnh có nhiều vùng màu và mật độ phân bố pixel màu trên mỗi vùng cũng khác nhau. Em muốn tìm mật độ pixel của một màu hoặc một dải màu phân bố trên một vùng cụ thể thì có thể làm cách nào ạ. Cảm ơn mọi người đã đọc viết của em ạ.","Chào mọi người ạ, em có một bài toán về xử lý ảnh. Trong một ảnh có nhiều vùng màu và mật độ phân bố pixel màu trên mỗi vùng cũng khác nhau. Em muốn tìm mật độ pixel của một màu hoặc một dải màu phân bố trên một vùng cụ thể thì có thể làm cách nào ạ. Cảm ơn mọi người đã đọc viết của em ạ.",,,,,
"#share
Công cụ plot neural network ra dạng Latex, gọn nhẹ và đẹp cho các bài báo cáo/ thuyết trình.","Công cụ plot neural network ra dạng Latex, gọn nhẹ và đẹp cho các bài báo cáo/ thuyết trình.",#share,,,,
"Em mới tìm hiểu Machine Learning được vài tháng, và với nhu cầu công việc e tìm hiểu các công nghệ OCR. E có thắc mắc là, tại sao các công nghệ OCR (open source dễ tìm kiếm trên google) hiện nay đều là sự kết hợp giữa CNN và RNN (hoặc mạnh hơn RNN như LSTM, Transformer), vậy tác dụng của RNN trong quy trình OCR là gì vậy ạ ?","Em mới tìm hiểu Machine Learning được vài tháng, và với nhu cầu công việc e tìm hiểu các công nghệ OCR. E có thắc mắc là, tại sao các công nghệ OCR (open source dễ tìm kiếm trên google) hiện nay đều là sự kết hợp giữa CNN và RNN (hoặc mạnh hơn RNN như LSTM, Transformer), vậy tác dụng của RNN trong quy trình OCR là gì vậy ạ ?",,,,,
"Xin chào Anh/Chị/Bạn,
Em bắt đầu học machine learning để apply cho longitudinal medical data. Dạ anh/chị/bạn cho em lời khuyên thuật toán nào tốt để cải thiện loại dữ liệu này ạ? Cần lưu ý điều gì khi train loại dữ liệu này? Giáo em cũng đưa ra một bài toán cho em về fusion of two cohorts, dạ thông thường anh/chị/bạn fusion như thế nào ạ? Em cảm ơn nhiều! Chúc mọi người ngày mới vui vẻ.","Xin chào Anh/Chị/Bạn, Em bắt đầu học machine learning để apply cho longitudinal medical data. Dạ anh/chị/bạn cho em lời khuyên thuật toán nào tốt để cải thiện loại dữ liệu này ạ? Cần lưu ý điều gì khi train loại dữ liệu này? Giáo em cũng đưa ra một bài toán cho em về fusion of two cohorts, dạ thông thường anh/chị/bạn fusion như thế nào ạ? Em cảm ơn nhiều! Chúc mọi người ngày mới vui vẻ.",,,,,
"Chào admin và mọi người.
Em mới bắt đầu tự học python được một thời gian, gốc là dân Điện tử-Viễn thông ạ.
Em đang dần làm quen với các giải thuật, mô hình toán học và code trong ML nên phần này em sẽ tiếp tục tìm hiểu và không hỏi quá nhiều.
Hiện tại em đang triển khai một hệ thống (cả phần cứng và phần mềm), trong đó phần cứng dùng để thu và xử lý dữ liệu (từ hệ cảm biến). Sau đó em sẽ truyền không dây lên webserver làm database (nguồn 1)
Một nguồn dữ liệu khác em sẽ dùng là từ một trang web cho phép import data (nguồn 2).
Em đang mắc ở chỗ này (em đã search nhưng vì kiến thức về web hạn chế nên chưa có nhiều kết quả). Bây giờ em muốn dùng dữ liệu từ 2 nguồn trên để xây dựng ML model thì nên tiếp cận như nào ạ:
- Viết chương trình đọc từ web (nguồn 2) và lưu về webserver chứa data (nguồn 1) rồi mới bắt đầu build model.
Em định sẽ làm như này vì em mong muốn xử lý real time, tuy nhiên chưa thể tìm ra công cụ cụ thể ạ.
Mong admin và mọi người giúp đỡ, em cảm ơn nhiều ạ!","Chào admin và mọi người. Em mới bắt đầu tự học python được một thời gian, gốc là dân Điện tử-Viễn thông ạ. Em đang dần làm quen với các giải thuật, mô hình toán học và code trong ML nên phần này em sẽ tiếp tục tìm hiểu và không hỏi quá nhiều. Hiện tại em đang triển khai một hệ thống (cả phần cứng và phần mềm), trong đó phần cứng dùng để thu và xử lý dữ liệu (từ hệ cảm biến). Sau đó em sẽ truyền không dây lên webserver làm database (nguồn 1) Một nguồn dữ liệu khác em sẽ dùng là từ một trang web cho phép import data (nguồn 2). Em đang mắc ở chỗ này (em đã search nhưng vì kiến thức về web hạn chế nên chưa có nhiều kết quả). Bây giờ em muốn dùng dữ liệu từ 2 nguồn trên để xây dựng ML model thì nên tiếp cận như nào ạ: - Viết chương trình đọc từ web (nguồn 2) và lưu về webserver chứa data (nguồn 1) rồi mới bắt đầu build model. Em định sẽ làm như này vì em mong muốn xử lý real time, tuy nhiên chưa thể tìm ra công cụ cụ thể ạ. Mong admin và mọi người giúp đỡ, em cảm ơn nhiều ạ!",,,,,
"Chào mn,
Cho mình hỏi nếu thu được loss train/val plot như hình thì mình nên lấy weight ở epoch nào nhỉ?
Mình cám ơn,","Chào mn, Cho mình hỏi nếu thu được loss train/val plot như hình thì mình nên lấy weight ở epoch nào nhỉ? Mình cám ơn,",,,,,
"Chào mọi người. Em đang làm về bài toán nhận dạng ảnh với 5 nhãn, train data gồm 7721 ảnh, test data gồm 856 ảnh. Kết quả bên dưới cho thấy model bị overfiting. Và một số nhãn cho kết quả dự đoán khá thấp (56 % và 58 %). Theo như em được biết thì một trong những cách để cải thiện các kết quả dự đoán là thêm training data. Không biết những đánh giá và những hiểu biết như vậy của em có đúng không ạ? Em cảm ơn mọi người!!!","Chào mọi người. Em đang làm về bài toán nhận dạng ảnh với 5 nhãn, train data gồm 7721 ảnh, test data gồm 856 ảnh. Kết quả bên dưới cho thấy model bị overfiting. Và một số nhãn cho kết quả dự đoán khá thấp (56 % và 58 %). Theo như em được biết thì một trong những cách để cải thiện các kết quả dự đoán là thêm training data. Không biết những đánh giá và những hiểu biết như vậy của em có đúng không ạ? Em cảm ơn mọi người!!!",,,,,
"Chào mọi người, em là sinh viên trái ngành và đang tự học về ML. Em đã hoàn thành khóa Machine Learning với khóa Deep Learning Specialization của thầy Andrew Ng, và khóa Specialization Tensorflow trên Coursera, em cũng xây dựng được một vài mô hình NN đơn giản với các dataset có sẵn trên mạng.
Các anh chị trong nghề cho em hỏi, AI engineer, cụ thể hơn ở mảng computer vision cần làm những gì ạ? Em cần phải có thêm kiến thức hay kinh nghiệm về cái gì nữa để xin được vị trí Internship/ Fresher vậy ạ? Kiểu như phải biết xây dựng 1 app/web AI hay sao ạ?
Em cảm ơn.","Chào mọi người, em là sinh viên trái ngành và đang tự học về ML. Em đã hoàn thành khóa Machine Learning với khóa Deep Learning Specialization của thầy Andrew Ng, và khóa Specialization Tensorflow trên Coursera, em cũng xây dựng được một vài mô hình NN đơn giản với các dataset có sẵn trên mạng. Các anh chị trong nghề cho em hỏi, AI engineer, cụ thể hơn ở mảng computer vision cần làm những gì ạ? Em cần phải có thêm kiến thức hay kinh nghiệm về cái gì nữa để xin được vị trí Internship/ Fresher vậy ạ? Kiểu như phải biết xây dựng 1 app/web AI hay sao ạ? Em cảm ơn.",,,,,
"Chào mọi người,
hiện mình đang làm về HPC, mình đang rất cần 1 anh expert có thể giúp mình cài đặt SLURM, tạo 1 cluster để chạy nextflow, quản lí jobs trên hệ thống nhiều server ubuntu. Các anh em có ai có kinh nghiệm phần này-slurm có thể giúp xin ib mình hoặc comment giúp với ạ. Bên mình rất tốt cho job này.
Hi all,
I am looking for an expert who can help me to install SLURM in my server ubuntu system, to build a cluster. Then I will integrate NEXTFLOW to slurm management. If you are available for this freelancer job, please dont hesitate to contact me. The rate for this job is very attractive!
thank you all,
Giang Vo: +84981419967","Chào mọi người, hiện mình đang làm về HPC, mình đang rất cần 1 anh expert có thể giúp mình cài đặt SLURM, tạo 1 cluster để chạy nextflow, quản lí jobs trên hệ thống nhiều server ubuntu. Các anh em có ai có kinh nghiệm phần này-slurm có thể giúp xin ib mình hoặc comment giúp với ạ. Bên mình rất tốt cho job này. Hi all, I am looking for an expert who can help me to install SLURM in my server ubuntu system, to build a cluster. Then I will integrate NEXTFLOW to slurm management. If you are available for this freelancer job, please dont hesitate to contact me. The rate for this job is very attractive! thank you all, Giang Vo: +84981419967",,,,,
"Dear ALL, hiện mình có 1 bài toán nhờ các bạn tư vấn giúp ah, mỗi sản phẩm mình có chụp lại ảnh bề mặt, khi sản phẩm nào bị lỗi gì đó bám lên bề mặt thì ảnh sẽ có các đốm sáng như ảnh đầu tiên, tùy vào hình dạng kích thước của đốm sáng mà sẽ quy định là lỗi A, lỗi B, lỗi C gì đó, hoặc nếu bề mặt bị xước một đoạn thì sẽ có 1 vệt sáng mờ trên ảnh như ảnh 2 chẳng hạn. Mình muốn áp dụng ML (image prediction) vào bài toán này, xin các bạn cho ý kiến và hướng nào có thể giải quyết được ạ.","Dear ALL, hiện mình có 1 bài toán nhờ các bạn tư vấn giúp ah, mỗi sản phẩm mình có chụp lại ảnh bề mặt, khi sản phẩm nào bị lỗi gì đó bám lên bề mặt thì ảnh sẽ có các đốm sáng như ảnh đầu tiên, tùy vào hình dạng kích thước của đốm sáng mà sẽ quy định là lỗi A, lỗi B, lỗi C gì đó, hoặc nếu bề mặt bị xước một đoạn thì sẽ có 1 vệt sáng mờ trên ảnh như ảnh 2 chẳng hạn. Mình muốn áp dụng ML (image prediction) vào bài toán này, xin các bạn cho ý kiến và hướng nào có thể giải quyết được ạ.",,,,,
"[Chia sẻ cơ hội]
WiDS DATATHON 2021
Dear các bạn,
Nhằm mục đích truyền cảm hứng và định hướng giáo dục cho các nhà khoa học dữ liệu tại Việt Nam, Tập đoàn Viettel đồng hành cùng tỏ chức Women in Data Science thúc đẩy cuộc thi Datathon và tổ chức Conference chuyên sâu về Khoa học dữ liệu. Cuộc thi là cơ hội rèn luyện và thể hiện bản thân cho tất cả những bạn yêu thích công nghệ và data science 😃
👉 Lập team và sign up tại https://www.kaggle.com/c/widsdatathon2021/
🏆Giải thưởng quy mô toàn cầu Datathon 2021:
🥇Giải nhất: $2,000 tiền mặt
🥈Giải nhì: $1,500 tiền mặt
🥉Giải ba: $1,000 tiền mặt
Các mốc thời gian quan trọng của cuộc thi:
• Ngày 26/2/2021: Thời hạn tập hợp nhóm. Đây là ngày cuối cùng các đối thủ có thể tham gia hoặc tập hợp nhóm.
• Ngày 1/3/2021: Hạn nộp Kaggle cuối cùng
• Ngày 8/3/2021: Người chiến thắng trên bản xếp hạng Datathon sẽ được công bố tại Hội nghị Toàn cầu WiDS.
📌Lưu ý là:
• Team tối đa 4 người, trong đó 50% là nữ.
• Mỗi team được submit tối đa 15 và được lựa chọn tối đa 2 kết quả submit tốt nhất để nộp lần cuối.
Ngoài ra, tại Việt Nam, các đội thi đạt thành tích cao sẽ nhận được giải thưởng tương xứng.
Đây là cơ hội tuyệt vời để rèn luyện và thể hiện khả năng của mình!
Team WiDS Vietnam sẽ hỗ trợ các bạn trong quá trình dự thi!
#wids2021
#widsvietnam
#womenindatascience
#vietteldataanalyticscenter
_____________
WOMAN IN DATA SCIENCE HANOI
➤ Official fanpage:
https://www.facebook.com/Women-in-data-science-Vietnam-Wids-103440151734352/
➤ Fanpage group:
https://www.facebook.com/groups/3744623902266580/
➤ LinkedIn: https://www.linkedin.com/company/wids-hanoi/
➤ Email: widshanoi@gmail.com","[Chia sẻ cơ hội] WiDS DATATHON 2021 Dear các bạn, Nhằm mục đích truyền cảm hứng và định hướng giáo dục cho các nhà khoa học dữ liệu tại Việt Nam, Tập đoàn Viettel đồng hành cùng tỏ chức Women in Data Science thúc đẩy cuộc thi Datathon và tổ chức Conference chuyên sâu về Khoa học dữ liệu. Cuộc thi là cơ hội rèn luyện và thể hiện bản thân cho tất cả những bạn yêu thích công nghệ và data science Lập team và sign up tại https://www.kaggle.com/c/widsdatathon2021/ Giải thưởng quy mô toàn cầu Datathon 2021: Giải nhất: $2,000 tiền mặt Giải nhì: $1,500 tiền mặt Giải ba: $1,000 tiền mặt Các mốc thời gian quan trọng của cuộc thi: • Ngày 26/2/2021: Thời hạn tập hợp nhóm. Đây là ngày cuối cùng các đối thủ có thể tham gia hoặc tập hợp nhóm. • Ngày 1/3/2021: Hạn nộp Kaggle cuối cùng • Ngày 8/3/2021: Người chiến thắng trên bản xếp hạng Datathon sẽ được công bố tại Hội nghị Toàn cầu WiDS. Lưu ý là: • Team tối đa 4 người, trong đó 50% là nữ. • Mỗi team được submit tối đa 15 và được lựa chọn tối đa 2 kết quả submit tốt nhất để nộp lần cuối. Ngoài ra, tại Việt Nam, các đội thi đạt thành tích cao sẽ nhận được giải thưởng tương xứng. Đây là cơ hội tuyệt vời để rèn luyện và thể hiện khả năng của mình! Team WiDS Vietnam sẽ hỗ trợ các bạn trong quá trình dự thi! _____________ WOMAN IN DATA SCIENCE HANOI Official fanpage: https://www.facebook.com/Women-in-data-science-Vietnam-Wids-103440151734352/ Fanpage group: https://www.facebook.com/groups/3744623902266580/ LinkedIn: https://www.linkedin.com/company/wids-hanoi/ Email: widshanoi@gmail.com",#wids2021	#widsvietnam	#womenindatascience	#vietteldataanalyticscenter,,,,
"[Chia sẻ cơ hội]
WiDS DATATHON 2021
Nhằm mục đích truyền cảm hứng và giáo dục hướng nghiệp cho các nhà khoa học dữ liệu tại Việt Nam, Tập đoàn Viettel đồng hành cùng tổ chức Women in Data Science thúc đẩy cuộc thi Datathon và Conference chuyên sâu về Khoa học dữ liệu. Cuộc thi là cơ hội rèn luyện và thể hiện bản thân cho tất cả những bạn yêu thích công nghệ và data science 😃
👉 Lập team và sign up tại https://www.kaggle.com/c/widsdatathon2021/
🏆Giải thưởng quy mô toàn cầu Datathon 2021:
🥇Giải nhất: $2,000 tiền mặt
🥈Giải nhì: $1,500 tiền mặt
🥉Giải ba: $1,000 tiền mặt
Các mốc thời gian quan trọng của cuộc thi:
• Ngày 26/2/2021: Thời hạn tập hợp nhóm. Đây là ngày cuối cùng các đối thủ có thể tham gia hoặc tập hợp nhóm.
• Ngày 1/3/2021: Hạn nộp Kaggle cuối cùng
• Ngày 8/3/2021: Người chiến thắng trên bản xếp hạng Datathon sẽ được công bố tại Hội nghị Toàn cầu WiDS.
📌Lưu ý là:
• Team tối đa 4 người, trong đó 50% là nữ.
• Mỗi team được submit tối đa 15 lần/ngày cho đến thời điểm deadline 1/3/2021 và được lựa chọn tối đa 2 kết quả submit tốt nhất để nộp lần cuối.
Ngoài ra, tại Việt Nam, các đội thi đạt thành tích cao sẽ nhận được giải thưởng tương xứng.
Đây là cơ hội tuyệt vời để rèn luyện và thể hiện khả năng của mình!
Team WiDS Vietnam sẽ hỗ trợ các bạn trong quá trình dự thi!
#wids2021
#widsvietnam
#womenindatascience
#vietteldataanalyticscenter
_____________
WOMEN IN DATA SCIENCE HANOI
➤ Official fanpage:
https://www.facebook.com/Women-in-data-science-Vietnam-Wids-103440151734352/
➤ Fanpage group:
https://www.facebook.com/groups/3744623902266580/
➤ LinkedIn: https://www.linkedin.com/company/wids-hanoi/
➤ Email: widshanoi@gmail.com","[Chia sẻ cơ hội] WiDS DATATHON 2021 Nhằm mục đích truyền cảm hứng và giáo dục hướng nghiệp cho các nhà khoa học dữ liệu tại Việt Nam, Tập đoàn Viettel đồng hành cùng tổ chức Women in Data Science thúc đẩy cuộc thi Datathon và Conference chuyên sâu về Khoa học dữ liệu. Cuộc thi là cơ hội rèn luyện và thể hiện bản thân cho tất cả những bạn yêu thích công nghệ và data science Lập team và sign up tại https://www.kaggle.com/c/widsdatathon2021/ Giải thưởng quy mô toàn cầu Datathon 2021: Giải nhất: $2,000 tiền mặt Giải nhì: $1,500 tiền mặt Giải ba: $1,000 tiền mặt Các mốc thời gian quan trọng của cuộc thi: • Ngày 26/2/2021: Thời hạn tập hợp nhóm. Đây là ngày cuối cùng các đối thủ có thể tham gia hoặc tập hợp nhóm. • Ngày 1/3/2021: Hạn nộp Kaggle cuối cùng • Ngày 8/3/2021: Người chiến thắng trên bản xếp hạng Datathon sẽ được công bố tại Hội nghị Toàn cầu WiDS. Lưu ý là: • Team tối đa 4 người, trong đó 50% là nữ. • Mỗi team được submit tối đa 15 lần/ngày cho đến thời điểm deadline 1/3/2021 và được lựa chọn tối đa 2 kết quả submit tốt nhất để nộp lần cuối. Ngoài ra, tại Việt Nam, các đội thi đạt thành tích cao sẽ nhận được giải thưởng tương xứng. Đây là cơ hội tuyệt vời để rèn luyện và thể hiện khả năng của mình! Team WiDS Vietnam sẽ hỗ trợ các bạn trong quá trình dự thi! _____________ WOMEN IN DATA SCIENCE HANOI Official fanpage: https://www.facebook.com/Women-in-data-science-Vietnam-Wids-103440151734352/ Fanpage group: https://www.facebook.com/groups/3744623902266580/ LinkedIn: https://www.linkedin.com/company/wids-hanoi/ Email: widshanoi@gmail.com",#wids2021	#widsvietnam	#womenindatascience	#vietteldataanalyticscenter,,,,
"Các anh/chị cho em hỏi thăm về việc đăng ký bộ dữ liệu VLSP. Em vào trang web chính là https://vlsp.org.vn/ thì không truy cập được. Vậy có cách nào khác không ạ? Em cảm ơn nhiều ạ.
Edit: Web mở lại rồi ạ, em cảm ơn mọi người.","Các anh/chị cho em hỏi thăm về việc đăng ký bộ dữ liệu VLSP. Em vào trang web chính là https://vlsp.org.vn/ thì không truy cập được. Vậy có cách nào khác không ạ? Em cảm ơn nhiều ạ. Edit: Web mở lại rồi ạ, em cảm ơn mọi người.",,,,,
"Xin phép Admin, mình là thành viên mới và thật sự không phải là dân công nghệ thông tin (mình làm bên mảng cơ khí chế tạo máy) và giờ công việc bắt buộc phải xử lý một vài số liệu thực nghiệm, mình đã gặp khó khăn và mong đưa bài lên đây để xin được những ý kiến trợ giúp mình.
Sau khi mô phỏng các tính toán vật lý của mình, mình đã có một bảng số liệu gồm có 144 datasets trong tệp ketqua.txt; mình chỉ quan tâm 2 cột đầu vào thứ hai (X1) và thứ ba (X2) trong file này; và tương ứng mình chỉ quan tâm cột thứ tư ( Y) là kết quả đầu ra của mình. Tức là: cột 4 là hàm phụ thuộc vào cột 2 và cột 3; hay: Y=f(X1,X2).
Giờ mình muốn xây dựng một Neural Network để xử lý bộ số liệu này, mình dùng Python với thư viện keras kết hợp với numpy để tạo Neural Network (code file mình có đính kèm). Tưởng chừng rất đơn giản nhưng nó thật khó khăn vì việc lựa chọn số hidden layers cho mạng và đặc biệt là việc lựa chọn các activation function tương ứng ! Trong code file mình trình bày mạng noron gồm: 2 đầu vào, một đầu ra và hai lớp ẩn: lớp ẩn 1 có 12 noron, lớp ẩn 2 có 8 noron. Nhưng mình chạy và thật sự không thu được kết quả phù hợp với bộ số liệu đã chỉ định.
Mình mong các bạn giúp mình hai vấn đề sau:
Lựa chọn số lớp ẩn cũng như số neuron của từng lớp để hợp lý cho kết quả tính toán (nâng cao accuracy của bài toán, hiện tại nó đang là 0, tức là không đạt yêu cầu);
Lựa chọn activation functions tương ứng.
Mình mong được trợ giúp sớm. Xin cảm ơn ạ.
Code file:
https://www.mediafire.com/file/kzdxvckqrmbjxg1/bTest.py/file
Data file:
https://www.mediafire.com/file/37k8i4e540uq2uv/Ketqua.txt/file","Xin phép Admin, mình là thành viên mới và thật sự không phải là dân công nghệ thông tin (mình làm bên mảng cơ khí chế tạo máy) và giờ công việc bắt buộc phải xử lý một vài số liệu thực nghiệm, mình đã gặp khó khăn và mong đưa bài lên đây để xin được những ý kiến trợ giúp mình. Sau khi mô phỏng các tính toán vật lý của mình, mình đã có một bảng số liệu gồm có 144 datasets trong tệp ketqua.txt; mình chỉ quan tâm 2 cột đầu vào thứ hai (X1) và thứ ba (X2) trong file này; và tương ứng mình chỉ quan tâm cột thứ tư ( Y) là kết quả đầu ra của mình. Tức là: cột 4 là hàm phụ thuộc vào cột 2 và cột 3; hay: Y=f(X1,X2). Giờ mình muốn xây dựng một Neural Network để xử lý bộ số liệu này, mình dùng Python với thư viện keras kết hợp với numpy để tạo Neural Network (code file mình có đính kèm). Tưởng chừng rất đơn giản nhưng nó thật khó khăn vì việc lựa chọn số hidden layers cho mạng và đặc biệt là việc lựa chọn các activation function tương ứng ! Trong code file mình trình bày mạng noron gồm: 2 đầu vào, một đầu ra và hai lớp ẩn: lớp ẩn 1 có 12 noron, lớp ẩn 2 có 8 noron. Nhưng mình chạy và thật sự không thu được kết quả phù hợp với bộ số liệu đã chỉ định. Mình mong các bạn giúp mình hai vấn đề sau: Lựa chọn số lớp ẩn cũng như số neuron của từng lớp để hợp lý cho kết quả tính toán (nâng cao accuracy của bài toán, hiện tại nó đang là 0, tức là không đạt yêu cầu); Lựa chọn activation functions tương ứng. Mình mong được trợ giúp sớm. Xin cảm ơn ạ. Code file: https://www.mediafire.com/file/kzdxvckqrmbjxg1/bTest.py/file Data file: https://www.mediafire.com/file/37k8i4e540uq2uv/Ketqua.txt/file",,,,,
"Hi các anh chị
Hiện e đang có bài toán chấm công ở công ty bằng mặt ngừoi
hiện em đang định đề xuất mua máy chủ để dành cho việc nhận dạng này.
hiện chắc sẽ dùng lib ngoài như dlib hay vgg16 hay 19, ....
nhờ A E trong nhóm tư vấn nên đầu tư máy chủ thế nào cho hợp lý,
cảm ơn AE","Hi các anh chị Hiện e đang có bài toán chấm công ở công ty bằng mặt ngừoi hiện em đang định đề xuất mua máy chủ để dành cho việc nhận dạng này. hiện chắc sẽ dùng lib ngoài như dlib hay vgg16 hay 19, .... nhờ A E trong nhóm tư vấn nên đầu tư máy chủ thế nào cho hợp lý, cảm ơn AE",,,,,
"Chào các bạn
Mình có dự án làm robot đi 2 chân, tìm hiểu mà k biết bắt đầu từ đâu, các bạn cho lời khuyên,
Mình cũng muốn thuê làm, thì không biết tìm người ở đâu và chi phí như thế nào
Mình muốn hỏi về phần machine learning, còn phần cứng và mạch điện điều khiển thì mình đã thiết kế và thấy ok rồi, hình dưới là phần thiết kế cơ khí mới hoàn thành, cần gia công cơ khí chính xác nữa là có mẫu và lắp mạch điều khiển nữa là có thể lập trình","Chào các bạn Mình có dự án làm robot đi 2 chân, tìm hiểu mà k biết bắt đầu từ đâu, các bạn cho lời khuyên, Mình cũng muốn thuê làm, thì không biết tìm người ở đâu và chi phí như thế nào Mình muốn hỏi về phần machine learning, còn phần cứng và mạch điện điều khiển thì mình đã thiết kế và thấy ok rồi, hình dưới là phần thiết kế cơ khí mới hoàn thành, cần gia công cơ khí chính xác nữa là có mẫu và lắp mạch điều khiển nữa là có thể lập trình",,,,,
"Trankit: A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing
https://arxiv.org/pdf/2101.03289.pdf
-------------------
Chào mọi người,
Mình muốn giới thiệu với mọi người bộ NLP toolkit đa ngôn ngữ do nhóm mình ở University of Oregon phát triển tên là Trankit: https://github.com/nlp-uoregon/trankit.
Trankit hiện đang giữ hiệu năng tốt nhất cho nhiều tasks (như tách câu, tách từ, gán nhãn từ loại, phân tích phụ thuộc) cho 56 ngôn ngữ khác nhau, vượt xa bộ toolkit đa ngôn ngữ tốt nhất hiện nay là Stanford NLP (Stanza) do Stanford phát triển.
Với Tiếng Việt, Trankit có khả năng xử lý dữ liệu văn bản thô (raw text) và cho ra kết quả tốt cho nhiều tasks khác nhau như: tách câu, tách từ, gán nhãn từ loại, phân tích ngữ pháp (dependency parsing), và nhận diện tên thực thể (named entity recognition). Hiện tại, bộ tách từ của Trankit được huấn luận trên bộ dữ liệu của VLSP và đang giữ hiệu năng tách từ tốt nhất cho Tiếng Việt với 98.03 F1.
Trankit được viết trên Python và dễ dàng cài đặt qua pip. Nhóm mình cũng tạo ra 1 demo website để mọi người có thể trực tiếp dùng thử Trankit tại đây: http://nlp.uoregon.edu/trankit
Để biết thêm thông tin về Trankit, các bạn có thể ghé qua trang github (https://github.com/nlp-uoregon/trankit), documentation (https://trankit.readthedocs.io/en/latest/index.html ), hoặc bài báo (https://arxiv.org/pdf/2101.03289.pdf) của nhóm mình.
Nhóm mình hi vọng Trankit sẽ góp phần phát triển các research về NLP cho Tiếng Việt.","Trankit: A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing https://arxiv.org/pdf/2101.03289.pdf ------------------- Chào mọi người, Mình muốn giới thiệu với mọi người bộ NLP toolkit đa ngôn ngữ do nhóm mình ở University of Oregon phát triển tên là Trankit: https://github.com/nlp-uoregon/trankit. Trankit hiện đang giữ hiệu năng tốt nhất cho nhiều tasks (như tách câu, tách từ, gán nhãn từ loại, phân tích phụ thuộc) cho 56 ngôn ngữ khác nhau, vượt xa bộ toolkit đa ngôn ngữ tốt nhất hiện nay là Stanford NLP (Stanza) do Stanford phát triển. Với Tiếng Việt, Trankit có khả năng xử lý dữ liệu văn bản thô (raw text) và cho ra kết quả tốt cho nhiều tasks khác nhau như: tách câu, tách từ, gán nhãn từ loại, phân tích ngữ pháp (dependency parsing), và nhận diện tên thực thể (named entity recognition). Hiện tại, bộ tách từ của Trankit được huấn luận trên bộ dữ liệu của VLSP và đang giữ hiệu năng tách từ tốt nhất cho Tiếng Việt với 98.03 F1. Trankit được viết trên Python và dễ dàng cài đặt qua pip. Nhóm mình cũng tạo ra 1 demo website để mọi người có thể trực tiếp dùng thử Trankit tại đây: http://nlp.uoregon.edu/trankit Để biết thêm thông tin về Trankit, các bạn có thể ghé qua trang github (https://github.com/nlp-uoregon/trankit), documentation (https://trankit.readthedocs.io/en/latest/index.html ), hoặc bài báo (https://arxiv.org/pdf/2101.03289.pdf) của nhóm mình. Nhóm mình hi vọng Trankit sẽ góp phần phát triển các research về NLP cho Tiếng Việt.",,,,,
"Mình đang học sử dụng Nearest Neighbor của gói scikit-learn để làm recommender sýtem (phương collaborating-based). Để có thể sử dụng model mình sẽ phải pivot dữ liệu với columns là các item nghiên cứu để recommend cho người dùng.
Tuy nhiên khi pivot_table thì mình gặp phải lỗi “unstack dataframe is too big, causing int32 overflow”. Mình đã cố gắng filtering bớt dữ liệu nhưng mình lo ngại việc filter quá nhiều sẽ ảnh hưởng đến chất lượng recommend.
Có ai có kinh nghiệm xử lý vấn đề này không nhỉ","Mình đang học sử dụng Nearest Neighbor của gói scikit-learn để làm recommender sýtem (phương collaborating-based). Để có thể sử dụng model mình sẽ phải pivot dữ liệu với columns là các item nghiên cứu để recommend cho người dùng. Tuy nhiên khi pivot_table thì mình gặp phải lỗi “unstack dataframe is too big, causing int32 overflow”. Mình đã cố gắng filtering bớt dữ liệu nhưng mình lo ngại việc filter quá nhiều sẽ ảnh hưởng đến chất lượng recommend. Có ai có kinh nghiệm xử lý vấn đề này không nhỉ",,,,,
"[TopDup - Core team recruitment]
Chào mọi người,

Như mọi người đã biết thì đầu tháng 12 năm nay anh Tiệp có giới thiệu với forum mình về dự án TopDup.

Nói sơ qua về dự án, ""TopDup là dự án mã nguồn mở được khởi xướng bởi Forum Machine Learning Cơ bản nhằm hỗ trợ các website, blog công nghệ bảo vệ bản quyền bài viết và chống sao chép.” Các bạn có thể đọc thêm thông tin chi tiết dự án tại: http://bit.ly/Topdup

Hiện tại dự án đã được chạy và bắt đầu Iteration 1, tuy nhiên tụi mình vẫn rất cần những bạn tình nguyện viên để đóng góp vào dự án với vai trò core team ở các vị trí sau:
DevOps
Backend Dev
Code Reviewer
UI/UX Designer
Tester
PR/Marketing Executive
HR Executive

Mô tả công việc, yêu cầu và chỉ tiêu của từng vị trí: http://bit.ly/TopDupcoreteam

Vì là dự án phi lợi nhuận nên các bạn tham gia vào sẽ không có lương, tuy nhiên đây chắc chắn sẽ là cơ hội tốt để các bạn có thể học hỏi, tạo network cũng như vận dụng kiến thức và kĩ năng để đóng góp cho cộng đồng.

Những bạn nào muốn tham gia dự án thì điền vào form này từ 31/12/2020 đến hết ngày 04/01/2021 https://forms.gle/crQzz5yuHLuLtQCF8
(Các bạn muốn contribute với tư cách là volunteer bình thường thì có thể join xuyên suốt quá trình thực hiện dự án nhé) 

Cảm ơn mọi người đã đọc bài và chúc mọi người năm mới vui vẻ!","[TopDup - Core team recruitment] Chào mọi người, Như mọi người đã biết thì đầu tháng 12 năm nay anh Tiệp có giới thiệu với forum mình về dự án TopDup. Nói sơ qua về dự án, ""TopDup là dự án mã nguồn mở được khởi xướng bởi Forum Machine Learning Cơ bản nhằm hỗ trợ các website, blog công nghệ bảo vệ bản quyền bài viết và chống sao chép.” Các bạn có thể đọc thêm thông tin chi tiết dự án tại: http://bit.ly/Topdup Hiện tại dự án đã được chạy và bắt đầu Iteration 1, tuy nhiên tụi mình vẫn rất cần những bạn tình nguyện viên để đóng góp vào dự án với vai trò core team ở các vị trí sau: DevOps Backend Dev Code Reviewer UI/UX Designer Tester PR/Marketing Executive HR Executive Mô tả công việc, yêu cầu và chỉ tiêu của từng vị trí: http://bit.ly/TopDupcoreteam Vì là dự án phi lợi nhuận nên các bạn tham gia vào sẽ không có lương, tuy nhiên đây chắc chắn sẽ là cơ hội tốt để các bạn có thể học hỏi, tạo network cũng như vận dụng kiến thức và kĩ năng để đóng góp cho cộng đồng. Những bạn nào muốn tham gia dự án thì điền vào form này từ 31/12/2020 đến hết ngày 04/01/2021 https://forms.gle/crQzz5yuHLuLtQCF8 (Các bạn muốn contribute với tư cách là volunteer bình thường thì có thể join xuyên suốt quá trình thực hiện dự án nhé) Cảm ơn mọi người đã đọc bài và chúc mọi người năm mới vui vẻ!",,,,,
"chào mọi người, mình đang tìm hiểu về weka dùng thuật toán association rule để phân lớp. có bạn nào học về weka xin giúp đỡ với, thanks mọi người","chào mọi người, mình đang tìm hiểu về weka dùng thuật toán association rule để phân lớp. có bạn nào học về weka xin giúp đỡ với, thanks mọi người",,,,,
"Em xin chào toàn thể anh chị em trong group ạ. Hiện tại e có một bài toán và vì mới học về ML nên e chưa biết cách nào để xử lí được vấn đề này ạ:
Khi vào một trang web bán hàng, đôi khi chúng ta thấy sản phẩm nào đó đang được đếm ngược, kiểu như là còn 01:20:30 nữa là sẽ kết thúc giảm giá, điều này sẽ khiến khách hàng ra quyết định sớm hơn. Hiện tại, em đang muốn detect được cái đồng hồ đếm ngược đó nằm ở đâu trong trang web. Ý tưởng của em là chụp ảnh màn hình trang web rồi detect phần countdown timer đó. Mọi người cho e hỏi là để detect được cái đó thì mình nên dùng phương pháp nào được ko ạ. Em cảm ơn cả nhà rất nhiều, chúc mn 1 tuần làm việc hiệu quả","Em xin chào toàn thể anh chị em trong group ạ. Hiện tại e có một bài toán và vì mới học về ML nên e chưa biết cách nào để xử lí được vấn đề này ạ: Khi vào một trang web bán hàng, đôi khi chúng ta thấy sản phẩm nào đó đang được đếm ngược, kiểu như là còn 01:20:30 nữa là sẽ kết thúc giảm giá, điều này sẽ khiến khách hàng ra quyết định sớm hơn. Hiện tại, em đang muốn detect được cái đồng hồ đếm ngược đó nằm ở đâu trong trang web. Ý tưởng của em là chụp ảnh màn hình trang web rồi detect phần countdown timer đó. Mọi người cho e hỏi là để detect được cái đó thì mình nên dùng phương pháp nào được ko ạ. Em cảm ơn cả nhà rất nhiều, chúc mn 1 tuần làm việc hiệu quả",,,,,
"ECCV 2020 và CPVR 2020
Mình đi học và project trong lớp là reproduce lại 1 bài của ECCV 2020 hoặc CPVR 2020, các bạn có gợi ý cho mình bài nào không?
Mục tiêu của mình:
- Bài liên quan về Segmentation
- Bài nào có data để train, GPU train tầm 12h thôi. Không cần cao như tác giả, có kết quả tương tự cũng được
- Có thể implement được, mình đã code được UNet, và những cái tương tự
- Có baseline code github cũng tốt
- Scope là project trong lớp học thôi nên cũng ko quá
Cám ơn các bạn.","ECCV 2020 và CPVR 2020 Mình đi học và project trong lớp là reproduce lại 1 bài của ECCV 2020 hoặc CPVR 2020, các bạn có gợi ý cho mình bài nào không? Mục tiêu của mình: - Bài liên quan về Segmentation - Bài nào có data để train, GPU train tầm 12h thôi. Không cần cao như tác giả, có kết quả tương tự cũng được - Có thể implement được, mình đã code được UNet, và những cái tương tự - Có baseline code github cũng tốt - Scope là project trong lớp học thôi nên cũng ko quá Cám ơn các bạn.",,,,,
"Chào các a/c, em có up bài viết bên forum mà chưa nhận được câu trả lời nên e xin hỏi luôn bên đây
Khi làm việc với các task NLP mình cần encode các sentence về dạng numerical, thì mình cần build được một list vocabulary gồm các token and ids từ các sentence, em muốn hỏi ở đây là mình build vocabulary này là trên tập train, tập test hay entire data vì em nghĩ có một số thứ sau đây xảy ra
1. Nếu build vocabulary trên tập train, thì model sẽ recognize khá nhiều OOV(out of vocabulary) trên cả tập dev/test
2. Nếu build vocabulary trên entire data thì có khả năng model sẽ ko gặp các unknown words vì mọi word đều đã có trong vocabulary, điều này có ảnh hưởng tới chất lượng của model khi đưa lên product hay ko? vì trên thực tế có nhiều OOV hơn ạ","Chào các a/c, em có up bài viết bên forum mà chưa nhận được câu trả lời nên e xin hỏi luôn bên đây Khi làm việc với các task NLP mình cần encode các sentence về dạng numerical, thì mình cần build được một list vocabulary gồm các token and ids từ các sentence, em muốn hỏi ở đây là mình build vocabulary này là trên tập train, tập test hay entire data vì em nghĩ có một số thứ sau đây xảy ra 1. Nếu build vocabulary trên tập train, thì model sẽ recognize khá nhiều OOV(out of vocabulary) trên cả tập dev/test 2. Nếu build vocabulary trên entire data thì có khả năng model sẽ ko gặp các unknown words vì mọi word đều đã có trong vocabulary, điều này có ảnh hưởng tới chất lượng của model khi đưa lên product hay ko? vì trên thực tế có nhiều OOV hơn ạ",,,,,
Mình đang có vấn đề là mình model bài toán NLP trên python nhưng muốn đưa lên product thì phải chuyển code sang tương ứng với Java. Các file model final đuôi .pickle và .sav. Mình muốn hỏi làm thế nào để triển khai model lên product nền tảng Java tốt nhất mà không phải đi code Java lại từ đầu.,Mình đang có vấn đề là mình model bài toán NLP trên python nhưng muốn đưa lên product thì phải chuyển code sang tương ứng với Java. Các file model final đuôi .pickle và .sav. Mình muốn hỏi làm thế nào để triển khai model lên product nền tảng Java tốt nhất mà không phải đi code Java lại từ đầu.,,,,,
"Hi guys
Mọi người phân tích ưu nhược điểm của phương pháp top- down( base về lập trình, framework) so với phương pháp bottom-up ( cần chuẩn bị base về Toán ) khi học Ai Ko ạ?","Hi guys Mọi người phân tích ưu nhược điểm của phương pháp top- down( base về lập trình, framework) so với phương pháp bottom-up ( cần chuẩn bị base về Toán ) khi học Ai Ko ạ?",,,,,
"#pytorch #melanoma
hi anh chị,
Update: hình như em hiểu vấn đề r ạ :3
Em đang trong quá trình làm luận văn tốt nghiệp về detection + deploy lên website (django). Và tìm được video của anh này. (youtube.com/watch?v=WaCFd-vL4HA&t=288s )
Sau khi xem xong video thì em có vọc vào dataset của kangle, định tải về thì tận 108GB.
Vậy mình nhất thiết phải tải không ạ. hay còn cách nào để nhẹ hơn và tiện hơn cho quả trình sau này deploy lên website không ạ.
Mong nhận đc góp ý của anh chị.
Cảm ơn mọi người.","hi anh chị, Update: hình như em hiểu vấn đề r ạ :3 Em đang trong quá trình làm luận văn tốt nghiệp về detection + deploy lên website (django). Và tìm được video của anh này. (youtube.com/watch?v=WaCFd-vL4HA&t=288s ) Sau khi xem xong video thì em có vọc vào dataset của kangle, định tải về thì tận 108GB. Vậy mình nhất thiết phải tải không ạ. hay còn cách nào để nhẹ hơn và tiện hơn cho quả trình sau này deploy lên website không ạ. Mong nhận đc góp ý của anh chị. Cảm ơn mọi người.",#pytorch	#melanoma,,,,
"ML Showcase 2020 - Cùng học kinh nghiệm của những ứng dụng AI thực tế.
https://www.facebook.com/vietaipublic/videos/882651942567916",ML Showcase 2020 - Cùng học kinh nghiệm của những ứng dụng AI thực tế. https://www.facebook.com/vietaipublic/videos/882651942567916,,,,,
"Kính chào các bác, đợt này em làm nhiều API cho Machine Learning nên có tìm hiểu món FastAPI. Thấy nó hay và nhiều ưu điểm quá nên mạnh dạn làm clip giới thiệu mong giúp được các bạn newbie nghiên cứu thêm.
Mong ad duyệt bài ah!","Kính chào các bác, đợt này em làm nhiều API cho Machine Learning nên có tìm hiểu món FastAPI. Thấy nó hay và nhiều ưu điểm quá nên mạnh dạn làm clip giới thiệu mong giúp được các bạn newbie nghiên cứu thêm. Mong ad duyệt bài ah!",,,,,
"Xin chào mọi người!
Hiện tại em đang làm một bài toán dự đoán, dữ liệu dạng time-series, chúng em muốn sử dụng nhiệt độ, thời tiết theo giờ để làm một đặc trưng. Tuy nhiên chúng em lại đang gặp phải việc không có dữ liệu về nó. Trong đây có ai đã có dữ liệu về nhiệt độ và thời tiết tại Hà Nội trong khoảng 12-15 năm gần đây không ạ. Nếu có thể cho chúng em xin với ạ (chúng em là sinh viên đang tìm hiểu và tham gia NCKH).
Chúng em cũng tìm thấy trang meteoblue.com tuy nhiên họ chỉ cho download free 2 tuần dữ liệu mua toàn bộ 30 năm mất 100 EUR. Hoặc nếu có trang nào cho phép crawl dữ liệu mọi người chỉ em với ạ.
Cám ơn mọi người đã rành thời gian cho bài viết của em <3","Xin chào mọi người! Hiện tại em đang làm một bài toán dự đoán, dữ liệu dạng time-series, chúng em muốn sử dụng nhiệt độ, thời tiết theo giờ để làm một đặc trưng. Tuy nhiên chúng em lại đang gặp phải việc không có dữ liệu về nó. Trong đây có ai đã có dữ liệu về nhiệt độ và thời tiết tại Hà Nội trong khoảng 12-15 năm gần đây không ạ. Nếu có thể cho chúng em xin với ạ (chúng em là sinh viên đang tìm hiểu và tham gia NCKH). Chúng em cũng tìm thấy trang meteoblue.com tuy nhiên họ chỉ cho download free 2 tuần dữ liệu mua toàn bộ 30 năm mất 100 EUR. Hoặc nếu có trang nào cho phép crawl dữ liệu mọi người chỉ em với ạ. Cám ơn mọi người đã rành thời gian cho bài viết của em <3",,,,,
Chào mọi người! Em có bộ dữ liệu gồm 5 nhãn được chia thế này thì có hợp lý chưa ạ? Và dữ liệu như thế này có bị imbalanced không? Em cảm ơn mọi người ạ.,Chào mọi người! Em có bộ dữ liệu gồm 5 nhãn được chia thế này thì có hợp lý chưa ạ? Và dữ liệu như thế này có bị imbalanced không? Em cảm ơn mọi người ạ.,,,,,
"Mọi người cho em hỏi chút ạ:
Hiện tại e có 1 ít data của 2 con cảm biến; mỗi con trả về 3 giá trị các trục X,Y,Z tại thời điểm lấy mẫu; trong 1s e lấy mẫu 1024 lần => trong file csv sẽ mỗi hàng sẽ chứa:
(6 cột x,y,z của 2 cảm biến + 1 cột timestamp) * 1024 lần lấy mẫu + cột cuối cùng là result 1 hoặc 0
*e tạo 1 file minh hoạ như hình dưới để mn dễ hình dung
với cấu trúc và tính chất dataset như vậy em nên dùng thuật toán machine learning hoặc model deep learning nào ạ?
e tham khảo 1 số nơi thì có 1 số hướng có vẻ liên quan như decision tree, lstm, linear regression, ... nhưng ko chắc nên dùng cái nào","Mọi người cho em hỏi chút ạ: Hiện tại e có 1 ít data của 2 con cảm biến; mỗi con trả về 3 giá trị các trục X,Y,Z tại thời điểm lấy mẫu; trong 1s e lấy mẫu 1024 lần => trong file csv sẽ mỗi hàng sẽ chứa: (6 cột x,y,z của 2 cảm biến + 1 cột timestamp) * 1024 lần lấy mẫu + cột cuối cùng là result 1 hoặc 0 *e tạo 1 file minh hoạ như hình dưới để mn dễ hình dung với cấu trúc và tính chất dataset như vậy em nên dùng thuật toán machine learning hoặc model deep learning nào ạ? e tham khảo 1 số nơi thì có 1 số hướng có vẻ liên quan như decision tree, lstm, linear regression, ... nhưng ko chắc nên dùng cái nào",,,,,
Chao cac ban. Cho minh hoi trong group co ai biet ve weka. Cho minh hoi ve Association rule tren weka voi. Thanks so much,Chao cac ban. Cho minh hoi trong group co ai biet ve weka. Cho minh hoi ve Association rule tren weka voi. Thanks so much,,,,,
"Em đang muốn tìm hiểu về Python, data wrangling, data visualization, data modeling với machine learning, mong mọi người chia sẻ cho em một số thông tin về một số khóa học cũng như tài liệu, sách ạ, em nên bắt đầu với lộ trình học như nào nữa ạ!
Em cảm ơn mọi người!!!!","Em đang muốn tìm hiểu về Python, data wrangling, data visualization, data modeling với machine learning, mong mọi người chia sẻ cho em một số thông tin về một số khóa học cũng như tài liệu, sách ạ, em nên bắt đầu với lộ trình học như nào nữa ạ! Em cảm ơn mọi người!!!!",,,,,
"Hi mọi người
Em đang làm dùng camera IP đếm số lượng người trong phòng và gán đúng ID và tên người đó. Nhưng có vấn đề xảy ra là trong phòng có toilet, khi người A đi vào toilet camera sẽ không tracking được, khi người A đi ra nó sẽ gán sai ID và tên của người A.
Không biết có cách nào khắc phục để nó biết được người A mới vào toilet và đi ra để nó gán đúng không ạ. Mong anh Thắng và các cao nhân chỉ giáo :)))
Em cám ơn.","Hi mọi người Em đang làm dùng camera IP đếm số lượng người trong phòng và gán đúng ID và tên người đó. Nhưng có vấn đề xảy ra là trong phòng có toilet, khi người A đi vào toilet camera sẽ không tracking được, khi người A đi ra nó sẽ gán sai ID và tên của người A. Không biết có cách nào khắc phục để nó biết được người A mới vào toilet và đi ra để nó gán đúng không ạ. Mong anh Thắng và các cao nhân chỉ giáo :))) Em cám ơn.",,,,,
"Chào các anh chị em đang học Machine Learning cơ bản, hiện e tìm hiều về kết hợp giữa mô hình RFM và K-means để giải quyết bài toán phân khúc khách hàng. 
Nhờ ơn các anh chị đi trước để lại rất nhiều tài liệu phân tích thì em có thực nghiệm việc phân khúc theo 2 hướng:
Hướng 1 là phân cụm từng thuộc tính R,F,M rồi ra rank chung(để dễ áp dụng trong trường hợp trọng số của các thuộc tính khác nhau).
Hướng 2 là phân cụm dựa trên dữ liệu 3 chiều xyz (tương ứng với 3 giá trị của RFM) để gom các đối tượng lại nhóm có trọng tâm gần nó nhất.
Em áp dụng độ lỗi elbow để xác định k theo phương pháp khủy tay, từ đó tìm ra số k tối ưu.
Hiện em muốn mở rộng việc tìm hiểu về vấn đề này nhưng không tìm được 1 hướng đi chuyên sâu hơn, các anh chị đã từng tìm hiểu về mô hình này có thể gợi ý giúp em với ạ. Em chân thành cảm ơn.","Chào các anh chị em đang học Machine Learning cơ bản, hiện e tìm hiều về kết hợp giữa mô hình RFM và K-means để giải quyết bài toán phân khúc khách hàng. Nhờ ơn các anh chị đi trước để lại rất nhiều tài liệu phân tích thì em có thực nghiệm việc phân khúc theo 2 hướng: Hướng 1 là phân cụm từng thuộc tính R,F,M rồi ra rank chung(để dễ áp dụng trong trường hợp trọng số của các thuộc tính khác nhau). Hướng 2 là phân cụm dựa trên dữ liệu 3 chiều xyz (tương ứng với 3 giá trị của RFM) để gom các đối tượng lại nhóm có trọng tâm gần nó nhất. Em áp dụng độ lỗi elbow để xác định k theo phương pháp khủy tay, từ đó tìm ra số k tối ưu. Hiện em muốn mở rộng việc tìm hiểu về vấn đề này nhưng không tìm được 1 hướng đi chuyên sâu hơn, các anh chị đã từng tìm hiểu về mô hình này có thể gợi ý giúp em với ạ. Em chân thành cảm ơn.",,,,,
"Dùng gabor filter trong nhận dạng khuôn mặt ai làm qua rồi cho e xin tài liệu hoặc code tham khảo dc không ạ, e dùng python, e cảm ơn ạ.","Dùng gabor filter trong nhận dạng khuôn mặt ai làm qua rồi cho e xin tài liệu hoặc code tham khảo dc không ạ, e dùng python, e cảm ơn ạ.",,,,,
"Chào mọi người,
Em đang làm bài về phân loại mã độc trên Android thông qua các file APK. Hiện tại em đang muốn lấy kết quả về tỷ lệ phân loại chính xác từ các chương trình có sẵn chạy trên mẫu của em để so sánh với phương pháp trong chương trình của mình nhưng không tìm được source nào public nên không biết có ai có source hay có thể chạy giúp em ra kết quả không ạ .","Chào mọi người, Em đang làm bài về phân loại mã độc trên Android thông qua các file APK. Hiện tại em đang muốn lấy kết quả về tỷ lệ phân loại chính xác từ các chương trình có sẵn chạy trên mẫu của em để so sánh với phương pháp trong chương trình của mình nhưng không tìm được source nào public nên không biết có ai có source hay có thể chạy giúp em ra kết quả không ạ .",,,,,
,nan,,,,,
Trong group của mình có ai có algorithm dự đoán giá cổ phiếu không ạ? Em đang làm một project về tài chính và rất muốn dùng ML/AI agorithm để đối chiếu kết quả. Em cảm ơn ạ,Trong group của mình có ai có algorithm dự đoán giá cổ phiếu không ạ? Em đang làm một project về tài chính và rất muốn dùng ML/AI agorithm để đối chiếu kết quả. Em cảm ơn ạ,,,,,
Có ai chỉ dùm em làm thuật toán K-Means train dữ liệu text để đưa ra quyết định không ạ? Em cảm ơn!,Có ai chỉ dùm em làm thuật toán K-Means train dữ liệu text để đưa ra quyết định không ạ? Em cảm ơn!,,,,,
"Chào mọị người. Mình có một câu hỏi nhanh liên quan đến thuật toán *Bag of Embedding* (BoE). (Paper: https://www.ijcai.org/Proceedings/16/Papers/401.pdf) Mình bắt gặp nó trong tutorial Text Classification của PyTorch (https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html). Trong tutorial đó, tác giả đã sử dụng một lớp *Bag of Embedding* và một lớp *linear* để thực hiện việc classify text. [Q]: Câu hỏi của mình đó là: 1. Cách BoE thực sự hoạt động? 2. Nó thực sự khác biệt gì so với bản word embedding truyền thống và tại sao / khi nào chúng ta cần nó? Hi vọng mình có thể nhận được những comment mang tính đóng góp tới từ các bạn.","Chào mọị người. Mình có một câu hỏi nhanh liên quan đến thuật toán *Bag of Embedding* (BoE). (Paper: https://www.ijcai.org/Proceedings/16/Papers/401.pdf) Mình bắt gặp nó trong tutorial Text Classification của PyTorch (https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html). Trong tutorial đó, tác giả đã sử dụng một lớp *Bag of Embedding* và một lớp *linear* để thực hiện việc classify text. [Q]: Câu hỏi của mình đó là: 1. Cách BoE thực sự hoạt động? 2. Nó thực sự khác biệt gì so với bản word embedding truyền thống và tại sao / khi nào chúng ta cần nó? Hi vọng mình có thể nhận được những comment mang tính đóng góp tới từ các bạn.",,,,,
"Xin phép Ad cho mình đăng bài ạ.
Mình đang cần tìm một team làm về dự án NLP - tiếng việt. Dự án đã có dữ liệu, yêu cầu rõ ràng có thể start ngay.
Anh chị em đi qua ai có thông tin cho mình xin liên lạc nhé.
cảm ơn mọi người ạ.","Xin phép Ad cho mình đăng bài ạ. Mình đang cần tìm một team làm về dự án NLP - tiếng việt. Dự án đã có dữ liệu, yêu cầu rõ ràng có thể start ngay. Anh chị em đi qua ai có thông tin cho mình xin liên lạc nhé. cảm ơn mọi người ạ.",,,,,
Có bạn nào từng làm qua dạng bài về 2-class Naive Bayes algorithm này chưa ạ?,Có bạn nào từng làm qua dạng bài về 2-class Naive Bayes algorithm này chưa ạ?,,,,,
Phương pháp training có thể giúp giải thích concept của dự đoán.,Phương pháp training có thể giúp giải thích concept của dự đoán.,,,,,
"[Imbalanced dataset]
Chào mọi người, em đang deal với bài toán classifications. Nhưng dataset của em đang có dấu hiệu imbalanced. Có 2 nhãn là 0 và 1, nhãn 0 chiếm khoảng 17% (dataset có khoảng 90k observations). Vậy trong trường hợp này mình có thể xem là imbalanced chưa ạ.
Xin cám ơn mọi người!","[Imbalanced dataset] Chào mọi người, em đang deal với bài toán classifications. Nhưng dataset của em đang có dấu hiệu imbalanced. Có 2 nhãn là 0 và 1, nhãn 0 chiếm khoảng 17% (dataset có khoảng 90k observations). Vậy trong trường hợp này mình có thể xem là imbalanced chưa ạ. Xin cám ơn mọi người!",,,,,
"📷📷[Các khóa học Academy - StandFord]📷📷
Ở lần trước mình đã giới thiệu các khóa học hand-on trong AI, Data Scientist.
Lần này mình sẽ giới thiệu tiếp các khóa học Acedemy trong lĩnh vực này. Đây là những khóa học nằm trong chương trình đào tạo của StandFord, được giảng dạy bởi những tiến sĩ, giáo sư hàng đầu.
📷 Với các bạn yêu thích thống kê có thể học các khóa:
1. CS109 (Probability for Computer Scientists)
2. CS229T (Statistical Learning Theory)
3. EE337 (Statistics and Information Theory)
4. CS246: Mining Massive Data Sets
📷 Về Machine Learning:
1. CS229 Machine Learning
2. CS229A Applied Machine Learning
3. CS224W Machine Learning with Graphs
📷 Deep Learning:
1. CS230 (Deep Learning)
2. CS231N Convolutional Neural Networks for Visual Recognition
3. CS224 (NLP), CS231 (Computer Vision)
4. CS234 (Reinforcement Learning)
5. CS330 (Deep Multi-Task and Meta Learning)
📷 Các khóa dành riêng cho Biology:
1. CS279 Computational Biology: Structure and Organization of Biomolecules and Cells
2. CS371 Computational Biology in Four Dimensions
📷 Robotic:
1. CS223A Introduction to Robotics
2. CS225A Experimental Robotics
3. CS326 Topics in Advanced Robotic Manipulation
4. CS327A Advanced Robotic Manipulation
5. CS333 Safe and Interactive Robotics
6. CS237A Principles of Robotic
https://ai.stanford.edu/courses/","[Các khóa học Academy - StandFord] Ở lần trước mình đã giới thiệu các khóa học hand-on trong AI, Data Scientist. Lần này mình sẽ giới thiệu tiếp các khóa học Acedemy trong lĩnh vực này. Đây là những khóa học nằm trong chương trình đào tạo của StandFord, được giảng dạy bởi những tiến sĩ, giáo sư hàng đầu. Với các bạn yêu thích thống kê có thể học các khóa: 1. CS109 (Probability for Computer Scientists) 2. CS229T (Statistical Learning Theory) 3. EE337 (Statistics and Information Theory) 4. CS246: Mining Massive Data Sets Về Machine Learning: 1. CS229 Machine Learning 2. CS229A Applied Machine Learning 3. CS224W Machine Learning with Graphs Deep Learning: 1. CS230 (Deep Learning) 2. CS231N Convolutional Neural Networks for Visual Recognition 3. CS224 (NLP), CS231 (Computer Vision) 4. CS234 (Reinforcement Learning) 5. CS330 (Deep Multi-Task and Meta Learning) Các khóa dành riêng cho Biology: 1. CS279 Computational Biology: Structure and Organization of Biomolecules and Cells 2. CS371 Computational Biology in Four Dimensions Robotic: 1. CS223A Introduction to Robotics 2. CS225A Experimental Robotics 3. CS326 Topics in Advanced Robotic Manipulation 4. CS327A Advanced Robotic Manipulation 5. CS333 Safe and Interactive Robotics 6. CS237A Principles of Robotic https://ai.stanford.edu/courses/",,,,,
"Mnist khi apply vào thực tế.
#Wowww",Mnist khi apply vào thực tế.,#Wowww,,,,
"Hôm trước em có hỏi về cách tách,  xử lý video và em đã làm được rồi ạ. Em cảm ơn mọi người ^^

Hiện tại em đang tìm cách thực hiện xử lý GAN qua webcam và real-time. Thời gian xử lý GAN và nhận diện object detection offline của em không quá lâu nhưng cũng không nhanh ngay lập tức được (tầm 1-2s). Không biết anh chị có lời khuyên hay nên search keyword gì để có thể tối ưu real-time processing time không ạ? (Hiện tại, trong đầu em đang nghĩ đến 1 số solution như convert sang tf lite, use google cloud)
Hiện tại pre-trained model chỉ chạy được trên một số image size nhất định. Không biết mọi người có thể gợi ý cho em cách chạy model trên mọi image size mà không phải train đi train lại nhiều lần trên từng image size khác nhau không ạ? Em xin cảm ơn mọi người nhiều. 
Thân,","Hôm trước em có hỏi về cách tách, xử lý video và em đã làm được rồi ạ. Em cảm ơn mọi người ^^ Hiện tại em đang tìm cách thực hiện xử lý GAN qua webcam và real-time. Thời gian xử lý GAN và nhận diện object detection offline của em không quá lâu nhưng cũng không nhanh ngay lập tức được (tầm 1-2s). Không biết anh chị có lời khuyên hay nên search keyword gì để có thể tối ưu real-time processing time không ạ? (Hiện tại, trong đầu em đang nghĩ đến 1 số solution như convert sang tf lite, use google cloud) Hiện tại pre-trained model chỉ chạy được trên một số image size nhất định. Không biết mọi người có thể gợi ý cho em cách chạy model trên mọi image size mà không phải train đi train lại nhiều lần trên từng image size khác nhau không ạ? Em xin cảm ơn mọi người nhiều. Thân,",,,,,
"Mọi người ơi cho em hỏi ạ, có khóa học online/ youtube video playlist nào dạy theo cuốn ""Pattern recognition and Machine Learning"" hoặc cuốn ""The elements of statistical learning"" không ạ?
Em cảm ơn mọi người ạ!","Mọi người ơi cho em hỏi ạ, có khóa học online/ youtube video playlist nào dạy theo cuốn ""Pattern recognition and Machine Learning"" hoặc cuốn ""The elements of statistical learning"" không ạ? Em cảm ơn mọi người ạ!",,,,,
"Chào mọi người,
Em đang có bài toán xử lý và trích xuất GAN với dữ liệu dạng video. Em hiểu được cách làm với hình ảnh nhưng không biết cách xử lý từng frame và ghép lại thành 1 video. Mảng này hơi mới với em nên rất cảm ơn mọi người ạ.
Thân,","Chào mọi người, Em đang có bài toán xử lý và trích xuất GAN với dữ liệu dạng video. Em hiểu được cách làm với hình ảnh nhưng không biết cách xử lý từng frame và ghép lại thành 1 video. Mảng này hơi mới với em nên rất cảm ơn mọi người ạ. Thân,",,,,,
"#Tuvan #MasterProgram
Chào mọi người. Mình đang làm kỹ sư về lĩnh vực Computer Vision. Trước mình học Tự động hóa. Bây giờ mình đang có kế hoạch học chương trình Thạc sỹ. Mình đang phân vân giữa chương trình đào thạc sỹ Khoa học dữ liệu của Đại học Bách Khoa Hà Nội và chương trình đào tạo thạc sỹ Cơ sở toán cho tin học của Đại học Khoa học tự nhiên. Mình đang có phần ưu tiên hơn cho Đại học Khoa học tự nhiên vì trước mình học không đúng ngành, mình tìm hiểu Đại học Khoa học tự nhiên các môn cơ bản về toán được đào tạo khá kỹ (đây mà điều mình đang rất cần). Dự định của mình có thiên hướng đi theo học thuật. Hướng nghiên cứu dự định của mình về Reinforcement Learning, Representation Learning, Meta Learning, Latent Reasoning, Generative Models,... Tuy nhiên mình vẫn chỉ tìm hiểu mà chưa trải nghiệm thực sự hai chương trình trên. Mình sẽ vừa làm vừa học. Mọi người đã ai học một trong hai chương trình ThS trên chưa có thể cho mình cảm nhận được không (như về các thầy cô giảng dậy, chương trình học, trải nghiệm quá trình học, lĩnh vực nghiên cứu, lịch học,...)? Hoặc bất kỳ ai có lời khuyên cho mình, mình rất mong nhận được sự tư vấn của mọi người. Mình cảm ơn mọi người rất nhiều.","Chào mọi người. Mình đang làm kỹ sư về lĩnh vực Computer Vision. Trước mình học Tự động hóa. Bây giờ mình đang có kế hoạch học chương trình Thạc sỹ. Mình đang phân vân giữa chương trình đào thạc sỹ Khoa học dữ liệu của Đại học Bách Khoa Hà Nội và chương trình đào tạo thạc sỹ Cơ sở toán cho tin học của Đại học Khoa học tự nhiên. Mình đang có phần ưu tiên hơn cho Đại học Khoa học tự nhiên vì trước mình học không đúng ngành, mình tìm hiểu Đại học Khoa học tự nhiên các môn cơ bản về toán được đào tạo khá kỹ (đây mà điều mình đang rất cần). Dự định của mình có thiên hướng đi theo học thuật. Hướng nghiên cứu dự định của mình về Reinforcement Learning, Representation Learning, Meta Learning, Latent Reasoning, Generative Models,... Tuy nhiên mình vẫn chỉ tìm hiểu mà chưa trải nghiệm thực sự hai chương trình trên. Mình sẽ vừa làm vừa học. Mọi người đã ai học một trong hai chương trình ThS trên chưa có thể cho mình cảm nhận được không (như về các thầy cô giảng dậy, chương trình học, trải nghiệm quá trình học, lĩnh vực nghiên cứu, lịch học,...)? Hoặc bất kỳ ai có lời khuyên cho mình, mình rất mong nhận được sự tư vấn của mọi người. Mình cảm ơn mọi người rất nhiều.",#Tuvan	#MasterProgram,,,,
"[Practical stats books]
Thực ra các thuật toán Machine Learning đều dựa trên nền của thống kê. Tuy nhiên bạn đã quá lâu chưa động lại đến toán, đến xác suất, thống kê. Giờ đọc các sách lý thuyết về thống kê cũng chán.
Nay mình giới thiệu 2 cuốn sách dạy thống kê theo kiểu thực hành, sẽ có code đi kèm và giải thích ý nghĩa của các khái niệm trong thống kê.","[Practical stats books] Thực ra các thuật toán Machine Learning đều dựa trên nền của thống kê. Tuy nhiên bạn đã quá lâu chưa động lại đến toán, đến xác suất, thống kê. Giờ đọc các sách lý thuyết về thống kê cũng chán. Nay mình giới thiệu 2 cuốn sách dạy thống kê theo kiểu thực hành, sẽ có code đi kèm và giải thích ý nghĩa của các khái niệm trong thống kê.",,,"#sharing, #math, #machine_learning",,
"Chào mọi người,
Em đang làm bài toán ""nhận dạng luống cây"" (crop row detection). Hiện tại, em đang dùng OpenCV theo các bước như sau: color filter -> edge detection -> hough line transform. Tuy nhiên cách này có vẻ chưa được tốt (như ảnh dưới). Nguyên nhân có lẽ do ảnh chụp gần và các khóm cây sát nhau.
Mong mọi người cho em xin một vài gợi ý để giải quyết.
Em cảm ơn!
P/S: yêu cầu chạy trên drone nên chắc khó có thể dùng DL.","Chào mọi người, Em đang làm bài toán ""nhận dạng luống cây"" (crop row detection). Hiện tại, em đang dùng OpenCV theo các bước như sau: color filter -> edge detection -> hough line transform. Tuy nhiên cách này có vẻ chưa được tốt (như ảnh dưới). Nguyên nhân có lẽ do ảnh chụp gần và các khóm cây sát nhau. Mong mọi người cho em xin một vài gợi ý để giải quyết. Em cảm ơn! P/S: yêu cầu chạy trên drone nên chắc khó có thể dùng DL.",,,,,
"[TRỰC TIẾP]
Bài giảng của GS. Vũ Hà Văn, Giám đốc Khoa học Vingroup Big Data Institute về Tăng tốc giải quyết ma trận khổng lồ: Fast computation - The magic of Sampling.
#math #DataScience #colloquium #sharing","[TRỰC TIẾP] Bài giảng của GS. Vũ Hà Văn, Giám đốc Khoa học Vingroup Big Data Institute về Tăng tốc giải quyết ma trận khổng lồ: Fast computation - The magic of Sampling.",#math	#DataScience	#colloquium	#sharing,,,,
"Phần Phụ lục về toán và các công cụ cho Học sâu cũng đã được hoàn thành.
https://d2l.aivivn.com/chapter_appendix-mathematics-for-deep-learning/index_vn.html
Như vậy cuốn sách đã được dịch xong sau hơn 10 tháng. Nhóm dịch sẽ sửa chữa các lỗi nhỏ trước khi tạo bản pdf.
Cảm ơn tất cả các thành viên trong nhóm dịch đã tích cực tham gia trong suốt nhiều tháng vừa rồi. Chúng tôi cũng cảm ơn các bạn độc giả đã ủng hộ. Hy vọng Forum có thể tiếp tục tổ chức thành công nhiều dự án cộng đồng khác nữa.",Phần Phụ lục về toán và các công cụ cho Học sâu cũng đã được hoàn thành. https://d2l.aivivn.com/chapter_appendix-mathematics-for-deep-learning/index_vn.html Như vậy cuốn sách đã được dịch xong sau hơn 10 tháng. Nhóm dịch sẽ sửa chữa các lỗi nhỏ trước khi tạo bản pdf. Cảm ơn tất cả các thành viên trong nhóm dịch đã tích cực tham gia trong suốt nhiều tháng vừa rồi. Chúng tôi cũng cảm ơn các bạn độc giả đã ủng hộ. Hy vọng Forum có thể tiếp tục tổ chức thành công nhiều dự án cộng đồng khác nữa.,,,,,
"Hi mọi người, có anh chị nào setup được gpu 3080 để chạy tensorflow object detection 1.x không ạ. Anh chị nào làm được thì chỉ em với ạ","Hi mọi người, có anh chị nào setup được gpu 3080 để chạy tensorflow object detection 1.x không ạ. Anh chị nào làm được thì chỉ em với ạ",,,,,
"Mình tính L2 Regularization Cost, bằng python, dưới đây là code mình viết:
"" L2_regularization_cost =(1/m)*(lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) ) ""
mình cảm thấy như vậy hơi dài, và chưa tận dụng hết python, bạn nào biết chỉ mình với, mình cám ơn nhiều ạ","Mình tính L2 Regularization Cost, bằng python, dưới đây là code mình viết: "" L2_regularization_cost =(1/m)*(lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) ) "" mình cảm thấy như vậy hơi dài, và chưa tận dụng hết python, bạn nào biết chỉ mình với, mình cám ơn nhiều ạ",,,,,
"Chào mọi người,
Vợ em rất sợ rắn, em đang làm một hệ thống cảnh báo phát hiện nếu như có con rắn nào bò vào nhà, sử dụng camera an ninh. các camera đặt trên cao (thường là từ 2,3 mét hoặc hơn) chất lượng hình ảnh ở mức chấp nhận được. hiện giờ em đang gặp khó khăn trong việc tìm dữ liệu để train. em đang cân nhắc việc đặt camera thấp xuống nhưng nếu thế thì ko dùng để quan sát an ninh trong nhà nếu như có kẻ trộm xâm nhập. em cũng định dùng thermal camera nhưng họ nói nó ko hoạt động tốt với các loài động vật máu lạnh và đặc biệt là nếu bọn chúng hơi nhỏ và chi phí cũng tốn kém.
Mọi người có thể cho em ít lời khuyên được không ạ?
em xin cảm ơn.","Chào mọi người, Vợ em rất sợ rắn, em đang làm một hệ thống cảnh báo phát hiện nếu như có con rắn nào bò vào nhà, sử dụng camera an ninh. các camera đặt trên cao (thường là từ 2,3 mét hoặc hơn) chất lượng hình ảnh ở mức chấp nhận được. hiện giờ em đang gặp khó khăn trong việc tìm dữ liệu để train. em đang cân nhắc việc đặt camera thấp xuống nhưng nếu thế thì ko dùng để quan sát an ninh trong nhà nếu như có kẻ trộm xâm nhập. em cũng định dùng thermal camera nhưng họ nói nó ko hoạt động tốt với các loài động vật máu lạnh và đặc biệt là nếu bọn chúng hơi nhỏ và chi phí cũng tốn kém. Mọi người có thể cho em ít lời khuyên được không ạ? em xin cảm ơn.",,,,,
"Nhờ các bạn trong group chia sẻ với các thế hệ tiếp theo. Nhóm STEAM for Vietnam này có rất nhiều thầy giỏi.
https://www.steamforvietnam.org/courses?fbclid=IwAR1YNwyDyEIu3-Vcw7ejV_-2BL24uPiFgSMQFTXXHIhopv_grkK1ZFP5V04
 — với Hung Tran.",Nhờ các bạn trong group chia sẻ với các thế hệ tiếp theo. Nhóm STEAM for Vietnam này có rất nhiều thầy giỏi. https://www.steamforvietnam.org/courses?fbclid=IwAR1YNwyDyEIu3-Vcw7ejV_-2BL24uPiFgSMQFTXXHIhopv_grkK1ZFP5V04 — với Hung Tran.,,,,,
"#chiase
Video đầu tiên trong series video về các bài toán OCR
Text detection với CRAFT",Video đầu tiên trong series video về các bài toán OCR Text detection với CRAFT,#chiase,,,,
"em mới nhập môn deep learning được chút, mọi người cho em hỏi là e training sao loss và acc lại cùng giảm là do lý do gì ạ, em cảm ơn ạ!","em mới nhập môn deep learning được chút, mọi người cho em hỏi là e training sao loss và acc lại cùng giảm là do lý do gì ạ, em cảm ơn ạ!",,,,,
"Kính chào các bác, lâu nay em vẫn thắc mắc khi ta đưa một vào mạng một hình ảnh (một cô gái chẳng hạn) thì ông CNN ông ấy nhìn thấy cô gái đó như nào? Có như ta nhìn không? Và tập trung vào phần nào của cô gái? Có giống chúng ta khi nhìn vào các cô gái thì hay tập trung vào……………không =))
Nên hôm nay em mạnh dạn tìm hiểu và chia sẻ cách visualize feature map và heatmap. Mong giúp được các bạn mới học, mong admin duyệt bài!","Kính chào các bác, lâu nay em vẫn thắc mắc khi ta đưa một vào mạng một hình ảnh (một cô gái chẳng hạn) thì ông CNN ông ấy nhìn thấy cô gái đó như nào? Có như ta nhìn không? Và tập trung vào phần nào của cô gái? Có giống chúng ta khi nhìn vào các cô gái thì hay tập trung vào……………không =)) Nên hôm nay em mạnh dạn tìm hiểu và chia sẻ cách visualize feature map và heatmap. Mong giúp được các bạn mới học, mong admin duyệt bài!",,,,,
"Hi all.
Mình muốn nhờ chút ạ.
Mình đang có file audio muốn chuyển sang word. Các bạn nào có source code thì có thể share cho mình được không ạ?
Hoặc là ibox riêng cho mình xin giá ạ?
Vì mình đang có người bác, muốn chuyển file thu âm thành word.
Mình cám ơn.","Hi all. Mình muốn nhờ chút ạ. Mình đang có file audio muốn chuyển sang word. Các bạn nào có source code thì có thể share cho mình được không ạ? Hoặc là ibox riêng cho mình xin giá ạ? Vì mình đang có người bác, muốn chuyển file thu âm thành word. Mình cám ơn.",,,,,
"Chào mn, em đang làm 1 bài toán về nhận diện cảm xúc mà lúc predict test toàn ra sad.
Chi tiết bài toán và code em để trong file sau:","Chào mn, em đang làm 1 bài toán về nhận diện cảm xúc mà lúc predict test toàn ra sad. Chi tiết bài toán và code em để trong file sau:",,,,,
"Hello mọi người ạ. Em thấy cái roadmap này với mấy cái em xem ở mấy trang khác sau nó ngược ngược kiểu gì, em ko biết phải bắt đầu từ đâu nữa ạ. Mà có mấy cái tra gg em cũng ko biết ra sao như là ""Just-In-Time"",..H em mông lung quá ko biết sao. Mọi người giúp em với ạ","Hello mọi người ạ. Em thấy cái roadmap này với mấy cái em xem ở mấy trang khác sau nó ngược ngược kiểu gì, em ko biết phải bắt đầu từ đâu nữa ạ. Mà có mấy cái tra gg em cũng ko biết ra sao như là ""Just-In-Time"",..H em mông lung quá ko biết sao. Mọi người giúp em với ạ",,,,,
"Em chào mọi người ạ!!
Dạ cho em hỏi là ở trong này, có ai theo ngành Computer Graphics hay là làm về Shape Matching, deformable modeling không ạ?
Em mới tập tễnh bước chân vào ngành này để tìm hiểu, hiện tại em đang thực tập tại phòng lab với Giáo sư và có ý định học lên master, anh chị em đi trước có kinh nghiệm truyền cho em ít để đọc paper với ạ, từ tiếng anh đã khó, từ chuyên ngành càng khó hơn, ai nhận để tử thì cho em ứng tuyển với, hứa ngoan ngoãn nghe lời ^^","Em chào mọi người ạ!! Dạ cho em hỏi là ở trong này, có ai theo ngành Computer Graphics hay là làm về Shape Matching, deformable modeling không ạ? Em mới tập tễnh bước chân vào ngành này để tìm hiểu, hiện tại em đang thực tập tại phòng lab với Giáo sư và có ý định học lên master, anh chị em đi trước có kinh nghiệm truyền cho em ít để đọc paper với ạ, từ tiếng anh đã khó, từ chuyên ngành càng khó hơn, ai nhận để tử thì cho em ứng tuyển với, hứa ngoan ngoãn nghe lời ^^",,,,,
"Chương trình giới thiệu học bổng Sau đại học VEF 2.0 du học tại Hoa Kỳ giành cho các ngành Khoa học, Kỹ thuật và Công nghệ năm nay khởi động tại ĐH Kinh tế quốc dân với buổi nói chuyện được tổ chức tại:
D2 302, Đại học Kinh tế Quốc dân, số 207 Giải Phóng, Hà Nội
Đặc biệt bạn Nguyễn Hữu Hoàng, cựu sinh viên NEU K55 sẽ chia sẻ về kinh nghiệm cho những bạn muốn tiếp tục theo học trong các lĩnh vực DA, BA, DE, DS, ...
Trân trọng kính mời các bạn quan tâm đến tham dự và tìm hiểu về cơ hội du học sau đại học tại Hoa Kỳ!","Chương trình giới thiệu học bổng Sau đại học VEF 2.0 du học tại Hoa Kỳ giành cho các ngành Khoa học, Kỹ thuật và Công nghệ năm nay khởi động tại ĐH Kinh tế quốc dân với buổi nói chuyện được tổ chức tại: D2 302, Đại học Kinh tế Quốc dân, số 207 Giải Phóng, Hà Nội Đặc biệt bạn Nguyễn Hữu Hoàng, cựu sinh viên NEU K55 sẽ chia sẻ về kinh nghiệm cho những bạn muốn tiếp tục theo học trong các lĩnh vực DA, BA, DE, DS, ... Trân trọng kính mời các bạn quan tâm đến tham dự và tìm hiểu về cơ hội du học sau đại học tại Hoa Kỳ!",,,,,
"[Danh sách các Startup trong lãnh vực AI / Machine Learning ở Việt Nam]
[Update: 03/01/2020]
Mình update danh sách thêm các công ty được các bạn comment bên dưới:
voicon.ai
sharitek.com
eyeq.tech
olli-ai.com
asilla.vn
trustingsocial.com
abivin.com
cinnamon.is
nextsmarty.com
hiip.asia
orm.vn
---
Mình đang tìm hiểu về các doanh nghiệp trẻ chuyên phát triển ứng dụng AI, Machine Learning ở VN, không tính các công ty lớn lâu năm có mảng AI mà chỉ công ty chuyên về AI. Tìm mãi mà chỉ tìm được vài công ty.
Mình tự hỏi lẽ nào VN mình có ít Startup trong AI thế? Có bạn nào biết còn công ty nào khác xin comment bên dưới giới thiệu giúp mình tên công ty & website với. Có danh sách này mình nghĩ cũng tiện cho các bạn nào muốn tìm việc làm trong lãnh vực AI.","[Danh sách các Startup trong lãnh vực AI / Machine Learning ở Việt Nam] [Update: 03/01/2020] Mình update danh sách thêm các công ty được các bạn comment bên dưới: voicon.ai sharitek.com eyeq.tech olli-ai.com asilla.vn trustingsocial.com abivin.com cinnamon.is nextsmarty.com hiip.asia orm.vn --- Mình đang tìm hiểu về các doanh nghiệp trẻ chuyên phát triển ứng dụng AI, Machine Learning ở VN, không tính các công ty lớn lâu năm có mảng AI mà chỉ công ty chuyên về AI. Tìm mãi mà chỉ tìm được vài công ty. Mình tự hỏi lẽ nào VN mình có ít Startup trong AI thế? Có bạn nào biết còn công ty nào khác xin comment bên dưới giới thiệu giúp mình tên công ty & website với. Có danh sách này mình nghĩ cũng tiện cho các bạn nào muốn tìm việc làm trong lãnh vực AI.",,,,,
"Chào các bác ạ, hiện tại em muốn làm 1 là course Machine learning theo kiểu cực kì cơ bản(nhập môn) cho mấy bạn không chuyên thì syllabus dưới này có ổn không ạ.
Em tính làm kiểu nói sơ cơ bản về lý thuyết và còn lại là code step by step.
Edit: Hiện tại mình chỉ vừa mới lên plans thôi vẫn chưa có content bên trong. Khi mình cảm thấy mọi thứ ổn rồi mình sẽ public free cho mọi người có thể tiếp cận.","Chào các bác ạ, hiện tại em muốn làm 1 là course Machine learning theo kiểu cực kì cơ bản(nhập môn) cho mấy bạn không chuyên thì syllabus dưới này có ổn không ạ. Em tính làm kiểu nói sơ cơ bản về lý thuyết và còn lại là code step by step. Edit: Hiện tại mình chỉ vừa mới lên plans thôi vẫn chưa có content bên trong. Khi mình cảm thấy mọi thứ ổn rồi mình sẽ public free cho mọi người có thể tiếp cận.",,,,,
"[Xin gợi ý tài liệu]
Xin chào anh chị em trong group.
Minh đang thực hiện đề tài về Speech-Command-Interface và TinyML cho vi điều khiển. Mình vốn không phải dân chuyên về AI và Deep  Learning nên cần mọi người trợ giúp chút xíu. Ai đã nghiên cứu qua đề  tài này có thể gợi ý cho mình ít tài liệu để tham khảo được không ạ.
Hiện tại mình mới chỉ đọc qua documentation của tensorflow và bài viết mô tả  về dataset của họ. Ngoài ra mình cũng đang đọc thêm các bài viết của  tác giả Jonathan Hui trên Medium. Do dịch bệnh nên thư viện chỗ mình  đóng cửa, nguồn tài liệu hơi khó tiếp cận. Vì vậy nên mong muốn là tài  liệu dạng bài viết hoặc nghiên cứu có thể trích xuất và truy cập bằng  internet được thì tuyệt vời.
Mong mọi người giúp ạ!
(ảnh chống trôi)","[Xin gợi ý tài liệu] Xin chào anh chị em trong group. Minh đang thực hiện đề tài về Speech-Command-Interface và TinyML cho vi điều khiển. Mình vốn không phải dân chuyên về AI và Deep Learning nên cần mọi người trợ giúp chút xíu. Ai đã nghiên cứu qua đề tài này có thể gợi ý cho mình ít tài liệu để tham khảo được không ạ. Hiện tại mình mới chỉ đọc qua documentation của tensorflow và bài viết mô tả về dataset của họ. Ngoài ra mình cũng đang đọc thêm các bài viết của tác giả Jonathan Hui trên Medium. Do dịch bệnh nên thư viện chỗ mình đóng cửa, nguồn tài liệu hơi khó tiếp cận. Vì vậy nên mong muốn là tài liệu dạng bài viết hoặc nghiên cứu có thể trích xuất và truy cập bằng internet được thì tuyệt vời. Mong mọi người giúp ạ! (ảnh chống trôi)",,,,,
Chào mn. Mình đang muốn bắt đầu nghiên cứu về ML nhưng hiện tại biết bắt đầu từ đâu và lộ trình thế nào. Mong mn tư vấn giúp mình,Chào mn. Mình đang muốn bắt đầu nghiên cứu về ML nhưng hiện tại biết bắt đầu từ đâu và lộ trình thế nào. Mong mn tư vấn giúp mình,,,,,
"Finding partners to join Discord study group for Data Structures and Algorithms specialisation - UCSD (Python)
Hi everyone,
This is not a promoting/advertising post.
I just want to invite who are studying the Data Structures and Algorithms specialisation (University of California San Diego) in Python and wish to have a group to frequently discuss the lectures/assignments/Pycharm related issues, to join our Discord study group. I'm just a beginner, have 1 year experience with Python (from DataCamp, CS50 and my MSc in Business Analytics). I'm studying full-time and hopefully could finish the specialisation in 2 months, so I would try to make the group as active as possible.
>>>> DM me for the Discord link.
Keep studying and Best regards.","Finding partners to join Discord study group for Data Structures and Algorithms specialisation - UCSD (Python) Hi everyone, This is not a promoting/advertising post. I just want to invite who are studying the Data Structures and Algorithms specialisation (University of California San Diego) in Python and wish to have a group to frequently discuss the lectures/assignments/Pycharm related issues, to join our Discord study group. I'm just a beginner, have 1 year experience with Python (from DataCamp, CS50 and my MSc in Business Analytics). I'm studying full-time and hopefully could finish the specialisation in 2 months, so I would try to make the group as active as possible. >>>> DM me for the Discord link. Keep studying and Best regards.",,,,,
"Các a/c cho e hỏi với ạ!
Khi inference model pytorch version 1.2 trên cpu support cả avx2 và avx512 thì lib mkldnn để default là avx512. Giờ em muốn config để chạy bằng avx2 thì cần set lại enviroment param như nào ạ?!
Em xin cảm ơn!",Các a/c cho e hỏi với ạ! Khi inference model pytorch version 1.2 trên cpu support cả avx2 và avx512 thì lib mkldnn để default là avx512. Giờ em muốn config để chạy bằng avx2 thì cần set lại enviroment param như nào ạ?! Em xin cảm ơn!,,,,,
"1 cuộc thi về ảnh y tế mới trên Kaggle do team mình đứng ra host vừa được launch: https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection
Sẽ có 1 giải đặc biệt ($8k) giành riêng cho team Việt Nam xuất sắc nhất nên mình hy vọng mọi người sẽ tham gia nhiệt tình 😁",1 cuộc thi về ảnh y tế mới trên Kaggle do team mình đứng ra host vừa được launch: https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection Sẽ có 1 giải đặc biệt ($8k) giành riêng cho team Việt Nam xuất sắc nhất nên mình hy vọng mọi người sẽ tham gia nhiệt tình,,,,,
"Hiện TopDup đang rất cần một bạn có kinh nghiệm UI/UX design để lead phần này của dự án. Nếu hứng thú, bạn có thể điền form để join slack hoặc ping mình trực tiếp có kèm email.
Cảm ơn bạn.","Hiện TopDup đang rất cần một bạn có kinh nghiệm UI/UX design để lead phần này của dự án. Nếu hứng thú, bạn có thể điền form để join slack hoặc ping mình trực tiếp có kèm email. Cảm ơn bạn.",,,,,
"Em chào mn. Em đang là sv năm cuối, sắp ra trg, tính mua 1 c laptop/desktop để training, học tập, giải trí.
Em tính mua chiếc msi gt73 i7 7700hq, 1080. Liệu có ổn ko mn nhỉ? Hay tầm tiền đó có thể mua đc 1 em rtx 2060+i7 9750 hơn?
Build desktop em sợ ko tiện lắm nên cũng đắn đo. Vì desk tầm 30tr là có 2080 rồi.
Em cảm ơn mn ạ.","Em chào mn. Em đang là sv năm cuối, sắp ra trg, tính mua 1 c laptop/desktop để training, học tập, giải trí. Em tính mua chiếc msi gt73 i7 7700hq, 1080. Liệu có ổn ko mn nhỉ? Hay tầm tiền đó có thể mua đc 1 em rtx 2060+i7 9750 hơn? Build desktop em sợ ko tiện lắm nên cũng đắn đo. Vì desk tầm 30tr là có 2080 rồi. Em cảm ơn mn ạ.",,,,,
"Xin chào mọi người, hiện tại mình được giao task trích xuất text trong hình ảnh, do mình không chuyên compuer vison nên mình muốn hỏi hiện tại mô hình hoặc pretrain nào tốt nhất để làm vấn đề này, mong mọi người chỉ giáo.","Xin chào mọi người, hiện tại mình được giao task trích xuất text trong hình ảnh, do mình không chuyên compuer vison nên mình muốn hỏi hiện tại mô hình hoặc pretrain nào tốt nhất để làm vấn đề này, mong mọi người chỉ giáo.",,,,,
Thêm một bài tổng hợp về các công ty làm MLOps năm 2020 https://huyenchip.com/2020/12/30/mlops-v2.html?fbclid=IwAR0RgMkzShsYTdCM5-2pWzIAeaZzpIeaMqdwV51st4FghQPDBMff4UaSQd8,Thêm một bài tổng hợp về các công ty làm MLOps năm 2020 https://huyenchip.com/2020/12/30/mlops-v2.html?fbclid=IwAR0RgMkzShsYTdCM5-2pWzIAeaZzpIeaMqdwV51st4FghQPDBMff4UaSQd8,,,,,
"Mọi người cho mình hỏi mình cài opencv contrib python bằng lệnh: pip install opencv-contrib-python bị lỗi như dưới đây, mọi người từng gặp có thể chỉ mình cách fix với, mình cảm ơn ạ","Mọi người cho mình hỏi mình cài opencv contrib python bằng lệnh: pip install opencv-contrib-python bị lỗi như dưới đây, mọi người từng gặp có thể chỉ mình cách fix với, mình cảm ơn ạ",,,,,
"[NEW BOOK] - PROBABILISTIC MACHINE LEARNING: AN INTRODUCTION (2021)
Đây là sách mới của Murphy, được updated từ quyển “Machine Learning: A Probabilistic Perspective” (2012) là một trong những cuốn sách nền tảng về xác suất ứng dụng trong Machine Learning.
Các bạn có thể chưa biết “Machine Learning: A Probabilistic Perspective” (2012) nhận được giải thưởng danh giá DeGroot Prize (https://bayesian.org/project/degroot-prize/) năm 2013 cho những đóng góp xuất sắc trong lĩnh vực khoa học thống kê. Cuốn sách covers rất nhiều topics khác nhau của xác suất thống kê và tính liên hệ mật thiết trong máy học như Probability Optimization, Conditional Random Fields, L1 Regularization, Bayesian Decision Theory, Gaussian Models và Deep Learning.
Năm 2012 được nhận định là khởi đầu của “kỷ nguyên học sâu” (Deep Learning Revolution). Khoảng 8 năm trở lại đây chúng ta đã chứng kiến những ứng dụng vô cùng nổi bật của Machine Learning và Deep Learning trong Computer Vision, Natural Language Processing và Reinforcement Learning. Năm 2018, Murphy nhận thấy còn nhiều thiếu sót trong quyển sách đầu tay của mình nên đã tiến hành viết hoàn thiện nó.
Bản thảo đầu tiên khi hoàn thành (2020) dài khoảng 1600 trang, tuy nhiên MIT Press không thể publish một quyển sách dài như vậy! Murphy quyết định chia thành 2 quyển khác nhau. Đó là “Probabilistic Machine Learning: An Introduction (2021)” và “Probabilistic Machine Learning: Advanced Topics (2022)”. Hai quyển kế thừa những điểm mấu chốt và xuất sắc từ quyển đầu tiên. Thêm vào đó là rất nhiều nội dung mới của “kỷ nguyên học sâu” như Generative Models, Variational Inference và Reinforcement Learning.
Với phong cách viết giản dị, gần gũi, dễ tiếp cận và hoàn chỉnh cho các thuật toán, đồng thời với rất nhiều hình ảnh và ví dụ trực quan, mình tin rằng các bạn sẽ học được rất nhiều điều thú vị và bổ ích trong quyển sách này.
Trang chủ và draft version (2021) của 2 quyển sách: https://probml.github.io/pml-book/
Happy Learning and Happy New Year! 😉
**************************************************
Mình có note lại danh sách những quyển sách “gối đầu” trong Machine Learning, các bạn có thể xem qua nhé!
Pattern Recognition and Machine Learning (2006) – Bishop (https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)
The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (2009) - Trevor Hastie et al. (https://web.stanford.edu/~hastie/ElemStatLearn/)
Machine Learning: A Probabilistic Perspective (2012) – Murphy (https://probml.github.io/pml-book/)
Deep Learning (2015) – Ian Goodfellow et al. (https://www.deeplearningbook.org/)
Foundations of Machine Learning, Second Edition (2018) - Mehryar Mohri et al. (https://cs.nyu.edu/~mohri/mlbook/)","[NEW BOOK] - PROBABILISTIC MACHINE LEARNING: AN INTRODUCTION (2021) Đây là sách mới của Murphy, được updated từ quyển “Machine Learning: A Probabilistic Perspective” (2012) là một trong những cuốn sách nền tảng về xác suất ứng dụng trong Machine Learning. Các bạn có thể chưa biết “Machine Learning: A Probabilistic Perspective” (2012) nhận được giải thưởng danh giá DeGroot Prize (https://bayesian.org/project/degroot-prize/) năm 2013 cho những đóng góp xuất sắc trong lĩnh vực khoa học thống kê. Cuốn sách covers rất nhiều topics khác nhau của xác suất thống kê và tính liên hệ mật thiết trong máy học như Probability Optimization, Conditional Random Fields, L1 Regularization, Bayesian Decision Theory, Gaussian Models và Deep Learning. Năm 2012 được nhận định là khởi đầu của “kỷ nguyên học sâu” (Deep Learning Revolution). Khoảng 8 năm trở lại đây chúng ta đã chứng kiến những ứng dụng vô cùng nổi bật của Machine Learning và Deep Learning trong Computer Vision, Natural Language Processing và Reinforcement Learning. Năm 2018, Murphy nhận thấy còn nhiều thiếu sót trong quyển sách đầu tay của mình nên đã tiến hành viết hoàn thiện nó. Bản thảo đầu tiên khi hoàn thành (2020) dài khoảng 1600 trang, tuy nhiên MIT Press không thể publish một quyển sách dài như vậy! Murphy quyết định chia thành 2 quyển khác nhau. Đó là “Probabilistic Machine Learning: An Introduction (2021)” và “Probabilistic Machine Learning: Advanced Topics (2022)”. Hai quyển kế thừa những điểm mấu chốt và xuất sắc từ quyển đầu tiên. Thêm vào đó là rất nhiều nội dung mới của “kỷ nguyên học sâu” như Generative Models, Variational Inference và Reinforcement Learning. Với phong cách viết giản dị, gần gũi, dễ tiếp cận và hoàn chỉnh cho các thuật toán, đồng thời với rất nhiều hình ảnh và ví dụ trực quan, mình tin rằng các bạn sẽ học được rất nhiều điều thú vị và bổ ích trong quyển sách này. Trang chủ và draft version (2021) của 2 quyển sách: https://probml.github.io/pml-book/ Happy Learning and Happy New Year! ************************************************** Mình có note lại danh sách những quyển sách “gối đầu” trong Machine Learning, các bạn có thể xem qua nhé! Pattern Recognition and Machine Learning (2006) – Bishop (https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (2009) - Trevor Hastie et al. (https://web.stanford.edu/~hastie/ElemStatLearn/) Machine Learning: A Probabilistic Perspective (2012) – Murphy (https://probml.github.io/pml-book/) Deep Learning (2015) – Ian Goodfellow et al. (https://www.deeplearningbook.org/) Foundations of Machine Learning, Second Edition (2018) - Mehryar Mohri et al. (https://cs.nyu.edu/~mohri/mlbook/)",,,,,
,nan,,,,,
"Chào mọi người,
Cho mình hỏi làm sao để tìm ra một ngưỡng threshold phù hợp để chuyển đổi từ thuộc tính có giá trị liên tục sang thuộc tính có dạng categorical ? Mình đang dùng ID3 cho Decision Tree.
Cảm ơn mọi người.","Chào mọi người, Cho mình hỏi làm sao để tìm ra một ngưỡng threshold phù hợp để chuyển đổi từ thuộc tính có giá trị liên tục sang thuộc tính có dạng categorical ? Mình đang dùng ID3 cho Decision Tree. Cảm ơn mọi người.",,,,,
https://m.facebook.com/groups/VietnamAiLlinkSharing/permalink/444600300280364/,https://m.facebook.com/groups/VietnamAiLlinkSharing/permalink/444600300280364/,,,,,
"Cho hỏi bạn nào biết đoạn code nào kết hợp 2 hay nhiều thuật toán phân cụm như :
Agglomerative Clustering, BIRCH,DBSCAN,K-Means,Mini-Batch K-Means, Mean Shift, OPTICS, Spectral Clustering, Mixture of
Gaussians lại với nhau không
Cảm ơn !","Cho hỏi bạn nào biết đoạn code nào kết hợp 2 hay nhiều thuật toán phân cụm như : Agglomerative Clustering, BIRCH,DBSCAN,K-Means,Mini-Batch K-Means, Mean Shift, OPTICS, Spectral Clustering, Mixture of Gaussians lại với nhau không Cảm ơn !",,,,,
"Mọi người cho em xin vài tựa sách (textbook), bài giảng (lecture) hoặc khoá học (course) về toán rời rạc đầy đủ kiến thức từ tập hợp-logic đến tổ hợp, đồ thị ạ.
P/s: tiếng Anh cũng được ạ.","Mọi người cho em xin vài tựa sách (textbook), bài giảng (lecture) hoặc khoá học (course) về toán rời rạc đầy đủ kiến thức từ tập hợp-logic đến tổ hợp, đồ thị ạ. P/s: tiếng Anh cũng được ạ.",,,,,
"Chào anh em,
Mình hiện tại bắt đầu học về ML với AI, nhờ anh em cho mình thêm lời khuyên, các lộ trình tự học và các khóa học tham khảo để có thể tự học nhanh và hiệu quả nhất.
Ngoài ra, để nghiên cứu chuyên sâu thì mình cần chuẩn bị kiến thức toán học gồm những phần nào là đủ vậy?
Hiện tại mình là .NET Developer, để học ML thì cần code được Python. Nhờ anh em introduce mình thêm các courses về Python for ML nhé.
Cám ơn mọi người rất nhiều.","Chào anh em, Mình hiện tại bắt đầu học về ML với AI, nhờ anh em cho mình thêm lời khuyên, các lộ trình tự học và các khóa học tham khảo để có thể tự học nhanh và hiệu quả nhất. Ngoài ra, để nghiên cứu chuyên sâu thì mình cần chuẩn bị kiến thức toán học gồm những phần nào là đủ vậy? Hiện tại mình là .NET Developer, để học ML thì cần code được Python. Nhờ anh em introduce mình thêm các courses về Python for ML nhé. Cám ơn mọi người rất nhiều.",,,,,
"CÁC A ƠI EM MUỐN SỬA ĐẦU VÀO CỦA BÀI NHẬN DIỆN BIỂN SỐ XE BẰNG WPOD VÀ TESSERACT OCR  LÀ FILE VIDEO CƠ MÀ BỊ LỖI CÁC BÁC SỬA GIÚP E VỚI Ạ . E CẢM ƠN MN
http://codepad.org/6SlQ0Vz1",CÁC A ƠI EM MUỐN SỬA ĐẦU VÀO CỦA BÀI NHẬN DIỆN BIỂN SỐ XE BẰNG WPOD VÀ TESSERACT OCR LÀ FILE VIDEO CƠ MÀ BỊ LỖI CÁC BÁC SỬA GIÚP E VỚI Ạ . E CẢM ƠN MN http://codepad.org/6SlQ0Vz1,,,,,
"Một trong những cuốn sách yêu thích của mình về Practical Machine Learning
https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/
Cuốn này đã được dịch ra rất nhiều thứ tiếng, hy vọng một ngày không xa sẽ có bản tiếng Việt ;)","Một trong những cuốn sách yêu thích của mình về Practical Machine Learning https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ Cuốn này đã được dịch ra rất nhiều thứ tiếng, hy vọng một ngày không xa sẽ có bản tiếng Việt ;)",,,,,
"#nlp
Mọi người cho em hỏi hiện nay biểu diễn vector từ tiếng anh để tính độ tương đồng thì pretrained nào tốt nhất ạ? Em cảm ơn",Mọi người cho em hỏi hiện nay biểu diễn vector từ tiếng anh để tính độ tương đồng thì pretrained nào tốt nhất ạ? Em cảm ơn,#nlp,,,,
"#hoidap #tensorflowSharp
mọi người ơi cho em hỏi vài câu:
https://teachablemachine.withgoogle.com/train/image
em dùng trang này để gen ra file .pb và labels.txt
em dùng https://github.com/Syn-McJ/TFClassify-Unity
và thế cái .pb đó vào code example : Assets/Scripts/Detector.cs line 41 thì gặp lỗi invalid graphDef
em tìm khắp nơi rồi nhưng cũng chỉ nói một vài thông tin ở python. còn C# thì không có function tương ứng ^^
HỎI: có dùng trang web của google kia train rồi lấy model về dùng cho TFSharp được không ?
có cách nào khác để Import không ?",mọi người ơi cho em hỏi vài câu: https://teachablemachine.withgoogle.com/train/image em dùng trang này để gen ra file .pb và labels.txt em dùng https://github.com/Syn-McJ/TFClassify-Unity và thế cái .pb đó vào code example : Assets/Scripts/Detector.cs line 41 thì gặp lỗi invalid graphDef em tìm khắp nơi rồi nhưng cũng chỉ nói một vài thông tin ở python. còn C# thì không có function tương ứng ^^ HỎI: có dùng trang web của google kia train rồi lấy model về dùng cho TFSharp được không ? có cách nào khác để Import không ?,#hoidap	#tensorflowSharp,,,,
"Kính chào các bác, tiếp tục series BERT. Hôm nay em xin mạnh dạn chia sẻ cùng anh em bài toán nhận diện cảm xúc văn bản Tiếng Việt sử dụng PhoBERT của VinAI mà em vừa học.
Hi vọng giúp được anh em trong các bài toán NLP.
Ps: Tài liệu chỉ mong giúp các anh em newbie mới học thôi ah. Mong ad duyệt bài!","Kính chào các bác, tiếp tục series BERT. Hôm nay em xin mạnh dạn chia sẻ cùng anh em bài toán nhận diện cảm xúc văn bản Tiếng Việt sử dụng PhoBERT của VinAI mà em vừa học. Hi vọng giúp được anh em trong các bài toán NLP. Ps: Tài liệu chỉ mong giúp các anh em newbie mới học thôi ah. Mong ad duyệt bài!",,,,,
"Chào mọi người, em hiện đang làm một cuộc khảo sát về việc ứng dụng machine learning hoặc deep learning trong công việc mọi người đang làm
Mọi người vui lòng click và trả lời khảo sát trên google form này giúp e nhé
Cảm ơn mọi người đã hợp tác","Chào mọi người, em hiện đang làm một cuộc khảo sát về việc ứng dụng machine learning hoặc deep learning trong công việc mọi người đang làm Mọi người vui lòng click và trả lời khảo sát trên google form này giúp e nhé Cảm ơn mọi người đã hợp tác",,,,,
"Machine learning models hiện đang ngày càng lớn và chạy ngày càng chậm. Nhiều người nghĩ việc này là một rào cản khiến cho việc đưa các chương trình này vào ứng dụng. Nhiều công ty không dám chạy online predictions mà chạy batch predictions.
Sau khi nói chuyện với các công ty lớn, mình nhận ra rằng một số công ty không chỉ chạy online predictions mà còn chạy online learning!
Bài viết này phân tích các ứng dụng của real-time machine learning dựa vào các cuộc nói chuyện với các công ty lớn ở cả Mỹ và Trung Quốc, bao gồm Netflix, DoorDash, Yelp, Tencent, Alibaba, TikTok, … Mình cũng khá ngạc nhiên khi gặp một số kỹ sư ở Việt Nam rất mạnh về khoản này, chuyên sâu hơn hẳn khá nhiều kỹ sư mình đã nói chuyện ở các công ty lớn của Mỹ.
Với real-time machine learning, infra là vô cùng quan trọng, và mình thấy các công ty TQ rất có thể đang đi trước các công ty Mỹ về mảng infra cho AI.
Nếu bạn nào trong nhóm làm về real-time machine learning, mình rất hy vọng có thể học hỏi từ bạn!","Machine learning models hiện đang ngày càng lớn và chạy ngày càng chậm. Nhiều người nghĩ việc này là một rào cản khiến cho việc đưa các chương trình này vào ứng dụng. Nhiều công ty không dám chạy online predictions mà chạy batch predictions. Sau khi nói chuyện với các công ty lớn, mình nhận ra rằng một số công ty không chỉ chạy online predictions mà còn chạy online learning! Bài viết này phân tích các ứng dụng của real-time machine learning dựa vào các cuộc nói chuyện với các công ty lớn ở cả Mỹ và Trung Quốc, bao gồm Netflix, DoorDash, Yelp, Tencent, Alibaba, TikTok, … Mình cũng khá ngạc nhiên khi gặp một số kỹ sư ở Việt Nam rất mạnh về khoản này, chuyên sâu hơn hẳn khá nhiều kỹ sư mình đã nói chuyện ở các công ty lớn của Mỹ. Với real-time machine learning, infra là vô cùng quan trọng, và mình thấy các công ty TQ rất có thể đang đi trước các công ty Mỹ về mảng infra cho AI. Nếu bạn nào trong nhóm làm về real-time machine learning, mình rất hy vọng có thể học hỏi từ bạn!",,,,,
"#hoidap
Mọi người ơi, cho em hỏi về việc phân tích sắc thái bình luận trong machine learning với ạ,em cũng mới học nghiên cứu nên chưa hiểu sâu và có tìm được 1 bài của 1 anh đã làm và có source code ở trên forum rồi nên là có ai đã từng xem qua bài trên này có thể giải thích giúp em với ko ạ?Em cảm ơn","Mọi người ơi, cho em hỏi về việc phân tích sắc thái bình luận trong machine learning với ạ,em cũng mới học nghiên cứu nên chưa hiểu sâu và có tìm được 1 bài của 1 anh đã làm và có source code ở trên forum rồi nên là có ai đã từng xem qua bài trên này có thể giải thích giúp em với ko ạ?Em cảm ơn",#hoidap,,,,
"Hello guys. Happy new year.
I decided to write a small blog during my new year vacation. It has been a while since my last blog, mostly due to the block of Medium in Vietnam 🙁
This time I will write about AI in audio industry. Not academic one but things/tips I learned during my 4 months working with them. Please don't asking for the code :))
https://sites.google.com/sssmar.../ai-in-audio-industry/home","Hello guys. Happy new year. I decided to write a small blog during my new year vacation. It has been a while since my last blog, mostly due to the block of Medium in Vietnam This time I will write about AI in audio industry. Not academic one but things/tips I learned during my 4 months working with them. Please don't asking for the code :)) https://sites.google.com/sssmar.../ai-in-audio-industry/home",,,,,
"E đang làm bài toán phân loại K means, cần phân loại ~ 150.000 vector với K=1500, nên chạy rất lâu ạ (~1h)
Mọi người cho e hỏi có cách nào để tăng tốc K means không ạ?","E đang làm bài toán phân loại K means, cần phân loại ~ 150.000 vector với K=1500, nên chạy rất lâu ạ (~1h) Mọi người cho e hỏi có cách nào để tăng tốc K means không ạ?",,,,,
"#computervision #face_recognition
Chào mọi người, mình có 1 bài toán sau mong mọi người giúp đỡ
Giả sử mình có 2 đoạn video được ghi bởi cùng 1 cam , cùng 1 góc cam, cùng thông số. Ví dụ với video 1 sau khi mình sử dụng face detection để cắt thì mình cắt được mặt của 3 người, mỗi người có tầm khoảng 5,6 ảnh
Yêu cầu đề bài là với video thứ 2 thì làm ntn để nhận diện ra 3 người mình đã cắt ra ở video 1
Hiện tại mình đã thử một vài cách :
- Xây dựng bộ classification từ mấy ảnh cắt ra từ video 1. Với video 2 thì mình dùng face detection và đưa vào bộ phân loại
- Sử dụng các pretrain để extract embedding -> sử dụng các hàm để tính similarity như Euclidean , Cosin
Với các cách của mình thì độ chính xác đang k được như mong muốn. Mọi người có giải pháp gì với bài toán của mình không thì chia sẻ nhé :D
Mình cảm ơn","Chào mọi người, mình có 1 bài toán sau mong mọi người giúp đỡ Giả sử mình có 2 đoạn video được ghi bởi cùng 1 cam , cùng 1 góc cam, cùng thông số. Ví dụ với video 1 sau khi mình sử dụng face detection để cắt thì mình cắt được mặt của 3 người, mỗi người có tầm khoảng 5,6 ảnh Yêu cầu đề bài là với video thứ 2 thì làm ntn để nhận diện ra 3 người mình đã cắt ra ở video 1 Hiện tại mình đã thử một vài cách : - Xây dựng bộ classification từ mấy ảnh cắt ra từ video 1. Với video 2 thì mình dùng face detection và đưa vào bộ phân loại - Sử dụng các pretrain để extract embedding -> sử dụng các hàm để tính similarity như Euclidean , Cosin Với các cách của mình thì độ chính xác đang k được như mong muốn. Mọi người có giải pháp gì với bài toán của mình không thì chia sẻ nhé :D Mình cảm ơn",#computervision	#face_recognition,,,,
"Hôm nay là sinh nhật lần thứ tư của Machine Learning Cơ Bản. Sau bốn năm, mình rất vui vì chúng ta đã có một cộng đồng lớn với nhiều dự án thú vị.
Hy vọng Machine Learning Cơ Bản sẽ tiếp tục mang đến những giá trị mới cho cộng đồng ML/AI Việt Nam.
 — với Cu Nguyen.","Hôm nay là sinh nhật lần thứ tư của Machine Learning Cơ Bản. Sau bốn năm, mình rất vui vì chúng ta đã có một cộng đồng lớn với nhiều dự án thú vị. Hy vọng Machine Learning Cơ Bản sẽ tiếp tục mang đến những giá trị mới cho cộng đồng ML/AI Việt Nam. — với Cu Nguyen.",,,,,
"Giải trí cuối tuần - mọi người nghĩ AI sẽ hỗ trợ hay đào thải nghệ sĩ truyền thống?
>>>Bài hát mô phỏng Katy Perry từ Jukebok - OpenAI: https://soundcloud.com/open.../jukebox-novel_lyrics-78968609",Giải trí cuối tuần - mọi người nghĩ AI sẽ hỗ trợ hay đào thải nghệ sĩ truyền thống? >>>Bài hát mô phỏng Katy Perry từ Jukebok - OpenAI: https://soundcloud.com/open.../jukebox-novel_lyrics-78968609,,,,,
Các cao nhân cho em hỏi về genetic network programming. Em tìm tài liệu và code demo mà khó quá. Em cảm ơn,Các cao nhân cho em hỏi về genetic network programming. Em tìm tài liệu và code demo mà khó quá. Em cảm ơn,,,,,
"Mình có nhiều file ảnh chụp các form khác nhau trên đó có các trường (dùng chủ yếu chữ in đánh máy, nhưng thi thoảng cũng có chỗ viết tay), cần có một tool có thể train để tách các dòng chữ từ các ảnh chứa form này (có thể có nhiều loại form như hồ sơ nhân sự, phiếu góp ý, bảng đăng ký dịch vụ...) và lưu ra file (hay database) thành các dòng và cột dữ liệu. Tên cột được đặt theo tên các trường trên form hay đặt thủ công cũng được.
Bạn nào đã dùng hay biết các công cụ loại này, xin giới thiệu giùm.
Nếu bạn làm được công cụ này và muốn bán thì xin liên hệ với mình.
Cám ơn các bạn.","Mình có nhiều file ảnh chụp các form khác nhau trên đó có các trường (dùng chủ yếu chữ in đánh máy, nhưng thi thoảng cũng có chỗ viết tay), cần có một tool có thể train để tách các dòng chữ từ các ảnh chứa form này (có thể có nhiều loại form như hồ sơ nhân sự, phiếu góp ý, bảng đăng ký dịch vụ...) và lưu ra file (hay database) thành các dòng và cột dữ liệu. Tên cột được đặt theo tên các trường trên form hay đặt thủ công cũng được. Bạn nào đã dùng hay biết các công cụ loại này, xin giới thiệu giùm. Nếu bạn làm được công cụ này và muốn bán thì xin liên hệ với mình. Cám ơn các bạn.",,,,,
"Dạ em xin chào các anh chị.
Hiện tại em đang có một bài tập ở trường là: phân loại xe bằng SVM. Em muốn tìm thêm tài liệu về SVM để đọc. Mọi người cho em xin ít tài liệu chất lượng được không ạ. E xin chân thành cám ơn.",Dạ em xin chào các anh chị. Hiện tại em đang có một bài tập ở trường là: phân loại xe bằng SVM. Em muốn tìm thêm tài liệu về SVM để đọc. Mọi người cho em xin ít tài liệu chất lượng được không ạ. E xin chân thành cám ơn.,,,,,
"Xin phép mọi người trong group và admin ạ
Hiện tại mình mới bắt đầu học AI nên mình định lập một nhóm nhỏ (chứ không add tràn lan và đương nhiên là sẽ có những tiêu chuẩn nhất định) để cùng nhau học. Hiện tại thì mình đang học song song 3 môn Calculus,Linear Algebra và Python. Nếu bạn nào quan tâm thì có thể ib mình để chúng ta cùng trao đổi thêm nhé","Xin phép mọi người trong group và admin ạ Hiện tại mình mới bắt đầu học AI nên mình định lập một nhóm nhỏ (chứ không add tràn lan và đương nhiên là sẽ có những tiêu chuẩn nhất định) để cùng nhau học. Hiện tại thì mình đang học song song 3 môn Calculus,Linear Algebra và Python. Nếu bạn nào quan tâm thì có thể ib mình để chúng ta cùng trao đổi thêm nhé",,,,,
"Bài chia sẻ đầu tiên về object detection solution với OneNet. Với content creator: Bé Na
Nếu các bạn muốn chia sẻ về các chủ đề AI thì hãy tham gia với bọn mình nhé",Bài chia sẻ đầu tiên về object detection solution với OneNet. Với content creator: Bé Na Nếu các bạn muốn chia sẻ về các chủ đề AI thì hãy tham gia với bọn mình nhé,,,,,
Mn cho mình hỏi làm sao để chuyển file .crash sang csv ạ,Mn cho mình hỏi làm sao để chuyển file .crash sang csv ạ,,,,,
"Chào mọi người
Em đang làm bài tập về Thị giác máy tính. Bài toán của em là từ ảnh, nhận diện ra product list và nhận diện text từ list. Product list có thể là có border hoặc không.
Em có 2 câu hỏi sau mong được giải đáp:
Hướng tiếp cận cho nhận diện ra productlist. Em đã thử các hướng Tabletnet, CascadTabNet, SSD, YOLO nhưng kết quả không tốt lắm.
Với nhận diện text trong product list, model thường không nhận diện được các vùng/ cột chỉ có số lượng là ""1"", ""x1"" . Em muốn hỏi về hướng xử lý trong trường hợp này.
Em cám ơn.","Chào mọi người Em đang làm bài tập về Thị giác máy tính. Bài toán của em là từ ảnh, nhận diện ra product list và nhận diện text từ list. Product list có thể là có border hoặc không. Em có 2 câu hỏi sau mong được giải đáp: Hướng tiếp cận cho nhận diện ra productlist. Em đã thử các hướng Tabletnet, CascadTabNet, SSD, YOLO nhưng kết quả không tốt lắm. Với nhận diện text trong product list, model thường không nhận diện được các vùng/ cột chỉ có số lượng là ""1"", ""x1"" . Em muốn hỏi về hướng xử lý trong trường hợp này. Em cám ơn.",,,,,
"Mình có các file PMML (Predictive Model Markup Language) lưu các trained model, kết quả của quá trình phân tích dữ liệu. Mình muốn chuyển đổi các trained model này thành các SQL function/stored proc để tính toán kết quả cho các dòng dữ liệu mới phát sinh cho tiện. Bạn nào có kinh nghiệm về vấn đề này xin góp ý giúp. Hoặc có thể dùng các PMML engine/library nhỏ gọn nào đó cho C# cũng được. Xin cám ơn.","Mình có các file PMML (Predictive Model Markup Language) lưu các trained model, kết quả của quá trình phân tích dữ liệu. Mình muốn chuyển đổi các trained model này thành các SQL function/stored proc để tính toán kết quả cho các dòng dữ liệu mới phát sinh cho tiện. Bạn nào có kinh nghiệm về vấn đề này xin góp ý giúp. Hoặc có thể dùng các PMML engine/library nhỏ gọn nào đó cho C# cũng được. Xin cám ơn.",,,,,
"#summarization
Em xin phép hỏi mọi người trong group ạ,
Em được giao task tóm tắt văn bản sử dụng PEGASUS hoặc BART, nhưng vấn đề là mô hình PEGASUS, BART chỉ cho max_length = 1024, em đang muốn finetuning với dữ liệu dài nên có điều chỉnh max_length = 2048, chi tiết e gửi trong code colab:
https://colab.research.google.com/drive/16XysqYk9Fef4R8HaJZI_4NTjpdIULP45?usp=sharing
Vấn đề em gặp phải là em bị tràn cuda memory, model PEGASUS em load lên chỉ 2,2GB, và em chỉ chỉnh sửa positional encoding. Dù em đã thử ít dữ liệu hơn (chỉ 10 điểm dữ liệu), batch_size = 1, nhưng ngay từ iter đầu tiên đã bị cuda out of memory. Em muốn hỏi:
Mọi người có thể xem và có thể cho em lí do tại sao bị out of memory cuda và hướng giải quyết.
Mọi người từng thử finetuning mô hình nào với dữ liệu dài hơn chưa ạ, có thể cho em xin chỉ dẫn với.
Em xin cảm ơn ạ!","Em xin phép hỏi mọi người trong group ạ, Em được giao task tóm tắt văn bản sử dụng PEGASUS hoặc BART, nhưng vấn đề là mô hình PEGASUS, BART chỉ cho max_length = 1024, em đang muốn finetuning với dữ liệu dài nên có điều chỉnh max_length = 2048, chi tiết e gửi trong code colab: https://colab.research.google.com/drive/16XysqYk9Fef4R8HaJZI_4NTjpdIULP45?usp=sharing Vấn đề em gặp phải là em bị tràn cuda memory, model PEGASUS em load lên chỉ 2,2GB, và em chỉ chỉnh sửa positional encoding. Dù em đã thử ít dữ liệu hơn (chỉ 10 điểm dữ liệu), batch_size = 1, nhưng ngay từ iter đầu tiên đã bị cuda out of memory. Em muốn hỏi: Mọi người có thể xem và có thể cho em lí do tại sao bị out of memory cuda và hướng giải quyết. Mọi người từng thử finetuning mô hình nào với dữ liệu dài hơn chưa ạ, có thể cho em xin chỉ dẫn với. Em xin cảm ơn ạ!",#summarization,,,,
Mình vừa làm 2 video về on-device machine learning (còn gọi là Edge ML) về cách chạy TensorFlow model trên mobile app và thiết bị IoT. Đây là video đầu tiên mới được up lên kênh TensorFlow trên YouTube. Mọi người xem tham khảo và góp ý nhé :),Mình vừa làm 2 video về on-device machine learning (còn gọi là Edge ML) về cách chạy TensorFlow model trên mobile app và thiết bị IoT. Đây là video đầu tiên mới được up lên kênh TensorFlow trên YouTube. Mọi người xem tham khảo và góp ý nhé :),,,,,
"Hi mọi người,
Em đang bắt đầu làm việc với mạng GAN. Xin hỏi mọi người, có cách nào xây dựng một mạng/ kĩ thuật để có thể từ một ảnh sinh ra hai ảnh với điều kiện hai ảnh có sự tương đồng lớn mà không giống nhau hoàn toàn hay không ạ?
Em xin cảm ơn.","Hi mọi người, Em đang bắt đầu làm việc với mạng GAN. Xin hỏi mọi người, có cách nào xây dựng một mạng/ kĩ thuật để có thể từ một ảnh sinh ra hai ảnh với điều kiện hai ảnh có sự tương đồng lớn mà không giống nhau hoàn toàn hay không ạ? Em xin cảm ơn.",,,,,
"#T5 #QA
Hi all, 
Em đang train T5 cho Question answering, 
em chưa rõ format của cái input như nào và encode như nào.
Em nghĩ là
input: ""question ...? context ...""
target: ""answer""
nhưng mà em không biết nên encode về ids như thế nào.
về cơ bản chắc vẫn là BPE nhưng mà đang thắc mắc là cái cái extra_ids (aka Sentinels) như nào
Mọi người giải đáp giúp em với ạ.","Hi all, Em đang train T5 cho Question answering, em chưa rõ format của cái input như nào và encode như nào. Em nghĩ là input: ""question ...? context ..."" target: ""answer"" nhưng mà em không biết nên encode về ids như thế nào. về cơ bản chắc vẫn là BPE nhưng mà đang thắc mắc là cái cái extra_ids (aka Sentinels) như nào Mọi người giải đáp giúp em với ạ.",#T5	#QA,,,,
"Em chào mọi người ạ,
Em đang muốn xây dựng một hệ thống gợi ý sản phẩm tiếp theo nên mua dựa trên data của người dùng. Data này bao gồm : tuổi, giới tính, những sản phẩm đã mua vừa qua.
Em có thử Time Series nhưng lại không phù hợp
Một ý tưởng khác là lập 1 tổ hợp các từ sao cho có nghĩa, rồi dùng NLP để gợi ý sản phẩm.
Mọi người cho em xin ý kiến ạ. Em cám ơn.","Em chào mọi người ạ, Em đang muốn xây dựng một hệ thống gợi ý sản phẩm tiếp theo nên mua dựa trên data của người dùng. Data này bao gồm : tuổi, giới tính, những sản phẩm đã mua vừa qua. Em có thử Time Series nhưng lại không phù hợp Một ý tưởng khác là lập 1 tổ hợp các từ sao cho có nghĩa, rồi dùng NLP để gợi ý sản phẩm. Mọi người cho em xin ý kiến ạ. Em cám ơn.",,,,,
Chào cả nhà. Mình đang làm thử một model để correct lại kết quả nhận diện OCR cho tiếng Việt. Đang tính sử dụng phoBERT với masked model language. Cơ mà vấn đề mình gặp phải bây giờ là làm sao để biết trong câu từ nào sai để mà mask nó lại. Các cao nhân ở đây có ai chỉ cho mình vài con đường lấp ló chút tia sáng được ko ạ?,Chào cả nhà. Mình đang làm thử một model để correct lại kết quả nhận diện OCR cho tiếng Việt. Đang tính sử dụng phoBERT với masked model language. Cơ mà vấn đề mình gặp phải bây giờ là làm sao để biết trong câu từ nào sai để mà mask nó lại. Các cao nhân ở đây có ai chỉ cho mình vài con đường lấp ló chút tia sáng được ko ạ?,,,,,
"❗ KHÔNG THỂ “ÔM” DỮ LIỆU MỘT MÌNH, ĐẶC BIỆT LÀ DỮ LIỆU Y SINH ❗
Nền tảng quản lý, phân tích dữ liệu y sinh của người Việt được kỳ vọng giải quyết nhu cầu tham chiếu dữ liệu suốt nhiều thập kỷ tại Việt Nam.
#sharing","KHÔNG THỂ “ÔM” DỮ LIỆU MỘT MÌNH, ĐẶC BIỆT LÀ DỮ LIỆU Y SINH Nền tảng quản lý, phân tích dữ liệu y sinh của người Việt được kỳ vọng giải quyết nhu cầu tham chiếu dữ liệu suốt nhiều thập kỷ tại Việt Nam.",#sharing,,,,
"NHỜ hỗ trợ giải thuật ANFIS (Python)
Xin chào cả nhà!
Mình đang tìm hiểu code python cho giải thuật ANFIS theo hướng dẫn: https://github.com/twmeggs/anfis
=> Mô tả này chạy cho 2 biến đầu vào và 1 biến đầu ra
Tuy nhiên, mình gặp khó khi chưa hiểu phải thay đổi tham số đầu vào như thế nào cho bộ dữ liệu của mình (5 biến đầu vào var1, ..., var5 và 1 biến đích target) trong mô hình của giải thuật ANFIS. Bộ dữ liệu của mình ko cần quan tâm đến Time Series.
Mình gửi tập dữ liệu mẫu kèm theo, xin nhờ mọi người trợ giúp tài liệu hoặc hướng dẫn cách thay đổi tham số cho phù hợp.
https://drive.google.com/file/d/1g7E_S8V5_N9GI9At1wZwQZEqEk59gQGq/view?usp=sharing
Trân trọng cảm ơn mọi người!","NHỜ hỗ trợ giải thuật ANFIS (Python) Xin chào cả nhà! Mình đang tìm hiểu code python cho giải thuật ANFIS theo hướng dẫn: https://github.com/twmeggs/anfis => Mô tả này chạy cho 2 biến đầu vào và 1 biến đầu ra Tuy nhiên, mình gặp khó khi chưa hiểu phải thay đổi tham số đầu vào như thế nào cho bộ dữ liệu của mình (5 biến đầu vào var1, ..., var5 và 1 biến đích target) trong mô hình của giải thuật ANFIS. Bộ dữ liệu của mình ko cần quan tâm đến Time Series. Mình gửi tập dữ liệu mẫu kèm theo, xin nhờ mọi người trợ giúp tài liệu hoặc hướng dẫn cách thay đổi tham số cho phù hợp. https://drive.google.com/file/d/1g7E_S8V5_N9GI9At1wZwQZEqEk59gQGq/view?usp=sharing Trân trọng cảm ơn mọi người!",,,,,
"#tensorflow
mình dùng
python 3.85
pip 20.3.2( cập nhật mới nhất)
lệnh cài đặt: pip install tensorflow
khi cài tensorfow bị lỗi không tìm được phiên bản. bạn mình dùng python 3.7 thì cài bình thường, tham khảo lỗi trên stackoverflow và github chưa thấy mấy ô chuyển về python 3.7 cài bình thường, còn 3.8 die hết. Group có ai cài được trên python3.8 không hay phải chuyển xuống python 3.7 vậy. 🤔","mình dùng python 3.85 pip 20.3.2( cập nhật mới nhất) lệnh cài đặt: pip install tensorflow khi cài tensorfow bị lỗi không tìm được phiên bản. bạn mình dùng python 3.7 thì cài bình thường, tham khảo lỗi trên stackoverflow và github chưa thấy mấy ô chuyển về python 3.7 cài bình thường, còn 3.8 die hết. Group có ai cài được trên python3.8 không hay phải chuyển xuống python 3.7 vậy.",#tensorflow,,,,
"Dear all, cho mình hỏi máy ở nhà dùng gpu rtx 3080 cuda 11.1 khi cài pytorch vào báo lỗi như hình sau. Làm thế nào để fix lỗi và cài pytorch được ạ. Nhờ mọi người chỉ giúp, xin chân thành cảm ơn.","Dear all, cho mình hỏi máy ở nhà dùng gpu rtx 3080 cuda 11.1 khi cài pytorch vào báo lỗi như hình sau. Làm thế nào để fix lỗi và cài pytorch được ạ. Nhờ mọi người chỉ giúp, xin chân thành cảm ơn.",,,,,
"Chào mọi người, trước đây em có up trong group mình về bài toán ước lượng khoảng cách giãn cách xã hội trong video. Hiện em đang thử nghiệm các phương pháp ước lượng với 2 phương pháp trong 2 trang này ạ:
OpenCV Social Distancing Detector - PyImageSearch
Subikshaa/Social-Distance-Detection-using-OpenCV (github.com)
Hiện tại em có một số thắc mắc như này ạ:
Giả sử với 2 phương pháp ước lượng khoảng cách ở trong 2 link trên, với trường hợp xét khoảng cách 2 người, phương pháp đầu cho khoảng cách ước lượng được là 5m, phương pháp thứ 2 cho khoảng cách ước lượng được là 4m. Vậy làm thế nào để biết được khoảng cách thực tế của 2 người trong video ( hay nói đúng hơn là tại 1 frame xác định trong video) là bao nhiêu để mình có thể đưa ra kết quả là phương pháp ước lượng nào tốt hơn?
Với trường hợp số lượng người cần tính là khá nhiều, khoảng từ 30 đến 40 người trở lên, thì mình có một số phương pháp hay kĩ thuật gì trong khi code để giảm thời gian tính toán của nó để cho ra kết quả nhanh hơn không ạ?
Hiện tại em đang thắc mắc như này ạ, mong mọi người có thể góp ý, tư vấn cho em thêm ạ.
Em cảm ơn mọi người.","Chào mọi người, trước đây em có up trong group mình về bài toán ước lượng khoảng cách giãn cách xã hội trong video. Hiện em đang thử nghiệm các phương pháp ước lượng với 2 phương pháp trong 2 trang này ạ: OpenCV Social Distancing Detector - PyImageSearch Subikshaa/Social-Distance-Detection-using-OpenCV (github.com) Hiện tại em có một số thắc mắc như này ạ: Giả sử với 2 phương pháp ước lượng khoảng cách ở trong 2 link trên, với trường hợp xét khoảng cách 2 người, phương pháp đầu cho khoảng cách ước lượng được là 5m, phương pháp thứ 2 cho khoảng cách ước lượng được là 4m. Vậy làm thế nào để biết được khoảng cách thực tế của 2 người trong video ( hay nói đúng hơn là tại 1 frame xác định trong video) là bao nhiêu để mình có thể đưa ra kết quả là phương pháp ước lượng nào tốt hơn? Với trường hợp số lượng người cần tính là khá nhiều, khoảng từ 30 đến 40 người trở lên, thì mình có một số phương pháp hay kĩ thuật gì trong khi code để giảm thời gian tính toán của nó để cho ra kết quả nhanh hơn không ạ? Hiện tại em đang thắc mắc như này ạ, mong mọi người có thể góp ý, tư vấn cho em thêm ạ. Em cảm ơn mọi người.",,,,,
"Xin chào a/c,
hiện em đang đọc dữ liệu từ camera ip (không phải mạng LAN) qua http bằng cv2.VideoCapture, sau đó dùng imshow() lên xem thì gặp hiện tượng bị bỏ frame (cứ 10 giây thì mới nhảy sang frame mới 1 lần, nghĩa là thời điểm a thì hiện lên frame a, sau đó đứng yên, sau đó 10s thì hiện frama a+10 luôn và bỏ qua đoạn ở giữa). Em mở camrera xem bằng trình duyệt web thì rất mượt.
Mọi người đã gặp case nào như này chưa ạ. Nếu có thì cho em xin chút kinh nghiệm để xử lý cho nó tuần tự với ạ. Em cảm ơn.","Xin chào a/c, hiện em đang đọc dữ liệu từ camera ip (không phải mạng LAN) qua http bằng cv2.VideoCapture, sau đó dùng imshow() lên xem thì gặp hiện tượng bị bỏ frame (cứ 10 giây thì mới nhảy sang frame mới 1 lần, nghĩa là thời điểm a thì hiện lên frame a, sau đó đứng yên, sau đó 10s thì hiện frama a+10 luôn và bỏ qua đoạn ở giữa). Em mở camrera xem bằng trình duyệt web thì rất mượt. Mọi người đã gặp case nào như này chưa ạ. Nếu có thì cho em xin chút kinh nghiệm để xử lý cho nó tuần tự với ạ. Em cảm ơn.",,,,,
"Danh sách các best paper trong năm 2020 với đầy đủ video demo, tóm tắt, paper và code cho các bạn tìm hiểu
#deeplearning","Danh sách các best paper trong năm 2020 với đầy đủ video demo, tóm tắt, paper và code cho các bạn tìm hiểu",#deeplearning,,,,
"Kính chào các bác, em đang học về phần thư viện Hugging Face để tập tọe làm BERT nên có viết một bài chia sẻ để các bạn mới học cùng tìm hiểu cho bớt bỡ ngỡ.
Mong ad duyệt bài và mong các bác chỉ giáo ah!","Kính chào các bác, em đang học về phần thư viện Hugging Face để tập tọe làm BERT nên có viết một bài chia sẻ để các bạn mới học cùng tìm hiểu cho bớt bỡ ngỡ. Mong ad duyệt bài và mong các bác chỉ giáo ah!",,,,,
"Xin chào mọi người !
Mình train model cho object detection, dataset khoảng 10k ảnh. Initial Lr = 0.01, decay 50%.
Dưới đây là đồ thị phần Training Loss. Mình muốn hỏi về đồ thị Regularization Loss. Mình đã nghĩ rằng đồ thị sẽ đạt trạng thái cân bằng, sau đó mình hạ Lr, đồ thị sẽ giảm hoặc tăng rồi lại cân bằng, sau đó mình tiếp tục hạ Lr.
Mình chưa hiểu tại sao đồ thị regularization loss lại như thế này và nó có ý nghĩa như thế nào ?
Mọi người lựa chọn Lr và decay factor như thế nào và dựa trên yếu tố gì ?
Mình xin cảm ơn trước !","Xin chào mọi người ! Mình train model cho object detection, dataset khoảng 10k ảnh. Initial Lr = 0.01, decay 50%. Dưới đây là đồ thị phần Training Loss. Mình muốn hỏi về đồ thị Regularization Loss. Mình đã nghĩ rằng đồ thị sẽ đạt trạng thái cân bằng, sau đó mình hạ Lr, đồ thị sẽ giảm hoặc tăng rồi lại cân bằng, sau đó mình tiếp tục hạ Lr. Mình chưa hiểu tại sao đồ thị regularization loss lại như thế này và nó có ý nghĩa như thế nào ? Mọi người lựa chọn Lr và decay factor như thế nào và dựa trên yếu tố gì ? Mình xin cảm ơn trước !",,,,,
"Đưa lại ở đây ý kiến của Ta Quang Dong - một trong những chuyên gia dịch thuật và ngôn ngữ có tên tuổi để trả lời câu hỏi của bạn Đăng Khôi về dịch Logistics Regression như thế nào?
PS: Thks TQĐ đã gửi cho link!
https://www.facebook.com/246328782728796/posts/400259637335709/?d=n",Đưa lại ở đây ý kiến của Ta Quang Dong - một trong những chuyên gia dịch thuật và ngôn ngữ có tên tuổi để trả lời câu hỏi của bạn Đăng Khôi về dịch Logistics Regression như thế nào? PS: Thks TQĐ đã gửi cho link! https://www.facebook.com/246328782728796/posts/400259637335709/?d=n,,,,,
"Có ai dịch được từ Logistic regression qua Tiếng Việt mà make sense chút chút được không. Đừng có dịch là hồi quy hậu cần nhé , Hjjjjjj.🙃","Có ai dịch được từ Logistic regression qua Tiếng Việt mà make sense chút chút được không. Đừng có dịch là hồi quy hậu cần nhé , Hjjjjjj.",,,,,
"How to build resnet model from scratch on tensorflow, pytorch, mxnet.","How to build resnet model from scratch on tensorflow, pytorch, mxnet.",,,,,
"Dạ em chào các anh/chị và các bạn ạ. Em hiện đang là web developer nhưng em có đam mê lớn với AI, đặc biệt là Machine Learning. Hiện nay em đang cố tìm tòi và học hỏi những tài liệu trên mạng, nhưng khi lên forum và xem cách các bạn hỏi lẫn cách giải đáp của mọi người, em mới thật sự cảm thấy cách học của em hiện đang không hiểu quả vì kiến thức của em vẫn còn hổng nhiều quá. Nên em lên đây mong các anh/chị và các bạn có thể cho em xin một vài lời khuyên về việc bắt đầu học cơ bản ML như thế nào được không ạ (em hiện code khá ổn Python rồi ạ)?
Em xin cảm ơn mọi người rất nhiều.","Dạ em chào các anh/chị và các bạn ạ. Em hiện đang là web developer nhưng em có đam mê lớn với AI, đặc biệt là Machine Learning. Hiện nay em đang cố tìm tòi và học hỏi những tài liệu trên mạng, nhưng khi lên forum và xem cách các bạn hỏi lẫn cách giải đáp của mọi người, em mới thật sự cảm thấy cách học của em hiện đang không hiểu quả vì kiến thức của em vẫn còn hổng nhiều quá. Nên em lên đây mong các anh/chị và các bạn có thể cho em xin một vài lời khuyên về việc bắt đầu học cơ bản ML như thế nào được không ạ (em hiện code khá ổn Python rồi ạ)? Em xin cảm ơn mọi người rất nhiều.",,,,,
"Chào anh chị em có đang làm project về sentiment analysis cho comment của khách hàng trên web. Em có gặp chút vấn đề về cleaning data trong tiếng việt khi người dùng viết tắt.
Mong anh chị trong nhóm giúp em với ạ. Em cảm ơn !",Chào anh chị em có đang làm project về sentiment analysis cho comment của khách hàng trên web. Em có gặp chút vấn đề về cleaning data trong tiếng việt khi người dùng viết tắt. Mong anh chị trong nhóm giúp em với ạ. Em cảm ơn !,,,,,
"RAdam (Rectified Adam) một trình tối ưu được Liyuan Liu đề xuất vào năm 2019.
Hiện VN mình thấy ít người viết về trình tối ưu này nên hôm nay mình xin gửi tơi bài viết về RAdam
Mong mọi người ủng hộ ạ 😃",RAdam (Rectified Adam) một trình tối ưu được Liyuan Liu đề xuất vào năm 2019. Hiện VN mình thấy ít người viết về trình tối ưu này nên hôm nay mình xin gửi tơi bài viết về RAdam Mong mọi người ủng hộ ạ,,,,,
https://fb.watch/2lBlu9wUKq/,https://fb.watch/2lBlu9wUKq/,,,,,
"Chào các anh/chị bạn mình có câu hỏi nhờ mọi người tư vấn giùm. Hình dưới là ảnh sau khi mình train model của mình với 300 epochs nhưng tại epoch 80 loss nó tăng vọt lên, mình rất muốn biết một vài lí do dẫn ra nguyên nhân này để khắc phục ạ!
Loss function: Adam, Lr=0.001, Bs=24,  dữ liệu khoảng 10000 ảnh, có normalize [0-1], có thực hiện data augmentation. 
Mình rất muốn biết một số nguyên nhân tại sao loss nhảy vọt lên 1 lần như vậy. 
Cảm ơn mọi người đã xem qua!","Chào các anh/chị bạn mình có câu hỏi nhờ mọi người tư vấn giùm. Hình dưới là ảnh sau khi mình train model của mình với 300 epochs nhưng tại epoch 80 loss nó tăng vọt lên, mình rất muốn biết một vài lí do dẫn ra nguyên nhân này để khắc phục ạ! Loss function: Adam, Lr=0.001, Bs=24, dữ liệu khoảng 10000 ảnh, có normalize [0-1], có thực hiện data augmentation. Mình rất muốn biết một số nguyên nhân tại sao loss nhảy vọt lên 1 lần như vậy. Cảm ơn mọi người đã xem qua!",,,,,
"Nay tụi mình có livestream về cuộc thi ML Showcase 2020, giải thưởng, kinh nghiệm thi, cách ứng dụng AI vào sản phẩm. Mọi người tham khảo nhé. Hi vọng sẽ có nhiều nhóm trong cộng đồng tham gia :D

https://www.facebook.com/vietaipublic/videos/141684164382216","Nay tụi mình có livestream về cuộc thi ML Showcase 2020, giải thưởng, kinh nghiệm thi, cách ứng dụng AI vào sản phẩm. Mọi người tham khảo nhé. Hi vọng sẽ có nhiều nhóm trong cộng đồng tham gia :D https://www.facebook.com/vietaipublic/videos/141684164382216",,,,,
"Hi all,
Mình muốn chia sẻ 1 tool mình mới được giới thiệu để học và thực hành build ML models, đặc biệt là đây là 1 visual tool nên sẽ đơn giản hơn cho người mới bắt đầu.
Nếu ai đã quen với những graphical programming tool như Node Red (cho Javascript) và Scratch (cho Arduino/Microcontrollers) thì chắc sẽ ko có vấn đề gì. Trên trang web có hướng dẫn cơ bản nha 😁
Have fun learning!","Hi all, Mình muốn chia sẻ 1 tool mình mới được giới thiệu để học và thực hành build ML models, đặc biệt là đây là 1 visual tool nên sẽ đơn giản hơn cho người mới bắt đầu. Nếu ai đã quen với những graphical programming tool như Node Red (cho Javascript) và Scratch (cho Arduino/Microcontrollers) thì chắc sẽ ko có vấn đề gì. Trên trang web có hướng dẫn cơ bản nha Have fun learning!",,,,,
Xin chào mọi người ạ. Em mới tập làm quen về machine learning. Em đã train xong model mà không biết nó có bị overfit hay không... Mong mọi người có thể nhận xét model của em ạ. Cám ơn mọi người đã giúp đỡ .,Xin chào mọi người ạ. Em mới tập làm quen về machine learning. Em đã train xong model mà không biết nó có bị overfit hay không... Mong mọi người có thể nhận xét model của em ạ. Cám ơn mọi người đã giúp đỡ .,,,,,
"Chào mọi người, em có một thắc mắc như thế này nhờ mọi người giúp ạ.
Em có một tấm hình con chó như bên dưới, khi đưa nó qua một embedding model thì output của nó là một embedding vector. Em thắc mắc một điều là có thể dùng embedding vector đó để phục hồi lại ảnh gốc bằng cách tính toán ngược lại không?
Em xin cảm ơn ạ","Chào mọi người, em có một thắc mắc như thế này nhờ mọi người giúp ạ. Em có một tấm hình con chó như bên dưới, khi đưa nó qua một embedding model thì output của nó là một embedding vector. Em thắc mắc một điều là có thể dùng embedding vector đó để phục hồi lại ảnh gốc bằng cách tính toán ngược lại không? Em xin cảm ơn ạ",,,,,
it is so true,it is so true,,,,,
"Em chào mọi người,
Hiện tại em đang làm 1 cái App theo dõi bệnh nhân tiểu đường, nhập số liệu đường huyết, thức ăn, hoạt động mỗi ngày, dữ liệu đủ lớn trên 7 ngày, em muốn tích hợp dự đoán chỉ số đường huyết của ngày tiếp theo. Mọi người có giải pháp gì giúp em. Cảm ơn mọi người! 🥰","Em chào mọi người, Hiện tại em đang làm 1 cái App theo dõi bệnh nhân tiểu đường, nhập số liệu đường huyết, thức ăn, hoạt động mỗi ngày, dữ liệu đủ lớn trên 7 ngày, em muốn tích hợp dự đoán chỉ số đường huyết của ngày tiếp theo. Mọi người có giải pháp gì giúp em. Cảm ơn mọi người!",,,,,
,nan,,,,,
"xin chào
Mọi người biết lỗi này sửa sao ko ạ, t search Google cũng ko fix dc , (ảnh dưới ghi thông tin keras và tensforflow t đang dùng)","xin chào Mọi người biết lỗi này sửa sao ko ạ, t search Google cũng ko fix dc , (ảnh dưới ghi thông tin keras và tensforflow t đang dùng)",,,,,
"[Deep Learning Project]
Một số project của các bạn học sinh khóa Deep Learning.
1. Giải hệ phương trình bằng ảnh: Input là ảnh chứa hệ phương trình, output sẽ là nghiệm của hệ phương trình.
2. Trả lời thông tin trong ảnh: Input là ảnh và câu hỏi liên quan tới nội dung bức ảnh, output là thông tin trả lời từ bức ảnh. Ví dụ: input ảnh 1 đội bóng, hỏi có bao nhiêu cầu thủ, thì sẽ trả lời được số lượng cầu thủ trong ảnh.
3. Tóm tắt văn bản: Dữ liệu lấy từ nội dung bình luận, nhận xét của người dùng trên amazon.","[Deep Learning Project] Một số project của các bạn học sinh khóa Deep Learning. 1. Giải hệ phương trình bằng ảnh: Input là ảnh chứa hệ phương trình, output sẽ là nghiệm của hệ phương trình. 2. Trả lời thông tin trong ảnh: Input là ảnh và câu hỏi liên quan tới nội dung bức ảnh, output là thông tin trả lời từ bức ảnh. Ví dụ: input ảnh 1 đội bóng, hỏi có bao nhiêu cầu thủ, thì sẽ trả lời được số lượng cầu thủ trong ảnh. 3. Tóm tắt văn bản: Dữ liệu lấy từ nội dung bình luận, nhận xét của người dùng trên amazon.",,,,,
"[Trực tiếp] HỢP TÁC QUỐC TẾ TRONG PHÁT TRIỂN Y HỌC CHÍNH XÁC - RA MẮT HỆ THỐNG PHÂN TÍCH DỮ LIỆU Y SINH LỚN NHẤT VIỆT NAM và TỌA ĐÀM: DỮ LIỆU LỚN TRONG Y HỌC - CƠ CHẾ QUẢN LÝ, KẾT NỐI VÀ CHIA SẺ
#bigdata #precisionmedicine","[Trực tiếp] HỢP TÁC QUỐC TẾ TRONG PHÁT TRIỂN Y HỌC CHÍNH XÁC - RA MẮT HỆ THỐNG PHÂN TÍCH DỮ LIỆU Y SINH LỚN NHẤT VIỆT NAM và TỌA ĐÀM: DỮ LIỆU LỚN TRONG Y HỌC - CƠ CHẾ QUẢN LÝ, KẾT NỐI VÀ CHIA SẺ",#bigdata	#precisionmedicine,,,,
"#YoloV4 
Chào mọi người,
Mình đang train YoloV4 cho dataset 200 ảnh tự label, dùng darknet framework ở đây: https://github.com/AlexeyAB/darknet
Vấn đề mình gặp phải là: 
Sau 350 epoches, loss của mình ở khoảng 1.6 (ảnh 1), code báo là ""performance bottleneck on CPU"" (do chưa được cty cấp GPU nên mình phải train tạm trên CPU xem sao).
Sang epoch 351, loss tăng lên 2.4 (ảnh 2). Và ở epoch này learning_rate tăng từ 1*10^-6 lên 2*10^-6.
Sang epoch 352, loss đột nhiên bị -nan (ảnh 3).
Những thông số chính trong file config mình set như sau:
batch=8
width=608
height=608
channels=3
momentum=0.949
decay=0.0005
learning_rate=0.0001
burn_in=1000
max_batches = 6000
policy=steps
steps=4800,5400

Mình có đọc hướng dẫn trong link github trên, tác giả nói là nếu bị NaN thì giảm learning rate. Nhưng mình thấy learning rate = 2*10^-6 là đã rất nhỏ rồi.

Không biết có ai từng gặp phải trường hợp này ko, có thể gợi ý hướng giải quyết cho mình dc ko? Mình cám ơn nhiều :D ","Chào mọi người, Mình đang train YoloV4 cho dataset 200 ảnh tự label, dùng darknet framework ở đây: https://github.com/AlexeyAB/darknet Vấn đề mình gặp phải là: Sau 350 epoches, loss của mình ở khoảng 1.6 (ảnh 1), code báo là ""performance bottleneck on CPU"" (do chưa được cty cấp GPU nên mình phải train tạm trên CPU xem sao). Sang epoch 351, loss tăng lên 2.4 (ảnh 2). Và ở epoch này learning_rate tăng từ 1*10^-6 lên 2*10^-6. Sang epoch 352, loss đột nhiên bị -nan (ảnh 3). Những thông số chính trong file config mình set như sau: batch=8 width=608 height=608 channels=3 momentum=0.949 decay=0.0005 learning_rate=0.0001 burn_in=1000 max_batches = 6000 policy=steps steps=4800,5400 Mình có đọc hướng dẫn trong link github trên, tác giả nói là nếu bị NaN thì giảm learning rate. Nhưng mình thấy learning rate = 2*10^-6 là đã rất nhỏ rồi. Không biết có ai từng gặp phải trường hợp này ko, có thể gợi ý hướng giải quyết cho mình dc ko? Mình cám ơn nhiều :D",#YoloV4,,,,
"Hi mọi người,
Mình đang chạy thử 3 phương pháp cho bài toán Instance Segmentation gồm: DetectoRS (A); Cascade Mask R-CNN (B); Mask R-CNN (C) trên ResNet-50.
Theo paper đã test trên COCO dataset thì độ chính xác của A>B>C.
Tuy nhiên khi mình thử trên 3-4 ảnh ngẫu nhiên thì đa phần A kém hơn so với B, C. Một thí dụ dưới đây mô tả độ chính xác của A<B<C, dễ dàng quan sát được bằng mắt.
Mọi người lý giải giúp tại sao lại như vậy nhỉ? 😒","Hi mọi người, Mình đang chạy thử 3 phương pháp cho bài toán Instance Segmentation gồm: DetectoRS (A); Cascade Mask R-CNN (B); Mask R-CNN (C) trên ResNet-50. Theo paper đã test trên COCO dataset thì độ chính xác của A>B>C. Tuy nhiên khi mình thử trên 3-4 ảnh ngẫu nhiên thì đa phần A kém hơn so với B, C. Một thí dụ dưới đây mô tả độ chính xác của A<B<C, dễ dàng quan sát được bằng mắt. Mọi người lý giải giúp tại sao lại như vậy nhỉ?",,,,,
"Em xin chào mọi người ạ.
Hiện tại em muốn chạy classifier bằng machine learning với C++, em có xem qua và thấy OpenCV, Dlib và LibSVM là có hỗ trợ trên C++. Nhưng OpenCV em test thử không ra được confidence( và kết quả margin em thấy có vẻ không ổn), Dlib thì xung đột với Libtorch.
Hiện tại em đang thiên về LibSVM nhưng source code train và predict quá dài, em mong mọi người có ai từng làm về LibSVM có thể share em code tiền xử lý dữ liệu và xử lý trực tiếp mà không phải qua file data & test hoặc có một hướng nào khác giúp em classifier dữ liệu và in ra được confidence không ạ.
Em xin chân thành cảm ơn ạ.","Em xin chào mọi người ạ. Hiện tại em muốn chạy classifier bằng machine learning với C++, em có xem qua và thấy OpenCV, Dlib và LibSVM là có hỗ trợ trên C++. Nhưng OpenCV em test thử không ra được confidence( và kết quả margin em thấy có vẻ không ổn), Dlib thì xung đột với Libtorch. Hiện tại em đang thiên về LibSVM nhưng source code train và predict quá dài, em mong mọi người có ai từng làm về LibSVM có thể share em code tiền xử lý dữ liệu và xử lý trực tiếp mà không phải qua file data & test hoặc có một hướng nào khác giúp em classifier dữ liệu và in ra được confidence không ạ. Em xin chân thành cảm ơn ạ.",,,,,
"Dependency Parser
Xin chào nhóm, mình đang làm việc trên cây phụ thuộc để trích xuất thông tin của một câu tiếng viêt.
Vấn đề là có khá nhiều từ viết tăt như: nsubj, case, nmod, ... Mình muốn hiểu ngữ nghĩa, cũng như các từ viết tắt đó theo tiếng việt thì mình có thể tìm tài liệu ở đâu được.
Mong mọi người tư vấn.
Xin cảm ơn.","Dependency Parser Xin chào nhóm, mình đang làm việc trên cây phụ thuộc để trích xuất thông tin của một câu tiếng viêt. Vấn đề là có khá nhiều từ viết tăt như: nsubj, case, nmod, ... Mình muốn hiểu ngữ nghĩa, cũng như các từ viết tắt đó theo tiếng việt thì mình có thể tìm tài liệu ở đâu được. Mong mọi người tư vấn. Xin cảm ơn.",,,,,
,nan,,,,,
"Chào mọi người, hiện tại mình đang thực hiện truy vấn ảnh (Image Retrieval) và đánh giá hiệu quả của thuật toán truy vấn. Mình tham khảo nhiều bài báo thì người ta đánh giá độ hiệu qủa của Image Retrieval thông qua Precision - Recall. Tuy nhiên mình đang gặp vướng mắc ở việc tính 
Recall  = (No. of relevant documents retrieved) / (No. of total relevant documents)
Mình thực hiện đánh giá trên một dataset có 5 categories, tuy nhiên mỗi category là do mình Crawl về với số lượng từng 1000~2000 ảnh/category, vì là dữ liệu crawl nên có nhiều ảnh dù trong Category 1 nhưng không hẳn nội dung của ảnh đã liên quan đến nó. nên việc tính Recall mình không biết là cứ nên tính dựa trên tổng số lượng ảnh của category đó hay phải kiểm tra lại từng ảnh để xác định ảnh nào mới thật sự liên quan đến ảnh truy vấn? Cảm ơn mọi người ạ 🙂","Chào mọi người, hiện tại mình đang thực hiện truy vấn ảnh (Image Retrieval) và đánh giá hiệu quả của thuật toán truy vấn. Mình tham khảo nhiều bài báo thì người ta đánh giá độ hiệu qủa của Image Retrieval thông qua Precision - Recall. Tuy nhiên mình đang gặp vướng mắc ở việc tính Recall = (No. of relevant documents retrieved) / (No. of total relevant documents) Mình thực hiện đánh giá trên một dataset có 5 categories, tuy nhiên mỗi category là do mình Crawl về với số lượng từng 1000~2000 ảnh/category, vì là dữ liệu crawl nên có nhiều ảnh dù trong Category 1 nhưng không hẳn nội dung của ảnh đã liên quan đến nó. nên việc tính Recall mình không biết là cứ nên tính dựa trên tổng số lượng ảnh của category đó hay phải kiểm tra lại từng ảnh để xác định ảnh nào mới thật sự liên quan đến ảnh truy vấn? Cảm ơn mọi người ạ",,,,,
"Mọi người cho mình hỏi, Với việc embeeding word bằng Tesorflow.keras.layers.Embeeding:  tf.keras.layers.Embedding thì Keras dùng methods gì ạ: Word2vect hay Glove. Mình cảm ơn rất nhiều","Mọi người cho mình hỏi, Với việc embeeding word bằng Tesorflow.keras.layers.Embeeding: tf.keras.layers.Embedding thì Keras dùng methods gì ạ: Word2vect hay Glove. Mình cảm ơn rất nhiều",,,,,
Giải thưởng nhỏ nhưng dữ liệu khá lớn (1M samples):,Giải thưởng nhỏ nhưng dữ liệu khá lớn (1M samples):,,,,,
"Mình đang phaỉ làm bài tập này nhưng đang lúng túng trong việc tính và update back propagation. Có bạn nào từng làm qua bài tập dạng này chưa, cho mình hỏi với","Mình đang phaỉ làm bài tập này nhưng đang lúng túng trong việc tính và update back propagation. Có bạn nào từng làm qua bài tập dạng này chưa, cho mình hỏi với",,,,,
"Chào mọi người,
Em đang làm project về object detection, các anh/em nào có làm qua cho e xin ý kiến với:
em có:
+ 10 camera IP ở 3 văn phòng khác nhau
+ 1 con server card 3090 24GB/10496 cuda cores, ram 64 GB - max 256 GB
+ mô hình nhận dạng object detection đã train và test với từng camera ở mạng local
Hiện em muốn stream từ 3 văn phòng đó về server em qua internet nhưng em không biết làm cách nào để stream về server rồi dùng opencv để đọc luồng stream về này.
Các anh chị nào làm rồi cho em hỏi làm sao để stream về server qua internet với ạ, với stream như vậy thì 1 con server như của em thì có thể chịu được bao nhiêu luồng cho mô hình object detection, stracking with deepsort ạ?
Cám ơn mọi người nhiều.","Chào mọi người, Em đang làm project về object detection, các anh/em nào có làm qua cho e xin ý kiến với: em có: + 10 camera IP ở 3 văn phòng khác nhau + 1 con server card 3090 24GB/10496 cuda cores, ram 64 GB - max 256 GB + mô hình nhận dạng object detection đã train và test với từng camera ở mạng local Hiện em muốn stream từ 3 văn phòng đó về server em qua internet nhưng em không biết làm cách nào để stream về server rồi dùng opencv để đọc luồng stream về này. Các anh chị nào làm rồi cho em hỏi làm sao để stream về server qua internet với ạ, với stream như vậy thì 1 con server như của em thì có thể chịu được bao nhiêu luồng cho mô hình object detection, stracking with deepsort ạ? Cám ơn mọi người nhiều.",,,,,
"🎯Kính chào các bác. Sau khi tìm hiểu về về Transformer, hôm nay em mạnh dạn tìm hiểu BERT vàì thấy BERT là một SOTA mới nổi trong làng NLP. Em thấy việc ứng dụng BERT sẽ làm cho model của các bạn tăng chất lượng và giảm thời gian xây dựng đáng kể.
Hi vọng giúp được các bạn newbie mới học phần nào!
Các cao thủ mong correct giúp em và mong ad duyệt bài!","Kính chào các bác. Sau khi tìm hiểu về về Transformer, hôm nay em mạnh dạn tìm hiểu BERT vàì thấy BERT là một SOTA mới nổi trong làng NLP. Em thấy việc ứng dụng BERT sẽ làm cho model của các bạn tăng chất lượng và giảm thời gian xây dựng đáng kể. Hi vọng giúp được các bạn newbie mới học phần nào! Các cao thủ mong correct giúp em và mong ad duyệt bài!",,,,,
"[PCA] Chào mọi người, mình mới chập chững học về data và đang bí hướng đi cho PCA. Đề bài đc giao là xử dụng PCA cho water level. Dạo một vòng trên google thì chẳng có ví dụ nào na ná như của mình. Mọi người có thể cho mình xin vài ý tưởng/hướng đi được không ạ. Chân thành cảm ơn :)","[PCA] Chào mọi người, mình mới chập chững học về data và đang bí hướng đi cho PCA. Đề bài đc giao là xử dụng PCA cho water level. Dạo một vòng trên google thì chẳng có ví dụ nào na ná như của mình. Mọi người có thể cho mình xin vài ý tưởng/hướng đi được không ạ. Chân thành cảm ơn :)",,,,,
"Học sâu (Deep learning), hành trình 70 năm đi tìm chân ái
(Bài mang tính chất giải trí là chính)
Hôm nay tranh thủ cậu con trai về quê với ông bà nội, có thời gian thảnh thơi đầu óc ngẫm nghĩ nhân tình thế thái. Lượn qua lượn lại trên các diễn đàn, hội nhóm thấy người ta nói nhiều về Deep Learning, đến cả đứa bạn mình học kinh tế cũng hỏi học code AI, Deep Learning .... Cao hứng chia sẻ ít hiểu biết bản thân về cái thứ “so deep” đang làm mưa làm gió khắp các mặt trận này.
Cái gì cũng có điểm bắt đầu, đối với Deep Learning điểm khởi thủy phù hợp nhất tôi nghĩ có lẽ là vào những năm 1950, ở một lĩnh vực không liên quan đến AI: Sinh học.
Người thợ xây đặt viên gạch đầu tiên này là David H. Hubel, ở một thí nghiệm không lấy gì làm liên quan. Ông và đồng nghiệp của mình (Wiesel) đặt những điện cực nhỏ vào trong phần vỏ não xử lý thị giác của những con mèo đã được gây mê, họ chiếu các kiểu tia sáng khác nhau lên một màn hình trước con mèo và nhận thấy một số neuron nhất định phản ứng nhanh hơn với một vài kiểu sáng này, trong khi các neuron khác phản nhanh hơn ở kiểu sáng khác. Tức mỗi neuron có 1 trạng thái kích thích khác nhau (weight) trước cùng một đối tượng . Từ đó mở ra một chương mới cho lịch sử nghiên cứu hoạt động của neuron thần kinh con người. Nhờ những đóng góp đó, năm 1981 ông được nhận giải Nobel về sinh học và y học (đồng nhân giải với Wiesel).
Nhưng có nằm mơ Hubel cũng không tưởng tượng ra được tác động lớn lao của những nghiên cứu của ông trên lên lĩnh vực AI những năm sau đó.
Vào một ngày đẹp trời mùa thu năm 1958, Frank Rosenblatt nhâm nhi một tách cafe trong phòng lab Naval Research, ông nhìn ra dòng kênh xanh ngát phía cửa sổ nghĩ miên man về cách làm cho máy tính trở nên hữu dụng hơn, giải được nhiều bài toán hơn và đặc biệt là có thể suy nghĩ như con người. Ông tình cờ đọc được nghiên cứu của Hubel về cơ chế vận hành của các neuron thần kinh, về cách chúng phản ứng nhiều ít khác nhau với cùng 1 kích thích.
Ông lẩm nhẩm: nếu mỗi neuron là 1 biến, và mức phản ứng là giá trị cao thấp của biến đó thì như nào nhỉ, có phải ...
Như sét đánh ngang tai!
Ông vỗ đùi cái rộp, rồi chạy ngay xuống phòng lab nơi chiếc máy tính IBM 704 nặng 5 tấn đang chạy các phép toán, tiếng ồn phát ra cỡ ngang lúc bạn đứng ở ngã tư Lê Văn Lương giờ tan tầm.
Vừa chạy vừa hô lớn: “Đù, tìm ra rồi! tìm ra rồi” (hoặc cũng có thể lịch sự hơn)
Rosenblatt đã tạo ra được hệ thống mô phỏng lại hoạt động của neuron não bộ (neural network) đầu tiên đầu tiên chỉ gồm 2 layer (input, output), đầu vào là ma trận 20x20 chứa pixel hình ảnh con vật, đầu ra chỉ có 2 node (2 class) phân biệt đó là chó hay mèo.
Cũng như trending AI hiện giờ, khi có 1 khám phá mới, báo chí sẽ viết bài rần rần. Trên tờ New York Time lúc đó có đưa bài trên trang nhất “NEW NAVY DEVICE LEARNS BY DOING: Psychologist Shows Embryo of Computer Designed to Read and Grow Wiser”. Đại khái dịch là những nhà tâm lý học đã làm cho máy tính có thể đọc, và trở nên thông minh hơn. Những phóng viên nhiệt tình không quên “chém”:  “Đối thủ nặng kỳ đầu tiên của não bộ con người đã được tạo ra”, rằng có thể chúng ta sắp mô phỏng lại được bộ não của con người, rằng máy tính sắp suy nghĩ, làm việc, giải quyết vấn đề như con người bla bla ... Sau hơn nửa thế kỷ điều đó vẫn chưa đến, thế mới thấy sự lạc quan của con người là vô hạn. May mà họ chưa lạc quan đến mức lo ngại về việc bị máy tính lấy mất việc làm của con người như chúng ta hiện nay.
Thời gian chứng minh chương trình của Rosenblatt là chưa hoàn hảo, vẫn nhiều dự đoán sai dù traning nhiều lần như nào chăng nữa. Mọi thứ lại đi vào bế tắc. Báo chí lại tiếp tục chuyển chủ đề sang cướp, giết, hiếp như thường lệ.
Phải đến năm 1960, khi Marvin Minsky nhà khoa học máy tính đến từ MIT trong cuốn sách mang tên “Perceptrons” của mình chứng minh bằng toán học rằng 2 layer là không đủ, chúng ta cần nhiều hơn thế. Đến nay đó vẫn được coi là một cuốn sách quan trọng trong lịch sử AI. Nhưng với trực giác thiên tài của mình, ông cũng không quên cảnh báo: “Không phải cứ cho càng nhiều node là ta giải quyết được mọi vấn đề đâu bro”. Đến tận ngày nay, sau 60 năm cuốn sách ra đời, những hậu bối chúng ta vẫn hay vướng phải lỗi đó. Mỗi khi model neuron network cho tỉ lệ chính xác thấp, việc đầu tiên chúng ta hay nghĩ đến là “add more node, more layer” (hay là chỉ mình tôi mắc lỗi tư duy đó?)
Trong suốt giai đoạn 1960-2000, neuron network chứng kiến sự hưng thịnh và thoái trào dần dần. Sự nhiệt tình nào rồi cũng lúc nguội lạnh khi đường đi ngày một tối.
Người kiên trì theo đuổi Neuron Network nhất có thể kể đến một vài cái tên như Yoshua Bengio, Geoff Hinton mà không thể không nhắc đến Yann Lecun.
Người đời nói rằng “tích cực quay tay, vận may sẽ đến”. Tôi phải khâm phục sự kiên trì của ông trong quá trình theo đuổi Neuron Network. Chúng ta phải trở lại bối cảnh giai đoạn năm 1990-2010. Lúc này Neuron Network ít nhiều thoái trào, nhường chỗ cho những phương pháp Marchine Learning thời thượng khác. Tiền ngân sách (funding) về Neuron Network thì khó xin như đi lên trời, các Shark quan tâm nhiều đến các hướng đi mới, đồng nghiệp, rồi nghiên cứu sinh dưới quyền ai cũng uể oải, chán chường rồi lũ lượt rủ nhau bỏ cuộc chơi.
Ấy vậy mà Yan Lecun vẫn không bỏ cuộc, không gì ngăn được sự nhiệt tình của tuổi trẻ. Năm  năm 1990 ông cùng đồng nghiệp ra được model nhận diện chữ viết tay có tên LeNet (đầu vào là ma trận 16x16 pixcel) cho độ chính xác khá cao, tạo được ít nhiều tiếng vang. Nhiều người thế hệ đó lại được truyền cảm hứng quay trở lại với neuron network. Ứng dụng rõ nhất thời điểm đó là trong các ngân hàng để nhận diện chữ viết tay. Ước lượng có khoảng 10% giao dịch bằng xéc (check) trên đất Mỹ được nhận diện qua hệ thống nhân diện tự động. Nhưng ấy là khi SVM chưa ra đời!
Năm 1992, Vapnik đồng nghiệp của Yan Lecun (phòng thí nghiệm AT&Bell), dìm Neuron Network chìm sâu vào vùng quên lãng bằng việc tạo ra thuật toán SVM (Support Vector Machine) cho độ chính xác hơn nhiều Neuron Network trong rất nhiều bài đặc biệt là nhận dạng chữ viết, phân loại ảnh, NLP (xử lý ngôn ngữ tự nhiên). Người ta ăn SVM, ngủ SVM, cà phê bàn luận về SVM. Tiền lại có nơi để đổ, báo chí lại được dịp ra bài lia lịa.
Giai đoạn này nghĩ cũng tội ông Yan Lecun, vẫn ra bài đều tầm 5-10 bài 1 năm nhưng quá nổi bật, không nhiều người  “like” (cited theo ngôn ngữ khoa học), mãi sau tên tuổi nổi như cồn thì lại thi nhau cite lại cả các bài cổ, người đời vốn vậy.
Năm 2012 diễn ra một biến cố quan trọng trong lĩnh vực AI, đặt nền móng cho Deep Learning ngày nay cũng như đưa Yan Lecun trở thành người mà ai cũng biết. Người đóng góp cho sự kiện này là thầy của Yan Lecun: Geoffrey Hinton. Nhưng hãy chờ, nói một chút về GPU đã.
Một trong những thách thức lớn nhất của mạng neuron (neural network) là việc tính toán rất nặng, thêm một node hay 1 layer vào thôi là phải thêm cả trăm, cả nghìn phép toán, cứ thế cấp số nhân lên. CPU thời bấy giờ vẫn cùi bắp, nhiều nhà khoa học thời bấy giờ chia sẻ, để training một mạng neuron cho tử tế có khi mất cả tuần, hoặc thậm chí tính bằng tháng. Loay hoay điều chỉnh tham số, chạy vài turn training như vậy không khéo lỡ toi mất kỳ báo báo.
GPU ra đời ban đầu với mục đích dung tục hơn là phục vụ các máy chơi game và xử lý đồ họa. Nhưng dần dà người ta phát hiện ra cách tính toán của GPU cực kỳ phù hợp với việc nhân chập ma trận (việc mà neuron network thường xuyên phải xử lý). Ấy thế là lại tìm ra ánh sáng cuối đường hầm. Nhờ GPU, giờ đây ta đã có thể training những mạng neuron rất lớn mà thời gian chờ không quá lâu.
Chuyện gì đến sẽ đến!
Từ những năm 2010, đội ngũ Hinton đã bắt đầu ứng dụng GPU vào việc tính toán neuron network, nhờ đó có thể huấn luyện được những mạng cỡ lớn. Năm 2012, team Hinton đạt được thành tựu lớn với việc phân loại ảnh trên data set ImageNet. Với sức mạnh của GPU, và tất nhiên cả “mạnh vì gạo, bạo vì tiền”, ông cùng đội nghiên cứu của mình (không có Yan Lecun) tiến hành traning (huấn luyện) trên network gồm 60 triệu tham số, 650.000 neuron, thực hiện trên training data set (dữ liệu huấn luyện) 1.2 triệu hình ảnh, 1000 class (thư mục hình ảnh),  test trên 150.000 hình ảnh với độ chính xác lên tơi 84%, gây xốc nặng giới computer science lúc bấy giờ (75% đã là cao hết tầm rồi). Một điều tôi rất nể Geoffrey Hinton ở chỗ, ông không quên cite bài của học trò của mình năm 1990 về nhận dạng chữ viết tay, nói rằng công trình của ông kế thừa nhiều từ bài báo đó. Yan Lecun lên hương, Geoffrey Hinton cũng hưởng đủ (thành lập công ty riêng về AI). Người quen hơn nhau là ở chỗ đó.
Từ những công trình của Geoffrey Hinton, hàng loạt net mới ra đời, với nhiều tham số hơn, nhiều layer hơn, data set khủng hơn, kỷ lục liên tục bị xô đổ. Deep Learning dần trở thành cuộc chơi của những đội lắm tiền, lắm gạo. Những diễn biến gần đây chắc ai cũng cập nhật rồi nên tôi ko nhắc lại nữa.
Google cũng không nằm ngoài trending này, DeepMind được google mua lại với giá 500 triệu đô trump năm 2014. Từ đây AlphaStar, AlphaGo, AlphaFold được ra đời và nổi tiếng toàn thế giới. Chỉ tháng trước thôi, AlphaFold đã dự đoán rất chính xác cấu trúc cuộn 3D của protein, viết nên lịch sử ngành sinh học phân tử.
Một con đường dài còn ở phía trước, phong trào nghiên cứu Deep Learning đang nổi lên rất mạnh trên toàn thế giới, trong đó người trẻ Việt Nam cũng tham gia rất tích cực. Ơ mây zing, gút chóp my friends.
Chém gió cho vui thế thôi, giờ phải đi thay bỉm cho con đã. Hẹn gặp mọi người gần đây nhất.

Edit: Sau comment của bạn Phong Thanh Dương, mình xin sửa lại thời điểm team Hinton làm bắt đầu ứng dụng GPU traning từ giai đoạn 2010, ra báo cáo (paper) nổi danh vào năm 2012. Xin lỗi các bạn vì sai mốc thời gian.

Dương Thịnh

Nguồn tham khảo:
https://en.wikipedia.org/wiki/David_H._Hubel
https://en.wikipedia.org/wiki/Frank_Rosenblatt
https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon
https://en.wikipedia.org/wiki/Yann_LeCun
https://en.wikipedia.org/wiki/Geoffrey_Hinton
Handwritten digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems (1990)
https://en.wikipedia.org/wiki/Support_vector_machine
https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
https://www.theguardian.com/technology/2020/nov/30/deepmind-ai-cracks-50-year-old-problem-of-biology-research","Học sâu (Deep learning), hành trình 70 năm đi tìm chân ái (Bài mang tính chất giải trí là chính) Hôm nay tranh thủ cậu con trai về quê với ông bà nội, có thời gian thảnh thơi đầu óc ngẫm nghĩ nhân tình thế thái. Lượn qua lượn lại trên các diễn đàn, hội nhóm thấy người ta nói nhiều về Deep Learning, đến cả đứa bạn mình học kinh tế cũng hỏi học code AI, Deep Learning .... Cao hứng chia sẻ ít hiểu biết bản thân về cái thứ “so deep” đang làm mưa làm gió khắp các mặt trận này. Cái gì cũng có điểm bắt đầu, đối với Deep Learning điểm khởi thủy phù hợp nhất tôi nghĩ có lẽ là vào những năm 1950, ở một lĩnh vực không liên quan đến AI: Sinh học. Người thợ xây đặt viên gạch đầu tiên này là David H. Hubel, ở một thí nghiệm không lấy gì làm liên quan. Ông và đồng nghiệp của mình (Wiesel) đặt những điện cực nhỏ vào trong phần vỏ não xử lý thị giác của những con mèo đã được gây mê, họ chiếu các kiểu tia sáng khác nhau lên một màn hình trước con mèo và nhận thấy một số neuron nhất định phản ứng nhanh hơn với một vài kiểu sáng này, trong khi các neuron khác phản nhanh hơn ở kiểu sáng khác. Tức mỗi neuron có 1 trạng thái kích thích khác nhau (weight) trước cùng một đối tượng . Từ đó mở ra một chương mới cho lịch sử nghiên cứu hoạt động của neuron thần kinh con người. Nhờ những đóng góp đó, năm 1981 ông được nhận giải Nobel về sinh học và y học (đồng nhân giải với Wiesel). Nhưng có nằm mơ Hubel cũng không tưởng tượng ra được tác động lớn lao của những nghiên cứu của ông trên lên lĩnh vực AI những năm sau đó. Vào một ngày đẹp trời mùa thu năm 1958, Frank Rosenblatt nhâm nhi một tách cafe trong phòng lab Naval Research, ông nhìn ra dòng kênh xanh ngát phía cửa sổ nghĩ miên man về cách làm cho máy tính trở nên hữu dụng hơn, giải được nhiều bài toán hơn và đặc biệt là có thể suy nghĩ như con người. Ông tình cờ đọc được nghiên cứu của Hubel về cơ chế vận hành của các neuron thần kinh, về cách chúng phản ứng nhiều ít khác nhau với cùng 1 kích thích. Ông lẩm nhẩm: nếu mỗi neuron là 1 biến, và mức phản ứng là giá trị cao thấp của biến đó thì như nào nhỉ, có phải ... Như sét đánh ngang tai! Ông vỗ đùi cái rộp, rồi chạy ngay xuống phòng lab nơi chiếc máy tính IBM 704 nặng 5 tấn đang chạy các phép toán, tiếng ồn phát ra cỡ ngang lúc bạn đứng ở ngã tư Lê Văn Lương giờ tan tầm. Vừa chạy vừa hô lớn: “Đù, tìm ra rồi! tìm ra rồi” (hoặc cũng có thể lịch sự hơn) Rosenblatt đã tạo ra được hệ thống mô phỏng lại hoạt động của neuron não bộ (neural network) đầu tiên đầu tiên chỉ gồm 2 layer (input, output), đầu vào là ma trận 20x20 chứa pixel hình ảnh con vật, đầu ra chỉ có 2 node (2 class) phân biệt đó là chó hay mèo. Cũng như trending AI hiện giờ, khi có 1 khám phá mới, báo chí sẽ viết bài rần rần. Trên tờ New York Time lúc đó có đưa bài trên trang nhất “NEW NAVY DEVICE LEARNS BY DOING: Psychologist Shows Embryo of Computer Designed to Read and Grow Wiser”. Đại khái dịch là những nhà tâm lý học đã làm cho máy tính có thể đọc, và trở nên thông minh hơn. Những phóng viên nhiệt tình không quên “chém”: “Đối thủ nặng kỳ đầu tiên của não bộ con người đã được tạo ra”, rằng có thể chúng ta sắp mô phỏng lại được bộ não của con người, rằng máy tính sắp suy nghĩ, làm việc, giải quyết vấn đề như con người bla bla ... Sau hơn nửa thế kỷ điều đó vẫn chưa đến, thế mới thấy sự lạc quan của con người là vô hạn. May mà họ chưa lạc quan đến mức lo ngại về việc bị máy tính lấy mất việc làm của con người như chúng ta hiện nay. Thời gian chứng minh chương trình của Rosenblatt là chưa hoàn hảo, vẫn nhiều dự đoán sai dù traning nhiều lần như nào chăng nữa. Mọi thứ lại đi vào bế tắc. Báo chí lại tiếp tục chuyển chủ đề sang cướp, giết, hiếp như thường lệ. Phải đến năm 1960, khi Marvin Minsky nhà khoa học máy tính đến từ MIT trong cuốn sách mang tên “Perceptrons” của mình chứng minh bằng toán học rằng 2 layer là không đủ, chúng ta cần nhiều hơn thế. Đến nay đó vẫn được coi là một cuốn sách quan trọng trong lịch sử AI. Nhưng với trực giác thiên tài của mình, ông cũng không quên cảnh báo: “Không phải cứ cho càng nhiều node là ta giải quyết được mọi vấn đề đâu bro”. Đến tận ngày nay, sau 60 năm cuốn sách ra đời, những hậu bối chúng ta vẫn hay vướng phải lỗi đó. Mỗi khi model neuron network cho tỉ lệ chính xác thấp, việc đầu tiên chúng ta hay nghĩ đến là “add more node, more layer” (hay là chỉ mình tôi mắc lỗi tư duy đó?) Trong suốt giai đoạn 1960-2000, neuron network chứng kiến sự hưng thịnh và thoái trào dần dần. Sự nhiệt tình nào rồi cũng lúc nguội lạnh khi đường đi ngày một tối. Người kiên trì theo đuổi Neuron Network nhất có thể kể đến một vài cái tên như Yoshua Bengio, Geoff Hinton mà không thể không nhắc đến Yann Lecun. Người đời nói rằng “tích cực quay tay, vận may sẽ đến”. Tôi phải khâm phục sự kiên trì của ông trong quá trình theo đuổi Neuron Network. Chúng ta phải trở lại bối cảnh giai đoạn năm 1990-2010. Lúc này Neuron Network ít nhiều thoái trào, nhường chỗ cho những phương pháp Marchine Learning thời thượng khác. Tiền ngân sách (funding) về Neuron Network thì khó xin như đi lên trời, các Shark quan tâm nhiều đến các hướng đi mới, đồng nghiệp, rồi nghiên cứu sinh dưới quyền ai cũng uể oải, chán chường rồi lũ lượt rủ nhau bỏ cuộc chơi. Ấy vậy mà Yan Lecun vẫn không bỏ cuộc, không gì ngăn được sự nhiệt tình của tuổi trẻ. Năm năm 1990 ông cùng đồng nghiệp ra được model nhận diện chữ viết tay có tên LeNet (đầu vào là ma trận 16x16 pixcel) cho độ chính xác khá cao, tạo được ít nhiều tiếng vang. Nhiều người thế hệ đó lại được truyền cảm hứng quay trở lại với neuron network. Ứng dụng rõ nhất thời điểm đó là trong các ngân hàng để nhận diện chữ viết tay. Ước lượng có khoảng 10% giao dịch bằng xéc (check) trên đất Mỹ được nhận diện qua hệ thống nhân diện tự động. Nhưng ấy là khi SVM chưa ra đời! Năm 1992, Vapnik đồng nghiệp của Yan Lecun (phòng thí nghiệm AT&Bell), dìm Neuron Network chìm sâu vào vùng quên lãng bằng việc tạo ra thuật toán SVM (Support Vector Machine) cho độ chính xác hơn nhiều Neuron Network trong rất nhiều bài đặc biệt là nhận dạng chữ viết, phân loại ảnh, NLP (xử lý ngôn ngữ tự nhiên). Người ta ăn SVM, ngủ SVM, cà phê bàn luận về SVM. Tiền lại có nơi để đổ, báo chí lại được dịp ra bài lia lịa. Giai đoạn này nghĩ cũng tội ông Yan Lecun, vẫn ra bài đều tầm 5-10 bài 1 năm nhưng quá nổi bật, không nhiều người “like” (cited theo ngôn ngữ khoa học), mãi sau tên tuổi nổi như cồn thì lại thi nhau cite lại cả các bài cổ, người đời vốn vậy. Năm 2012 diễn ra một biến cố quan trọng trong lĩnh vực AI, đặt nền móng cho Deep Learning ngày nay cũng như đưa Yan Lecun trở thành người mà ai cũng biết. Người đóng góp cho sự kiện này là thầy của Yan Lecun: Geoffrey Hinton. Nhưng hãy chờ, nói một chút về GPU đã. Một trong những thách thức lớn nhất của mạng neuron (neural network) là việc tính toán rất nặng, thêm một node hay 1 layer vào thôi là phải thêm cả trăm, cả nghìn phép toán, cứ thế cấp số nhân lên. CPU thời bấy giờ vẫn cùi bắp, nhiều nhà khoa học thời bấy giờ chia sẻ, để training một mạng neuron cho tử tế có khi mất cả tuần, hoặc thậm chí tính bằng tháng. Loay hoay điều chỉnh tham số, chạy vài turn training như vậy không khéo lỡ toi mất kỳ báo báo. GPU ra đời ban đầu với mục đích dung tục hơn là phục vụ các máy chơi game và xử lý đồ họa. Nhưng dần dà người ta phát hiện ra cách tính toán của GPU cực kỳ phù hợp với việc nhân chập ma trận (việc mà neuron network thường xuyên phải xử lý). Ấy thế là lại tìm ra ánh sáng cuối đường hầm. Nhờ GPU, giờ đây ta đã có thể training những mạng neuron rất lớn mà thời gian chờ không quá lâu. Chuyện gì đến sẽ đến! Từ những năm 2010, đội ngũ Hinton đã bắt đầu ứng dụng GPU vào việc tính toán neuron network, nhờ đó có thể huấn luyện được những mạng cỡ lớn. Năm 2012, team Hinton đạt được thành tựu lớn với việc phân loại ảnh trên data set ImageNet. Với sức mạnh của GPU, và tất nhiên cả “mạnh vì gạo, bạo vì tiền”, ông cùng đội nghiên cứu của mình (không có Yan Lecun) tiến hành traning (huấn luyện) trên network gồm 60 triệu tham số, 650.000 neuron, thực hiện trên training data set (dữ liệu huấn luyện) 1.2 triệu hình ảnh, 1000 class (thư mục hình ảnh), test trên 150.000 hình ảnh với độ chính xác lên tơi 84%, gây xốc nặng giới computer science lúc bấy giờ (75% đã là cao hết tầm rồi). Một điều tôi rất nể Geoffrey Hinton ở chỗ, ông không quên cite bài của học trò của mình năm 1990 về nhận dạng chữ viết tay, nói rằng công trình của ông kế thừa nhiều từ bài báo đó. Yan Lecun lên hương, Geoffrey Hinton cũng hưởng đủ (thành lập công ty riêng về AI). Người quen hơn nhau là ở chỗ đó. Từ những công trình của Geoffrey Hinton, hàng loạt net mới ra đời, với nhiều tham số hơn, nhiều layer hơn, data set khủng hơn, kỷ lục liên tục bị xô đổ. Deep Learning dần trở thành cuộc chơi của những đội lắm tiền, lắm gạo. Những diễn biến gần đây chắc ai cũng cập nhật rồi nên tôi ko nhắc lại nữa. Google cũng không nằm ngoài trending này, DeepMind được google mua lại với giá 500 triệu đô trump năm 2014. Từ đây AlphaStar, AlphaGo, AlphaFold được ra đời và nổi tiếng toàn thế giới. Chỉ tháng trước thôi, AlphaFold đã dự đoán rất chính xác cấu trúc cuộn 3D của protein, viết nên lịch sử ngành sinh học phân tử. Một con đường dài còn ở phía trước, phong trào nghiên cứu Deep Learning đang nổi lên rất mạnh trên toàn thế giới, trong đó người trẻ Việt Nam cũng tham gia rất tích cực. Ơ mây zing, gút chóp my friends. Chém gió cho vui thế thôi, giờ phải đi thay bỉm cho con đã. Hẹn gặp mọi người gần đây nhất. Edit: Sau comment của bạn Phong Thanh Dương, mình xin sửa lại thời điểm team Hinton làm bắt đầu ứng dụng GPU traning từ giai đoạn 2010, ra báo cáo (paper) nổi danh vào năm 2012. Xin lỗi các bạn vì sai mốc thời gian. Dương Thịnh Nguồn tham khảo: https://en.wikipedia.org/wiki/David_H._Hubel https://en.wikipedia.org/wiki/Frank_Rosenblatt https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon https://en.wikipedia.org/wiki/Yann_LeCun https://en.wikipedia.org/wiki/Geoffrey_Hinton Handwritten digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems (1990) https://en.wikipedia.org/wiki/Support_vector_machine https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf https://www.theguardian.com/technology/2020/nov/30/deepmind-ai-cracks-50-year-old-problem-of-biology-research",,,,,
"HƯỚNG DẪN LẬP TRÌNH IRON MAN FRIDAY ĐIỀU KHIỂN MÁY TÍNH BẰNG GIỌNG NÓI VỚI PYTHON
Hello mọi người,
Nhóm mình có cái clip chia sẻ cách làm trợ lý ảo Friday giống của Iron Man trên PC.Với Friday chỉ cần ra lệnh bằng giọng nói mọi người có thể:
Tự động mở nhạc hoặc xem phim
Tìm kiếm trên Google và Youtube
Xem thời gian
Hi vọng clip sẽ giúp ích được nhiều cho các bạn đam mê lập trình.","HƯỚNG DẪN LẬP TRÌNH IRON MAN FRIDAY ĐIỀU KHIỂN MÁY TÍNH BẰNG GIỌNG NÓI VỚI PYTHON Hello mọi người, Nhóm mình có cái clip chia sẻ cách làm trợ lý ảo Friday giống của Iron Man trên PC.Với Friday chỉ cần ra lệnh bằng giọng nói mọi người có thể: Tự động mở nhạc hoặc xem phim Tìm kiếm trên Google và Youtube Xem thời gian Hi vọng clip sẽ giúp ích được nhiều cho các bạn đam mê lập trình.",,,,,
Mọi người cho mình hỏi nếu muốn process image 10k ảnh trở lên với opencv trên colab thì phải làm ntn ạ vì nó cứ báo bị tràn ram. Em có thử làm chỉ với 5k ảnh nhưng đến khi convert sang numpy thì nó bị như vậy tiếp. Có cách nào xử lý k ạ mình cảm ơn,Mọi người cho mình hỏi nếu muốn process image 10k ảnh trở lên với opencv trên colab thì phải làm ntn ạ vì nó cứ báo bị tràn ram. Em có thử làm chỉ với 5k ảnh nhưng đến khi convert sang numpy thì nó bị như vậy tiếp. Có cách nào xử lý k ạ mình cảm ơn,,,,,
"Chào mọi người, em đang làm bài tập lớn về NLP sử dụng framework LSTM của Keras để train model, sau đó em sử dụng save để lưu model và dùng load_model để gọi mô hình để dự đoán. Kết quả dự đoán trước khi lưu model với sau khi lưu model khác nhau hoàn toàn ạ. Mọi người ai đã từng gặp trường hợp này có thể với giúp em với ạ, em cám ơn nhiều ạ","Chào mọi người, em đang làm bài tập lớn về NLP sử dụng framework LSTM của Keras để train model, sau đó em sử dụng save để lưu model và dùng load_model để gọi mô hình để dự đoán. Kết quả dự đoán trước khi lưu model với sau khi lưu model khác nhau hoàn toàn ạ. Mọi người ai đã từng gặp trường hợp này có thể với giúp em với ạ, em cám ơn nhiều ạ",,,,,
"chào mọi ngưởi, em đang sử dụng yolov4 thì gặp lỗi tương tự như này. mọi ngưởi có ai có cách fix không ạ. 
link bài viết :object detection - YOLO cannot display the class name on prediction picture - Stack Overflow","chào mọi ngưởi, em đang sử dụng yolov4 thì gặp lỗi tương tự như này. mọi ngưởi có ai có cách fix không ạ. link bài viết :object detection - YOLO cannot display the class name on prediction picture - Stack Overflow",,,,,
"Mọi người đã ai làm về thuật toán word beam search trên linux mà khi load thư viện TFWordBeamSearch.so bị lỗi ntn chưa ạ !
Em gặp lỗi này mà chưa biết cách fix thế nào ! Em cảm ơn !
tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory",Mọi người đã ai làm về thuật toán word beam search trên linux mà khi load thư viện TFWordBeamSearch.so bị lỗi ntn chưa ạ ! Em gặp lỗi này mà chưa biết cách fix thế nào ! Em cảm ơn ! tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory,,,,,
"Chào mọi người, hiện tại em đang phát triển một số ứng dụng Desktop sử dụng python, em muốn hỏi là có cách nào để bảo vệ ứng dụng của mình tránh bị dịch ngược không ạ. Em đã có google xem nhiều phương pháp trước rồi, em viết bài này để mong nhận được nhiều sự góp ý của các anh chị trong nhóm. Cảm ơn Admin đã cho em đăng bài ạ, chúc mọi người một ngày tốt lành.","Chào mọi người, hiện tại em đang phát triển một số ứng dụng Desktop sử dụng python, em muốn hỏi là có cách nào để bảo vệ ứng dụng của mình tránh bị dịch ngược không ạ. Em đã có google xem nhiều phương pháp trước rồi, em viết bài này để mong nhận được nhiều sự góp ý của các anh chị trong nhóm. Cảm ơn Admin đã cho em đăng bài ạ, chúc mọi người một ngày tốt lành.",,,,,
Chào mọi người. Hiện tại em đang có đề tài làm chatbot bằng Rasa. Và được yêu cầu tích hợn spaCy hoặc underthesea để phân tích ý định đầu vào của người dùng. Nhưng hiện tại trên spaCy thì không có model sẵn của tiếng Việt còn underthesea thì không tương thích với Rasa 2.1(nếu dowgrade Rasa xuống 2.0 thì ko test đc trên Rasa X). Mọi người ai từng làm bài toán ntn có thể hỗ trợ em được không ạ. Em cảm ơn mọi người.,Chào mọi người. Hiện tại em đang có đề tài làm chatbot bằng Rasa. Và được yêu cầu tích hợn spaCy hoặc underthesea để phân tích ý định đầu vào của người dùng. Nhưng hiện tại trên spaCy thì không có model sẵn của tiếng Việt còn underthesea thì không tương thích với Rasa 2.1(nếu dowgrade Rasa xuống 2.0 thì ko test đc trên Rasa X). Mọi người ai từng làm bài toán ntn có thể hỗ trợ em được không ạ. Em cảm ơn mọi người.,,,,,
"SPEECH RECOGNITION SERIES - JONATHAN HUI
Jonathan Hui - Một trong những tác giả nổi bật và viết có tâm nhất trên medium. Các bài viết của ông viết rất dễ hiểu, kiến thức từ nền tảng tới chi tiết với những phân tích rõ ràng và trực quan. Đây là 1 trong những nguồn tài liệu rất quý.  Dưới đây là 1 series của ông mà mình muốn giới thiệu tới các bạn mới trong lĩnh vực xử lí tiếng nói - Speech recognition series.
https://jonathan-hui.medium.com/speech-recognition-series-71fd6784551a","SPEECH RECOGNITION SERIES - JONATHAN HUI Jonathan Hui - Một trong những tác giả nổi bật và viết có tâm nhất trên medium. Các bài viết của ông viết rất dễ hiểu, kiến thức từ nền tảng tới chi tiết với những phân tích rõ ràng và trực quan. Đây là 1 trong những nguồn tài liệu rất quý. Dưới đây là 1 series của ông mà mình muốn giới thiệu tới các bạn mới trong lĩnh vực xử lí tiếng nói - Speech recognition series. https://jonathan-hui.medium.com/speech-recognition-series-71fd6784551a",,,,,
"Kính chào các bác, dạo này trên Group có nhiều bạn hỏi về train Colab. Có bạn hiểu sai cách dùng Colab và mang code lên chạy trên colab để vẽ và hiển thị ảnh thay vì dùng nó để train model.
Do vậy em làm clip này mong giúp được anh em newbie. Clip siêu chi tiết dài gần 1 tiếng ah!
Mong ad duyệt bài!","Kính chào các bác, dạo này trên Group có nhiều bạn hỏi về train Colab. Có bạn hiểu sai cách dùng Colab và mang code lên chạy trên colab để vẽ và hiển thị ảnh thay vì dùng nó để train model. Do vậy em làm clip này mong giúp được anh em newbie. Clip siêu chi tiết dài gần 1 tiếng ah! Mong ad duyệt bài!",,,,,
"[ Phân cụm dữ liệu HDBSCAN]
[Giảm chiều dữ liệu UMAP]
Hi cộng đồng, mọi người có ai từng làm về HDBSCAN và UMAP chưa ạ ? Không biết có thể chia sẽ một ít cách để đánh giá 2 thuật toán đó được không ạ.
Em cảm ơn mọi người !!!","[ Phân cụm dữ liệu HDBSCAN] [Giảm chiều dữ liệu UMAP] Hi cộng đồng, mọi người có ai từng làm về HDBSCAN và UMAP chưa ạ ? Không biết có thể chia sẽ một ít cách để đánh giá 2 thuật toán đó được không ạ. Em cảm ơn mọi người !!!",,,,,
"Xin chia sẻ các bạn tổng hợp những kinh nghiệm thực tế về phỏng vấn ML engineer va Data scientist. Hy vọng hữu ích và giúp các bạn luyện hiệu quả hơn.
https://rebrand.ly/MLInterview",Xin chia sẻ các bạn tổng hợp những kinh nghiệm thực tế về phỏng vấn ML engineer va Data scientist. Hy vọng hữu ích và giúp các bạn luyện hiệu quả hơn. https://rebrand.ly/MLInterview,,,,,
"Xin được giới thiệu với mọi người dự án ALBERT for Vietnamese do 1 bạn trẻ tài năng(Ngoan Phạm) thực hiện. Đến thời điểm hiện tại đây là ngôn ngữ thứ 3 sau English/Chinese có hiện thực 1 trong những state of the arts của NLP inn 2019. Mời mọi người contribute/use/build applications để cộng đồng phát triển hơn nữa.
Chia sẻ thêm là Ngoan cũng là người thắng giải Zalo AI Chanllenge 2019 cho bài Vietnamese Wikipedia Question Answering. Các bạn có thể tham khảo thêm solution của bạn ấy tại git repo.
ALBERT < 3
https://github.com/ngoanpv/albert_vi",Xin được giới thiệu với mọi người dự án ALBERT for Vietnamese do 1 bạn trẻ tài năng(Ngoan Phạm) thực hiện. Đến thời điểm hiện tại đây là ngôn ngữ thứ 3 sau English/Chinese có hiện thực 1 trong những state of the arts của NLP inn 2019. Mời mọi người contribute/use/build applications để cộng đồng phát triển hơn nữa. Chia sẻ thêm là Ngoan cũng là người thắng giải Zalo AI Chanllenge 2019 cho bài Vietnamese Wikipedia Question Answering. Các bạn có thể tham khảo thêm solution của bạn ấy tại git repo. ALBERT < 3 https://github.com/ngoanpv/albert_vi,,,,,
"Mời tham gia challenge mobile-captured receipt recognition (MC-OCR). Chào cả nhà, nhờ dữ liệu hoá đơn mà ACE diễn đàn cung cấp, cùng với dữ liệu được tài trợ. MC-OCR đã có thể chính thức diễn ra cùng với RIVF2021. Mời ACE quan tâm tới bàn toán nhận dạng OCR cho hoá đơn đăng ký tham gia ở đây [1]. Call 4 participants [2]. Ngay trong ngày mai là đã có dữ liệu warm-up rồi nhé cả nhà.
[1] https://rivf2021-mc-ocr.vietnlp.com/home
[2] https://drive.google.com/file/d/14bLt1HE38Gtpv7-YLYRjsf9pbWtV_v0a/view?usp=sharing
 — với Tiep VuHuu.","Mời tham gia challenge mobile-captured receipt recognition (MC-OCR). Chào cả nhà, nhờ dữ liệu hoá đơn mà ACE diễn đàn cung cấp, cùng với dữ liệu được tài trợ. MC-OCR đã có thể chính thức diễn ra cùng với RIVF2021. Mời ACE quan tâm tới bàn toán nhận dạng OCR cho hoá đơn đăng ký tham gia ở đây [1]. Call 4 participants [2]. Ngay trong ngày mai là đã có dữ liệu warm-up rồi nhé cả nhà. [1] https://rivf2021-mc-ocr.vietnlp.com/home [2] https://drive.google.com/file/d/14bLt1HE38Gtpv7-YLYRjsf9pbWtV_v0a/view?usp=sharing — với Tiep VuHuu.",,,,,
"Chào mọi người, em hiện tại đang gặp vấn đề với tensorflow.js em đã gg nhưng kết quả không khả quan, Hy vọng anh chị, hoặc bạn nào có kinh nghiệm về tfjs giúp em với ạ.
Em đang dùng deeplab để segmentation ảnh, đoạn code dưới là em lấy ảnh, processing và predict nhưng nó bị lỗi khi predict ạ. Em đã thử nhiều cách nhưng không thể sửa, mong mọi người cho em lời khuyên, cảm ơn ạ
Dưới đây là toàn bộ đoạn code của em ạ :
https://drive.google.com/file/d/1an0l73LbzPnti3RLnVYaf1ATKQmhazN5/view?usp=sharing
Edit: em làm theo git này ạ
https://github.com/tensorflow/tfjs-models/tree/master/deeplab","Chào mọi người, em hiện tại đang gặp vấn đề với tensorflow.js em đã gg nhưng kết quả không khả quan, Hy vọng anh chị, hoặc bạn nào có kinh nghiệm về tfjs giúp em với ạ. Em đang dùng deeplab để segmentation ảnh, đoạn code dưới là em lấy ảnh, processing và predict nhưng nó bị lỗi khi predict ạ. Em đã thử nhiều cách nhưng không thể sửa, mong mọi người cho em lời khuyên, cảm ơn ạ Dưới đây là toàn bộ đoạn code của em ạ : https://drive.google.com/file/d/1an0l73LbzPnti3RLnVYaf1ATKQmhazN5/view?usp=sharing Edit: em làm theo git này ạ https://github.com/tensorflow/tfjs-models/tree/master/deeplab",,,,,
"#AVATECH #ShareData
Có khi nào bạn cảm thấy việc lượn một vòng siêu thị rồi nhanh tay lượm, lượm hàng tá đồ dùng, các thứ đầy giỏ thực sự là một đam mê nhưng nghĩ đến cảnh sẽ phải xếp hàng dài chờ tới lượt mình thanh toán thì ôi thôi, thật ngán ngẩm!
Thông thường tại các hệ thống siêu thị lớn sẽ có khoảng gần chục cửa thanh toán kèm nhân viên thu ngân nhưng có vẻ so với số lượng sản phẩm và lượng người tiêu dùng lớn như vậy thì việc check mã từng sản phẩm không còn tối ưu nữa. AVA đã thử nghiệm hệ thống tính tiền tự động sử dụng công nghệ Deep Learning cho phép thanh toán nhiều sản phẩm cùng lúc.
Các bạn có thể sử dụng bộ dữ liệu ảnh dưới đây để xây dựng một hệ thống cho riêng mình nhé.
Link download dữ liệu: http://avatech.com.vn/phan-loai-san-pham/he-thong-tinh-tien-tu-dong-su-dung-cong-nghe-deep-learning","Có khi nào bạn cảm thấy việc lượn một vòng siêu thị rồi nhanh tay lượm, lượm hàng tá đồ dùng, các thứ đầy giỏ thực sự là một đam mê nhưng nghĩ đến cảnh sẽ phải xếp hàng dài chờ tới lượt mình thanh toán thì ôi thôi, thật ngán ngẩm! Thông thường tại các hệ thống siêu thị lớn sẽ có khoảng gần chục cửa thanh toán kèm nhân viên thu ngân nhưng có vẻ so với số lượng sản phẩm và lượng người tiêu dùng lớn như vậy thì việc check mã từng sản phẩm không còn tối ưu nữa. AVA đã thử nghiệm hệ thống tính tiền tự động sử dụng công nghệ Deep Learning cho phép thanh toán nhiều sản phẩm cùng lúc. Các bạn có thể sử dụng bộ dữ liệu ảnh dưới đây để xây dựng một hệ thống cho riêng mình nhé. Link download dữ liệu: http://avatech.com.vn/phan-loai-san-pham/he-thong-tinh-tien-tu-dong-su-dung-cong-nghe-deep-learning",#AVATECH	#ShareData,,,,
"Chúc mừng năm mới 2020.
Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 1 vào comment của post này.
Chúc các bạn năm mới nhiều thành công mới.","Chúc mừng năm mới 2020. Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện tháng 1 vào comment của post này. Chúc các bạn năm mới nhiều thành công mới.",,,,,
"Sách mới free của John Wright và Yi Ma, hai tên tuổi lớn trong lĩnh vực sparse coding, về ""High-Dimensional Data Analysis with Low-Dimensional Models"":","Sách mới free của John Wright và Yi Ma, hai tên tuổi lớn trong lĩnh vực sparse coding, về ""High-Dimensional Data Analysis with Low-Dimensional Models"":",,,,,
"Advanced IT paves the way for online learning which is advantageous to anyone who want to expand their knowledge or even switch to a new area. So, I'd like to share with you my online courses for Data Science program from Coursera that I have been taking for the last 2 years.","Advanced IT paves the way for online learning which is advantageous to anyone who want to expand their knowledge or even switch to a new area. So, I'd like to share with you my online courses for Data Science program from Coursera that I have been taking for the last 2 years.",,,,,
"TÀI LIỆU TỰ HỌC DEEP LEARNING NLP CHO MỌI NGƯỜI
Hiện nay, số lượng bạn có nhu cầu học NLP đang tăng cao, bao gồm các bạn Advanced và mới bắt đầu. Tuy nhiên, tài liệu đa số từ nước ngoài và chưa có thư viện tập hợp, hoặc có thì cũng rải rác và khó cho các bạn mới và tự học.
Đó là lý do VietAI kêu gọi & kết nối các bạn trong cộng đồng AI/ML/DL để đóng góp vào Thư viện Tự học NLP:
📌Những bạn Chia sẻ: chia sẻ nguồn học/tài liệu NLP/paper từ mọi nơi trên thế giới
📌 Những bạn được chia sẻ: xem, tự học, tìm hiểu & chủ động chia sẻ thêm
Tất cả các bạn chia sẻ sẽ nhận CREDIT trực tiếp trên file và tùy thuộc vào mức độ đóng góp, VietAI sẽ hỗ trợ để cùng bạn tổ chức nhiều hoạt động chia sẻ bổ ích.
Thư viện đã có mục lục có sẵn. Mọi người chỉ cần chia sẻ (paste link) vào. Nếu các bạn có đóng góp về cách thức xây dựng mục lục của Thư viện có thể comment phía dưới hoặc inbox An. File sẽ được monitor thường xuyên để đảm bảo rõ ràng và credit đủ cho tất cả anh/chị đóng góp.
About us: VietAI is a non-profit organization. Our mission is to build a community of world-­class AI talents in Vietnam to solve meaningful and impactful problems for not just Vietnam but also the world.","TÀI LIỆU TỰ HỌC DEEP LEARNING NLP CHO MỌI NGƯỜI Hiện nay, số lượng bạn có nhu cầu học NLP đang tăng cao, bao gồm các bạn Advanced và mới bắt đầu. Tuy nhiên, tài liệu đa số từ nước ngoài và chưa có thư viện tập hợp, hoặc có thì cũng rải rác và khó cho các bạn mới và tự học. Đó là lý do VietAI kêu gọi & kết nối các bạn trong cộng đồng AI/ML/DL để đóng góp vào Thư viện Tự học NLP: Những bạn Chia sẻ: chia sẻ nguồn học/tài liệu NLP/paper từ mọi nơi trên thế giới Những bạn được chia sẻ: xem, tự học, tìm hiểu & chủ động chia sẻ thêm Tất cả các bạn chia sẻ sẽ nhận CREDIT trực tiếp trên file và tùy thuộc vào mức độ đóng góp, VietAI sẽ hỗ trợ để cùng bạn tổ chức nhiều hoạt động chia sẻ bổ ích. Thư viện đã có mục lục có sẵn. Mọi người chỉ cần chia sẻ (paste link) vào. Nếu các bạn có đóng góp về cách thức xây dựng mục lục của Thư viện có thể comment phía dưới hoặc inbox An. File sẽ được monitor thường xuyên để đảm bảo rõ ràng và credit đủ cho tất cả anh/chị đóng góp. About us: VietAI is a non-profit organization. Our mission is to build a community of world-­class AI talents in Vietnam to solve meaningful and impactful problems for not just Vietnam but also the world.",,,,,
"Chào các bro, e đang làm 1 model để forecast VN30 futures sử dụng Deep Q NN, mô hình e NN sử dụng là RNN/ GRU
E đánh giá mô hình với 2 chỉ số:
(1): Profit/ Max profit ~ 80% với profit là tổng giá trị (thắng - thua) trong 1 kỳ; max profit là tổng giá trị có thể thắng trong kỳ đó.
(2): win/loss ~ 4-6 lần với win là tổng giá trị thắng; loss là tổng giá trị thua trong kỳ.
VD: Với chuỗi giá và hành động như sau:
Prices = [1, 3, 2, 5, 7] ; Actions = [mua, giữ, bán, mua]
Max profit = 2 + 0 + 3 + 2 = 7
Profit = 2 - 1 + 0 + 2 = 3
Win = 2 + 2 = 4
Loss = -1
Kỳ có thể là weekly, monthly, quaterly.
E không có benchmark về các chỉ số này để biết model như thế nào là tốt, ACE chỉa sẻ e chút kinh nghiệm, đặc biệt là các chỉ số để đánh giá model performance nhé.
E chạy validate với VNindex và Down jones 30 thì Profit/ Max profit ~ 45-55%","Chào các bro, e đang làm 1 model để forecast VN30 futures sử dụng Deep Q NN, mô hình e NN sử dụng là RNN/ GRU E đánh giá mô hình với 2 chỉ số: (1): Profit/ Max profit ~ 80% với profit là tổng giá trị (thắng - thua) trong 1 kỳ; max profit là tổng giá trị có thể thắng trong kỳ đó. (2): win/loss ~ 4-6 lần với win là tổng giá trị thắng; loss là tổng giá trị thua trong kỳ. VD: Với chuỗi giá và hành động như sau: Prices = [1, 3, 2, 5, 7] ; Actions = [mua, giữ, bán, mua] Max profit = 2 + 0 + 3 + 2 = 7 Profit = 2 - 1 + 0 + 2 = 3 Win = 2 + 2 = 4 Loss = -1 Kỳ có thể là weekly, monthly, quaterly. E không có benchmark về các chỉ số này để biết model như thế nào là tốt, ACE chỉa sẻ e chút kinh nghiệm, đặc biệt là các chỉ số để đánh giá model performance nhé. E chạy validate với VNindex và Down jones 30 thì Profit/ Max profit ~ 45-55%",,,,,
"Xin lỗi các bác cho em hỏi chút về statistics 2 công thức trong hình
Công thức đầu tiên là residual sum of squares (RSS) là giá trị quan sát được observed trừ đi estimation theo model thì sao lại bằng sigma vốn dĩ dùng mean.
Công thức thứ 2 thì em ngu luôn không hiểu sao ra được như vậy!
Nguồn sách:. An Introduction to Statistical Learning: with Applications in R (Springer Texts in Statistics) trang 76",Xin lỗi các bác cho em hỏi chút về statistics 2 công thức trong hình Công thức đầu tiên là residual sum of squares (RSS) là giá trị quan sát được observed trừ đi estimation theo model thì sao lại bằng sigma vốn dĩ dùng mean. Công thức thứ 2 thì em ngu luôn không hiểu sao ra được như vậy! Nguồn sách:. An Introduction to Statistical Learning: with Applications in R (Springer Texts in Statistics) trang 76,,,"#Q&A, #math",,
"Paper khá hay cho mọi người muốn dựng App mobile 2d to 3d
Chuyển view để dựng 3d từ MỘT hoặc nhiều ảnh 2d.",Paper khá hay cho mọi người muốn dựng App mobile 2d to 3d Chuyển view để dựng 3d từ MỘT hoặc nhiều ảnh 2d.,,,,,
"Xin lỗi mọi người,
Em thử dùng pretrain gpt2 của transformer để generate text, mà bị lỗi như này 😢. Em có search trên stackoverflow nhưng vẫn chưa tìm ra giải pháp.
Link google colab:
https://colab.research.google.com/drive/1ksOkmNLRSAkur3Ym-IudkTWDkGrz0f9f?usp=sharing","Xin lỗi mọi người, Em thử dùng pretrain gpt2 của transformer để generate text, mà bị lỗi như này . Em có search trên stackoverflow nhưng vẫn chưa tìm ra giải pháp. Link google colab: https://colab.research.google.com/drive/1ksOkmNLRSAkur3Ym-IudkTWDkGrz0f9f?usp=sharing",,,,,
"Giới thiệu dự án TopDup
""TopDup là dự án mã nguồn mở được khởi xướng bởi Forum Machine Learning Cơ bản nhằm hỗ trợ các website, blog công nghệ bảo vệ bản quyền bài viết và chống sao chép.
Ý tưởng cơ bản của dự án là quét toàn bộ tin tức từ các website, blog công nghệ phổ biến tại Việt Nam, phân tích để xác định những bài viết giống nhau và liệt kê tất cả những trường hợp trùng lặp phát hiện được. Danh sách trùng lặp được công bố công khai tại website của dự án: https://topdup.xyz, từ đó giúp tác giả bài viết / chủ blog / website dễ dàng phát hiện nếu bài viết của mình bị sao chép trái phép.""
Dự án rất mong nhận được sự góp ý từ cộng đồng về ý nghĩa, tính khả thi của dự án và các ý tưởng để có sản phẩm tốt hơn. Thông tin chi tiết về dự án có thể được xem thêm tại https://docs.google.com/document/d/12_ogaEmYt_E0PlxdN5vWQJ5uYoMs5y70EVzenw71f0c/edit?pli=1.
Nhóm phát triển ban đầu cũng rất sẵn lòng tích hợp TopDup vào các nền tảng đọc báo công nghệ đang hoạt động.
Mình xin chân thành cảm ơn Đặng Hải Lộc (founder VNAlert) và các bạn kỹ sư bên Viblo đã tham gia khởi tạo dự án này.","Giới thiệu dự án TopDup ""TopDup là dự án mã nguồn mở được khởi xướng bởi Forum Machine Learning Cơ bản nhằm hỗ trợ các website, blog công nghệ bảo vệ bản quyền bài viết và chống sao chép. Ý tưởng cơ bản của dự án là quét toàn bộ tin tức từ các website, blog công nghệ phổ biến tại Việt Nam, phân tích để xác định những bài viết giống nhau và liệt kê tất cả những trường hợp trùng lặp phát hiện được. Danh sách trùng lặp được công bố công khai tại website của dự án: https://topdup.xyz, từ đó giúp tác giả bài viết / chủ blog / website dễ dàng phát hiện nếu bài viết của mình bị sao chép trái phép."" Dự án rất mong nhận được sự góp ý từ cộng đồng về ý nghĩa, tính khả thi của dự án và các ý tưởng để có sản phẩm tốt hơn. Thông tin chi tiết về dự án có thể được xem thêm tại https://docs.google.com/document/d/12_ogaEmYt_E0PlxdN5vWQJ5uYoMs5y70EVzenw71f0c/edit?pli=1. Nhóm phát triển ban đầu cũng rất sẵn lòng tích hợp TopDup vào các nền tảng đọc báo công nghệ đang hoạt động. Mình xin chân thành cảm ơn Đặng Hải Lộc (founder VNAlert) và các bạn kỹ sư bên Viblo đã tham gia khởi tạo dự án này.",,,,,
"Chào mn, em đang đọc về face_recognition sau khi embedding face, sao không dùng euclidean distance mà lại dùng cosine distance để so sánh sự giống nhau của 2 vecto vậy ạ, em mới học nên mong mn chỉ giáo ạ!","Chào mn, em đang đọc về face_recognition sau khi embedding face, sao không dùng euclidean distance mà lại dùng cosine distance để so sánh sự giống nhau của 2 vecto vậy ạ, em mới học nên mong mn chỉ giáo ạ!",,,,,
Please allow me to share this. Thank you.,Please allow me to share this. Thank you.,,,,,
"Chào mọi người trong group,
Em đang tìm hiểu về bài toán tính khoảng cách giãn cách xã hội trong video. Em đang tìm hiểu theo 2 trang này:
OpenCV Social Distancing Detector - PyImageSearch
Subikshaa/Social-Distance-Detection-using-OpenCV (github.com)
Thì trong link github em đang xem về phương pháp Triangle similarity, trong đó phải xác định được tiêu cự F (Focal length), nhưng trong code thì đã set sẵn là 615. Còn chiều cao người H thì họ để trung bình là 165cm. Cho em hỏi F mình có cách nào tính rõ ràng không ạ vì trong github đã có công thức tính nhưng họ đã để 1 giá trị nào đó nhưng em vẫn chưa hiểu vì sao họ để như vậy ạ. Còn chiều cao trung bình của người thì mình có thể dựa vào đâu để có thể set cho hợp lý ạ?
Vì em đang làm đồ án tốt nghiệp và sau này khi viết báo cáo em có đủ căn cứ để có thể giải thích được cho hội đồng nếu hội đồng có hỏi về cái này ạ. Nếu mọi người đã từng làm về đề tài này hoặc có tìm hiểu qua về phương pháp triangle similarity này có thể giải thích hoặc gợi ý để em hiểu thêm về vấn đề em vừa nêu ở trên ạ.
Em cảm ơn mọi người.","Chào mọi người trong group, Em đang tìm hiểu về bài toán tính khoảng cách giãn cách xã hội trong video. Em đang tìm hiểu theo 2 trang này: OpenCV Social Distancing Detector - PyImageSearch Subikshaa/Social-Distance-Detection-using-OpenCV (github.com) Thì trong link github em đang xem về phương pháp Triangle similarity, trong đó phải xác định được tiêu cự F (Focal length), nhưng trong code thì đã set sẵn là 615. Còn chiều cao người H thì họ để trung bình là 165cm. Cho em hỏi F mình có cách nào tính rõ ràng không ạ vì trong github đã có công thức tính nhưng họ đã để 1 giá trị nào đó nhưng em vẫn chưa hiểu vì sao họ để như vậy ạ. Còn chiều cao trung bình của người thì mình có thể dựa vào đâu để có thể set cho hợp lý ạ? Vì em đang làm đồ án tốt nghiệp và sau này khi viết báo cáo em có đủ căn cứ để có thể giải thích được cho hội đồng nếu hội đồng có hỏi về cái này ạ. Nếu mọi người đã từng làm về đề tài này hoặc có tìm hiểu qua về phương pháp triangle similarity này có thể giải thích hoặc gợi ý để em hiểu thêm về vấn đề em vừa nêu ở trên ạ. Em cảm ơn mọi người.",,,,,
"Trước đi học đh cũng rớt môn xstk sml, giờ mình chia sẻ lại kênh này cho các bạn, kênh này đã giúp mình qua môn","Trước đi học đh cũng rớt môn xstk sml, giờ mình chia sẻ lại kênh này cho các bạn, kênh này đã giúp mình qua môn",,,"#sharing, #math",,
"Group Vector and Clustering
chào mọi người
E đang gặp vấn đề này ở bước 1 k biết xử lý như nào.
ban đầu mình có ma trận 1 triệu milion và 200 dimension
bước 1: mình phải giảm số chiều và sorting ma trận này thành nhiều block khác nhau, tương ứng là các vector cũng phải sắp xếp theo kiểu giống nhau thành 1 block.
bước 2: sau đó get từng block rồi chạy thử vs dbscan , kmeans, optics cho từng block để đánh giá
bước 3 : có thể chạy product quantization với 1 block và sau đó chạy thuật toán PQKMEANS để tăng tốc clustering
nhưng em k biết cái bước số 1 mình có thể dùng thuật toán gì để vector quantization và làm sao để group các vector lại với nhau được thành 1 block.
e đã thử nearpy, faiss distance scipy nhưng đó chỉ là 1 số thuật toán indexx dữ liệu và dùng cho việc tìm kiếm.
mọi người có ý tưởng gì thì giúp e với nhé, thanks
ảnh mô tả quá trình bước 1 .","Group Vector and Clustering chào mọi người E đang gặp vấn đề này ở bước 1 k biết xử lý như nào. ban đầu mình có ma trận 1 triệu milion và 200 dimension bước 1: mình phải giảm số chiều và sorting ma trận này thành nhiều block khác nhau, tương ứng là các vector cũng phải sắp xếp theo kiểu giống nhau thành 1 block. bước 2: sau đó get từng block rồi chạy thử vs dbscan , kmeans, optics cho từng block để đánh giá bước 3 : có thể chạy product quantization với 1 block và sau đó chạy thuật toán PQKMEANS để tăng tốc clustering nhưng em k biết cái bước số 1 mình có thể dùng thuật toán gì để vector quantization và làm sao để group các vector lại với nhau được thành 1 block. e đã thử nearpy, faiss distance scipy nhưng đó chỉ là 1 số thuật toán indexx dữ liệu và dùng cho việc tìm kiếm. mọi người có ý tưởng gì thì giúp e với nhé, thanks ảnh mô tả quá trình bước 1 .",,,,,
"Chào các anh/chị, em đang là sinh viên chuyên ngành SE. Hiện tại thì em muốn thử sức với ML ở mức là sử dụng được những model có sẵn thì em nên bắt đầu từ đâu ạ? ( em không biết là dùng từ như này có đúng hay không ).
Nếu câu hỏi của em có ngớ ngẫn quá thì cmt để em xoá, em xin lỗi trước :((","Chào các anh/chị, em đang là sinh viên chuyên ngành SE. Hiện tại thì em muốn thử sức với ML ở mức là sử dụng được những model có sẵn thì em nên bắt đầu từ đâu ạ? ( em không biết là dùng từ như này có đúng hay không ). Nếu câu hỏi của em có ngớ ngẫn quá thì cmt để em xoá, em xin lỗi trước :((",,,,,
"Mọi người ơi cho mình hỏi ngành này mình muốn kiếm việc làm thì focus vào 1 thứ như Computer Vision , Deep learning hoặc NLP , .. chứ ngta ko yêu cầu phải biết hết đúng k ạ","Mọi người ơi cho mình hỏi ngành này mình muốn kiếm việc làm thì focus vào 1 thứ như Computer Vision , Deep learning hoặc NLP , .. chứ ngta ko yêu cầu phải biết hết đúng k ạ",,,,,
"Mọi người cho mình hỏi mình muốn focus vào 1 mảng NLP để đi xin việc làm thì path carrer của NLP là ntn ạ , em cảm ơn mn ạ","Mọi người cho mình hỏi mình muốn focus vào 1 mảng NLP để đi xin việc làm thì path carrer của NLP là ntn ạ , em cảm ơn mn ạ",,,,,
"Share cho các bạn trong nhóm cuộc thi ML showcase 2020, cuộc thi cuối năm vinh danh các ứng dụng ML có ứng dụng trong thực tế.
Giải thưởng trong link nhé mọi người :D","Share cho các bạn trong nhóm cuộc thi ML showcase 2020, cuộc thi cuối năm vinh danh các ứng dụng ML có ứng dụng trong thực tế. Giải thưởng trong link nhé mọi người :D",,,,,
Các master xác suất cho em hỏi là sách sai hay em sai vậy ạ? Cái P(A) ấy mọi người. E cảm ơn ạ!,Các master xác suất cho em hỏi là sách sai hay em sai vậy ạ? Cái P(A) ấy mọi người. E cảm ơn ạ!,,,"#Q&A, #math",,
"Mọi người ai có kinh nghiệm build tensorflow cpp trên windows chưa ạ?
Cho em xin ít kinh nghiệm. Em build ra rồi mà có vẻ vẫn bị thiếu 1 vài file header ạ",Mọi người ai có kinh nghiệm build tensorflow cpp trên windows chưa ạ? Cho em xin ít kinh nghiệm. Em build ra rồi mà có vẻ vẫn bị thiếu 1 vài file header ạ,,,,,
"[Shameless self advertisement - my apologies] As NeurIPS 2020 (a top-tier conference in machine learning) is starting soon, I would like to shamelessly advertise our work presented at the conference, which is in bandit theory; federated learning with selfish agents; and topological machine learning (see details below). If you have time, please take a look at them, and if still interested, please stop by the sessions to discuss with my students and I:
1. Adversarial blocking bandits: Tuesday 8 Dec, 5-7pm UK time (9-11am PST)
https://neurips.cc/.../poster...
In this work, we extend Basu et al. (2019)'s blocking bandit paper to the adversarial setting. We first prove that the underlying offline problem, which is a non-preemptive interval scheduling problem, is strongly NP-hard. Then we provide a approximation ratio for the online greedy (AFAIK this is the first approx. result for online non-preemptive deterministic interval scheduling algs). Then we prove that in the bandit setting, sub-linear regret cannot be achieved without having an additional bound on the total path variation. We then propose 2 algorithms with sub-linear regret bounds for the case when we know this budget, and when we don't respectively.
2. Optimal learning from verified training data: Thursday 10 Dec, 5-7pm UK time (9-11am PST)
https://neurips.cc/.../poster...
This paper looks at a federated learning problem with selfish participants, where each participant can submit their data to a central learner for joint training. The goal of the selfish agents is to manipulate the central learning process so that the outcome is more favourable to them (e.g., becomes closer to a model that they prefer). We ask the question whether we can do something clever against this selfish behaviour. To answer this, we look at the Stackelberg prediction game (Brückner and Scheffer 2011) with linear regression models (SPG). Our main, and perhaps quite surprising, result is that this problem can be solved optimally (most results in SPG and bilevel optimisation can at most guarantee convergence to local optima). In particular, we show that solving an SPG of this type is equivalent to solving a quadratically constrained quadratic fractional program with a single constraint. Then, by using a novel combination of Dinkelbach’s lemmas for fractional programming and the S-lemma for quadratic programs, we show that a combination of bisection search and semidefinite programming can be used to converge to global optima.
3. Two papers on topological ML at the TDA & Beyond Workshop: Friday 11 Dec (the whole day):
One is a spotlight presentation in which we prove that the class of hypotheses with the same persistent homology is non-uniformly learnable:
https://openreview.net/attachment?id=Ay-RgChnje&name=Poster
This work justifies the inclusion of topological losses as loss function/as a regulariser in the training phase (as has been done by many until now without theoretical justification).
The other work enables the usage of fuzzy clustering directly on the persistent diagram space:
https://openreview.net/attachment?id=I49l3mLYXl6&name=Poster
This work allows us to automatically process the topology of data without requiring addition of domain knowledge (as has been done until now).
We also show that it has great potential in transfer learning: When applied to a new domain, we can use fuzzy clustering to identify the closest topological clusters and use pre-trained models which have decision boundaries with most similar topologies to the new data set. Our experiments show that by doing so, the performance of transfer learning can be significantly improved.","[Shameless self advertisement - my apologies] As NeurIPS 2020 (a top-tier conference in machine learning) is starting soon, I would like to shamelessly advertise our work presented at the conference, which is in bandit theory; federated learning with selfish agents; and topological machine learning (see details below). If you have time, please take a look at them, and if still interested, please stop by the sessions to discuss with my students and I: 1. Adversarial blocking bandits: Tuesday 8 Dec, 5-7pm UK time (9-11am PST) https://neurips.cc/.../poster... In this work, we extend Basu et al. (2019)'s blocking bandit paper to the adversarial setting. We first prove that the underlying offline problem, which is a non-preemptive interval scheduling problem, is strongly NP-hard. Then we provide a approximation ratio for the online greedy (AFAIK this is the first approx. result for online non-preemptive deterministic interval scheduling algs). Then we prove that in the bandit setting, sub-linear regret cannot be achieved without having an additional bound on the total path variation. We then propose 2 algorithms with sub-linear regret bounds for the case when we know this budget, and when we don't respectively. 2. Optimal learning from verified training data: Thursday 10 Dec, 5-7pm UK time (9-11am PST) https://neurips.cc/.../poster... This paper looks at a federated learning problem with selfish participants, where each participant can submit their data to a central learner for joint training. The goal of the selfish agents is to manipulate the central learning process so that the outcome is more favourable to them (e.g., becomes closer to a model that they prefer). We ask the question whether we can do something clever against this selfish behaviour. To answer this, we look at the Stackelberg prediction game (Brückner and Scheffer 2011) with linear regression models (SPG). Our main, and perhaps quite surprising, result is that this problem can be solved optimally (most results in SPG and bilevel optimisation can at most guarantee convergence to local optima). In particular, we show that solving an SPG of this type is equivalent to solving a quadratically constrained quadratic fractional program with a single constraint. Then, by using a novel combination of Dinkelbach’s lemmas for fractional programming and the S-lemma for quadratic programs, we show that a combination of bisection search and semidefinite programming can be used to converge to global optima. 3. Two papers on topological ML at the TDA & Beyond Workshop: Friday 11 Dec (the whole day): One is a spotlight presentation in which we prove that the class of hypotheses with the same persistent homology is non-uniformly learnable: https://openreview.net/attachment?id=Ay-RgChnje&name=Poster This work justifies the inclusion of topological losses as loss function/as a regulariser in the training phase (as has been done by many until now without theoretical justification). The other work enables the usage of fuzzy clustering directly on the persistent diagram space: https://openreview.net/attachment?id=I49l3mLYXl6&name=Poster This work allows us to automatically process the topology of data without requiring addition of domain knowledge (as has been done until now). We also show that it has great potential in transfer learning: When applied to a new domain, we can use fuzzy clustering to identify the closest topological clusters and use pre-trained models which have decision boundaries with most similar topologies to the new data set. Our experiments show that by doing so, the performance of transfer learning can be significantly improved.",,,,,
"Chào các bác, mấy hôm rồi chạy quanh Group thấy anh em nhiều lỗi khi học AI , Deep Learning trên Windows quá (đặc biệt là thằng Dlib) nên mình ra bài này hi vọng giúp được anh em newbie mới học.
Mong ad duyệt bài!","Chào các bác, mấy hôm rồi chạy quanh Group thấy anh em nhiều lỗi khi học AI , Deep Learning trên Windows quá (đặc biệt là thằng Dlib) nên mình ra bài này hi vọng giúp được anh em newbie mới học. Mong ad duyệt bài!",,,,,
"Em chào anh/chị, gần đây em có tìm hiểu về YOLOv3 để làm 1 project nhỏ về detect playing card game. vì trong pre-trained model ko có data là playing card game nên em quyết định tự train 1 cái custom data set, em train trên local machine ạ. Tuy nhiên lúc train thì em bị kẹt ở chỗ `can not load image` và `STB Reason: can't fopen` ạ. Em có search google và fix theo hướng dẫn nhưng vẫn không đc. Anh/chị nào biết thì giúp em với ạ, em cảm ơn ạ.","Em chào anh/chị, gần đây em có tìm hiểu về YOLOv3 để làm 1 project nhỏ về detect playing card game. vì trong pre-trained model ko có data là playing card game nên em quyết định tự train 1 cái custom data set, em train trên local machine ạ. Tuy nhiên lúc train thì em bị kẹt ở chỗ `can not load image` và `STB Reason: can't fopen` ạ. Em có search google và fix theo hướng dẫn nhưng vẫn không đc. Anh/chị nào biết thì giúp em với ạ, em cảm ơn ạ.",,,,,
"Việc chia dữ liệu làm mấy phần là một bước cơ bản, tưởng đơn giản mà lại rất dễ gây nhầm lẫn trong quá trình xây dựng mô hình học máy.
Blog lần này chia sẻ một chút kinh nghiệm của mình về chủ đề này... Còn bạn thì sao? Bạn có kinh nghiệm gì thú vị về việc chia dữ liệu thì hãy chia sẻ để mọi người cùng học hỏi nha :)","Việc chia dữ liệu làm mấy phần là một bước cơ bản, tưởng đơn giản mà lại rất dễ gây nhầm lẫn trong quá trình xây dựng mô hình học máy. Blog lần này chia sẻ một chút kinh nghiệm của mình về chủ đề này... Còn bạn thì sao? Bạn có kinh nghiệm gì thú vị về việc chia dữ liệu thì hãy chia sẻ để mọi người cùng học hỏi nha :)",,,,,
Em đã hoàn thành xong khoá ML(Coursera của gs Andrew NG) giờ em muốn học DL thì khoá nào thì có lý thuyết + code ạ,Em đã hoàn thành xong khoá ML(Coursera của gs Andrew NG) giờ em muốn học DL thì khoá nào thì có lý thuyết + code ạ,,,,,
"Xin chào mn!
Em mới tìm hiểu về cách đánh label cho ảnh, em làm theo link bên dưới. Em dùng Anaconda trên window, nhưng khi mở labelImg lên rồi mở Open file ảnh lên thì ảnh nó tự phóng to ra, một hồi sau là labelImg tự động tắt.
Mn cho em hỏi trường hợp này là lỗi gì vậy ạ? Và cách khắc phục như thế nào?
Em xin cảm ơn!
https://github.com/tzutalin/labelImg","Xin chào mn! Em mới tìm hiểu về cách đánh label cho ảnh, em làm theo link bên dưới. Em dùng Anaconda trên window, nhưng khi mở labelImg lên rồi mở Open file ảnh lên thì ảnh nó tự phóng to ra, một hồi sau là labelImg tự động tắt. Mn cho em hỏi trường hợp này là lỗi gì vậy ạ? Và cách khắc phục như thế nào? Em xin cảm ơn! https://github.com/tzutalin/labelImg",,,,,
"Kính chào các bác. Để cập nhật kiến thức em cũng mạnh dạn tìm hiểu và làm clip về Trànormer để làm nền tảng cho clip tới sẽ học về BERT và ứng dụng của BERT.
Mong giúp được các bạn newbie, các cao thủ đi qua comment giúp nếu em sai để em sửa kiến thức cho chuẩn ah.
Mong ad duyệt bài!","Kính chào các bác. Để cập nhật kiến thức em cũng mạnh dạn tìm hiểu và làm clip về Trànormer để làm nền tảng cho clip tới sẽ học về BERT và ứng dụng của BERT. Mong giúp được các bạn newbie, các cao thủ đi qua comment giúp nếu em sai để em sửa kiến thức cho chuẩn ah. Mong ad duyệt bài!",,,,,
"Cho mình xin được biết: tại sao tính liên tục không được chú trọng với các active functions cũng như đạo hàm của nó? Mình xem rất nhiều active function thông dụng nhưng ít thấy hàm nào vừa đảm bảo được tính liên tục của chính nó và cho cả đạo hàm của nó. Nó có ảnh hưởng gì đến quá trình tính toán của mô hình deep learning tại những điểm gián đoạn không? Các hàm/thư viện trên Python chỉ cung cấp ở dạng API để khai thác chứ nó không có document để mô tả việc thiết lập các API này, nên mình không thể biết bên trong nó đã xử lý thế nào.
Mình xin lỗi khi đề cập đến vấn đề này, nhưng nó là cái mình không hiểu nên mình mạnh dạn hỏi, xin các bạn chỉ đáp, mình xin cảm ơn!","Cho mình xin được biết: tại sao tính liên tục không được chú trọng với các active functions cũng như đạo hàm của nó? Mình xem rất nhiều active function thông dụng nhưng ít thấy hàm nào vừa đảm bảo được tính liên tục của chính nó và cho cả đạo hàm của nó. Nó có ảnh hưởng gì đến quá trình tính toán của mô hình deep learning tại những điểm gián đoạn không? Các hàm/thư viện trên Python chỉ cung cấp ở dạng API để khai thác chứ nó không có document để mô tả việc thiết lập các API này, nên mình không thể biết bên trong nó đã xử lý thế nào. Mình xin lỗi khi đề cập đến vấn đề này, nhưng nó là cái mình không hiểu nên mình mạnh dạn hỏi, xin các bạn chỉ đáp, mình xin cảm ơn!",,,,,
"Em chào mọi người, hiện tại em đang tìm hiểu bài toán về bóc tách layout và thứ tự đọc, bước đầu đã ghép được các layout và ghép các paragraph, sentence với nhau. Giờ đang vướng mắc 1 vài vấn đề. về thứ tự đọc cũng như các layout khó. Không biết mọi người có thể gợi ý một vài hướng đi được không ạ?
Và tiện đây em muốn xin code của của paper dưới đây do web của họ đã không còn giữ ạ.
Em cảm ơn mọi người và chúc mọi người 1 ngày tốt lành.","Em chào mọi người, hiện tại em đang tìm hiểu bài toán về bóc tách layout và thứ tự đọc, bước đầu đã ghép được các layout và ghép các paragraph, sentence với nhau. Giờ đang vướng mắc 1 vài vấn đề. về thứ tự đọc cũng như các layout khó. Không biết mọi người có thể gợi ý một vài hướng đi được không ạ? Và tiện đây em muốn xin code của của paper dưới đây do web của họ đã không còn giữ ạ. Em cảm ơn mọi người và chúc mọi người 1 ngày tốt lành.",,,,,
"Cho em hỏi có ai cài được thuật toán BERT đạt độ chính xác >80% như trong báo cáo của google nói chưa (link: https://arxiv.org/pdf/1810.04805.pdf)
E chạy được với epoch =600 (hơn nữa thì chạy quá lâu, vượt quá thời gian cho free của colab). Mà độ chính xác khoảng 70% thôi.
Thầy yêu cầu phải chỉnh đến khi nào đạt >80% mới thôi. Có ai giúp em với, đuối quá.","Cho em hỏi có ai cài được thuật toán BERT đạt độ chính xác >80% như trong báo cáo của google nói chưa (link: https://arxiv.org/pdf/1810.04805.pdf) E chạy được với epoch =600 (hơn nữa thì chạy quá lâu, vượt quá thời gian cho free của colab). Mà độ chính xác khoảng 70% thôi. Thầy yêu cầu phải chỉnh đến khi nào đạt >80% mới thôi. Có ai giúp em với, đuối quá.",,,,,
"Mọi người cho e hỏi, làm thế nào để biết trong 1 tập các vector thì có những vector nào giống nhau ạ? E dùng set để add thì set không add đc kiểu vector, ndarray ạ?","Mọi người cho e hỏi, làm thế nào để biết trong 1 tập các vector thì có những vector nào giống nhau ạ? E dùng set để add thì set không add đc kiểu vector, ndarray ạ?",,,,,
"tiếp tục series reinforcement learning với team AI camp tại Quy Nhơn
Link bài viết gốc : https://www.facebook.com/QNAICommunity/posts/176477823998645",tiếp tục series reinforcement learning với team AI camp tại Quy Nhơn Link bài viết gốc : https://www.facebook.com/QNAICommunity/posts/176477823998645,,,,,
"1 Trang khá hay dành cho những bạn muốn vẽ schematic NN architecture đơn giản mà không cần lầy lội với draw.io
http://alexlenail.me/NN-SVG/LeNet.html",1 Trang khá hay dành cho những bạn muốn vẽ schematic NN architecture đơn giản mà không cần lầy lội với draw.io http://alexlenail.me/NN-SVG/LeNet.html,,,,,
"Nhiều bạn muốn làm các model về language nhiều khi không có dữ liệu trong khi facebook lại quá nhiều thông tin luôn. Do đó em mạnh dạn làm clip chia sẻ các crawl dữ liệu từ facebook. Mong giúp được các bạn newbie mới học
Em cũng biết là còn nhiều món cần làm như: fake ip, change agent... tránh bị block. Nếu các bạn cần em sẽ làm trong một video khác ah.","Nhiều bạn muốn làm các model về language nhiều khi không có dữ liệu trong khi facebook lại quá nhiều thông tin luôn. Do đó em mạnh dạn làm clip chia sẻ các crawl dữ liệu từ facebook. Mong giúp được các bạn newbie mới học Em cũng biết là còn nhiều món cần làm như: fake ip, change agent... tránh bị block. Nếu các bạn cần em sẽ làm trong một video khác ah.",,,,,
"Việc tách nền được thực hiện nhờ U^2-Net (U square net).
Nếu các bạn có bất cứ câu hỏi gì về app này, mình sẽ nhiệt tình trả lời, hi vọng chúng ta sẽ có những thảo luận hữu ích. Rất mong nhận được các ý kiến đóng góp từ mọi người ạ!","Việc tách nền được thực hiện nhờ U^2-Net (U square net). Nếu các bạn có bất cứ câu hỏi gì về app này, mình sẽ nhiệt tình trả lời, hi vọng chúng ta sẽ có những thảo luận hữu ích. Rất mong nhận được các ý kiến đóng góp từ mọi người ạ!",,,,,
"Em đang có cùng câu hỏi ạ.
Mọi người tư vấn giúp em, cảm ơn mọi người.
Thực tập sinh Machine Learning, Deep Learning - Jobs/Events - Diễn đàn Machine Learning cơ bản (machinelearningcoban.com)","Em đang có cùng câu hỏi ạ. Mọi người tư vấn giúp em, cảm ơn mọi người. Thực tập sinh Machine Learning, Deep Learning - Jobs/Events - Diễn đàn Machine Learning cơ bản (machinelearningcoban.com)",,,,,
,nan,,,,,
"Facebook AI Introduces Linformer: A New Transformer Architecture To Catch Hate Speech And Content That Incites Violence
Paper: https://arxiv.org/abs/2006.04768?
Github: https://github.com/pytorch/fairseq/tree/master/examples/linformer?
#FacebookAi #Opensource #speech",Facebook AI Introduces Linformer: A New Transformer Architecture To Catch Hate Speech And Content That Incites Violence Paper: https://arxiv.org/abs/2006.04768? Github: https://github.com/pytorch/fairseq/tree/master/examples/linformer?,#FacebookAi	#Opensource	#speech,,,,
"Em chào mọi người ạ.
Hiện tại em đang tìm cách chạy model train bằng pytorch trên C++, em có tìm thấy ONNX và Caffe2 nhưng em không biết convert sang format nào và dùng cách nào để chạy trên C++. Mong mọi người giúp đỡ.
Em xin cảm ơn ạ.","Em chào mọi người ạ. Hiện tại em đang tìm cách chạy model train bằng pytorch trên C++, em có tìm thấy ONNX và Caffe2 nhưng em không biết convert sang format nào và dùng cách nào để chạy trên C++. Mong mọi người giúp đỡ. Em xin cảm ơn ạ.",,,,,
"Xin chào mọi người,
Em đang làm luận văn tốt nghiệp, đề tài là về Nhận dạng người bằng vân tay (fingertips) và giọng nói (speaker identification).
Mọi người cho em hỏi là em nên học khóa học nào có liên quan đến đề tài ạ, em đọc các bài báo với sách cũng nhiều mà thấy vẫn chưa đủ ạ
Em xin cảm ơn!","Xin chào mọi người, Em đang làm luận văn tốt nghiệp, đề tài là về Nhận dạng người bằng vân tay (fingertips) và giọng nói (speaker identification). Mọi người cho em hỏi là em nên học khóa học nào có liên quan đến đề tài ạ, em đọc các bài báo với sách cũng nhiều mà thấy vẫn chưa đủ ạ Em xin cảm ơn!",,,,,
"Chào mọi người e đang học ML ạ. E có 1 bài tập là mình có 1 tập dữ liệu yêu cầu xử lý tập dữ liệu bằng phương pháp cây quyết định. Trong tập dữ liệu ở phần nhãn có kiểu chuỗi, e chạy thì gặp lỗi như bên dưới mong mọi người giúp đỡ ạ.","Chào mọi người e đang học ML ạ. E có 1 bài tập là mình có 1 tập dữ liệu yêu cầu xử lý tập dữ liệu bằng phương pháp cây quyết định. Trong tập dữ liệu ở phần nhãn có kiểu chuỗi, e chạy thì gặp lỗi như bên dưới mong mọi người giúp đỡ ạ.",,,,,
"[Deep Learning Project]
Một số project của các bạn học sinh khóa Deep Learning.
1. Sinh thơ lục bát: Dữ liệu là các bài thơ lục bát được crawl về từ thivien.net
2. Hệ thống Chabot: dữ liệu lấy từ đoạn hội thoại của các bộ phim.
3. Đánh giá sản phẩm trên trang Lazada thông qua bình luận của khách hàng: Crawl bình luận của mỗi sản phẩm, phân loại bình luận tích cực, tiêu cực và đưa ra khuyến nghị cho khách hàng.
Slide và code tham khảo ở đây: https://drive.google.com/drive/folders/1ab6ct5xDF8oJmki8-0RJZl3V4mha6yew?usp=sharing
P/s: Ảnh dưới được AI sinh ra sau khi cho học từ thơ lục bát, mọi người thấy sinh ra có câu 6/8, có vần khá tốt.","[Deep Learning Project] Một số project của các bạn học sinh khóa Deep Learning. 1. Sinh thơ lục bát: Dữ liệu là các bài thơ lục bát được crawl về từ thivien.net 2. Hệ thống Chabot: dữ liệu lấy từ đoạn hội thoại của các bộ phim. 3. Đánh giá sản phẩm trên trang Lazada thông qua bình luận của khách hàng: Crawl bình luận của mỗi sản phẩm, phân loại bình luận tích cực, tiêu cực và đưa ra khuyến nghị cho khách hàng. Slide và code tham khảo ở đây: https://drive.google.com/drive/folders/1ab6ct5xDF8oJmki8-0RJZl3V4mha6yew?usp=sharing P/s: Ảnh dưới được AI sinh ra sau khi cho học từ thơ lục bát, mọi người thấy sinh ra có câu 6/8, có vần khá tốt.",,,,,
"Mọi người giúp em với ạ, em cảm ơn mn 🥰🥰","Mọi người giúp em với ạ, em cảm ơn mn",,,,,
"Xin chào mọi người, mình đang làm một đề tài nhỏ là so sánh các cơ chế kiểm soát (regularization) cho bài toán phân loại (classification) trên mẫu dữ liệu số viết tay MNIST. Các phương pháp đang so sánh ở đây là Lasso, minimax concave penalty (MCP), smoothly clipped absolute deviation (SCAD). Các thư viện mình tìm thấy trên R hoặc Python đều không hỗ trợ cùng lúc softmax (multinomial) regression và các penalty ớt trên ngoại trừ thư viện ncpen trên R.
Mình sử dụng thư viện ncpen thì bị báo lỗi std::bad_alloc. Có bác nào ở đây đã từng sử dụng qua thư viện này xin giúp mình lỗi này hoặc có bất cứ thư viện nào trên R hay Python có hỗ trợ softmax (multinomial) regression cùng với các penalty ở trên thì cho mình xin. Cảm ơn mọi người!
Mong ad duyệt bài
Tài liệu gốc","Xin chào mọi người, mình đang làm một đề tài nhỏ là so sánh các cơ chế kiểm soát (regularization) cho bài toán phân loại (classification) trên mẫu dữ liệu số viết tay MNIST. Các phương pháp đang so sánh ở đây là Lasso, minimax concave penalty (MCP), smoothly clipped absolute deviation (SCAD). Các thư viện mình tìm thấy trên R hoặc Python đều không hỗ trợ cùng lúc softmax (multinomial) regression và các penalty ớt trên ngoại trừ thư viện ncpen trên R. Mình sử dụng thư viện ncpen thì bị báo lỗi std::bad_alloc. Có bác nào ở đây đã từng sử dụng qua thư viện này xin giúp mình lỗi này hoặc có bất cứ thư viện nào trên R hay Python có hỗ trợ softmax (multinomial) regression cùng với các penalty ở trên thì cho mình xin. Cảm ơn mọi người! Mong ad duyệt bài Tài liệu gốc",,,,,
"Hi group,
Em viết stt mong ACE share tên một vài tutorial/book hay về xử lý dữ liệu trước giai đoạn apply training model.
Em cảm ơn.","Hi group, Em viết stt mong ACE share tên một vài tutorial/book hay về xử lý dữ liệu trước giai đoạn apply training model. Em cảm ơn.",,,,,
Xin phép ad cho mình chia sẻ về blog nho nhỏ của mình . Mình là sinh viên cũng đang nghiên cứu về amngr machine learning . Mình tạo ra 1 blog với mục đích chia sẻ lại những gì mình đã học được từ trên mạng ... Đây là bài mới nhất của blog . Cách nhận diện vật thể realtime với opencv,Xin phép ad cho mình chia sẻ về blog nho nhỏ của mình . Mình là sinh viên cũng đang nghiên cứu về amngr machine learning . Mình tạo ra 1 blog với mục đích chia sẻ lại những gì mình đã học được từ trên mạng ... Đây là bài mới nhất của blog . Cách nhận diện vật thể realtime với opencv,,,,,
"Chào mọi người, hiện tại em đang làm với bài toán bóc tách được layout của 1 bài báo đơn giản. Hiện tại em sử dụng split and merge từ kết quả trả ra của pdf minner và cho ra kết quả khá ổn nhưng khi gặp các bài báo có thứ tự đọc phức tạp hơn thì kết quả không tốt.
Hiện tại em đang có hướng suy nghĩ tới việc ghép các câu dựa theo ý nghĩa. Không biết mọi người có thể cho em xin 1 vài gợi ý hoặc các hướng tiếp cận được không ạ?
Em cảm ơn mọi người đã đọc ạ.","Chào mọi người, hiện tại em đang làm với bài toán bóc tách được layout của 1 bài báo đơn giản. Hiện tại em sử dụng split and merge từ kết quả trả ra của pdf minner và cho ra kết quả khá ổn nhưng khi gặp các bài báo có thứ tự đọc phức tạp hơn thì kết quả không tốt. Hiện tại em đang có hướng suy nghĩ tới việc ghép các câu dựa theo ý nghĩa. Không biết mọi người có thể cho em xin 1 vài gợi ý hoặc các hướng tiếp cận được không ạ? Em cảm ơn mọi người đã đọc ạ.",,,,,
Mọi người cho e hỏi khái niệm Convolutional Neural Network có phải là thuật toán ko ạ,Mọi người cho e hỏi khái niệm Convolutional Neural Network có phải là thuật toán ko ạ,,,,,
"DEEP LEARNING AND BEYOND: A TUTORIAL
On Dec 1st 2020 I will give a #tutorial on the current state of the arts in #DeepLearning and what might push the field forward beyond what we are experiencing. The tutorial is hosted by the 2020 IEEE Symposium Series on Computational #Intelligence, running virtual this year.
The topics include #transformer, #graph neural networks, deep #unsupervised learning, #BERT, fast weights, neural #memories, neural #reasoning, and neural #theoryofmind.
Slides are included here. Video will be available after the talk: https://truyentran.github.io/ssci2020-tute.html","DEEP LEARNING AND BEYOND: A TUTORIAL On Dec 1st 2020 I will give a on the current state of the arts in and what might push the field forward beyond what we are experiencing. The tutorial is hosted by the 2020 IEEE Symposium Series on Computational running virtual this year. The topics include neural networks, deep learning, fast weights, neural neural and neural Slides are included here. Video will be available after the talk: https://truyentran.github.io/ssci2020-tute.html","#tutorial	#DeepLearning	#Intelligence,	#transformer,	#graph	#unsupervised	#BERT,	#memories,	#reasoning,	#theoryofmind.",,,,
"https://youtu.be/Zg0t3f90n6Q
bai huong dan cua anh Le Viet Khanh dung tensorflow lite tren mobile device",https://youtu.be/Zg0t3f90n6Q bai huong dan cua anh Le Viet Khanh dung tensorflow lite tren mobile device,,,,,
"Chào các bạn,
Mình mới tìm hiểu một chút về computer vision, mình thấy có một khái niệm là visual codebook. Mình cũng không hiểu cụ thể ý nghĩa của nó, và làm thế nào để generate ra nó. Mình thấy nó hay được nhắc tới cùng với bài toán k means clustering.
Bạn nào đó có thể giải thích hoặc giới thiệu tài liệu để giải thích về chủ đề này được không nhỉ ? Mình đã tham khảo cuốn Multiple View và cuốn CV của Szeliski nhưng không thấy nói cụ thể.
Thanks in advance !","Chào các bạn, Mình mới tìm hiểu một chút về computer vision, mình thấy có một khái niệm là visual codebook. Mình cũng không hiểu cụ thể ý nghĩa của nó, và làm thế nào để generate ra nó. Mình thấy nó hay được nhắc tới cùng với bài toán k means clustering. Bạn nào đó có thể giải thích hoặc giới thiệu tài liệu để giải thích về chủ đề này được không nhỉ ? Mình đã tham khảo cuốn Multiple View và cuốn CV của Szeliski nhưng không thấy nói cụ thể. Thanks in advance !",,,,,
"Tối ưu hóa quá trình reinforcement learning với data augmentation và latent space representations
Bác nào đang làm về simulator hay forecasting có thể apply",Tối ưu hóa quá trình reinforcement learning với data augmentation và latent space representations Bác nào đang làm về simulator hay forecasting có thể apply,,,,,
"Chào các anh chị, mọi người cho em hỏi 1 câu về xử lý số liệu. Giả sử em có 2 bộ X, Y chứa 10 thông số đầu vào X = (x1, x2, ..., x10) và Y = (y1, y2, ..., y10).
Giả sử Z chứa dữ liệu đầu ra phụ thuộc vào X và Y: X, Y -> Z.
Vấn đề là, với mỗi cặp (x, y) sẽ cho ra 5 giá trị z observed (data lấy từ experiments nên mỗi lần làm experiments lại ra 1 số) và nó không khác biệt quá quá lớn nhưng vẫn có sự khác biệt.
Câu hỏi của em là, trong trường hợp trên mọi người xử lý thế nào ạ ? lấy giá trị Mean của 5 z đó để tạo dataset mỗi x, y chỉ cho ra 1 z hay có cách nào khác ạ ? Cảm ơn mọi người nhiều.
Ngoài ra, mọi người thường dùng phương pháp gì đơn giản và hiệu quả để remove outliers mà không làm giảm hoặc mất properties của dataset ạ ?","Chào các anh chị, mọi người cho em hỏi 1 câu về xử lý số liệu. Giả sử em có 2 bộ X, Y chứa 10 thông số đầu vào X = (x1, x2, ..., x10) và Y = (y1, y2, ..., y10). Giả sử Z chứa dữ liệu đầu ra phụ thuộc vào X và Y: X, Y -> Z. Vấn đề là, với mỗi cặp (x, y) sẽ cho ra 5 giá trị z observed (data lấy từ experiments nên mỗi lần làm experiments lại ra 1 số) và nó không khác biệt quá quá lớn nhưng vẫn có sự khác biệt. Câu hỏi của em là, trong trường hợp trên mọi người xử lý thế nào ạ ? lấy giá trị Mean của 5 z đó để tạo dataset mỗi x, y chỉ cho ra 1 z hay có cách nào khác ạ ? Cảm ơn mọi người nhiều. Ngoài ra, mọi người thường dùng phương pháp gì đơn giản và hiệu quả để remove outliers mà không làm giảm hoặc mất properties của dataset ạ ?",,,,,
"Em đang làm đồ án Machine Learning, phát hiện PHP Webshell dựa trên học máy sử dụng CNN Model. Ai có phương hướng nào chia sẻ cho e với ạ","Em đang làm đồ án Machine Learning, phát hiện PHP Webshell dựa trên học máy sử dụng CNN Model. Ai có phương hướng nào chia sẻ cho e với ạ",,,,,
"Anh,chị nào đã cài được thuật toán BERT chạy trên colab cho em hỏi là có bị lỗi phiên bản không ạ. Em vật lộn mấy tuần bất lực rồi, haizz
Code: https://colab.research.google.com/drive/1b_XfIgexXofEh4zjQmNxNImQvCVOqKIl?usp=sharing&fbclid=IwAR3idu4q3FM-XI9XwsxPZzJluXwBjT6IytVWBbflahO_XWBVhVXTw82GyWg
Folder:
https://drive.google.com/drive/u/1/folders/1GjxRGjGYHUafgPso2cSGo98ECEJ_QTPR?fbclid=IwAR3R20FX24ISAe9mG9U7Vf4buz70M0nKY40UHrlihBoDtfWnod7QL-LdMrI","Anh,chị nào đã cài được thuật toán BERT chạy trên colab cho em hỏi là có bị lỗi phiên bản không ạ. Em vật lộn mấy tuần bất lực rồi, haizz Code: https://colab.research.google.com/drive/1b_XfIgexXofEh4zjQmNxNImQvCVOqKIl?usp=sharing&fbclid=IwAR3idu4q3FM-XI9XwsxPZzJluXwBjT6IytVWBbflahO_XWBVhVXTw82GyWg Folder: https://drive.google.com/drive/u/1/folders/1GjxRGjGYHUafgPso2cSGo98ECEJ_QTPR?fbclid=IwAR3R20FX24ISAe9mG9U7Vf4buz70M0nKY40UHrlihBoDtfWnod7QL-LdMrI",,,,,
"Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện vào trong comment của post này.
Chúc các bạn ngày mới tháng mới vui vẻ.","Các bạn vui lòng đăng tin tuyển sinh, tuyển dụng, sự kiện vào trong comment của post này. Chúc các bạn ngày mới tháng mới vui vẻ.",,,,,
mình đang tìm hiểu về machine learning. Mình đang crawl 2 web truyện nhưng có những truyện 2 bên tên nó đặt khác nhau không giống nhau hoàn toàn mình muốn sử dụng ml để dự đoán truyện đã có chưa dựa vào name và other name thì cần tìm hiểu thư viện nào,mình đang tìm hiểu về machine learning. Mình đang crawl 2 web truyện nhưng có những truyện 2 bên tên nó đặt khác nhau không giống nhau hoàn toàn mình muốn sử dụng ml để dự đoán truyện đã có chưa dựa vào name và other name thì cần tìm hiểu thư viện nào,,,,,
"[Product quantization]
Chào mọi người
mình dùng thư viện PKmeans trong Python để nén dataset ban đầu.mục đích làm giảm số chiều của bộ dataset.
Library: https://pypi.org/project/pqkmeans/
code on github : https://github.com/DwangoMediaVillage/pqkmeans
hiện tại mình đang bị mắc phần get subvector sau khi chuyển đổi từ vector nhiều chiều sang sub-vector với không gian nhỏ hơn.
ví dụ : vector (1000, 100 dimension ) = compress -> 5 lần x (1000 x 20 dimension).
sau đó mình muốn get từng subvector để test từng subvector với từng thuật toán clustering và thống kê lỗi của tưng thuat toán, rât tiếc phần subvector chưa có lấy được.
trong ví dụ của họ thì chỉ có phần nén dataset . (thư mục tutorial /1 ). còn phần lấy tưng subvector thì k thây nói đến .
Các bạn đã từng làm Product quantization thì giup mình chỗ này nhé ?
cảm ơn mọi người đã đọc tin .","[Product quantization] Chào mọi người mình dùng thư viện PKmeans trong Python để nén dataset ban đầu.mục đích làm giảm số chiều của bộ dataset. Library: https://pypi.org/project/pqkmeans/ code on github : https://github.com/DwangoMediaVillage/pqkmeans hiện tại mình đang bị mắc phần get subvector sau khi chuyển đổi từ vector nhiều chiều sang sub-vector với không gian nhỏ hơn. ví dụ : vector (1000, 100 dimension ) = compress -> 5 lần x (1000 x 20 dimension). sau đó mình muốn get từng subvector để test từng subvector với từng thuật toán clustering và thống kê lỗi của tưng thuat toán, rât tiếc phần subvector chưa có lấy được. trong ví dụ của họ thì chỉ có phần nén dataset . (thư mục tutorial /1 ). còn phần lấy tưng subvector thì k thây nói đến . Các bạn đã từng làm Product quantization thì giup mình chỗ này nhé ? cảm ơn mọi người đã đọc tin .",,,,,
"Em dùng YOLOv4 để train bộ data PlantDoc trên roboflow với 2569 ảnh và 30 class, nhưng đến tận 20000 iterations mà loss vẫn chưa xuống dưới 1. Mọi người cho em hỏi là:
Theo hướng dẫn của darknet thì số lần iteration = số class*2000, tức là sau 60000 thì liệu nó có đảm bảo kết quả tốt không ạ? 
Và 60000 lần iteration (tốn khoảng 65 tiếng khi dùng colab pro) là bình thường ạ?
Có cách nào để nó hội tụ sớm hơn không ạ? 
Em có biết Zalo hiện có 1 cuộc thi trong đó có bài nhận diện biển số xe mà ko được dùng pretrained. Mặc dù ko tham gia, nhưng em có thắc mắc là với bộ dataset khoảng 2000 ảnh trên đã tốn ~65h train, thì mọi người dùng phương pháp nào để tối ưu train bộ dataset ~16000 ảnh (hình như vậy) vậy ạ?
Dataset: https://public.roboflow.com/object-detection/plantdoc
Em cảm ơn. ","Em dùng YOLOv4 để train bộ data PlantDoc trên roboflow với 2569 ảnh và 30 class, nhưng đến tận 20000 iterations mà loss vẫn chưa xuống dưới 1. Mọi người cho em hỏi là: Theo hướng dẫn của darknet thì số lần iteration = số class*2000, tức là sau 60000 thì liệu nó có đảm bảo kết quả tốt không ạ? Và 60000 lần iteration (tốn khoảng 65 tiếng khi dùng colab pro) là bình thường ạ? Có cách nào để nó hội tụ sớm hơn không ạ? Em có biết Zalo hiện có 1 cuộc thi trong đó có bài nhận diện biển số xe mà ko được dùng pretrained. Mặc dù ko tham gia, nhưng em có thắc mắc là với bộ dataset khoảng 2000 ảnh trên đã tốn ~65h train, thì mọi người dùng phương pháp nào để tối ưu train bộ dataset ~16000 ảnh (hình như vậy) vậy ạ? Dataset: https://public.roboflow.com/object-detection/plantdoc Em cảm ơn.",,,,,
"Em chào các Anh Chị. Em là newbie mới làm quen với ML thôi. Em có một bài tập lấy trong M5 competition. Nhiệm vụ là dùng thuật toán Moving Average để dự đoán số sản phẩm đc bán ra trong các ngày tiếp theo. Các anh chị có thể giúp em một số thắc mắc được không ạ?
1/ Yêu cầu như trong hình vẽ, thì có chỗ “k is selected from [2,5] by minimizing MSE” em không hiểu xử lí với nó ntn?
2/ Giá tiền sp thì liên quan ntn đến số lượng bán ra vậy ạ?
Em cám ơn mng.","Em chào các Anh Chị. Em là newbie mới làm quen với ML thôi. Em có một bài tập lấy trong M5 competition. Nhiệm vụ là dùng thuật toán Moving Average để dự đoán số sản phẩm đc bán ra trong các ngày tiếp theo. Các anh chị có thể giúp em một số thắc mắc được không ạ? 1/ Yêu cầu như trong hình vẽ, thì có chỗ “k is selected from [2,5] by minimizing MSE” em không hiểu xử lí với nó ntn? 2/ Giá tiền sp thì liên quan ntn đến số lượng bán ra vậy ạ? Em cám ơn mng.",,,,,
"[Xin phép Admin] Chào mọi người, post này dành cho những anh/chị/bạn quan tâm đến metaheuristic optimization trong machine learning và AI ạ.
[Update: Matlab code: https://www.mathworks.com/matlabcentral/fileexchange/76299-forensic-based-investigation-algorithm-fbi]
Nếu mọi người đang tìm 1 thuật toán tối ưu xuất sắc outperform các thuật toán khác (gồm những thuật toán cũ rích như GA, PSO, DE,... hay những thuật toán gần đây như SOS, TLBO, WOA,...), thì hãy thử thuật toán FBI của mình, vừa được publish trên Applied Soft Computing. Người VN dùng hàng VN chất lượng cao ạ, mình tin mọi người sẽ không thất vọng ^^ English below ạ!
To Researchers: If you are facing an optimization problem, my newly developed algorithm can help.
The main motivation for developing a new algorithm is its capacity to effectively and efficiently solve various optimization problems. A novel optimization method, the forensic-based investigation algorithm (FBI), is developed to determine global solutions for continuous nonlinear functions with low computational effort and high accuracy. FBI is inspired by the suspect investigation–location–pursuit process of police officers. Main features of FBI:
(1) FBI is a parameter-free optimization algorithm.
(2) FBI remarkably outperformed the well-known and newly developed algorithms.
(3) FBI has short computational time and rapidly reaches the optimal solutions in solving problems.
(4) FBI is effective in solving high-dimensional problems (D=1000).
(5) FBI structure has two teams that well balance exploration and exploitation.
Details can be found at: Chou J-S, Nguyen N-M, FBI inspired meta-optimization, Applied Soft Computing, 2020:106339, ISSN 1568-4946, https://doi.org/10.1016/j.asoc.2020.106339.
I would like to thank Distinguished Professor Jui-Sheng Chou, my husband Prof. Minh-Tu Cao, and my good friend Dr. Duc-Hoc Tran for their great supports.","[Xin phép Admin] Chào mọi người, post này dành cho những anh/chị/bạn quan tâm đến metaheuristic optimization trong machine learning và AI ạ. [Update: Matlab code: https://www.mathworks.com/matlabcentral/fileexchange/76299-forensic-based-investigation-algorithm-fbi] Nếu mọi người đang tìm 1 thuật toán tối ưu xuất sắc outperform các thuật toán khác (gồm những thuật toán cũ rích như GA, PSO, DE,... hay những thuật toán gần đây như SOS, TLBO, WOA,...), thì hãy thử thuật toán FBI của mình, vừa được publish trên Applied Soft Computing. Người VN dùng hàng VN chất lượng cao ạ, mình tin mọi người sẽ không thất vọng ^^ English below ạ! To Researchers: If you are facing an optimization problem, my newly developed algorithm can help. The main motivation for developing a new algorithm is its capacity to effectively and efficiently solve various optimization problems. A novel optimization method, the forensic-based investigation algorithm (FBI), is developed to determine global solutions for continuous nonlinear functions with low computational effort and high accuracy. FBI is inspired by the suspect investigation–location–pursuit process of police officers. Main features of FBI: (1) FBI is a parameter-free optimization algorithm. (2) FBI remarkably outperformed the well-known and newly developed algorithms. (3) FBI has short computational time and rapidly reaches the optimal solutions in solving problems. (4) FBI is effective in solving high-dimensional problems (D=1000). (5) FBI structure has two teams that well balance exploration and exploitation. Details can be found at: Chou J-S, Nguyen N-M, FBI inspired meta-optimization, Applied Soft Computing, 2020:106339, ISSN 1568-4946, https://doi.org/10.1016/j.asoc.2020.106339. I would like to thank Distinguished Professor Jui-Sheng Chou, my husband Prof. Minh-Tu Cao, and my good friend Dr. Duc-Hoc Tran for their great supports.",,,,,
"Chào các bạn, mình có một bài toán nhỏ mong được tư vấn (mình là dân ngoại đạo): một bộ số liệu của mình có 3 inputs và 7 outputs. 3 cột bôi màu vàng trong file excel dưới đây là các inputs (x1, x2, x3), những cột còn lại (7 cột) là các outputs (f1, f2,..., f7). Mình đã sử dụng hồi quy tuyến tính ở dạng fi=W0+W1*x1+W2*x2+W3*x3 (với i=1...7) và các trọng số Wi được mình xác định ngay phía dưới của bộ số liệu của mình. Tuy nhiên mình cảm thấy mô hình đã chọn không phù hợp với ""quy luật"" của bộ số liệu phía trên. Mình mong các bạn tư vấn cho mình hướng chọn mô hình cho các outputs của mình để nó phù hợp nhất với bộ số liệu ở trên ! Mong admin duyệt bài cho mình để mình giải quyết được vấn đề và học hỏi thêm lĩnh vực này. Mình xin cảm ơn.
Data: http://www.mediafire.com/file/cab9c6r3os6kiq2/Experiment_Data.xlsx/file","Chào các bạn, mình có một bài toán nhỏ mong được tư vấn (mình là dân ngoại đạo): một bộ số liệu của mình có 3 inputs và 7 outputs. 3 cột bôi màu vàng trong file excel dưới đây là các inputs (x1, x2, x3), những cột còn lại (7 cột) là các outputs (f1, f2,..., f7). Mình đã sử dụng hồi quy tuyến tính ở dạng fi=W0+W1*x1+W2*x2+W3*x3 (với i=1...7) và các trọng số Wi được mình xác định ngay phía dưới của bộ số liệu của mình. Tuy nhiên mình cảm thấy mô hình đã chọn không phù hợp với ""quy luật"" của bộ số liệu phía trên. Mình mong các bạn tư vấn cho mình hướng chọn mô hình cho các outputs của mình để nó phù hợp nhất với bộ số liệu ở trên ! Mong admin duyệt bài cho mình để mình giải quyết được vấn đề và học hỏi thêm lĩnh vực này. Mình xin cảm ơn. Data: http://www.mediafire.com/file/cab9c6r3os6kiq2/Experiment_Data.xlsx/file",,,,,
"Mình mới tìm hiểu về ML, được bạn giới thiệu cho Project nhận diện trái cây, mà mình cài đủ thư viện rồi nó vẫn ko tìm thấy là sao ạ. Cám ơn","Mình mới tìm hiểu về ML, được bạn giới thiệu cho Project nhận diện trái cây, mà mình cài đủ thư viện rồi nó vẫn ko tìm thấy là sao ạ. Cám ơn",,,,,
"Xin cho hỏi, mấy anh chị em có ai từng nghe qua Viện Trí Tuệ Nhân Tạo VAIC chưa vậy? Mình cần liên hệ làm việc, mà sao ko tìm thấy web site. Mà thấy cũng quản cáo rầm rộ.","Xin cho hỏi, mấy anh chị em có ai từng nghe qua Viện Trí Tuệ Nhân Tạo VAIC chưa vậy? Mình cần liên hệ làm việc, mà sao ko tìm thấy web site. Mà thấy cũng quản cáo rầm rộ.",,,,,
"hi group.
Em vừa xong khóa Neural network bên Andrew Ng.
Và đang đọc tutorial của pytorch.
Em có thắc mắc là ở forward đáng nhẽ họ phải lấy w1.T.dot(x) nhưng họ lại lấy ngược lại.
Anh chị giải thích chỗ này cho em với.
Cảm ơn mọi người.",hi group. Em vừa xong khóa Neural network bên Andrew Ng. Và đang đọc tutorial của pytorch. Em có thắc mắc là ở forward đáng nhẽ họ phải lấy w1.T.dot(x) nhưng họ lại lấy ngược lại. Anh chị giải thích chỗ này cho em với. Cảm ơn mọi người.,,,,,
"Kính chào cả nhà, em xin phép ra tiếp bài tiếp theo của series Mì Python với phần build BackEnd Server cho các bài toán về AI bằng Flask.
Mong ad duyệt bài để các bạn newbie có tài liệu tham khảo ah.","Kính chào cả nhà, em xin phép ra tiếp bài tiếp theo của series Mì Python với phần build BackEnd Server cho các bài toán về AI bằng Flask. Mong ad duyệt bài để các bạn newbie có tài liệu tham khảo ah.",,,,,
"Xin chào mọi người, tui là dân tay ngang đang học hỏi về ML. Bạn tui chỉ là mới bắt đầu thì nên đọc vietsub trước để hiểu concept xong đọc bản E nên tui đang tìm ""Recursive deep models for semantic compositionality over a sentiment treebank in Stanford NLP"" bản tiếng Việt. Không biết có bản tiếng Việt không ạ ? Nếu có thì vui lòng cho tui xin với ạ. Xin cám ơn.
Đây là bản tiếng Anh ạ","Xin chào mọi người, tui là dân tay ngang đang học hỏi về ML. Bạn tui chỉ là mới bắt đầu thì nên đọc vietsub trước để hiểu concept xong đọc bản E nên tui đang tìm ""Recursive deep models for semantic compositionality over a sentiment treebank in Stanford NLP"" bản tiếng Việt. Không biết có bản tiếng Việt không ạ ? Nếu có thì vui lòng cho tui xin với ạ. Xin cám ơn. Đây là bản tiếng Anh ạ",,,,,
"[ML scientist interview]
Chào anh chị và các bạn, mình có một câu hỏi phỏng vấn sau về xác xuất(Bayesian) nhưng không chắc về kết quả nên rất mong mọi người góp ý ạ.
""Consider a binary classification task with a prior probability p of class 1. Assume that each assessor labeling an instance makes an error with probability q. A given instance is labeled by n assessors and m of them assigned the label 1. What is the probability that the true label is 1?""","[ML scientist interview] Chào anh chị và các bạn, mình có một câu hỏi phỏng vấn sau về xác xuất(Bayesian) nhưng không chắc về kết quả nên rất mong mọi người góp ý ạ. ""Consider a binary classification task with a prior probability p of class 1. Assume that each assessor labeling an instance makes an error with probability q. A given instance is labeled by n assessors and m of them assigned the label 1. What is the probability that the true label is 1?""",,,,,
"[AI News]
Một trong những hướng đi chính của Facebook AI hiện nay là triển khai các công nghệ học máy để bảo vệ người dùng khỏi những nội dung độc hại. Mục tiêu là nhận ra nhanh và chính xác những đoạn phát ngôn mang tính tấn công hoặc gây thù ghét, những thông tin sai sự thật hoặc những nội dung vi phạm pháp luật bằng mọi ngôn ngữ trên thế giới.
AI hiện nay đã cải thiện rất nhiều, chủ động phát hiện 94.7% các đoạn phát ngôn không hợp lệ bị xóa khỏi Facebook, so với 80,5% của 2019 và 24% của 2017. Điều này được thực hiện bởi các công nghệ nâng cao về nhận diện tự động khác nhau như XLM, Reinforced Integrity Optimizer, Linformer AI architecture, SimSearchNet, ...","[AI News] Một trong những hướng đi chính của Facebook AI hiện nay là triển khai các công nghệ học máy để bảo vệ người dùng khỏi những nội dung độc hại. Mục tiêu là nhận ra nhanh và chính xác những đoạn phát ngôn mang tính tấn công hoặc gây thù ghét, những thông tin sai sự thật hoặc những nội dung vi phạm pháp luật bằng mọi ngôn ngữ trên thế giới. AI hiện nay đã cải thiện rất nhiều, chủ động phát hiện 94.7% các đoạn phát ngôn không hợp lệ bị xóa khỏi Facebook, so với 80,5% của 2019 và 24% của 2017. Điều này được thực hiện bởi các công nghệ nâng cao về nhận diện tự động khác nhau như XLM, Reinforced Integrity Optimizer, Linformer AI architecture, SimSearchNet, ...",,,,,
"Mình muốn học khoá học ML cơ bản của Viện AI này! Thấy đọc thông tin trên web của Viện cũng ok lắm!
Bạn nào học rồi cho mình ít review với!
Ps: mình đăng bài hỏi ý kiến như này mấy hôm, sao ko được duyệt. Mong ad duyệt bài!","Mình muốn học khoá học ML cơ bản của Viện AI này! Thấy đọc thông tin trên web của Viện cũng ok lắm! Bạn nào học rồi cho mình ít review với! Ps: mình đăng bài hỏi ý kiến như này mấy hôm, sao ko được duyệt. Mong ad duyệt bài!",,,,,
"Cần mọi người giúp đỡ đánh giá chất lượng hệ thống thử ảo quần áo.
Mục đích là chọn ra ảnh (tự nhiên nhất, chân thực nhất).
Tổng cộng có 100 trường hợp (tốn khoảng 10 phút) ạ.
Mong được mọi người hỗ trợ.
Chân thành cảm ơn.","Cần mọi người giúp đỡ đánh giá chất lượng hệ thống thử ảo quần áo. Mục đích là chọn ra ảnh (tự nhiên nhất, chân thực nhất). Tổng cộng có 100 trường hợp (tốn khoảng 10 phút) ạ. Mong được mọi người hỗ trợ. Chân thành cảm ơn.",,,,,
,nan,,,,,
"Hi cả nhà, em đang học lại về Machine Learning và đang học đến phần K-Means và các chọn Số cụm K. Em mạnh dạn làm clip để tổng hợp kiến thức và cũng là để chia sẻ cho các bạn mới học!
Do em cũng không học trường lớp nào mà tự google nên nếu sai mong được corect để học tốt hơn ah.
Mong ad duyệt bài!","Hi cả nhà, em đang học lại về Machine Learning và đang học đến phần K-Means và các chọn Số cụm K. Em mạnh dạn làm clip để tổng hợp kiến thức và cũng là để chia sẻ cho các bạn mới học! Do em cũng không học trường lớp nào mà tự google nên nếu sai mong được corect để học tốt hơn ah. Mong ad duyệt bài!",,,,,
"Các bạn vui lòng đăng tin tuyển dụng, tuyển sinh, sự kiện tháng 12 bên dưới comment của post này.
Chúc các bạn một tháng vui vẻ.","Các bạn vui lòng đăng tin tuyển dụng, tuyển sinh, sự kiện tháng 12 bên dưới comment của post này. Chúc các bạn một tháng vui vẻ.",,,,,
"GÓC DEBUG!!
recognition_google không khả dụng nữa hả mn? Cao nhân debug giúp",GÓC DEBUG!! recognition_google không khả dụng nữa hả mn? Cao nhân debug giúp,,,,,
"Mình đã mở anacoda dưới quyền admin mà khi chạy vấn bị ""Permission denied"".
Mọi người giúp em fix lỗi này với ạ","Mình đã mở anacoda dưới quyền admin mà khi chạy vấn bị ""Permission denied"". Mọi người giúp em fix lỗi này với ạ",,,,,
"Chào a/c,
Em là sinh viên năm 4, ngành điện điện tử, e muốn ra trường làm về ML, nên e tính kiếm chỗ thực tập về ML gần (q9,hcm), e đã có nền về python và một ít về DL, e chỉ mong muốn học thêm kiến thức để có thể đi lm được nên ko cần lương ạ! Anh chị nào thương thì giúp em.","Chào a/c, Em là sinh viên năm 4, ngành điện điện tử, e muốn ra trường làm về ML, nên e tính kiếm chỗ thực tập về ML gần (q9,hcm), e đã có nền về python và một ít về DL, e chỉ mong muốn học thêm kiến thức để có thể đi lm được nên ko cần lương ạ! Anh chị nào thương thì giúp em.",,,,,
"Cái bài này có vẻ thú vị
The $25,000,000,000 Eigenvector: The Linear Algebra behind Google","Cái bài này có vẻ thú vị The $25,000,000,000 Eigenvector: The Linear Algebra behind Google",,,,,
"Chúc mừng Huyền Chip vào top!
 — với Huyen Nguyen.",Chúc mừng Huyền Chip vào top! — với Huyen Nguyen.,,,,,
"Em đang test mạng CNN cơ bản theo video hướng dẫn mà em thấy training hơi bị lâu ạ(gấp 10 lần trong video) mặc dù chương trình chạy trên GPU và trong video họ dùng GTX1050 và em dùng GTX1070ti ạ.
Mọi người cho em hỏi việc training chậm là do đâu vậy ạ và hướng khắc phục là như nào ạ?
Em cảm ơn!!!",Em đang test mạng CNN cơ bản theo video hướng dẫn mà em thấy training hơi bị lâu ạ(gấp 10 lần trong video) mặc dù chương trình chạy trên GPU và trong video họ dùng GTX1050 và em dùng GTX1070ti ạ. Mọi người cho em hỏi việc training chậm là do đâu vậy ạ và hướng khắc phục là như nào ạ? Em cảm ơn!!!,,,,,
"chào mọi người và các bạn, hiện tại mình đang làm việc trong mảng Big Data, mình nhận thấy thời gian này vòng đời technique quay rất nhanh... mình thắc mắc mãi là không biết nên tìm kênh nào để update xu hướng công nghệ, mình cũng hay tìm sách để đọc, nhưng đôi khi thấy những trend mà nhà xuất bản viết cũng không đúng lắm, nên viết lên đây để mong nhận được sự chia sẻ của mọi người. Mình xin cảm ơn.","chào mọi người và các bạn, hiện tại mình đang làm việc trong mảng Big Data, mình nhận thấy thời gian này vòng đời technique quay rất nhanh... mình thắc mắc mãi là không biết nên tìm kênh nào để update xu hướng công nghệ, mình cũng hay tìm sách để đọc, nhưng đôi khi thấy những trend mà nhà xuất bản viết cũng không đúng lắm, nên viết lên đây để mong nhận được sự chia sẻ của mọi người. Mình xin cảm ơn.",,,,,
"Chào mn ạ,
Cho em xin phép được hỏi cách giải quyết câu 5. phần early stopping với các model ở câu 4.(đã implement bằng sklearn) ạ.","Chào mn ạ, Cho em xin phép được hỏi cách giải quyết câu 5. phần early stopping với các model ở câu 4.(đã implement bằng sklearn) ạ.",,,,,
"Xin phép Ad,
chào các a/c, mọi người ai đã từng đẩy GPU passthrough từ host vào virtual machine có thể cho e ít lời khuyên ko ạ (e mong muốn tạo 1 máy ảo ubuntu dùng được GPU từ host để chạy model). E đã tìm hiểu thì biết được VMware Sphere có hỗ trợ, nhưng khi e cài Sphere xong thì GPU ko active như thế này, a/c đã từng làm qua có thể cho e vài lời khuyên ạ, e cám ơn mn!","Xin phép Ad, chào các a/c, mọi người ai đã từng đẩy GPU passthrough từ host vào virtual machine có thể cho e ít lời khuyên ko ạ (e mong muốn tạo 1 máy ảo ubuntu dùng được GPU từ host để chạy model). E đã tìm hiểu thì biết được VMware Sphere có hỗ trợ, nhưng khi e cài Sphere xong thì GPU ko active như thế này, a/c đã từng làm qua có thể cho e vài lời khuyên ạ, e cám ơn mn!",,,,,
"🎯Kính chào các bác, hôm nay em đang học về phần mạng InceptionNet nên em mạnh dạn viết một chút vừa note lại kiến thức vừa chia sẻ cho các bạn mới học.
🎯Mong admin duyệt bài ah!","Kính chào các bác, hôm nay em đang học về phần mạng InceptionNet nên em mạnh dạn viết một chút vừa note lại kiến thức vừa chia sẻ cho các bạn mới học. Mong admin duyệt bài ah!",,,,,
"TÀI LIỆU HỌC MACHINE LEARNING CHO NGƯỜI MỚI BẮT ĐẦU
Chia sẻ từ Nghiên cứu sinh Tiến sĩ Khánh Nguyễn, đại học Maryland, College Park. Chuyên nghiên cứu về NLP thông qua giảng dạy và học tập, với những nghiên cứu mũi nhọn tập trung vào phương pháp học tương tác dựa trên cấu trúc có sẵn (instruction-based interactive learning). Vinh dự khi là đại diện diễn thuyết tại Global Voices với chủ đề ""Dữ liệu đánh giá đa ngôn ngữ mới nhất"" (new evaluation dataset for cross-lingual summarization). Nghiên cứu của anh (Visual Navigation with Multimodal Natural Assistance) đã vinh dự nằm trong danh sách EMNLP2019 (tỷ lệ chấp thuận: 23.8%).","TÀI LIỆU HỌC MACHINE LEARNING CHO NGƯỜI MỚI BẮT ĐẦU Chia sẻ từ Nghiên cứu sinh Tiến sĩ Khánh Nguyễn, đại học Maryland, College Park. Chuyên nghiên cứu về NLP thông qua giảng dạy và học tập, với những nghiên cứu mũi nhọn tập trung vào phương pháp học tương tác dựa trên cấu trúc có sẵn (instruction-based interactive learning). Vinh dự khi là đại diện diễn thuyết tại Global Voices với chủ đề ""Dữ liệu đánh giá đa ngôn ngữ mới nhất"" (new evaluation dataset for cross-lingual summarization). Nghiên cứu của anh (Visual Navigation with Multimodal Natural Assistance) đã vinh dự nằm trong danh sách EMNLP2019 (tỷ lệ chấp thuận: 23.8%).",,,,,
Giới thiệu với các bạn một cuộc thi mới do Bộ KHCN đồng tổ chức sắp tới,Giới thiệu với các bạn một cuộc thi mới do Bộ KHCN đồng tổ chức sắp tới,,,,,
"Xin chào các anh chị và các bạn,
Mình là dân ngoại đạo đang học ML để phục vụ công việc. Mình muốn hỏi một vấn đề này, rất mong mọi người góp ý.
Mình triển khai các thuật toán ML cổ điển bằng sklearn thì thấy mỗi điểm dữ liệu đầu vào là 1 vector 1 chiều chứa n feature values. Đối với Deep Learning sau khi sử dụng các lớp tích chập để trích xuất được các đặc trưng của ảnh thì vẫn qua 1 lớp flatten để trải phẳng ma trận 2 chiều thành vector 1 chiều trước khi đưa vào 1 full connected layer.
Hiện nay các điểm dữ liệu của mình đang có dạng là 1 ma trận 2 chiều đối xứng như hình vẽ. Vậy mình muốn hỏi là có cách nào đưa dạng dữ liệu này vào các thuật toán ML không, hay bắt buộc phải trải phẳng ra, và việc trải phẳng ra như vậy có ảnh hưởng đến chất lượng mô hình không ạ?","Xin chào các anh chị và các bạn, Mình là dân ngoại đạo đang học ML để phục vụ công việc. Mình muốn hỏi một vấn đề này, rất mong mọi người góp ý. Mình triển khai các thuật toán ML cổ điển bằng sklearn thì thấy mỗi điểm dữ liệu đầu vào là 1 vector 1 chiều chứa n feature values. Đối với Deep Learning sau khi sử dụng các lớp tích chập để trích xuất được các đặc trưng của ảnh thì vẫn qua 1 lớp flatten để trải phẳng ma trận 2 chiều thành vector 1 chiều trước khi đưa vào 1 full connected layer. Hiện nay các điểm dữ liệu của mình đang có dạng là 1 ma trận 2 chiều đối xứng như hình vẽ. Vậy mình muốn hỏi là có cách nào đưa dạng dữ liệu này vào các thuật toán ML không, hay bắt buộc phải trải phẳng ra, và việc trải phẳng ra như vậy có ảnh hưởng đến chất lượng mô hình không ạ?",,,,,
"Xin chào các bạn,
tôi có một bài báo vừa được đăng trên Journal of Systems and Software của Elsevier, một tập san khá tốt trong ngành Software Engineering (SE). Bài nói về ứng dụng mạng CNN trong phân loại metamodels (một kiểu dữ liệu). Mạng CNN đề xuất không có gì đặc biệt, không quá sâu (deep) hay rộng (wide). Nếu mang nó so với các cấu trúc mạng mà nhiều bạn ở đây hay dùng để nhận dạng ảnh thì giống như mang con tem dán lên tờ giấy A4 :).
Trọng tâm của bài là cách thức để chuyển metamodels thành một loại format tương tự ảnh, từ đó có thể tận dụng các ưu thế của CNN với ảnh để phân loại metamodels. Tôi nghĩ có thể áp dụng kỹ thuật này với các loại dữ liệu khác, ví dụ như văn bản, tạo tiền đề để triển khai Deep Learning trên các loại dữ liệu này.
Elsevier cho tải miễn phí trong giai đoạn đầu khi bài mới được đăng, mời các bạn quan tâm ghé qua xem tại địa chỉ sau: https://authors.elsevier.com/c/1c3xebKHp2KQi
Hiện nay, tôi đang có ý tưởng về một bài báo dài hơi với chủ đề là về SE + ML, tôi cần có thêm đồng tác giả (khoảng 3 bạn nữa) để cùng đọc tài liệu và viết bài, dựa trên phân công công việc cụ thể. Nếu bạn nào đang nghiên cứu (PhD, postdoc) cùng lĩnh vực, có hứng thú hợp tác, cũng như là có thể dành thời gian cho công việc này, xin hãy liên hệ với tôi. Sản phẩm cuối cùng là bài báo và (rất tiếc là) không có lợi ích về mặt tài chính, nhưng chắc chắn sẽ có ý nghĩa trong hành trang của mỗi người làm nghiên cứu.
Hy vọng có cơ hội được hợp tác với các bạn có cùng đam mê.
Cảm ơn mọi người đã đọc tin và chúc cuối tuần vui vẻ. Stay safe!
NTP","Xin chào các bạn, tôi có một bài báo vừa được đăng trên Journal of Systems and Software của Elsevier, một tập san khá tốt trong ngành Software Engineering (SE). Bài nói về ứng dụng mạng CNN trong phân loại metamodels (một kiểu dữ liệu). Mạng CNN đề xuất không có gì đặc biệt, không quá sâu (deep) hay rộng (wide). Nếu mang nó so với các cấu trúc mạng mà nhiều bạn ở đây hay dùng để nhận dạng ảnh thì giống như mang con tem dán lên tờ giấy A4 :). Trọng tâm của bài là cách thức để chuyển metamodels thành một loại format tương tự ảnh, từ đó có thể tận dụng các ưu thế của CNN với ảnh để phân loại metamodels. Tôi nghĩ có thể áp dụng kỹ thuật này với các loại dữ liệu khác, ví dụ như văn bản, tạo tiền đề để triển khai Deep Learning trên các loại dữ liệu này. Elsevier cho tải miễn phí trong giai đoạn đầu khi bài mới được đăng, mời các bạn quan tâm ghé qua xem tại địa chỉ sau: https://authors.elsevier.com/c/1c3xebKHp2KQi Hiện nay, tôi đang có ý tưởng về một bài báo dài hơi với chủ đề là về SE + ML, tôi cần có thêm đồng tác giả (khoảng 3 bạn nữa) để cùng đọc tài liệu và viết bài, dựa trên phân công công việc cụ thể. Nếu bạn nào đang nghiên cứu (PhD, postdoc) cùng lĩnh vực, có hứng thú hợp tác, cũng như là có thể dành thời gian cho công việc này, xin hãy liên hệ với tôi. Sản phẩm cuối cùng là bài báo và (rất tiếc là) không có lợi ích về mặt tài chính, nhưng chắc chắn sẽ có ý nghĩa trong hành trang của mỗi người làm nghiên cứu. Hy vọng có cơ hội được hợp tác với các bạn có cùng đam mê. Cảm ơn mọi người đã đọc tin và chúc cuối tuần vui vẻ. Stay safe! NTP",,,,,
"Chào anh chị.
E là sv năm 2 đang suy nghĩ ý tưởng về đề tài ""ứng dụng AI vào mô hình nhà nổi chống lũ"" và có ý tưởng là dùng AI hoặc IoT để phân tích mực nước + tốc độ dòng chảy, sau đó điều chỉnh tốc độ và hướng của cánh quạt gắn dưới nhà nổi để tăng độ ổn định của nhà nổi. (kiểu như cánh quạt của thuyền á ạ)
Điều này về mặt ý tưởng có khả thi không ạ? (chỉ dừng lại ở mặt ý tưởng ko cần chi tiết về kỹ thuật)
Ngoài ra có thể ứng dụng AI vào đâu nữa ko ạ?
Em cám ơn nhiều ạ.","Chào anh chị. E là sv năm 2 đang suy nghĩ ý tưởng về đề tài ""ứng dụng AI vào mô hình nhà nổi chống lũ"" và có ý tưởng là dùng AI hoặc IoT để phân tích mực nước + tốc độ dòng chảy, sau đó điều chỉnh tốc độ và hướng của cánh quạt gắn dưới nhà nổi để tăng độ ổn định của nhà nổi. (kiểu như cánh quạt của thuyền á ạ) Điều này về mặt ý tưởng có khả thi không ạ? (chỉ dừng lại ở mặt ý tưởng ko cần chi tiết về kỹ thuật) Ngoài ra có thể ứng dụng AI vào đâu nữa ko ạ? Em cám ơn nhiều ạ.",,,,,
"Chào các anh chị, em đang tìm hiểu về Self-SVM thì có đoạn như hình. Em muốn hỏi là trong sklearn làm thế nào để lấy giá trị object function ạ. Em là newbie mong được các anh chị hướng dẫn ạ. Em xin cảm ơn!","Chào các anh chị, em đang tìm hiểu về Self-SVM thì có đoạn như hình. Em muốn hỏi là trong sklearn làm thế nào để lấy giá trị object function ạ. Em là newbie mong được các anh chị hướng dẫn ạ. Em xin cảm ơn!",,,,,
"Các bạn khi làm các bài toán nhận diện qua camera hay bị chậm, giật, lag và có rất nhiều câu hỏi kiểu ""Làm sao cho nhanh lên anh ơi"", ""Chậm quá anh ah!"", hay ""GPU là gì anh, mua thêm có nhanh hơn không?"".
Hãy cùng tìm hiểu qua video này nhé!
Mong ad duyệt bài cho các bạn newbie học hỏi thêm ah!","Các bạn khi làm các bài toán nhận diện qua camera hay bị chậm, giật, lag và có rất nhiều câu hỏi kiểu ""Làm sao cho nhanh lên anh ơi"", ""Chậm quá anh ah!"", hay ""GPU là gì anh, mua thêm có nhanh hơn không?"". Hãy cùng tìm hiểu qua video này nhé! Mong ad duyệt bài cho các bạn newbie học hỏi thêm ah!",,,,,
"Chào mọi người!
Mình xin chia sẻ một pre-trained language model tiếng Việt cho mô hình mới nổi gần đây là ELECTRA. Mô hình này dựa trên ý tưởng của GAN, huấn luyện một mô hình generator để đoán từ bị mask (giống BERT) và một mô hình discriminator để đoán xem từ nào bị thay thế. Mô hình này theo như công bố đạt được SOTA trên rất nhiều task NLP mà kích thước và thời gian train model lại thấp hơn đáng kể. (Đây là một trong số ít các nghiên cứu của google mà không lấy tài nguyên đè người 🥳)
Pre-trained mình chia sẻ là mô hình ELECTRA small được train trên dữ liệu khá lớn (~50GB text raw). Do giới hạn tài nguyên nên mới chỉ train được 400k steps trên 1 GPU 2080ti. Tuy thế nhưng phần generator và discriminator hoạt động khá tốt (mọi người xem qua tut trong repo). Mình đã port model sang tf2 và tách biệt hai model generator và discriminator để mọi người có thể sử dụng.
Link github: https://github.com/nguyenvulebinh/vietnamese-electra
PS. Nghiên cứu này có sự ghóp mặt của hai pro người Việt rất nổi tiếng là anh Thắng Lương và Quốc Lê. 😍
 — với Phí Mạnh Kiên và Nguyen Minh Phuong.","Chào mọi người! Mình xin chia sẻ một pre-trained language model tiếng Việt cho mô hình mới nổi gần đây là ELECTRA. Mô hình này dựa trên ý tưởng của GAN, huấn luyện một mô hình generator để đoán từ bị mask (giống BERT) và một mô hình discriminator để đoán xem từ nào bị thay thế. Mô hình này theo như công bố đạt được SOTA trên rất nhiều task NLP mà kích thước và thời gian train model lại thấp hơn đáng kể. (Đây là một trong số ít các nghiên cứu của google mà không lấy tài nguyên đè người ) Pre-trained mình chia sẻ là mô hình ELECTRA small được train trên dữ liệu khá lớn (~50GB text raw). Do giới hạn tài nguyên nên mới chỉ train được 400k steps trên 1 GPU 2080ti. Tuy thế nhưng phần generator và discriminator hoạt động khá tốt (mọi người xem qua tut trong repo). Mình đã port model sang tf2 và tách biệt hai model generator và discriminator để mọi người có thể sử dụng. Link github: https://github.com/nguyenvulebinh/vietnamese-electra PS. Nghiên cứu này có sự ghóp mặt của hai pro người Việt rất nổi tiếng là anh Thắng Lương và Quốc Lê. — với Phí Mạnh Kiên và Nguyen Minh Phuong.",,,,,
"Em xin phép chia sẻ bài tiếp theo của series Mì Python. Mong giúp được những kiến thức cơ bản nhất cho các bạn newbie.
Mong ad duyệt bài!",Em xin phép chia sẻ bài tiếp theo của series Mì Python. Mong giúp được những kiến thức cơ bản nhất cho các bạn newbie. Mong ad duyệt bài!,,,,,
"Xin chào mọi người,
Cho mình hỏi co cái project nào về ML trích xuất thông tin hóa đơn tự động hay không? Thông tin người bán, người mua, ngày hóa đơn, mã số hóa đơn, sản phẩm... Hoặc có công ty nào cung cấp dịch vụ này ko, hoặc ai có ý tưởng nào xin cho mình cái gợi ý
Cảm ơn mọi người","Xin chào mọi người, Cho mình hỏi co cái project nào về ML trích xuất thông tin hóa đơn tự động hay không? Thông tin người bán, người mua, ngày hóa đơn, mã số hóa đơn, sản phẩm... Hoặc có công ty nào cung cấp dịch vụ này ko, hoặc ai có ý tưởng nào xin cho mình cái gợi ý Cảm ơn mọi người",,,,,
"Sau khi hoàn thiện dịch cuốn ""Machine Learning Yearning"" và đang ở trong giai đoạn cuối của cuốn ""Dive into Deep Learning"", nhóm dịch thuật MLCB có kế hoạch dịch thêm các cuốn sách giá trị khác.
Mình đặc biệt thích cuốn ""Hands-On Machine Learning with Scikit-Learn and TensorFlow 2nd edition"" (link trong comment) và mong muốn phổ biến cuốn sách này tới các bạn đọc sử dụng tiếng Việt. Có một điểm khác biệt đó là cuốn sách này có bản quyền nên nhóm không thể tự dịch và công bố được. Thay vào đó, nhóm cần làm hợp đồng với O'Reilly và cam kết các vấn đề về bản quyền, xuất bản cũng như giá bán. Nhóm dịch thuật làm việc hoàn toàn không vì lợi nhuận; nếu không vì bản quyền, chúng tôi đã kêu gọi cộng đồng cùng dịch như hai cuốn bên trên.
Xin các bạn một phút điền bản khảo sát này để chúng tôi hiểu rõ hơn nhu cầu của cộng đồng trước khi đưa ra quyết định ký hợp đồng. Cảm ơn các bạn.
https://docs.google.com/forms/d/e/1FAIpQLSeO9gzKUOW9D9tR_aFJ-grDY7UM-SjxPLLifdlSX5qyh-2rvQ/viewform
Vì sách có thể cập nhật thêm theo phiên bản của scikit learn và TensorFlow nên chúng tôi chỉ dự định làm ebook cho bản hiện tại.","Sau khi hoàn thiện dịch cuốn ""Machine Learning Yearning"" và đang ở trong giai đoạn cuối của cuốn ""Dive into Deep Learning"", nhóm dịch thuật MLCB có kế hoạch dịch thêm các cuốn sách giá trị khác. Mình đặc biệt thích cuốn ""Hands-On Machine Learning with Scikit-Learn and TensorFlow 2nd edition"" (link trong comment) và mong muốn phổ biến cuốn sách này tới các bạn đọc sử dụng tiếng Việt. Có một điểm khác biệt đó là cuốn sách này có bản quyền nên nhóm không thể tự dịch và công bố được. Thay vào đó, nhóm cần làm hợp đồng với O'Reilly và cam kết các vấn đề về bản quyền, xuất bản cũng như giá bán. Nhóm dịch thuật làm việc hoàn toàn không vì lợi nhuận; nếu không vì bản quyền, chúng tôi đã kêu gọi cộng đồng cùng dịch như hai cuốn bên trên. Xin các bạn một phút điền bản khảo sát này để chúng tôi hiểu rõ hơn nhu cầu của cộng đồng trước khi đưa ra quyết định ký hợp đồng. Cảm ơn các bạn. https://docs.google.com/forms/d/e/1FAIpQLSeO9gzKUOW9D9tR_aFJ-grDY7UM-SjxPLLifdlSX5qyh-2rvQ/viewform Vì sách có thể cập nhật thêm theo phiên bản của scikit learn và TensorFlow nên chúng tôi chỉ dự định làm ebook cho bản hiện tại.",,,,,
Cheat Sheets for Machine Learning Interview,Cheat Sheets for Machine Learning Interview,,,,,
"Dear all,
Em muốn mua máy tính để bàn, giá khoảng 40tr để chạy code deep learning. Ai có cũ bán giá rẻ thì hay quá. Hoặc ai có thể tư vấn chỗ mua hay cấu hình máy thì em cám ơn. Do em chỉ là giáo viên tin học cấp ba ở quê mà đam mê nghiên cứu nên mong mọi người giúp đỡ giùm.","Dear all, Em muốn mua máy tính để bàn, giá khoảng 40tr để chạy code deep learning. Ai có cũ bán giá rẻ thì hay quá. Hoặc ai có thể tư vấn chỗ mua hay cấu hình máy thì em cám ơn. Do em chỉ là giáo viên tin học cấp ba ở quê mà đam mê nghiên cứu nên mong mọi người giúp đỡ giùm.",,,,,
"Chào các anh chị,
em đang được giao tìm hiểu về một bài toán kiểu như đếm số lượng của một loại object trong một hình ảnh.
các object này nói chung là gần giống nhau, kiểu như đếm số đồng xu cùng loại đặt trên bàn chẳng hạn.
em muốn hỏi là bài toán này thì thuộc loại bài toán nào trong ML và nên tìm kiếm theo từ khóa nào nhỉ ?","Chào các anh chị, em đang được giao tìm hiểu về một bài toán kiểu như đếm số lượng của một loại object trong một hình ảnh. các object này nói chung là gần giống nhau, kiểu như đếm số đồng xu cùng loại đặt trên bàn chẳng hạn. em muốn hỏi là bài toán này thì thuộc loại bài toán nào trong ML và nên tìm kiếm theo từ khóa nào nhỉ ?",,,,,
"Mình đang sử dụng thư viện VNCoreNLP để tách từ. Có một số từ thư viện tách không chính xác, ví dụ ""đăk nông"" sau khi tách từ thành 2 token ""đăk"" và ""nông"". Mình sau đó có thêm gạch nối ở giữa, thành ""đăk_nông"" nhưng VnCoreNLP lại tách ra thành 3 token ""đăk"", ""_"" và ""nông"" chứ không coi ""đăk_nông"" là 1 token.
Không biết mọi người có cách nào để khắc phục vấn đề VNCoreNLP tách một số từ không chuẩn ở trên không?","Mình đang sử dụng thư viện VNCoreNLP để tách từ. Có một số từ thư viện tách không chính xác, ví dụ ""đăk nông"" sau khi tách từ thành 2 token ""đăk"" và ""nông"". Mình sau đó có thêm gạch nối ở giữa, thành ""đăk_nông"" nhưng VnCoreNLP lại tách ra thành 3 token ""đăk"", ""_"" và ""nông"" chứ không coi ""đăk_nông"" là 1 token. Không biết mọi người có cách nào để khắc phục vấn đề VNCoreNLP tách một số từ không chuẩn ở trên không?",,,,,
Dạ em chào mấy anh chị ạ. Em mới bắt đầu học về Machine Learning nhưng có thấy là phải tốt về toán. Anh/ chị có thể cho em biết là em phải học những dạng toán nào để có thể tiếp cận được với Machine Learning không ạ. Em cám ơn ạ,Dạ em chào mấy anh chị ạ. Em mới bắt đầu học về Machine Learning nhưng có thấy là phải tốt về toán. Anh/ chị có thể cho em biết là em phải học những dạng toán nào để có thể tiếp cận được với Machine Learning không ạ. Em cám ơn ạ,,,"#Q&A, #math, #machine_learning",,
"#Conditional_Random_Fields
Hi all,
Em đang muốn dùng CRF layer trong Tensorflow Keras mà khi load model không load được CRF layer. Mọi người ai từng implement CRF với Tensorflow Keras rồi thì giúp em với ạ. Em build on top để classify từ BiLSTM.","Hi all, Em đang muốn dùng CRF layer trong Tensorflow Keras mà khi load model không load được CRF layer. Mọi người ai từng implement CRF với Tensorflow Keras rồi thì giúp em với ạ. Em build on top để classify từ BiLSTM.",#Conditional_Random_Fields,,,,
"Bàn chuyện minh họa các con số, không bàn chuyện chính trị. Các comment dạng hate speech hoặc gây hấn sẽ bị block.
Sự việc xảy ra tại Milwaukee, Wisconsin trong đêm bầu cử tổng thống Mỹ. Có một thời điểm đường màu xanh ""bỗng dưng nhảy vọt"" dẫn đến nghi ngờ về khả năng gian lận bầu cử vì điều này phi lý. Mình không khẳng định có gian lận hay không, mình chỉ xin nói rằng việc này hoàn toàn dễ hiểu.
Ở Fig 1, hình bên trái là số lượng phiếu mà bên xanh và đỏ nhận được theo thời gian. Hình bên phải là bức hình cường điệu hình bên trái được chia sẻ rất rộng rãi trên các trang mạng xã hội. Nhìn thoáng qua có vẻ hai hình như một. Nếu nhìn hình bên phải, ta sẽ thấy thực sự phi lý vì có một thời điểm mà 100% số phiếu bầu được gán cho bên xanh và 0% cho bên đỏ. Nhưng sự thật hoàn toàn khác, cảm giác ban đầu này là do cách minh hoạ tạo ra.
Nhìn kỹ hình gốc, ta thấy rằng tại thời điểm xảy ra sự việc, cả đường màu xanh và mảu đỏ đều có sự thay đổi với đường màu xanh thay đổi khoảng 4-5 lần so với đường màu đỏ. Sau đó hai đường gần như bằng nhau trong một khoảng thời gian, nhưng do đường màu xanh ""đè"" lên đường màu đỏ nên ta có cảm giác đường màu đỏ đứng im.
Bây giờ chúng ta đi sâu một chút vào ""domain knowledge"" để thấy rằng điều này là bình thường. Tại thời điểm này, rất nhiều phiếu bầu cử được đếm xong tại Milwaukee (một trong những thành phố lớn nhất bang Wisconsin) và tổng số phiếu được nhập một lượt thay vì được nhập theo một lượng phiếu nhỏ như các vùng khác. Và nếu các bạn biết về xu hướng chính trị của nước Mỹ hơn, bạn sẽ hiểu rằng các thành phố lớn có xu hướng thích màu xanh hơn màu đỏ, các vùng nông thôn thì ngược lại. Với thành phố này, năm 2016 màu xanh giành được 77% số phiếu so với 18% của màu đỏ (5% phiếu còn lại có màu khác). Đồng thời, do dân chúng thích màu xanh cũng thích gửi phiếu qua thư nên trong tập hợp các phiếu qua thư này, tỉ lệ xanh/đỏ có thể còn cao hơn nữa.
Cảm giác ""bất thường"" này xảy ra một phần do thứ tự đếm. Nếu kết quả được cập nhật mỗi khi một phiếu được đếm thì chúng ta sẽ không thấy điều này. Lấy thêm tấm hình ở Pennsylvania (Fig 2) để các bạn thấy rõ màu đỏ cũng từng có đoạn nhảy vọt so với màu xanh như thế với một lượng lớn hơn nhiều. Quan trọng là màu nào được đếm trước mà thôi.
Rất tiếc hình ảnh bên phải trong Fig 1 lại được chia sẻ rộng rãi thậm chí bởi cả những bạn bè của mình đang làm nghiên cứu khoa học. Thậm chí có bạn còn cười cợt làm hẳn một bài báo khoa học với hình bên phải để chỉ ra rằng đây là những điều phí lý và chắc chắn có gian lận bầu cử.
Mình không khẳng định dữ liệu có bị xào nấu hay không, việc này chờ tòa án vào việc. Mình chỉ muốn nói rằng các bạn nên có trách nhiệm với những gì mình chia sẻ. Một chi tiết nhỏ có thể dẫn tới một cuộc chiến tranh to.
Mình đang đọc cuốn ""How to lie with statistics"", khi nào đọc xong sẽ tóm tắt lại và đăng lên đây chúng ta cùng thảo luận.","Bàn chuyện minh họa các con số, không bàn chuyện chính trị. Các comment dạng hate speech hoặc gây hấn sẽ bị block. Sự việc xảy ra tại Milwaukee, Wisconsin trong đêm bầu cử tổng thống Mỹ. Có một thời điểm đường màu xanh ""bỗng dưng nhảy vọt"" dẫn đến nghi ngờ về khả năng gian lận bầu cử vì điều này phi lý. Mình không khẳng định có gian lận hay không, mình chỉ xin nói rằng việc này hoàn toàn dễ hiểu. Ở Fig 1, hình bên trái là số lượng phiếu mà bên xanh và đỏ nhận được theo thời gian. Hình bên phải là bức hình cường điệu hình bên trái được chia sẻ rất rộng rãi trên các trang mạng xã hội. Nhìn thoáng qua có vẻ hai hình như một. Nếu nhìn hình bên phải, ta sẽ thấy thực sự phi lý vì có một thời điểm mà 100% số phiếu bầu được gán cho bên xanh và 0% cho bên đỏ. Nhưng sự thật hoàn toàn khác, cảm giác ban đầu này là do cách minh hoạ tạo ra. Nhìn kỹ hình gốc, ta thấy rằng tại thời điểm xảy ra sự việc, cả đường màu xanh và mảu đỏ đều có sự thay đổi với đường màu xanh thay đổi khoảng 4-5 lần so với đường màu đỏ. Sau đó hai đường gần như bằng nhau trong một khoảng thời gian, nhưng do đường màu xanh ""đè"" lên đường màu đỏ nên ta có cảm giác đường màu đỏ đứng im. Bây giờ chúng ta đi sâu một chút vào ""domain knowledge"" để thấy rằng điều này là bình thường. Tại thời điểm này, rất nhiều phiếu bầu cử được đếm xong tại Milwaukee (một trong những thành phố lớn nhất bang Wisconsin) và tổng số phiếu được nhập một lượt thay vì được nhập theo một lượng phiếu nhỏ như các vùng khác. Và nếu các bạn biết về xu hướng chính trị của nước Mỹ hơn, bạn sẽ hiểu rằng các thành phố lớn có xu hướng thích màu xanh hơn màu đỏ, các vùng nông thôn thì ngược lại. Với thành phố này, năm 2016 màu xanh giành được 77% số phiếu so với 18% của màu đỏ (5% phiếu còn lại có màu khác). Đồng thời, do dân chúng thích màu xanh cũng thích gửi phiếu qua thư nên trong tập hợp các phiếu qua thư này, tỉ lệ xanh/đỏ có thể còn cao hơn nữa. Cảm giác ""bất thường"" này xảy ra một phần do thứ tự đếm. Nếu kết quả được cập nhật mỗi khi một phiếu được đếm thì chúng ta sẽ không thấy điều này. Lấy thêm tấm hình ở Pennsylvania (Fig 2) để các bạn thấy rõ màu đỏ cũng từng có đoạn nhảy vọt so với màu xanh như thế với một lượng lớn hơn nhiều. Quan trọng là màu nào được đếm trước mà thôi. Rất tiếc hình ảnh bên phải trong Fig 1 lại được chia sẻ rộng rãi thậm chí bởi cả những bạn bè của mình đang làm nghiên cứu khoa học. Thậm chí có bạn còn cười cợt làm hẳn một bài báo khoa học với hình bên phải để chỉ ra rằng đây là những điều phí lý và chắc chắn có gian lận bầu cử. Mình không khẳng định dữ liệu có bị xào nấu hay không, việc này chờ tòa án vào việc. Mình chỉ muốn nói rằng các bạn nên có trách nhiệm với những gì mình chia sẻ. Một chi tiết nhỏ có thể dẫn tới một cuộc chiến tranh to. Mình đang đọc cuốn ""How to lie with statistics"", khi nào đọc xong sẽ tóm tắt lại và đăng lên đây chúng ta cùng thảo luận.",,,,,
AI đi cùng Covid-19,AI đi cùng Covid-19,,,,,
"#AVATECH #SHAREDATA
Một số lỗi rất dễ xảy ra trong ngành sản xuất vải như là rách, vết cào, xước trên bề mặt vải, thiếu sợi,...lại gây ảnh hưởng lớn đến chất lượng vải. Điều đặc biết các loại lỗi này khá khó phát hiện, dễ gây thiếu sót khi sử dụng các phương pháp truyền thống. Hiện nay một số nước trên thế giới đã sử dụng hệ thống AI học trực tiếp dữ liệu từ hình ảnh và cho kết quả nhanh hơn, chính xác hơn.
Hi vọng bộ dữ liệu dưới đây sẽ giúp các bạn nghiên cứu và giải quyết bài toán này.
http://avatech.com.vn/kiem-tra-phat-hien-loi-san-pham/kiem-tra-chat-luong-vai-soi","Một số lỗi rất dễ xảy ra trong ngành sản xuất vải như là rách, vết cào, xước trên bề mặt vải, thiếu sợi,...lại gây ảnh hưởng lớn đến chất lượng vải. Điều đặc biết các loại lỗi này khá khó phát hiện, dễ gây thiếu sót khi sử dụng các phương pháp truyền thống. Hiện nay một số nước trên thế giới đã sử dụng hệ thống AI học trực tiếp dữ liệu từ hình ảnh và cho kết quả nhanh hơn, chính xác hơn. Hi vọng bộ dữ liệu dưới đây sẽ giúp các bạn nghiên cứu và giải quyết bài toán này. http://avatech.com.vn/kiem-tra-phat-hien-loi-san-pham/kiem-tra-chat-luong-vai-soi",#AVATECH	#SHAREDATA,,,,
"Bayesian Optimization là một thuật toán tối ưu thường dùng trong các bài tối tối ưu hàm số hộp đen (black-box functions) khi mà chúng ta không có công thức tường minh (hoặc công thức tường minh quá khó để phân tích), chi phí tính toán hàm số lớn. Trong bài viết này chúng ta cùng khảo sát việc ""khai thác"" & ""khám phá"" đánh đổi (exploitation vs. exploration tradeoff) như thế nào trong bài toán này, và chiến thuật của Bayesian Optimization là gì thông qua phần cài đặt đơn giản được trực quan hóa!
<3 ThetaLog - Nhật ký Theta!","Bayesian Optimization là một thuật toán tối ưu thường dùng trong các bài tối tối ưu hàm số hộp đen (black-box functions) khi mà chúng ta không có công thức tường minh (hoặc công thức tường minh quá khó để phân tích), chi phí tính toán hàm số lớn. Trong bài viết này chúng ta cùng khảo sát việc ""khai thác"" & ""khám phá"" đánh đổi (exploitation vs. exploration tradeoff) như thế nào trong bài toán này, và chiến thuật của Bayesian Optimization là gì thông qua phần cài đặt đơn giản được trực quan hóa! <3 ThetaLog - Nhật ký Theta!",,,"#sharing, #math",,
"Em chào mọi người, em là newbie:
em có train một mô hình deep learning LSTM bằng colab và lưu lại file .h5
****
nhưng khi load file h5 vào model thì  câu trả lời do mô hình sinh ra không có ý nghĩa như tự train
****
em có tìm hiểu thì những model khác ngta có load file weights, vậy file h5 và file weight thì anh thấy em nên chọn load file nào hay thường thì ngta sẽ load cả 2 loại vào ạ, em cảm ơn ạ","Em chào mọi người, em là newbie: em có train một mô hình deep learning LSTM bằng colab và lưu lại file .h5 **** nhưng khi load file h5 vào model thì câu trả lời do mô hình sinh ra không có ý nghĩa như tự train **** em có tìm hiểu thì những model khác ngta có load file weights, vậy file h5 và file weight thì anh thấy em nên chọn load file nào hay thường thì ngta sẽ load cả 2 loại vào ạ, em cảm ơn ạ",,,,,
"xin chào anh chị và các bạn.
em đang code 1 chương trình xài CNN của pytorch nhưng em đang gặp phải 1 vấn đề rằng khi em dùng dataloader có shuffle = True thì chương trình hiện ra lỗi then stack expects each tensor to be equal size, but got [107] at entry 0 and [82] at entry 1. Nhưng khi em cho shuffle = Flase chương trình không sảy ra lỗi.
class FruitDataLoader(Dataset):
def __init__(self,dataset):
#random.shuffle(dataset)
self.__len__ = len(dataset)
self.datas = dataset
self.train_transform = transforms.Compose([
transforms.Resize((100,100)),
transforms.ToTensor()
])
def __getitem__(self,index):
x = read_image(self.datas[index][0])
y = self.datas[index][-1]
x = self.train_transform(x)
return x,y
def __len__(self):
return self.__len__
model = FruitCNN(262)
train_dataset = FruitDataLoader(dataset)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
batch_size=BATCH_SIZE,
shuffle=True)
đây là code của em mong mọi người giúp đỡ ạ em đã search và không biết fix ở đâu để có thể shuffle được. À còn nữa em có sài Batch_size = 1 thì shuffle=True được luôn :( .","xin chào anh chị và các bạn. em đang code 1 chương trình xài CNN của pytorch nhưng em đang gặp phải 1 vấn đề rằng khi em dùng dataloader có shuffle = True thì chương trình hiện ra lỗi then stack expects each tensor to be equal size, but got [107] at entry 0 and [82] at entry 1. Nhưng khi em cho shuffle = Flase chương trình không sảy ra lỗi. class FruitDataLoader(Dataset): def __init__(self,dataset): self.__len__ = len(dataset) self.datas = dataset self.train_transform = transforms.Compose([ transforms.Resize((100,100)), transforms.ToTensor() ]) def __getitem__(self,index): x = read_image(self.datas[index][0]) y = self.datas[index][-1] x = self.train_transform(x) return x,y def __len__(self): return self.__len__ model = FruitCNN(262) train_dataset = FruitDataLoader(dataset) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True) đây là code của em mong mọi người giúp đỡ ạ em đã search và không biết fix ở đâu để có thể shuffle được. À còn nữa em có sài Batch_size = 1 thì shuffle=True được luôn :( .",#random.shuffle(dataset),,,,
"Chào anh chị và các bạn, Em đang làm hệ thống SNORT Bằng việc sử dụng thuật toán KNN để lọc các cảnh báo từ hệ thống Snort. Mọi người cho em xin hướng giải quyết với ạ. Em cảm ơn mọi người","Chào anh chị và các bạn, Em đang làm hệ thống SNORT Bằng việc sử dụng thuật toán KNN để lọc các cảnh báo từ hệ thống Snort. Mọi người cho em xin hướng giải quyết với ạ. Em cảm ơn mọi người",,,,,
"Em chào các anh. Em đang tham dự cuộc thi https://challenge.zalo.ai/.
Đề bài Traffic Sign Detection yêu cầu ko cho dùng pretrain. Vậy các a cho em hỏi em dùng YOLO , em có train vói data của Zalo cho thì có gọi pretrain ko ah? Hay transfer learning!?","Em chào các anh. Em đang tham dự cuộc thi https://challenge.zalo.ai/. Đề bài Traffic Sign Detection yêu cầu ko cho dùng pretrain. Vậy các a cho em hỏi em dùng YOLO , em có train vói data của Zalo cho thì có gọi pretrain ko ah? Hay transfer learning!?",,,,,
"Chào mọi người,
Có ai biết có trang web nào đăng cần tìm người làm project machine learning ngoài giờ mà có trả tiền, kiểu làm cuối tuần hoặc làm ban đêm. Lấy ví dụ như người ta đưa mình 1 project, yêu cầu làm trong vòng 30 ngày, với có yêu cầu về metric. Mấy hôm trước mình có tìm thấy 1 trang web như thế nhưng giờ lại tìm ko ra.
Thanks mọi người.","Chào mọi người, Có ai biết có trang web nào đăng cần tìm người làm project machine learning ngoài giờ mà có trả tiền, kiểu làm cuối tuần hoặc làm ban đêm. Lấy ví dụ như người ta đưa mình 1 project, yêu cầu làm trong vòng 30 ngày, với có yêu cầu về metric. Mấy hôm trước mình có tìm thấy 1 trang web như thế nhưng giờ lại tìm ko ra. Thanks mọi người.",,,,,
"Chào mọi người, hiện em đang làm đề tài về đếm người trong hồ bơi thông qua camera an ninh. Em có thử dùng yolo v4 để đếm tuy nhiên model chỉ đem lại hiệu quả với những ảnh cận còn những ảnh của camera an ninh thì hơi khó (trong ảnh dưới). Mong mọi người cho em lời khuyên hay kinh nghiệm về xử lý đề tài này. Em xin cảm ơn","Chào mọi người, hiện em đang làm đề tài về đếm người trong hồ bơi thông qua camera an ninh. Em có thử dùng yolo v4 để đếm tuy nhiên model chỉ đem lại hiệu quả với những ảnh cận còn những ảnh của camera an ninh thì hơi khó (trong ảnh dưới). Mong mọi người cho em lời khuyên hay kinh nghiệm về xử lý đề tài này. Em xin cảm ơn",,,,,
"Chào cả nhà, mình sắp tới có nói chuyện ở Toronto Machine Learning Society annual event và có 2 free tickets. Bạn nào muốn đi thì email mình chip@huyenchip.com nhé! https://www.eventbrite.ca/e/toronto-machine-learning-society-tmls-2020-annual-conference-expo-tickets-115917822327","Chào cả nhà, mình sắp tới có nói chuyện ở Toronto Machine Learning Society annual event và có 2 free tickets. Bạn nào muốn đi thì email mình chip@huyenchip.com nhé! https://www.eventbrite.ca/e/toronto-machine-learning-society-tmls-2020-annual-conference-expo-tickets-115917822327",,,,,
"Hi mn.
Mn cho em hỏi là Em nhận dạng vật thể phải resize ảnh về kích thước 32x32. Nhưng đối với những ảnh kích thước nhỏ hơn ko resize về được thì chương trình bị lỗi.
Em người cho em hỏi cách để fix với ạ. E cảm ơn",Hi mn. Mn cho em hỏi là Em nhận dạng vật thể phải resize ảnh về kích thước 32x32. Nhưng đối với những ảnh kích thước nhỏ hơn ko resize về được thì chương trình bị lỗi. Em người cho em hỏi cách để fix với ạ. E cảm ơn,,,,,
"chào m.n
e có làm 1 projcet nhỏ phát hiện đối tượng với yolo4, e gặp 1 vấn đề nho nhỏ là e dùng 2 webcam tốc độ khá chậm(0.33 fps)
cho e hỏi là có cách nào để trong1 thời điểm chỉ có 1 webcam chạy không ạ? hay có cách nào để tăng tốc độ lên k ạ@","chào m.n e có làm 1 projcet nhỏ phát hiện đối tượng với yolo4, e gặp 1 vấn đề nho nhỏ là e dùng 2 webcam tốc độ khá chậm(0.33 fps) cho e hỏi là có cách nào để trong1 thời điểm chỉ có 1 webcam chạy không ạ? hay có cách nào để tăng tốc độ lên k ạ@",,,,,
"Mọi người cho em hỏi, cái này là vấn đề lý thuyết thôi ạ.
Trong K-means clustering, ban đầu chọn random K điểm làm centroids, và xếp các data points vào K clusters dựa vào khoảng cách tới các centroids. Giả sử có điểm X1 có cùng khoảng cách tới 2 centroids là C1 và C2, thì ở lần phân bổ data này làm sao để chọn X1 thuộc C1 hay C2 ạ?","Mọi người cho em hỏi, cái này là vấn đề lý thuyết thôi ạ. Trong K-means clustering, ban đầu chọn random K điểm làm centroids, và xếp các data points vào K clusters dựa vào khoảng cách tới các centroids. Giả sử có điểm X1 có cùng khoảng cách tới 2 centroids là C1 và C2, thì ở lần phân bổ data này làm sao để chọn X1 thuộc C1 hay C2 ạ?",,,,,
"Mình có đang làm một project về Virtual Background. Tuy không to tát gì cho lắm nhưng cũng xin mạn phép chia sẻ với mọi người. Project ở link https://github.com/tamnguyenvan/beecam.
Về đại khái, chương trình cho phép thay đổi hình nền từ nguồn webcam/camera theo thời gian thực trên CPU. Hỗ trợ virtual camera qua đó có thể kết nối trực tiếp tới các phần mềm như Skype, Zoom. Mình có build sẵn một bản cho Windows chỉ cần tải về và chạy.
Hy vọng nó có ích cho ai đó ^^","Mình có đang làm một project về Virtual Background. Tuy không to tát gì cho lắm nhưng cũng xin mạn phép chia sẻ với mọi người. Project ở link https://github.com/tamnguyenvan/beecam. Về đại khái, chương trình cho phép thay đổi hình nền từ nguồn webcam/camera theo thời gian thực trên CPU. Hỗ trợ virtual camera qua đó có thể kết nối trực tiếp tới các phần mềm như Skype, Zoom. Mình có build sẵn một bản cho Windows chỉ cần tải về và chạy. Hy vọng nó có ích cho ai đó ^^",,,,,
"mình đang tìm hiểu về Yolov5, không biết mấy số mình khoan tròn đỏ có phải xác suất phân loại không?, làm sao mình có thể lấy được nó về sau khi test từng hình","mình đang tìm hiểu về Yolov5, không biết mấy số mình khoan tròn đỏ có phải xác suất phân loại không?, làm sao mình có thể lấy được nó về sau khi test từng hình",,,,,
"Em kính chào các bác. Nhân dịp đang nghiên cứu về Liveness Detection / Chống giả mạo trong nhận diện khuôn mặt và có mượn được một chiếc Camera 3D nên em xin mạn phép làm bài review demo mong giúp đỡ được các bạn mới học
Mong ad duyệt bài!",Em kính chào các bác. Nhân dịp đang nghiên cứu về Liveness Detection / Chống giả mạo trong nhận diện khuôn mặt và có mượn được một chiếc Camera 3D nên em xin mạn phép làm bài review demo mong giúp đỡ được các bạn mới học Mong ad duyệt bài!,,,,,
"Dạ em chào mọi người. Có cách nào để tách các số trong ảnh ra để train , và nhận biết tọa độ của của con số mình mong muốn không ạ","Dạ em chào mọi người. Có cách nào để tách các số trong ảnh ra để train , và nhận biết tọa độ của của con số mình mong muốn không ạ",,,,,
"Hi mọi người,
Em vừa mới bắt đầu học Computer Vision cho mảng VR/AR. Có anh chị nào trong group mình có kinh nghiệm về mảng này cho em hỏi nên bắt đầu học từ đâu ạ? Em xin cảm ơn.
P/S: Em đang muốn đào sâu về áp dụng GANs trong 3D Object/Human shape. Các technique với 2D image thì em hiểu nhưng còn 3D thì em thấy còn phụ thuộc nhiều về system và rendering nữa, em không rành nên hỏi mọi người ạ.","Hi mọi người, Em vừa mới bắt đầu học Computer Vision cho mảng VR/AR. Có anh chị nào trong group mình có kinh nghiệm về mảng này cho em hỏi nên bắt đầu học từ đâu ạ? Em xin cảm ơn. P/S: Em đang muốn đào sâu về áp dụng GANs trong 3D Object/Human shape. Các technique với 2D image thì em hiểu nhưng còn 3D thì em thấy còn phụ thuộc nhiều về system và rendering nữa, em không rành nên hỏi mọi người ạ.",,,,,
Có anh/chị nào đã apply financial aid thành công ở khoá deep learning specialization trên coursera chưa ạ kinh nghiệm fill form với ạ em xin cảm ơn.,Có anh/chị nào đã apply financial aid thành công ở khoá deep learning specialization trên coursera chưa ạ kinh nghiệm fill form với ạ em xin cảm ơn.,,,,,
"#crawler
Chào mọi người.
Hiện tại em có 1 list rất nhiều từ tiếng anh, và em muốn thu thập các câu chứa mỗi từ đó. Vậy cho em hỏi có cách nào để crawl các câu của mỗi từ trên internet không ạ ? Ví dụ trong wiki thì em chỉ cần các câu ở phần định nghĩa khi search từ đó. Không biết có ai có kinh nghiệm về vấn đề này không ạ ?
Em cảm ơn ạ.","Chào mọi người. Hiện tại em có 1 list rất nhiều từ tiếng anh, và em muốn thu thập các câu chứa mỗi từ đó. Vậy cho em hỏi có cách nào để crawl các câu của mỗi từ trên internet không ạ ? Ví dụ trong wiki thì em chỉ cần các câu ở phần định nghĩa khi search từ đó. Không biết có ai có kinh nghiệm về vấn đề này không ạ ? Em cảm ơn ạ.",#crawler,,,,
"Gần đây mình có chia sẻ bài báo Vision Transformer, giờ đã được đăng trên ArXiv https://arxiv.org/pdf/2010.11929.pdf. Tác giả đã chia sẻ source code gốc (viết bằng JAX) tại đây https://github.com/google-research/vision_transformer/tree/master/vit_jax; Hay code được viết trên Pytorch và pretrained weights cũng đã được port sang PT tại đây https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py; Code viết bằng TensorFlow tại đây https://github.com/emla2805/vision-trànormer.
Lưu ý: ViT cần rất nhiều dữ liệu để train nên ảnh hưởng của Transfer Learning đến việc ứng dụng vào dataset của các bạn là rất lớn. Nên mình khuyên hãy sử dụng original source code vì có pretrained weights. Tuy nhiên, nó được viết bằng JAX, 1 framework do Google phát triển nhưng chưa phổ biến lắm có thể gây khó khăn ban đầu. Code viết trên TF tác giả chưa convert weights nên các bạn phải tự làm. Theo mình code của anh Wightman trên PT rất sạch, anh ấy đã port weights sang PT và down-Scale từ ViT-Base xuống ViT-Small với image size 224 sẽ rất tiện để train với tài nguyên tính toán hạn chế.","Gần đây mình có chia sẻ bài báo Vision Transformer, giờ đã được đăng trên ArXiv https://arxiv.org/pdf/2010.11929.pdf. Tác giả đã chia sẻ source code gốc (viết bằng JAX) tại đây https://github.com/google-research/vision_transformer/tree/master/vit_jax; Hay code được viết trên Pytorch và pretrained weights cũng đã được port sang PT tại đây https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py; Code viết bằng TensorFlow tại đây https://github.com/emla2805/vision-trànormer. Lưu ý: ViT cần rất nhiều dữ liệu để train nên ảnh hưởng của Transfer Learning đến việc ứng dụng vào dataset của các bạn là rất lớn. Nên mình khuyên hãy sử dụng original source code vì có pretrained weights. Tuy nhiên, nó được viết bằng JAX, 1 framework do Google phát triển nhưng chưa phổ biến lắm có thể gây khó khăn ban đầu. Code viết trên TF tác giả chưa convert weights nên các bạn phải tự làm. Theo mình code của anh Wightman trên PT rất sạch, anh ấy đã port weights sang PT và down-Scale từ ViT-Base xuống ViT-Small với image size 224 sẽ rất tiện để train với tài nguyên tính toán hạn chế.",,,,,
"#Share_AIData
AVA chia sẻ bộ dữ liệu ảnh giúp các bạn thực hành huấn luyện mô hình AI.
Bộ dữ liệu gồm 2670 ảnh là ba mẫu hình dạng thuốc cơ bản thường thấy trên thị trường với hai lỗi hay gặp trong quá trình sản xuất thuốc là dính tạp chất và nứt vỡ.
Dựa vào bộ dữ liệu các bạn có thể tiến hành thử nghiệm các bài toán phân loại, phát hiện lỗi trền bề mặt sản phẩm.
Chi tiết link download dữ liệu và một số kết quả AVA chạy được mời các bạn xem tại: http://avatech.com.vn/kiem-tra-phat-hien-loi-san-pham/kiem-tra-chat-luong-vien-thuoc","AVA chia sẻ bộ dữ liệu ảnh giúp các bạn thực hành huấn luyện mô hình AI. Bộ dữ liệu gồm 2670 ảnh là ba mẫu hình dạng thuốc cơ bản thường thấy trên thị trường với hai lỗi hay gặp trong quá trình sản xuất thuốc là dính tạp chất và nứt vỡ. Dựa vào bộ dữ liệu các bạn có thể tiến hành thử nghiệm các bài toán phân loại, phát hiện lỗi trền bề mặt sản phẩm. Chi tiết link download dữ liệu và một số kết quả AVA chạy được mời các bạn xem tại: http://avatech.com.vn/kiem-tra-phat-hien-loi-san-pham/kiem-tra-chat-luong-vien-thuoc",#Share_AIData,,,,
"Hội mình có ai có máy tính desktop có cấu hình i7, gpu gtx 970, ram 16 gb ko ạ?
Cho mình mượn chạy inference để so sánh cho fair với bài của người ta, (tại máy mình chênh lệch hiệu năng quá so với cấu hình này), hoặc có suggest gì để 1 máy mạnh hơn chạy yếu hơn để so sánh công bằng ko ạ?
Cảm ơn mọi người.
(Thời gian mượn chắc độ 30 phút là xong).","Hội mình có ai có máy tính desktop có cấu hình i7, gpu gtx 970, ram 16 gb ko ạ? Cho mình mượn chạy inference để so sánh cho fair với bài của người ta, (tại máy mình chênh lệch hiệu năng quá so với cấu hình này), hoặc có suggest gì để 1 máy mạnh hơn chạy yếu hơn để so sánh công bằng ko ạ? Cảm ơn mọi người. (Thời gian mượn chắc độ 30 phút là xong).",,,,,
"Hi mn, mình hỏi ý kiếm mn với ạ. Hiện tại mình chưa hề biết gì và chưa có kiến thức về Machine Learning, mình có khoảng 3 tháng rảnh không vướng bận cv để học. mình có ý định ra HCM để học 1 khóa ở 1 trung tâm nào đó (hiện tại mình thấy có fpt software) cho dễ vào hơn rồi mới tự mò. m.n có khóa học nào ở HCM tốt không ạ.","Hi mn, mình hỏi ý kiếm mn với ạ. Hiện tại mình chưa hề biết gì và chưa có kiến thức về Machine Learning, mình có khoảng 3 tháng rảnh không vướng bận cv để học. mình có ý định ra HCM để học 1 khóa ở 1 trung tâm nào đó (hiện tại mình thấy có fpt software) cho dễ vào hơn rồi mới tự mò. m.n có khóa học nào ở HCM tốt không ạ.",,,,,
"Hôm trước có bạn nào hỏi phần xóa phông cho mobile apps.
Đây là bài SOTA mới về phần này : high res, high accuracy.
Chỉ là chưa convert sang mobile.","Hôm trước có bạn nào hỏi phần xóa phông cho mobile apps. Đây là bài SOTA mới về phần này : high res, high accuracy. Chỉ là chưa convert sang mobile.",,,,,
"Chào mọi người, mình đang vướng ở đề tài huấn luyện để nhận diện mã vạch.
Ở trong nhóm có ai đã tìm hiểu thì có thể chỉ mình về mảng này được không? Mình không chuyên về mảng này.
Xin cảm ơn mọi người!","Chào mọi người, mình đang vướng ở đề tài huấn luyện để nhận diện mã vạch. Ở trong nhóm có ai đã tìm hiểu thì có thể chỉ mình về mảng này được không? Mình không chuyên về mảng này. Xin cảm ơn mọi người!",,,,,
"Camera sử dụng AI để tự động quay và focus vào trái bóng. Tuy nhiên, trong phần lớn thời gian của trận đấu, khán giả được chiêm ngưỡng những bước chạy của trọng tài biên thay vì trái bóng, ... bởi vì ông có 1 cái đầu trọc... 
Lần sau có lẽ bác nên đội mũ để AI tránh nhầm lẫn với trái bóng. Một ví dụ về AI trong ứng dụng đời sống..., và điều gì sẽ xảy ra khi xe tự động lái cũng có những nhầm lẫn tương tự?
#adversarial_attack","Camera sử dụng AI để tự động quay và focus vào trái bóng. Tuy nhiên, trong phần lớn thời gian của trận đấu, khán giả được chiêm ngưỡng những bước chạy của trọng tài biên thay vì trái bóng, ... bởi vì ông có 1 cái đầu trọc... Lần sau có lẽ bác nên đội mũ để AI tránh nhầm lẫn với trái bóng. Một ví dụ về AI trong ứng dụng đời sống..., và điều gì sẽ xảy ra khi xe tự động lái cũng có những nhầm lẫn tương tự?",#adversarial_attack,,,,
"Hi mọi người,
Chuyện là em có train 1 custom model với google automl vision. Giờ em muốn xem architecture của model đó với file model.json và weights.bin thì có cách nào không ạ ?","Hi mọi người, Chuyện là em có train 1 custom model với google automl vision. Giờ em muốn xem architecture của model đó với file model.json và weights.bin thì có cách nào không ạ ?",,,,,
"[MÌ AI #4] - THỬ LẬP TRÌNH XE OTO TỰ LÁI
Link bài viết: http://miai.vn/2019/08/19/computer-vision-thu-lap-trinh-xe-o-to-dua-tu-lai/
Chào tuần mới cả nhà, chúng ta lại cùng ăn mì cùng Mì AI – học AI theo cách mì ăn liền. Mình sẽ cố gắng đơn giản hóa các khái niệm phức tạp, để làm sao các bạn dễ tiếp cận và làm AI một cách Mì ăn liền. Đích đến cuối cùng là Ai cũng làm được AI 😀
Okie, hẳn các bạn đã nghe đến nhiều dự án xe tự lái của Google đúng không? Xe sẽ tự hành và được điều khiển hoàn toàn bằng máy tính trên xe, không cần con người can thiệp. Hôm nay chúng ta sẽ cùng nhau làm một model xe tự lái đơn giản để hiểu được nguyên lý, cách làm xe tự lái nhé.","[MÌ AI - THỬ LẬP TRÌNH XE OTO TỰ LÁI Link bài viết: http://miai.vn/2019/08/19/computer-vision-thu-lap-trinh-xe-o-to-dua-tu-lai/ Chào tuần mới cả nhà, chúng ta lại cùng ăn mì cùng Mì AI – học AI theo cách mì ăn liền. Mình sẽ cố gắng đơn giản hóa các khái niệm phức tạp, để làm sao các bạn dễ tiếp cận và làm AI một cách Mì ăn liền. Đích đến cuối cùng là Ai cũng làm được AI Okie, hẳn các bạn đã nghe đến nhiều dự án xe tự lái của Google đúng không? Xe sẽ tự hành và được điều khiển hoàn toàn bằng máy tính trên xe, không cần con người can thiệp. Hôm nay chúng ta sẽ cùng nhau làm một model xe tự lái đơn giản để hiểu được nguyên lý, cách làm xe tự lái nhé.",#4],,,,
"Chào mọi người.
Mình đang viết chương trình training model trên nhiều gpus, hiện tại là 2 gpu, mình dùng tf.distribute.MirroredStrategy(tensorflow 2.x). Mình chạy source tutorial của tensorflow , nhưng mà training trên 2gpu lại không nhanh hơn 1gpu, thậm chí trên 1gpu lại nhanh hơn 1 chút, mình đã tìm nguyên nhân trên google nhưng mà vẫn chưa giải quyết được. Ai đã từng làm rồi có thể giúp mình với được không ạ.
Cảm ơn mọi người","Chào mọi người. Mình đang viết chương trình training model trên nhiều gpus, hiện tại là 2 gpu, mình dùng tf.distribute.MirroredStrategy(tensorflow 2.x). Mình chạy source tutorial của tensorflow , nhưng mà training trên 2gpu lại không nhanh hơn 1gpu, thậm chí trên 1gpu lại nhanh hơn 1 chút, mình đã tìm nguyên nhân trên google nhưng mà vẫn chưa giải quyết được. Ai đã từng làm rồi có thể giúp mình với được không ạ. Cảm ơn mọi người",,,,,
"URL2Video, một nghiên cứu mới của Google, cho phép tự động tạo một video ngắn từ trang web.","URL2Video, một nghiên cứu mới của Google, cho phép tự động tạo một video ngắn từ trang web.",,,,,
"Xin chào Anh/Chị,
Em đang có một bài toán tìm quãng đường đi ngắn nhất trên map.
Anh/Chị gợi ý giúp e một số cách xử lý bằng ML.
Data em đang có từ Google API
+ Start point (lat/long)
+ End point (lat/long)
+ Distance Google API
+ Labled by bikers.
Kết quả mong muốn:
+ Optimize lại quãng đường đã đi bằng 1 lịch trình mới ngắn hơn mà k bị vi phạm luật giao thông (đi vào đường cấm, đường ngược chiều)
+ Khi input 1 Start/End Point mới, ML sẽ output lịch trình tốt nhất là data point mà bikers cần đi qua và distance của trip đó.
Em xin cảm ơn.","Xin chào Anh/Chị, Em đang có một bài toán tìm quãng đường đi ngắn nhất trên map. Anh/Chị gợi ý giúp e một số cách xử lý bằng ML. Data em đang có từ Google API + Start point (lat/long) + End point (lat/long) + Distance Google API + Labled by bikers. Kết quả mong muốn: + Optimize lại quãng đường đã đi bằng 1 lịch trình mới ngắn hơn mà k bị vi phạm luật giao thông (đi vào đường cấm, đường ngược chiều) + Khi input 1 Start/End Point mới, ML sẽ output lịch trình tốt nhất là data point mà bikers cần đi qua và distance của trip đó. Em xin cảm ơn.",,,,,
"Chào mn, em đang đọc về face_recognition và có đọc đến facenet, và đọc về pre trained của nó nhưng em đang không hiểu là nó trainning kiểu gì ạ, đầu vào là ảnh; đầu ra chỉ là 1 vector 128D, vậy label là gì, và nó tối ưu cái mạng facenet này như nào ạ. Em chưa nói đến phần classify ạ, ở đây em mới đang thắc mắc về cái facenet để extracting ảnh mặt người thành vector 128D ạ. Em cảm ơn ạ!","Chào mn, em đang đọc về face_recognition và có đọc đến facenet, và đọc về pre trained của nó nhưng em đang không hiểu là nó trainning kiểu gì ạ, đầu vào là ảnh; đầu ra chỉ là 1 vector 128D, vậy label là gì, và nó tối ưu cái mạng facenet này như nào ạ. Em chưa nói đến phần classify ạ, ở đây em mới đang thắc mắc về cái facenet để extracting ảnh mặt người thành vector 128D ạ. Em cảm ơn ạ!",,,,,
"Chào mọi người, mình đang có 1 project cần build tool để detect các triệu chứng về da mặt khi upload hình lên API.
Mình tìm thấy tài liệu về 1 team đã làm phần này rất ổn. Mọi người xem qua, ai làm được thì PM mình nhé.","Chào mọi người, mình đang có 1 project cần build tool để detect các triệu chứng về da mặt khi upload hình lên API. Mình tìm thấy tài liệu về 1 team đã làm phần này rất ổn. Mọi người xem qua, ai làm được thì PM mình nhé.",,,,,
"Hi mọi người,
Mình định build một pc để nghiên cứu Deep learning theo hướng dự báo time series, thỉnh thoảng có chạm vào computer vision nhưng k phải là hướng nghiên cứu chính. Với chi phí tầm 30tr, mình có tham khảo được một cấu hình như bên dưới. Nhờ mọi người tư vấn giúp cấu hình như vậy có ổn không, có cần thay đổi gì không?
Cảm ơn mọi người.","Hi mọi người, Mình định build một pc để nghiên cứu Deep learning theo hướng dự báo time series, thỉnh thoảng có chạm vào computer vision nhưng k phải là hướng nghiên cứu chính. Với chi phí tầm 30tr, mình có tham khảo được một cấu hình như bên dưới. Nhờ mọi người tư vấn giúp cấu hình như vậy có ổn không, có cần thay đổi gì không? Cảm ơn mọi người.",,,,,
"Xin chào mọi người, em là dev mobile. Hiện em đang làm tính năng virtual background trên video call. Hướng đi cụ thể của em là lấy từng frame , thực hiện detect (Image segmentation) và biết được giá trị của pixel (Có phải người hay 1 đối tượng nào khác không) và thay thế từng pixel KHÔNG PHẢI NGƯỜI thành pixel của background (ý em là chép từng pixel).
Em đang sử dung Tensorflow (https://www.tensorflow.org/lite/models/segmentation/overview) để đọc model deeplabv3, kết hợp với opencv để xử lý hình ảnh
Câu hỏi của em là: Có model nào khác sử lý Image segmentation nhan hơn không ạ. Bác nào từng có kinh nghiệm giải quyết vấn đề kết hợp với mobile kiểu này thì cho em xin vài lời khuyên với ạ
Mong mọi người giúp em ạ. Đã gần 2 tháng em vật lộn với cái này và bắt đầu bí ý tưởng và không biết làm gì để tối ưu. Lần đầu tiên tiếp xúc với ML nên kiến thức của em vô cùng nông cạn và lung tung, mong được mọi người khai sáng","Xin chào mọi người, em là dev mobile. Hiện em đang làm tính năng virtual background trên video call. Hướng đi cụ thể của em là lấy từng frame , thực hiện detect (Image segmentation) và biết được giá trị của pixel (Có phải người hay 1 đối tượng nào khác không) và thay thế từng pixel KHÔNG PHẢI NGƯỜI thành pixel của background (ý em là chép từng pixel). Em đang sử dung Tensorflow (https://www.tensorflow.org/lite/models/segmentation/overview) để đọc model deeplabv3, kết hợp với opencv để xử lý hình ảnh Câu hỏi của em là: Có model nào khác sử lý Image segmentation nhan hơn không ạ. Bác nào từng có kinh nghiệm giải quyết vấn đề kết hợp với mobile kiểu này thì cho em xin vài lời khuyên với ạ Mong mọi người giúp em ạ. Đã gần 2 tháng em vật lộn với cái này và bắt đầu bí ý tưởng và không biết làm gì để tối ưu. Lần đầu tiên tiếp xúc với ML nên kiến thức của em vô cùng nông cạn và lung tung, mong được mọi người khai sáng",,,,,
,nan,,,,,
"[thiết bị] [camera]
Chào mọi người hiện tại em đang làm đề tài có phần liên quan đến nhận diện biển số xe.
Em dùng yolov4 để detect license plate sau đó dùng tesseract để chiết tự, kết quả không chính xác lắm nên em train CNN để enhance tuy nhiên cũng ko cải thiện được nhiều.
Khi log file binary ra thì mới biết ảnh quá mờ để nhận dạng. Vì thế cho em hỏi có camera nào trên thị trường đủ tốt để lấy biển số từ cao và mức giá phù hợp cho sinh viên không ạ (tụi em mua 2 cái, mỗi cái tầm 6-700k đổ lại).
Hiện tại em dùng 2 camera IP Ezviz ạ, thích hợp cho nhận diện gần, hàng mới.
https://tiki.vn/camera-ip-wifi-ezviz-c6n-1080p-hang-chinh-hang-p41319582.html
Bác nào có con camera phù hợp với đồ án thì tụi em có thể mua lại hoặc trade ạ, em ở TP HCM.
Hoặc có cách nào improve accuracy thì cũng mong sự tư vấn ạ, em cảm ơn rất nhiều vì giúp đỡ.","[thiết bị] [camera] Chào mọi người hiện tại em đang làm đề tài có phần liên quan đến nhận diện biển số xe. Em dùng yolov4 để detect license plate sau đó dùng tesseract để chiết tự, kết quả không chính xác lắm nên em train CNN để enhance tuy nhiên cũng ko cải thiện được nhiều. Khi log file binary ra thì mới biết ảnh quá mờ để nhận dạng. Vì thế cho em hỏi có camera nào trên thị trường đủ tốt để lấy biển số từ cao và mức giá phù hợp cho sinh viên không ạ (tụi em mua 2 cái, mỗi cái tầm 6-700k đổ lại). Hiện tại em dùng 2 camera IP Ezviz ạ, thích hợp cho nhận diện gần, hàng mới. https://tiki.vn/camera-ip-wifi-ezviz-c6n-1080p-hang-chinh-hang-p41319582.html Bác nào có con camera phù hợp với đồ án thì tụi em có thể mua lại hoặc trade ạ, em ở TP HCM. Hoặc có cách nào improve accuracy thì cũng mong sự tư vấn ạ, em cảm ơn rất nhiều vì giúp đỡ.",,,,,
"Kính chào các member, các bạn mới học thì hay làm bài tập trên PC hoặc Colab nên khi train hay bị tràn bộ nhớ khi load nhiều dữ liệu (đặc biệt là các bài CNN, dữ liệu ảnh). Em xin mạnh dạn chia sẻ bài viết về xử lý vấn đề này mong giúp được các bạn newbie mới học ah. Cảm ơn và mong ad duyệt bài!","Kính chào các member, các bạn mới học thì hay làm bài tập trên PC hoặc Colab nên khi train hay bị tràn bộ nhớ khi load nhiều dữ liệu (đặc biệt là các bài CNN, dữ liệu ảnh). Em xin mạnh dạn chia sẻ bài viết về xử lý vấn đề này mong giúp được các bạn newbie mới học ah. Cảm ơn và mong ad duyệt bài!",,,,,
"Kính chào các bạn, mình có làm một game nhỏ dùng OpenCV và Dlib nên chia sẻ cùng các bạn newbie mới học. Mong giúp được các bạn kỹ năng code từ đầu một bài toán ah.
Mong ad duyệt bài!","Kính chào các bạn, mình có làm một game nhỏ dùng OpenCV và Dlib nên chia sẻ cùng các bạn newbie mới học. Mong giúp được các bạn kỹ năng code từ đầu một bài toán ah. Mong ad duyệt bài!",,,,,
"Em chaof mọi người, hiện em đang có làm 1 project nhỏ về 1 cái máy cứ có rác trong phòng là sẽ tự chạy đến quét đi.
Vấn đề của em hiện tại: Em sẽ cho ""rác"" là những thứ đại loại như mẩu giấy vụn, vỏ chai nhựa.
Vậy có cách nào xử lý được nhanh(như opencv) việc này không mọi người nhỉ?
Cảm ơn mọi người ạ!","Em chaof mọi người, hiện em đang có làm 1 project nhỏ về 1 cái máy cứ có rác trong phòng là sẽ tự chạy đến quét đi. Vấn đề của em hiện tại: Em sẽ cho ""rác"" là những thứ đại loại như mẩu giấy vụn, vỏ chai nhựa. Vậy có cách nào xử lý được nhanh(như opencv) việc này không mọi người nhỉ? Cảm ơn mọi người ạ!",,,,,
"Chào các a/c,
Em đang làm bài toán về Classification, model e trained dựa trên tập data Táo (3 bệnh, 1 khoẻ), và em dùng model này để chạy trên tập data Bưởi (tương tự như Táo cũng 3 bệnh, 1 khoẻ). Lí do em làm thế này là vì em không có bộ data thu thập thực tế, nên em muốn dùng model chạy trên 1 tập khác có đặc tính tương tự, rồi sau đó e sẽ cải thiện model bằng cách tác động sâu hơn vào kiến trúc của model như điều chình các thông số: Hyperparameter, Regularization và Optimation... A/c nào đã từng làm qua bài toán tương tự có thể cho em vài gợi ý, hoặc suggest cho e vài bài paper đc ko ạ, em cám ơn mn.","Chào các a/c, Em đang làm bài toán về Classification, model e trained dựa trên tập data Táo (3 bệnh, 1 khoẻ), và em dùng model này để chạy trên tập data Bưởi (tương tự như Táo cũng 3 bệnh, 1 khoẻ). Lí do em làm thế này là vì em không có bộ data thu thập thực tế, nên em muốn dùng model chạy trên 1 tập khác có đặc tính tương tự, rồi sau đó e sẽ cải thiện model bằng cách tác động sâu hơn vào kiến trúc của model như điều chình các thông số: Hyperparameter, Regularization và Optimation... A/c nào đã từng làm qua bài toán tương tự có thể cho em vài gợi ý, hoặc suggest cho e vài bài paper đc ko ạ, em cám ơn mn.",,,,,
"Các bác bên ML cho em hỏi xíu là, em xây dựng model ML của em rồi, và nhận thấy ma trận W và b nó có nhiều số 0 quá, nên đang muốn tìm cách xóa nó đi, thay đổi kích thước của W và b, để giảm thời gian tính toán cho model. Không biết có phương pháp hay package nào có thể giúp làm chuyện này tốt không? Cám ơn các bác.","Các bác bên ML cho em hỏi xíu là, em xây dựng model ML của em rồi, và nhận thấy ma trận W và b nó có nhiều số 0 quá, nên đang muốn tìm cách xóa nó đi, thay đổi kích thước của W và b, để giảm thời gian tính toán cho model. Không biết có phương pháp hay package nào có thể giúp làm chuyện này tốt không? Cám ơn các bác.",,,,,
"https://youtu.be/uGrBHohIgQY
co ban nao muon thu suc voi cuoc thi google kickstart khong?",https://youtu.be/uGrBHohIgQY co ban nao muon thu suc voi cuoc thi google kickstart khong?,,,,,
"Em chào mọi người, em có làm một ios App dùng deep learning để nhận diện bạch cầu qua hình ảnh cho Sanford hackathon. Hackathon này có giải cho project được nhiều vote nhất nên cả nhà có thời gian thì vote giùm em ạ. Mọi người có thể xem và vote cho project của em ở link dưới đây:
Ps: Nếu post của em không phù hợp với quy định của group thì xin admin xoá bài giùm em.","Em chào mọi người, em có làm một ios App dùng deep learning để nhận diện bạch cầu qua hình ảnh cho Sanford hackathon. Hackathon này có giải cho project được nhiều vote nhất nên cả nhà có thời gian thì vote giùm em ạ. Mọi người có thể xem và vote cho project của em ở link dưới đây: Ps: Nếu post của em không phù hợp với quy định của group thì xin admin xoá bài giùm em.",,,,,
1 phương pháp cho các bác đang nghiên cứu về unsupervised learnings,1 phương pháp cho các bác đang nghiên cứu về unsupervised learnings,,,,,
"Em chào các anh chị, em có một vấn đề khi sử dụng yolov3 mong các anh chị giúp đỡ.
Em đã dùng Yolo v3 để training nhận dạng tế bào máu, và thu được file weight ở hơn 4000 vòng. Data của em gồm có 400 ảnh với kích thước 640x480 và 330 ảnh có kích thước 1392x1038 với màu sắc tế bào có nhiều khác biệt do em thu thập từ nhiều nguồn.
Tuy nhiên khi em test thì yolov3 chỉ nhận diện được các ảnh gốc có kích thước 640x480, còn các ảnh khác không nhận diện được. Em cũng đã thử crop ảnh 1392x1038 về kích thước 640x480 cũng không thể nhận diện được . Vậy mong các anh có cách nào giúp em để em có thể nhận diện được ảnh hoặc hướng dẫn em làm cách nào để phát hiện ra vấn đề. Em cảm ơn nhiều!
Trong đoạn code: blob = cv2.dnn.blobFromImage(img, 0.00392, (896, 896), (0, 0, 0), em để kích thước 896 thì nhận diện được ảnh có kích thước 640x480, Còn nếu em để (416x416) thì ảnh có kích thước 640x480 cũng không nhận diện được.","Em chào các anh chị, em có một vấn đề khi sử dụng yolov3 mong các anh chị giúp đỡ. Em đã dùng Yolo v3 để training nhận dạng tế bào máu, và thu được file weight ở hơn 4000 vòng. Data của em gồm có 400 ảnh với kích thước 640x480 và 330 ảnh có kích thước 1392x1038 với màu sắc tế bào có nhiều khác biệt do em thu thập từ nhiều nguồn. Tuy nhiên khi em test thì yolov3 chỉ nhận diện được các ảnh gốc có kích thước 640x480, còn các ảnh khác không nhận diện được. Em cũng đã thử crop ảnh 1392x1038 về kích thước 640x480 cũng không thể nhận diện được . Vậy mong các anh có cách nào giúp em để em có thể nhận diện được ảnh hoặc hướng dẫn em làm cách nào để phát hiện ra vấn đề. Em cảm ơn nhiều! Trong đoạn code: blob = cv2.dnn.blobFromImage(img, 0.00392, (896, 896), (0, 0, 0), em để kích thước 896 thì nhận diện được ảnh có kích thước 640x480, Còn nếu em để (416x416) thì ảnh có kích thước 640x480 cũng không nhận diện được.",,,,,
"Em chào các anh chị ạ
Hiện tại em đang xây dựng mạng VGGNET16 để phân biệt tư thế người và có khoảng 17000 ảnh chia làm 3 lớp ứng với nằm, đứng và ngồi. Tuy nhiên hiện tại ở bước tiền xử lý ảnh đầu vào đang gặp một chút vấn đề mà em đã google rồi nhưng chưa tìm được cách giải quyết. Em rất mong ai đã gặp vấn đề này rồi cho em lời khuyên với ạ =( .
Theo yêu cầu của mạng VGGNET thì kích thước ảnh đầu vào là 224x224. Và ở bước tiền xử lý em đã thực hiện việc resize tất cả 17000 ảnh về 224x224 như đoạn code ở trên. Tuy nhiên, em thấy nó ngốn rất nhiều RAM trên Google Colab, và khi resize được hết thì Colab nó tự bị ngắt kết nối ( lúc resize xong là hết tầm 11GB RAM của Colab). Em đoán là do bị tràn RAM mà vẫn chưa tìm được hướng giải quyết. Trước đó em thử resize về 64x64 thì vẫn thực hiện được (mất khoảng 2 tiếng) nhưng khi là 224x224 thì bị lỗi như em mô tả ở trên.
Em rất mong nhận được lời khuyên từ các anh/chị. Cảm ơn mọi người nhiều !","Em chào các anh chị ạ Hiện tại em đang xây dựng mạng VGGNET16 để phân biệt tư thế người và có khoảng 17000 ảnh chia làm 3 lớp ứng với nằm, đứng và ngồi. Tuy nhiên hiện tại ở bước tiền xử lý ảnh đầu vào đang gặp một chút vấn đề mà em đã google rồi nhưng chưa tìm được cách giải quyết. Em rất mong ai đã gặp vấn đề này rồi cho em lời khuyên với ạ =( . Theo yêu cầu của mạng VGGNET thì kích thước ảnh đầu vào là 224x224. Và ở bước tiền xử lý em đã thực hiện việc resize tất cả 17000 ảnh về 224x224 như đoạn code ở trên. Tuy nhiên, em thấy nó ngốn rất nhiều RAM trên Google Colab, và khi resize được hết thì Colab nó tự bị ngắt kết nối ( lúc resize xong là hết tầm 11GB RAM của Colab). Em đoán là do bị tràn RAM mà vẫn chưa tìm được hướng giải quyết. Trước đó em thử resize về 64x64 thì vẫn thực hiện được (mất khoảng 2 tiếng) nhưng khi là 224x224 thì bị lỗi như em mô tả ở trên. Em rất mong nhận được lời khuyên từ các anh/chị. Cảm ơn mọi người nhiều !",,,,,
"Chào mọi người, em hiện đang làm đồ án về real time semantic segmentation. Nhưng em đang gặp vấn đề về nhận diện mặt đường, còn lại các vật thể khác bình thường.
HIện em đang sử dụng Mask RCNN với 5000 ảnh và 10 lớp nhưng detect ra thì mặt đường bị lem khá nhiều. Mọi người cố thể cho em xin cách giải quyêt và một số mô hình nào phù hợp hơn được không ạ.
Em xin cám ơn ạ.","Chào mọi người, em hiện đang làm đồ án về real time semantic segmentation. Nhưng em đang gặp vấn đề về nhận diện mặt đường, còn lại các vật thể khác bình thường. HIện em đang sử dụng Mask RCNN với 5000 ảnh và 10 lớp nhưng detect ra thì mặt đường bị lem khá nhiều. Mọi người cố thể cho em xin cách giải quyêt và một số mô hình nào phù hợp hơn được không ạ. Em xin cám ơn ạ.",,,,,
"Xin chào các bác, hôm nay em xin tiếp theo series về Python, Keras và CNN. Mong rằng qua bài này bạn nào mới học cũng có thể xây dựng được model CNN cho mình!
Mong ad duyệt bài!","Xin chào các bác, hôm nay em xin tiếp theo series về Python, Keras và CNN. Mong rằng qua bài này bạn nào mới học cũng có thể xây dựng được model CNN cho mình! Mong ad duyệt bài!",,,,,
"Chào cả nhà,
Bên mình là cty sxuat đang cần tìm giải pháp để tự động hoá 1 số khâu sản xuất, dùng camera detection để phân loại sản phẩm theo 5-6 tiêu chí và loại bỏ hàng k đạt yêu cầu.
Bạn nào quan tâm và có được giải pháp thid inbox mình hoặc liên hệ qua zalo 0778107721 để trao đổi cụ thể hơn nhé. Mình xin cảm ơn.","Chào cả nhà, Bên mình là cty sxuat đang cần tìm giải pháp để tự động hoá 1 số khâu sản xuất, dùng camera detection để phân loại sản phẩm theo 5-6 tiêu chí và loại bỏ hàng k đạt yêu cầu. Bạn nào quan tâm và có được giải pháp thid inbox mình hoặc liên hệ qua zalo 0778107721 để trao đổi cụ thể hơn nhé. Mình xin cảm ơn.",,,,,
"Em chào mọi người ạ.
Hiện tại, em đang có folder dữ liệu ảnh với các nhãn tương ứng. Em muốn chia các dữ liệu ảnh này thành 75% train, 15% validation và 10% dùng để test dạng như ảnh bên dưới để load generator vào các thư viện kiểu như flow_from_directory của keras. Em đã tra trên Tensorflow thì thấy họ chỉ làm với hàm tf.keras.preprocessing.image_dataset_from_directory, tức là học chỉ chia thành 2 phần train và validation. Bây giờ, em muốn chia làm 3 như ảnh trên thì có hướng giải quyết nào không ạ.
Rất mong mọi người cho em lời khuyên ạ. Cảm ơn mọi người nhiều.","Em chào mọi người ạ. Hiện tại, em đang có folder dữ liệu ảnh với các nhãn tương ứng. Em muốn chia các dữ liệu ảnh này thành 75% train, 15% validation và 10% dùng để test dạng như ảnh bên dưới để load generator vào các thư viện kiểu như flow_from_directory của keras. Em đã tra trên Tensorflow thì thấy họ chỉ làm với hàm tf.keras.preprocessing.image_dataset_from_directory, tức là học chỉ chia thành 2 phần train và validation. Bây giờ, em muốn chia làm 3 như ảnh trên thì có hướng giải quyết nào không ạ. Rất mong mọi người cho em lời khuyên ạ. Cảm ơn mọi người nhiều.",,,,,
"Hi mọi người,
Em có cái mô hình chạy bằng sklearn , decision tree,
Nhưng nuốn deploy lên android nên cần model xuất ra là đuôi . Tflite
Có cách nào lưu hay convert model sklearn thành .tf không ạ?
( em định train lại bằng tensoflow nhưng lại k tìm dc package để train )","Hi mọi người, Em có cái mô hình chạy bằng sklearn , decision tree, Nhưng nuốn deploy lên android nên cần model xuất ra là đuôi . Tflite Có cách nào lưu hay convert model sklearn thành .tf không ạ? ( em định train lại bằng tensoflow nhưng lại k tìm dc package để train )",,,,,
"AI for Accessibility Virtual Hackathon from Microsoft APAC
⭐ Chương trình AI for Accessibility Virtual Hackathon là cuộc thi tìm kiếm ý tưởng và xây dưng prototype hỗ trợ người khuyết tật đến gần với công việc để có thể có thu nhập và cải thiện hoạt động hàng ngày của cá nhân (đi chợ, đi bệnh viện, đi xung quanh 😊) và có thể link đến public transport. Các bạn thành lập team (2-5 người) và đăng ký sẽ được hỗ trợ từ các chuyên gia Trí Tuệ Nhân Tạo của Microsoft coach và tham gia các khóa học online được thiết kế riêng cho chương trình này. Sau vòng shortlist, những presentation & demo ý tưởng sẽ thi cùng nhau ở việt nam để chọn ra 1 winner.
⭐ Cuộc thi này APAC có 11 nước cùng tham gia. Đại diện mỗi nước sẽ đc tranh tài trong cuộc thi của APAC và tranh giải Microsoft AI for Good của global, giải thưởng up to US$20,000 Azure Credits. Và được tuyển thẳng vào ctrinh Microsoft Starts up competition, người thắng sẽ có fund support up to US$20 million.
⭐ Như vậy ctrinh này sẽ có lợi cho người tham dự về mặt nâng cao kỹ năng presentation và pitch ideas, một qui trình hoàn chỉnh về xây dựng và phát triển các sáng kiến cộng đồng, đặc biệt là người khuyết tật. Năm nay 3 chủ đề chính sẽ tập trung vào người khiếm thị. Mục đích cuối cùng là mong nâng cao nhận thức về khả năng đóng góp và impact của các bạn có thể tạo ra để phục vụ cộng đồng, điều đó không khó và hoàn toàn trong tầm tay các bạn.
Link:","AI for Accessibility Virtual Hackathon from Microsoft APAC Chương trình AI for Accessibility Virtual Hackathon là cuộc thi tìm kiếm ý tưởng và xây dưng prototype hỗ trợ người khuyết tật đến gần với công việc để có thể có thu nhập và cải thiện hoạt động hàng ngày của cá nhân (đi chợ, đi bệnh viện, đi xung quanh ) và có thể link đến public transport. Các bạn thành lập team (2-5 người) và đăng ký sẽ được hỗ trợ từ các chuyên gia Trí Tuệ Nhân Tạo của Microsoft coach và tham gia các khóa học online được thiết kế riêng cho chương trình này. Sau vòng shortlist, những presentation & demo ý tưởng sẽ thi cùng nhau ở việt nam để chọn ra 1 winner. Cuộc thi này APAC có 11 nước cùng tham gia. Đại diện mỗi nước sẽ đc tranh tài trong cuộc thi của APAC và tranh giải Microsoft AI for Good của global, giải thưởng up to US$20,000 Azure Credits. Và được tuyển thẳng vào ctrinh Microsoft Starts up competition, người thắng sẽ có fund support up to US$20 million. Như vậy ctrinh này sẽ có lợi cho người tham dự về mặt nâng cao kỹ năng presentation và pitch ideas, một qui trình hoàn chỉnh về xây dựng và phát triển các sáng kiến cộng đồng, đặc biệt là người khuyết tật. Năm nay 3 chủ đề chính sẽ tập trung vào người khiếm thị. Mục đích cuối cùng là mong nâng cao nhận thức về khả năng đóng góp và impact của các bạn có thể tạo ra để phục vụ cộng đồng, điều đó không khó và hoàn toàn trong tầm tay các bạn. Link:",,,,,
"Chào mọi người ạ, em là thành viên mới và là newbie mới bước chân vào học ngành này.
Hiện tại em đang tự làm 1 bài toán nhỏ đó là phân cụm văn bản bằng thuật toán k-means. Em đã crawl data, tiền xử lý, vector hóa văn bản thành các điểm và phân cụm thành công. Tuy nhiên lật lại thì em gặp 1 số vướng mắc, đó là làm sao để biêt văn bản nào thuộc về cụm nào? Vì mọi văn bản đều đc vector hóa nên về cơ bản là phân cụm các con số, sau khi phân cụm thì làm cách nào để ánh xạ lại vector đó trỏ về văn bản tương ứng ạ?
Lần đầu đăng lên group có chỗ nào chưa đúng nội quy mong mọi người nhắc nhỏ chứ đừng đá em đi.","Chào mọi người ạ, em là thành viên mới và là newbie mới bước chân vào học ngành này. Hiện tại em đang tự làm 1 bài toán nhỏ đó là phân cụm văn bản bằng thuật toán k-means. Em đã crawl data, tiền xử lý, vector hóa văn bản thành các điểm và phân cụm thành công. Tuy nhiên lật lại thì em gặp 1 số vướng mắc, đó là làm sao để biêt văn bản nào thuộc về cụm nào? Vì mọi văn bản đều đc vector hóa nên về cơ bản là phân cụm các con số, sau khi phân cụm thì làm cách nào để ánh xạ lại vector đó trỏ về văn bản tương ứng ạ? Lần đầu đăng lên group có chỗ nào chưa đúng nội quy mong mọi người nhắc nhỏ chứ đừng đá em đi.",,,,,
"chào mấy a/c, a Tiep,
cho em hỏi câu ngoài lề xíu, google facebook dùng cái gì để lưu trữ hàng tỉ Gb thông tin người dùng ạ, sao các camera giám sát họ không dùng cloud để lưu video để cho hình ảnh chất lượng hơn, ko lo hết bộ nhớ!","chào mấy a/c, a Tiep, cho em hỏi câu ngoài lề xíu, google facebook dùng cái gì để lưu trữ hàng tỉ Gb thông tin người dùng ạ, sao các camera giám sát họ không dùng cloud để lưu video để cho hình ảnh chất lượng hơn, ko lo hết bộ nhớ!",,,,,
"Chào mn ạ ,em là newbie
Mn cho em hỏi em đang dùng hệ đh Linux 16.04 . Em vừa cài CudaToolkit thì bây giờ làm sao để biết mình đang train model bằng CPU hay GPU ạ ! Em cảm ơn","Chào mn ạ ,em là newbie Mn cho em hỏi em đang dùng hệ đh Linux 16.04 . Em vừa cài CudaToolkit thì bây giờ làm sao để biết mình đang train model bằng CPU hay GPU ạ ! Em cảm ơn",,,,,
"Mọi người ơi, hiện tại em đang làm một project liên quan đến nhận diện biển số xe. Phần detect biển số em đã hoàn thành xong. Nhưng đến phần Segment thì gặp vấn đề. Em sử dụng findCoutours() của OpenCV vào bức hình đã chuyển về binary. Kết quả ra như bên dưới. Làm sao để xử lý đoạn này ạ. Em cám ơn mọi
người.","Mọi người ơi, hiện tại em đang làm một project liên quan đến nhận diện biển số xe. Phần detect biển số em đã hoàn thành xong. Nhưng đến phần Segment thì gặp vấn đề. Em sử dụng findCoutours() của OpenCV vào bức hình đã chuyển về binary. Kết quả ra như bên dưới. Làm sao để xử lý đoạn này ạ. Em cám ơn mọi người.",,,,,
"[AI News - Video Completion]
Đội ngũ nghiên cứu nhà Virginia Tech and Facebook gần đây đã công bố thuật toán xóa vật thể chuyển động trong video ứng dụng AI có độ chính xác cao. Người dùng chỉ cần khoanh kín vùng có đối tượng và AI sẽ loại bỏ hoàn toàn đối tượng đó trong video mà không cần sử dụng các kỹ năng, thao tác phức tạp.
Trong vài năm gần đây, cũng đã có các thuật toán AI xóa vật thể chuyển động ra đời, tuy nhiên kết quả vẫn còn nhìn rõ bóng hoặc dấu vết các cạnh của vật thể. Thuật toán mới này gần như khắc phục được các hạn chế của các thuật toán hiện tại. Flow edges, Non-local flow, Seamless blending và Memory efficiency làm nên thành công của thuật toán này. Đây là thuật toán cho ra kết quả tốt nhất so với các thuật toán hiện tại.
Bên dưới là video được áp dụng thuật toán. Không biết có phải do bốn mắt không mà mình chẳng thấy sự khác biệt gì về khung cảnh giữa trước và sau khi xóa vật thể cả. Còn các bạn thì sao?
Paper và code cho bạn nào có hứng thú:
Paper: https://arxiv.org/abs/2009.01835
Code: https://github.com/vt-vl-lab/FGVC","[AI News - Video Completion] Đội ngũ nghiên cứu nhà Virginia Tech and Facebook gần đây đã công bố thuật toán xóa vật thể chuyển động trong video ứng dụng AI có độ chính xác cao. Người dùng chỉ cần khoanh kín vùng có đối tượng và AI sẽ loại bỏ hoàn toàn đối tượng đó trong video mà không cần sử dụng các kỹ năng, thao tác phức tạp. Trong vài năm gần đây, cũng đã có các thuật toán AI xóa vật thể chuyển động ra đời, tuy nhiên kết quả vẫn còn nhìn rõ bóng hoặc dấu vết các cạnh của vật thể. Thuật toán mới này gần như khắc phục được các hạn chế của các thuật toán hiện tại. Flow edges, Non-local flow, Seamless blending và Memory efficiency làm nên thành công của thuật toán này. Đây là thuật toán cho ra kết quả tốt nhất so với các thuật toán hiện tại. Bên dưới là video được áp dụng thuật toán. Không biết có phải do bốn mắt không mà mình chẳng thấy sự khác biệt gì về khung cảnh giữa trước và sau khi xóa vật thể cả. Còn các bạn thì sao? Paper và code cho bạn nào có hứng thú: Paper: https://arxiv.org/abs/2009.01835 Code: https://github.com/vt-vl-lab/FGVC",,,,,
Cả nhà có kinh nghiệm xử lý text classification và semantic search ntn bằng việc xử dụng Reformer (Transformer) không? Mình cảm ơn,Cả nhà có kinh nghiệm xử lý text classification và semantic search ntn bằng việc xử dụng Reformer (Transformer) không? Mình cảm ơn,,,,,
"Respect!
 — với Huyen Nguyen.",Respect! — với Huyen Nguyen.,,,,,
"Em chào các anh/chị ạ

Em đang làm 1 bài toán forecasting (multivariate) sử dụng LSTM (implement bằng torch) và gặp 1 số vấn đề khi update weights mong anh/chị góp ý ạ:

Ở dưới đây thì em dùng batch size = 64, learning rate = 0.001 thì training loss (MSE) có giảm nhẹ trong quá trình training nhưng ko đáng kể. Còn những hyperparam khác thì loss đều không giảm
Em có plot thay đổi của loss theo từng batch với mỗi training epoch và plot của các epoch đều gần như y hệt nhau như weights ko đc updates vậy
Em cảm ơn ạ","Em chào các anh/chị ạ Em đang làm 1 bài toán forecasting (multivariate) sử dụng LSTM (implement bằng torch) và gặp 1 số vấn đề khi update weights mong anh/chị góp ý ạ: Ở dưới đây thì em dùng batch size = 64, learning rate = 0.001 thì training loss (MSE) có giảm nhẹ trong quá trình training nhưng ko đáng kể. Còn những hyperparam khác thì loss đều không giảm Em có plot thay đổi của loss theo từng batch với mỗi training epoch và plot của các epoch đều gần như y hệt nhau như weights ko đc updates vậy Em cảm ơn ạ",,,,,
"Chào mọi người, mình dự định xây dựng mô hình phân lớp cho văn bản bằng BERT.
Mỗi văn bản thuộc một lĩnh vực nào đó.
Mình thắc mắc ở chỗ là BERT chỉ cho phép tối đa 512 tokens, còn văn bản của mình thì có khi lớn hơn nhiều, việc truncating có thể làm mất đi ý nghĩa của dữ liệu.
Vậy có hướng nào để giải quyết không ạ?","Chào mọi người, mình dự định xây dựng mô hình phân lớp cho văn bản bằng BERT. Mỗi văn bản thuộc một lĩnh vực nào đó. Mình thắc mắc ở chỗ là BERT chỉ cho phép tối đa 512 tokens, còn văn bản của mình thì có khi lớn hơn nhiều, việc truncating có thể làm mất đi ý nghĩa của dữ liệu. Vậy có hướng nào để giải quyết không ạ?",,,,,
"Hi mọi người,
Cho em hỏi là thường trong bài toán phân loại ảnh với Conv2D, trước lớp cuối cùng là Dense(num_classes, activation=""sigmoid/softmax"") thì có 1 layer nữa là Dense(128, activation='relu')).
Tuỳ vào bài mà sẽ có số khác 128, vậy con số này là dựa vào đâu để tìm ạ. Em xin cảm ơn ^^","Hi mọi người, Cho em hỏi là thường trong bài toán phân loại ảnh với Conv2D, trước lớp cuối cùng là Dense(num_classes, activation=""sigmoid/softmax"") thì có 1 layer nữa là Dense(128, activation='relu')). Tuỳ vào bài mà sẽ có số khác 128, vậy con số này là dựa vào đâu để tìm ạ. Em xin cảm ơn ^^",,,,,
"Em chào các anh chị ạ, em có câu hỏi là trong bài toán dịch máy tự động khi áp dụng cho tiếng việt + tiếng anh thì ngoài điểm BLEU ra mình có thể dùng điểm nào khác để đánh giá mô hình không ạ?","Em chào các anh chị ạ, em có câu hỏi là trong bài toán dịch máy tự động khi áp dụng cho tiếng việt + tiếng anh thì ngoài điểm BLEU ra mình có thể dùng điểm nào khác để đánh giá mô hình không ạ?",,,,,
"Kỹ thuật phân chia dữ liệu cho ba nhóm: training và test (thêm Validation)
Chào mọi người, mình đang có một bài tập nhỏ về phân chia dữ liệu cho một mô hình dự đoán trong bảo hiểm. Dữ liệu được chuẩn bị là một pandas với nhiều biến khác nhau. Mình cần chia dữ liệu thành ba bộ phận nói trên: training, validation và test. Mình có mấy câu hỏi cần sự giúp đỡ của các bạn như sau:
1, Chỉ cần chia thành training và test hay nhất thiết phải có phần thứ 3 là validation. Tỉ lệ hay gặp nhất giữa các phần là bao nhiêu?
2. Ban đầu mình định lấy ngẫu nhiên 80% cho training và 20% cho test nhưng lại sợ phân bố dữ liệu không đều nên mình thay đổi là: Lấy index chia cho 5: nếu index chia hết cho 5 thì lấy vào bộ test, còn lại sẽ cho vào bộ training.
Vậy hai cách của mình có hợp lý không?
3. Bạn nào biết về cách phân chia dữ liệu thì chia sẻ với mình bằng cách bình luận tại bài viết này, đặc biệt là mình cần phần code Python cụ thể để hiểu thêm kỹ thuật này!
Cảm ơn các bạn nhiều!","Kỹ thuật phân chia dữ liệu cho ba nhóm: training và test (thêm Validation) Chào mọi người, mình đang có một bài tập nhỏ về phân chia dữ liệu cho một mô hình dự đoán trong bảo hiểm. Dữ liệu được chuẩn bị là một pandas với nhiều biến khác nhau. Mình cần chia dữ liệu thành ba bộ phận nói trên: training, validation và test. Mình có mấy câu hỏi cần sự giúp đỡ của các bạn như sau: 1, Chỉ cần chia thành training và test hay nhất thiết phải có phần thứ 3 là validation. Tỉ lệ hay gặp nhất giữa các phần là bao nhiêu? 2. Ban đầu mình định lấy ngẫu nhiên 80% cho training và 20% cho test nhưng lại sợ phân bố dữ liệu không đều nên mình thay đổi là: Lấy index chia cho 5: nếu index chia hết cho 5 thì lấy vào bộ test, còn lại sẽ cho vào bộ training. Vậy hai cách của mình có hợp lý không? 3. Bạn nào biết về cách phân chia dữ liệu thì chia sẻ với mình bằng cách bình luận tại bài viết này, đặc biệt là mình cần phần code Python cụ thể để hiểu thêm kỹ thuật này! Cảm ơn các bạn nhiều!",,,,,
"Có anh chị nào biết platform đằng sau của dịch vụ OCR trong trang này không https://www.zoho.com/creator/newhelp/forms/fields/ocr/
Em xin nhận chia sẻ kinh nghiệm ạ!",Có anh chị nào biết platform đằng sau của dịch vụ OCR trong trang này không https://www.zoho.com/creator/newhelp/forms/fields/ocr/ Em xin nhận chia sẻ kinh nghiệm ạ!,,,,,
"Chào mọi người.
Cho mình hỏi một chút về PySpark và Keras ạ. Có cách nào hợp lý và đúng logic kết hợp PySpark pandas_udf với model Keras để sử dụng cho PySpark readStream() không ạ?",Chào mọi người. Cho mình hỏi một chút về PySpark và Keras ạ. Có cách nào hợp lý và đúng logic kết hợp PySpark pandas_udf với model Keras để sử dụng cho PySpark readStream() không ạ?,,,,,
"em đang họp với người nhật và người ấn độ khá nhiều. em đang tìm một tool để nhận diện giọng nói của mọi người để dễ dàng nghe được hơn. Yêu cầu thì cần realtime, không quá chậm đề dùng lúc họp mặt luôn ạ.
Liệu giờ học máy đã có sản phẩm nào hỗ trợ được tốc độ và vừa túi tiền không ạ. ( em nghĩ chi phí một tháng em bỏ ra được chắc sẽ là khoảng max 3 triệu thui ạ ).
P/S em ko thuê làm tùe đaùa mà đang tìm một product nào sẵn họ bán cho nhiều user. Mà hiện tại em tìm product như thế mà không thấy nên nghĩ mọi người làm nên sẽ biết các sản phẩm nổi tiếng ạ. 3 Triệu là tiền mua license thui ạ.","em đang họp với người nhật và người ấn độ khá nhiều. em đang tìm một tool để nhận diện giọng nói của mọi người để dễ dàng nghe được hơn. Yêu cầu thì cần realtime, không quá chậm đề dùng lúc họp mặt luôn ạ. Liệu giờ học máy đã có sản phẩm nào hỗ trợ được tốc độ và vừa túi tiền không ạ. ( em nghĩ chi phí một tháng em bỏ ra được chắc sẽ là khoảng max 3 triệu thui ạ ). P/S em ko thuê làm tùe đaùa mà đang tìm một product nào sẵn họ bán cho nhiều user. Mà hiện tại em tìm product như thế mà không thấy nên nghĩ mọi người làm nên sẽ biết các sản phẩm nổi tiếng ạ. 3 Triệu là tiền mua license thui ạ.",,,,,
"Chào mọi người!
Mình đang muốn dùng google_image_download để tạo database training yolo. Nhưng khi chạy chương trình thì gặp lỗi này.
Mọi người chỉ mình cách khác phụ với ạ. Hay có phương pháp khác cũng được
Mình cảm ơn",Chào mọi người! Mình đang muốn dùng google_image_download để tạo database training yolo. Nhưng khi chạy chương trình thì gặp lỗi này. Mọi người chỉ mình cách khác phụ với ạ. Hay có phương pháp khác cũng được Mình cảm ơn,,,,,
"Chào anh chị, em đang có một bài tập về two-way ANOVA và Tukey HSD trên Python, em có 2 câu hỏi:
Câu 1: tập dữ liệu của em gồm 3 thuộc tính là Pclass (loại vé mà hành khách sử dụng, như là thương gia, bình thường,... được chuyển đổi thành số), Sex (giới tính gồm male và female), cuối cùng là Fare (số tiền bỏ ra mua vé của một người). Em đã làm xong two-way ANOVA rồi, mọi người cho em hỏi H0 của em lúc này có phải là 3 thuộc tính Pclass, Sex, Fare là độc lập không ạ.
Câu 2: em tiến hành phân tích Tukey HSD và thu được bản dưới đây, thì em có đọc tài liệu thì được biết là nếu reject = False thì mình sẽ bác bỏ H0, nhưng H0 lúc này là H0 ở câu 1 hay là một phát biểu khác ạ, em cảm ơn.","Chào anh chị, em đang có một bài tập về two-way ANOVA và Tukey HSD trên Python, em có 2 câu hỏi: Câu 1: tập dữ liệu của em gồm 3 thuộc tính là Pclass (loại vé mà hành khách sử dụng, như là thương gia, bình thường,... được chuyển đổi thành số), Sex (giới tính gồm male và female), cuối cùng là Fare (số tiền bỏ ra mua vé của một người). Em đã làm xong two-way ANOVA rồi, mọi người cho em hỏi H0 của em lúc này có phải là 3 thuộc tính Pclass, Sex, Fare là độc lập không ạ. Câu 2: em tiến hành phân tích Tukey HSD và thu được bản dưới đây, thì em có đọc tài liệu thì được biết là nếu reject = False thì mình sẽ bác bỏ H0, nhưng H0 lúc này là H0 ở câu 1 hay là một phát biểu khác ạ, em cảm ơn.",,,,,
"Chào mọi người. Mong tìm được cao nhân nào giúp em vụ này! Em đg cố convert h5 model qua đuôi mlmodel bằng coremltools nhưng gặp lỗi: Keras layer’<class ’tensorflow.python.keras.layers.convolutional.Conv2D’>’ not supported. Mò mẫm google mãi cũng không biết làm sao sửa. Btw em đang dùng python 3.7, coremltools 4.0, tensorflow 2.3.1 và keras 2.2.4; còn model này là nhận diện và phân loại biển báo giao thông. Mong mọi người giúp đỡ!","Chào mọi người. Mong tìm được cao nhân nào giúp em vụ này! Em đg cố convert h5 model qua đuôi mlmodel bằng coremltools nhưng gặp lỗi: Keras layer’<class ’tensorflow.python.keras.layers.convolutional.Conv2D’>’ not supported. Mò mẫm google mãi cũng không biết làm sao sửa. Btw em đang dùng python 3.7, coremltools 4.0, tensorflow 2.3.1 và keras 2.2.4; còn model này là nhận diện và phân loại biển báo giao thông. Mong mọi người giúp đỡ!",,,,,
"Chào mọi người !
Em đang có file ảnh lưu các ảnh theo dạng train_i.png với i từ 0 -> 2000. Em muốn mở từng file theo thứ tự tuy nhiên nó lại sắp xếp theo kiểu rất khó hiểu. Mọi người giúp em với ạ. Em cảm ơn!",Chào mọi người ! Em đang có file ảnh lưu các ảnh theo dạng train_i.png với i từ 0 -> 2000. Em muốn mở từng file theo thứ tự tuy nhiên nó lại sắp xếp theo kiểu rất khó hiểu. Mọi người giúp em với ạ. Em cảm ơn!,,,,,
"Chào mọi người ạ!
Do vấn đề về memory GPU nên em dùng đặt model.fit trong vòng lặp for, (3500 phần tử e chia thành 70 loops 50 phần tử) nhưng có vẻ như càng về loops sau thời gian train càng lâu, có vẻ là do RAM lưu vẫn còn lưu dữ liệu preprocess nên bị tràn thì phải. Mọi người giúp em khắc phục với ạ! Em xin cảm ơn ạ!","Chào mọi người ạ! Do vấn đề về memory GPU nên em dùng đặt model.fit trong vòng lặp for, (3500 phần tử e chia thành 70 loops 50 phần tử) nhưng có vẻ như càng về loops sau thời gian train càng lâu, có vẻ là do RAM lưu vẫn còn lưu dữ liệu preprocess nên bị tràn thì phải. Mọi người giúp em khắc phục với ạ! Em xin cảm ơn ạ!",,,,,
"Em convert darknet model sang file .pb gặp lỗi này mong ae giải đáp, em cảm ơn ạ !","Em convert darknet model sang file .pb gặp lỗi này mong ae giải đáp, em cảm ơn ạ !",,,,,
"các bác ơi có khoá học python, SQL online ổn ổn nào cho beginner không ạ? hoặc youtube cũng được, em đang học ở lớp mà thầy dạy sương sương thui k hiểu lắm ạ hic, phải tự tìm hiểu là chính.","các bác ơi có khoá học python, SQL online ổn ổn nào cho beginner không ạ? hoặc youtube cũng được, em đang học ở lớp mà thầy dạy sương sương thui k hiểu lắm ạ hic, phải tự tìm hiểu là chính.",,,,,
"chào a Tiep, các a/c
cho mình hỏi có ai có ebook machine learning cơ bản có màu của a Tiep ko ạ, cho mình xin với, cái bản mình kiếm trên forum là bản trắng đen, mình đặt mua sách nhưng hiện ko có xuất bản nữa! thanks m.n","chào a Tiep, các a/c cho mình hỏi có ai có ebook machine learning cơ bản có màu của a Tiep ko ạ, cho mình xin với, cái bản mình kiếm trên forum là bản trắng đen, mình đặt mua sách nhưng hiện ko có xuất bản nữa! thanks m.n",,,,,
"Hi mọi người,
Cho em hỏi làm cách nào để flow ImageDataGenerator từ tfds ạ? (Em biết cách làm với local dataset bằng flow_from_directory nhưng còn data từ tensorflow thì luôn bị 'path should be string, bytes, os.PathLike, integer or None, not DatasetV1Adapter')
Em xin cảm ơn ^^.","Hi mọi người, Cho em hỏi làm cách nào để flow ImageDataGenerator từ tfds ạ? (Em biết cách làm với local dataset bằng flow_from_directory nhưng còn data từ tensorflow thì luôn bị 'path should be string, bytes, os.PathLike, integer or None, not DatasetV1Adapter') Em xin cảm ơn ^^.",,,,,
Có tools này có vẻ hay cho anh em làm về DNN.,Có tools này có vẻ hay cho anh em làm về DNN.,,,,,
"cho em hỏi xong vấn đề này em sẽ xóa bài ^^.
lần đầu tiên em dùng tới cnn của Tensorflow nên hỏi không hiểu lắm về Checkpoint, theo như em đọc trên mạng thì checkpoint có khả năng lưu lại sau mỗi lần kết thúc Epoch, mình có thể dùng ckpt để tính Accuracy gần nhất mà ckpt đó lưu được, nhưng mà em không biết nếu đang train trên 50 epoch, mình lưu đc 10 epoch đầu thì có cách nào load cái checkpoint đó để chạy tiếp 40 epoch sau không ạ.
em cảm ơn. ^^","cho em hỏi xong vấn đề này em sẽ xóa bài ^^. lần đầu tiên em dùng tới cnn của Tensorflow nên hỏi không hiểu lắm về Checkpoint, theo như em đọc trên mạng thì checkpoint có khả năng lưu lại sau mỗi lần kết thúc Epoch, mình có thể dùng ckpt để tính Accuracy gần nhất mà ckpt đó lưu được, nhưng mà em không biết nếu đang train trên 50 epoch, mình lưu đc 10 epoch đầu thì có cách nào load cái checkpoint đó để chạy tiếp 40 epoch sau không ạ. em cảm ơn. ^^",,,,,
"Có thể cho mình một ví dụ nhỏ với source code sự khác nhau giữa deep learning và machine learning không? Hiện tại mình chưa phân biệt được chúng, đọc lý thuyết mình cũng không thể phân biệt được. Source code với những bài thực hành nhỏ có lẽ mình sẽ hiểu được nó dễ dàng hơn. Mình xin cảm ơn.","Có thể cho mình một ví dụ nhỏ với source code sự khác nhau giữa deep learning và machine learning không? Hiện tại mình chưa phân biệt được chúng, đọc lý thuyết mình cũng không thể phân biệt được. Source code với những bài thực hành nhỏ có lẽ mình sẽ hiểu được nó dễ dàng hơn. Mình xin cảm ơn.",,,,,
"Xin phép ad một ít tài nguyên của nhóm. Cảm ơn
Xin chào các bạn. Hiện tại mình đang xây dựng dự án. Tra cứu bác sĩ. Đã có demo ở đây: http://huhivn90.pythonanywhere.com/
Ý tưởng là 1 Dự án này miễn phí hoàn toàn để các bệnh viện hoặc phòng khám nhỏ có thể triển khai với chi phí tối thiểu. Nôm na nó như một mạng xã hội mini cho bác sĩ và bệnh nhân. Có tính năng book lịch khám và đánh giá, hỏi đáp.
Hiện ứng dụng đã báo cáo tại hội thảo ứng dụng cntt về y tế do cục cntt trao chứng nhận. (Hội thảo cntt về y tế tại tp vinh ngày 24/10, google xem thêm, mình k tiện dẫn link)
Một mình làm hiện hơi đuối sức vì mình đang có ý tưởng tích hợp thêm tính năng tự động gán nhãn cho các bình luận và các mô tả lúc đăng ký khám.
Vì thế, bạn nào có chung ý tưởng hoặc hứng thú với dự án này muốn cùng xây dựng từ sản phẩm demo đến product. Thì liên hệ với mình nhé. Cảm ơn đã đọc tin","Xin phép ad một ít tài nguyên của nhóm. Cảm ơn Xin chào các bạn. Hiện tại mình đang xây dựng dự án. Tra cứu bác sĩ. Đã có demo ở đây: http://huhivn90.pythonanywhere.com/ Ý tưởng là 1 Dự án này miễn phí hoàn toàn để các bệnh viện hoặc phòng khám nhỏ có thể triển khai với chi phí tối thiểu. Nôm na nó như một mạng xã hội mini cho bác sĩ và bệnh nhân. Có tính năng book lịch khám và đánh giá, hỏi đáp. Hiện ứng dụng đã báo cáo tại hội thảo ứng dụng cntt về y tế do cục cntt trao chứng nhận. (Hội thảo cntt về y tế tại tp vinh ngày 24/10, google xem thêm, mình k tiện dẫn link) Một mình làm hiện hơi đuối sức vì mình đang có ý tưởng tích hợp thêm tính năng tự động gán nhãn cho các bình luận và các mô tả lúc đăng ký khám. Vì thế, bạn nào có chung ý tưởng hoặc hứng thú với dự án này muốn cùng xây dựng từ sản phẩm demo đến product. Thì liên hệ với mình nhé. Cảm ơn đã đọc tin",,,,,
Chào mọi người. Hiện tại em cần down một lượng lớn video từ youtube cụ thể là tập dataset MS ASL. Em có dùng script Python download nhưng chỉ download được vài video thì nó báo quá nhiều request. Mn có giải pháp gì không ạ. Em cảm ơn ạ.,Chào mọi người. Hiện tại em cần down một lượng lớn video từ youtube cụ thể là tập dataset MS ASL. Em có dùng script Python download nhưng chỉ download được vài video thì nó báo quá nhiều request. Mn có giải pháp gì không ạ. Em cảm ơn ạ.,,,,,
"Xin chào mọi người, khi mình dùng lệnh pip install fastBPE thì bị lỗi ntn , mình có tải buildtools rồi mà vẫn lỗi như kìa, mong mọi người giúp đỡ ạ :((","Xin chào mọi người, khi mình dùng lệnh pip install fastBPE thì bị lỗi ntn , mình có tải buildtools rồi mà vẫn lỗi như kìa, mong mọi người giúp đỡ ạ :((",,,,,
Cho mình hỏi sau khi tìm được mô hình dự đoán ngoài thay tập test vào để so sánh thì có cần phải làm thêm bước nào để kiểm tra mô hình nữa không ?,Cho mình hỏi sau khi tìm được mô hình dự đoán ngoài thay tập test vào để so sánh thì có cần phải làm thêm bước nào để kiểm tra mô hình nữa không ?,,,,,
"Lastly, the game in vision recognition tasks can be changed from CNN to Transformer!
Cuối cùng cũng như mong đợi là bài báo Vision Transformer (AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE) đã được công bố source code. Và cũng không ngoài dự đoán của mình rằng bài báo do nhóm nghiên cứu của Google.
Đây là bài báo: https://openreview.net/pdf?id=YicbFdNTTy
Hay ở trên Arxiv https://arxiv.org/pdf/2010.11929.pdf
Đây là code được Ross Wightman port sang PyTorch: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
Đây là source code của bài báo được viết bằng jax thay vì TF trên Python https://github.com/google-research/vision_transformer
Chúc cả nhà cuối tuần vui vẻ!","Lastly, the game in vision recognition tasks can be changed from CNN to Transformer! Cuối cùng cũng như mong đợi là bài báo Vision Transformer (AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE) đã được công bố source code. Và cũng không ngoài dự đoán của mình rằng bài báo do nhóm nghiên cứu của Google. Đây là bài báo: https://openreview.net/pdf?id=YicbFdNTTy Hay ở trên Arxiv https://arxiv.org/pdf/2010.11929.pdf Đây là code được Ross Wightman port sang PyTorch: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py Đây là source code của bài báo được viết bằng jax thay vì TF trên Python https://github.com/google-research/vision_transformer Chúc cả nhà cuối tuần vui vẻ!",,,,,
"Kính gửi đến cả nhà clip chia sẻ về 🚩[Mỳ Python] Bài 4. Python với Keras (Phần 1)
✏️Link video: https://www.youtube.com/watch?v=hPhnqTtidnA
✏️Link playlist full bộ: https://www.youtube.com/watch?v=TY43q0w1Eh0&list=PLZPCoTKpEddAay-lItE-pn27uNNrApORH
Phần này hi vọng sẽ mang đến cho các bạn mới học cái nhìn cơ bản về Python, Keras, về cách xây dựng một mạng NN cơ bản bằng Keras, cách chia dữ liệu train, test, validation...
Mong các cao thủ chỉ giáo và mong ad duyệt bài!","Kính gửi đến cả nhà clip chia sẻ về [Mỳ Python] Bài 4. Python với Keras (Phần 1) Link video: https://www.youtube.com/watch?v=hPhnqTtidnA Link playlist full bộ: https://www.youtube.com/watch?v=TY43q0w1Eh0&list=PLZPCoTKpEddAay-lItE-pn27uNNrApORH Phần này hi vọng sẽ mang đến cho các bạn mới học cái nhìn cơ bản về Python, Keras, về cách xây dựng một mạng NN cơ bản bằng Keras, cách chia dữ liệu train, test, validation... Mong các cao thủ chỉ giáo và mong ad duyệt bài!",,,,,
"[PaddleOCR]
Hệ thống Nhận dạng chữ viết (OCR) đã được sử dụng rộng rãi trong thực tế như nhận dạng chứng minh thư, bằng lái xe. Tuy nhiên, OCR vẫn là một nhiệm vụ đầy thách thức về cả độ chính xác và tốc độ tính toán. Do đó, PaddleOCR ra đời hỗ trợ nhận dạng tiếng Trung, tiếng Anh, chữ số và nhận dạng các văn bản dài. Ngoài ra, PaddleOCR còn hỗ trợ nhận dạng nhiều ngôn ngữ: Hàn Quốc, Nhật Bản, Đức, Pháp. Đặc biệt là PaddleOCR rất nhẹ do sử dụng các phương pháp giảm kích thước mô hình.
Paper: https://arxiv.org/abs/2009.09941
Github: https://github.com/PaddlePaddle/PaddleOCR","[PaddleOCR] Hệ thống Nhận dạng chữ viết (OCR) đã được sử dụng rộng rãi trong thực tế như nhận dạng chứng minh thư, bằng lái xe. Tuy nhiên, OCR vẫn là một nhiệm vụ đầy thách thức về cả độ chính xác và tốc độ tính toán. Do đó, PaddleOCR ra đời hỗ trợ nhận dạng tiếng Trung, tiếng Anh, chữ số và nhận dạng các văn bản dài. Ngoài ra, PaddleOCR còn hỗ trợ nhận dạng nhiều ngôn ngữ: Hàn Quốc, Nhật Bản, Đức, Pháp. Đặc biệt là PaddleOCR rất nhẹ do sử dụng các phương pháp giảm kích thước mô hình. Paper: https://arxiv.org/abs/2009.09941 Github: https://github.com/PaddlePaddle/PaddleOCR",,,,,
Em đã convert .py to .exe thành công. Nhưng khi chạy file exe thì bị lỗi như này ạ. Ai gặp lỗi tương tự rồi giúp em với ạ. Em xin cảm ơn ạ,Em đã convert .py to .exe thành công. Nhưng khi chạy file exe thì bị lỗi như này ạ. Ai gặp lỗi tương tự rồi giúp em với ạ. Em xin cảm ơn ạ,,,,,
Mọi người giúp em câu 3 với ạ,Mọi người giúp em câu 3 với ạ,,,,,
Mn ơi cho em hỏi có APIs nào tự summarize paragraph tiếng việt ko ạ. Em cảm ơn ạ,Mn ơi cho em hỏi có APIs nào tự summarize paragraph tiếng việt ko ạ. Em cảm ơn ạ,,,,,
"[AI News - Multilingual Machine Translation]
Phần lớn các mô hình dịch máy giữa hai ngôn ngữ bất kỳ hiện nay đều dựa vào ngôn ngữ trung gian là tiếng Anh. Tuy nhiên, việc dịch qua ngôn ngữ trung gian mất nhiều thời gian, giảm độ chính xác và ý nghĩa cũng sẽ khó được lưu giữ trọn vẹn.
Gần đây, Facebook AI đã phát triển mô hình dịch máy đa ngôn ngữ, có thể dịch giữa các ngôn ngữ không cần thông qua tiếng Anh. Mô hình M2M-100 này là mô hình đầu tiên có thể dịch trực tiếp bất kỳ cặp nào giữa 100 ngôn ngữ khác nhau, đạt được hơn 10 điểm BLEU (điểm đánh giá song ngữ) tốt hơn so với các mô hình thông qua tiếng anh hiện tại. Mô hình đang tiếp tục được cải thiện để có thể ứng dụng vào trình dịch trên Facebook.
Paper: https://ai.facebook.com/research/publications/Beyond-English-Centric-Multilingual-Machine-Translation/
Github: https://github.com/pytorch/fairseq/tree/master/examples/m2m_100","[AI News - Multilingual Machine Translation] Phần lớn các mô hình dịch máy giữa hai ngôn ngữ bất kỳ hiện nay đều dựa vào ngôn ngữ trung gian là tiếng Anh. Tuy nhiên, việc dịch qua ngôn ngữ trung gian mất nhiều thời gian, giảm độ chính xác và ý nghĩa cũng sẽ khó được lưu giữ trọn vẹn. Gần đây, Facebook AI đã phát triển mô hình dịch máy đa ngôn ngữ, có thể dịch giữa các ngôn ngữ không cần thông qua tiếng Anh. Mô hình M2M-100 này là mô hình đầu tiên có thể dịch trực tiếp bất kỳ cặp nào giữa 100 ngôn ngữ khác nhau, đạt được hơn 10 điểm BLEU (điểm đánh giá song ngữ) tốt hơn so với các mô hình thông qua tiếng anh hiện tại. Mô hình đang tiếp tục được cải thiện để có thể ứng dụng vào trình dịch trên Facebook. Paper: https://ai.facebook.com/research/publications/Beyond-English-Centric-Multilingual-Machine-Translation/ Github: https://github.com/pytorch/fairseq/tree/master/examples/m2m_100",,,,,
"[MÌ AI #32] [Nhận diện biển số xe] Chương 5 – Nhận diện biển số xe bằng WPOD và SVM
Link bài viết: https://www.miai.vn/2019/12/05/nhan-dien-bien-so-xe-chuong-5-nhan-dien-bien-so-xe-bang-wpod-va-svm/?fbclid=IwAR3gpWwRFDeNL52f-iFNUpkobH2ODay5otSF5llnVJBQL7IukqnjgFlU-rA
(Có share full bộ dữ liệu ký tự, tải về xài được ngay không phải tạo dữ liệu)
Xin chào cả nhà, Tết đến nơi rồi nên bận kinh khủng, tuần vừa qua mình tối mắt tối mũi thanh ra không thể ra bài kịp thời mong các bạn thông cảm nhé. Hôm nay chúng ta sẽ cùng nhau Nhận diện biển số xe bằng WPOD và SVM nhé!
Hôm trước mình có guide các bạn cách nhận diện biển số bằng WPOD và Tesseract OCR.Cách đó thì đơn giản nhưng sẽ bị phụ thuộc vào Tess nhiều, và nhiều bạn nói nó ""tù"". Do vậy bây giờ chúng ta sẽ cùng nhau tự làm bằng OpenCV và dùng SVM để nhận ký tự nhé.
miaifull #MìAI 
Fanpage: http://facebook.com/miaiblog 
Group trao đổi, chia sẻ: https://facebook.com/groups/miaigroup 
Website: http://ainoodle.tech 
Youtube: http://bit.ly/miaiyoutube","[MÌ AI [Nhận diện biển số xe] Chương 5 – Nhận diện biển số xe bằng WPOD và SVM Link bài viết: https://www.miai.vn/2019/12/05/nhan-dien-bien-so-xe-chuong-5-nhan-dien-bien-so-xe-bang-wpod-va-svm/?fbclid=IwAR3gpWwRFDeNL52f-iFNUpkobH2ODay5otSF5llnVJBQL7IukqnjgFlU-rA (Có share full bộ dữ liệu ký tự, tải về xài được ngay không phải tạo dữ liệu) Xin chào cả nhà, Tết đến nơi rồi nên bận kinh khủng, tuần vừa qua mình tối mắt tối mũi thanh ra không thể ra bài kịp thời mong các bạn thông cảm nhé. Hôm nay chúng ta sẽ cùng nhau Nhận diện biển số xe bằng WPOD và SVM nhé! Hôm trước mình có guide các bạn cách nhận diện biển số bằng WPOD và Tesseract OCR.Cách đó thì đơn giản nhưng sẽ bị phụ thuộc vào Tess nhiều, và nhiều bạn nói nó ""tù"". Do vậy bây giờ chúng ta sẽ cùng nhau tự làm bằng OpenCV và dùng SVM để nhận ký tự nhé. miaifull Fanpage: http://facebook.com/miaiblog Group trao đổi, chia sẻ: https://facebook.com/groups/miaigroup Website: http://ainoodle.tech Youtube: http://bit.ly/miaiyoutube",#32]	#MìAI,,,,
"#retinaface
Hi anh em,
Mình đang sử dụng retinaface để trích xuất gương mặt nhưng hiện có một vấn đề là khi mình crop và lưu gương mặt xuống folder thì bị lỗi như thế này. Theo mình research trên google thì thấy họ bảo là check các shape trước, lúc mình print các shape ra thì thấy shape vẫn có gía trị được xuất ra.
Ae nào đã gặp vấn đề này cho mình xin cách fĩx với ạ. Mình cám ơn.","Hi anh em, Mình đang sử dụng retinaface để trích xuất gương mặt nhưng hiện có một vấn đề là khi mình crop và lưu gương mặt xuống folder thì bị lỗi như thế này. Theo mình research trên google thì thấy họ bảo là check các shape trước, lúc mình print các shape ra thì thấy shape vẫn có gía trị được xuất ra. Ae nào đã gặp vấn đề này cho mình xin cách fĩx với ạ. Mình cám ơn.",#retinaface,,,,
"Hôm trước có 1 số bạn hỏi nên mình share source cho solution của mình trong cuộc thi ERC2019 https://erc2019.com/ (Audio Emotion Recognition).
https://github.com/suicao/Pytorch-Audio-Emotion-Recognition
Đây là cuộc thi phân loại cảm xúc tuy nhiên model này có thể dùng cho audio classifcation nói chung, mình lấy lại từ cuộc thi Freesound 2019 của Kaggle thôi.
Một số key features:
Mixup + SpecAugment for data augmentation.
CNN for audio classification with customized pooling.
Cosine annealing learning scheduler.
Test time augmentation (TTA)
Enjoy.","Hôm trước có 1 số bạn hỏi nên mình share source cho solution của mình trong cuộc thi ERC2019 https://erc2019.com/ (Audio Emotion Recognition). https://github.com/suicao/Pytorch-Audio-Emotion-Recognition Đây là cuộc thi phân loại cảm xúc tuy nhiên model này có thể dùng cho audio classifcation nói chung, mình lấy lại từ cuộc thi Freesound 2019 của Kaggle thôi. Một số key features: Mixup + SpecAugment for data augmentation. CNN for audio classification with customized pooling. Cosine annealing learning scheduler. Test time augmentation (TTA) Enjoy.",,,,,
"http://szeliski.org/Book/
sách giáo khoa về computer vision cho ae nào cần (bản draft)",http://szeliski.org/Book/ sách giáo khoa về computer vision cho ae nào cần (bản draft),,,,,
"Mọi người cho mình hỏi là GRU giải quyết vanishing gradient như thế nào ạ?
Mình hiểu rằng GRU có thể chắt lọc những thông tin cần thiết và bỏ những thông tin thừa thải để mang những htoong tin quan trọng đi đến những layer xa hơn. Nhưng vẫn chưa thực sự hiểu việc khi thông tin qua cổng update = 0, tức giữ nguyên giá trị trước đó lại có thể giải quyết vấn đề vanishing gradient.
Mình cảm ơn trước.","Mọi người cho mình hỏi là GRU giải quyết vanishing gradient như thế nào ạ? Mình hiểu rằng GRU có thể chắt lọc những thông tin cần thiết và bỏ những thông tin thừa thải để mang những htoong tin quan trọng đi đến những layer xa hơn. Nhưng vẫn chưa thực sự hiểu việc khi thông tin qua cổng update = 0, tức giữ nguyên giá trị trước đó lại có thể giải quyết vấn đề vanishing gradient. Mình cảm ơn trước.",,,,,
"Hi mọi người, hiện tại, em đang có một bài toán về table/document extraction cho tờ khai hải quan. Em muốn extract mỗi cái subtable ""Số lượng và mô tả chi tiết nội dung bưu phẩm + Khối lượng + Giá trị"" và subtable ""Dành cho bưu phẩm là hàng hoá. Nếu biết số thuế HS"".
Em thử dùng Google Form Parser nhưng vẫn ko thành công như mong muốn. Hiện tại, em nghĩ về cách sử dụng deep learning để locate bounding box để crop rồi dùng Google vision để extract info. Nhưng em muốn nghe lời khuyên từ các anh chị về cách approach bài toán này vì em ko biết cách làm của hiện tại có phải là most efficient không. Em cảm ơn nhiều ạ và mong nhận được lời khuyên từ mọi người.","Hi mọi người, hiện tại, em đang có một bài toán về table/document extraction cho tờ khai hải quan. Em muốn extract mỗi cái subtable ""Số lượng và mô tả chi tiết nội dung bưu phẩm + Khối lượng + Giá trị"" và subtable ""Dành cho bưu phẩm là hàng hoá. Nếu biết số thuế HS"". Em thử dùng Google Form Parser nhưng vẫn ko thành công như mong muốn. Hiện tại, em nghĩ về cách sử dụng deep learning để locate bounding box để crop rồi dùng Google vision để extract info. Nhưng em muốn nghe lời khuyên từ các anh chị về cách approach bài toán này vì em ko biết cách làm của hiện tại có phải là most efficient không. Em cảm ơn nhiều ạ và mong nhận được lời khuyên từ mọi người.",,,,,
Các bạn nào có điều kiện tham gia thì cùng chung tay với Lộc.,Các bạn nào có điều kiện tham gia thì cùng chung tay với Lộc.,,,,,
,nan,,,,,
"Khi nhắc đến COVID-19, có lẽ mọi người đã quá quen với những chỉ số như số ca nhiễm mới, số ca tử vong, số xét nghiệm được thực hiện,…
Tuy nhiên, có một chỉ số khác cũng rất quan trọng để đánh giá mức độ nghiêm trọng của một dịch bệnh. Bài hôm nay mình xin giới thiệu qua về hệ số lây nhiễm R và ứng dụng của chỉ số này.","Khi nhắc đến COVID-19, có lẽ mọi người đã quá quen với những chỉ số như số ca nhiễm mới, số ca tử vong, số xét nghiệm được thực hiện,… Tuy nhiên, có một chỉ số khác cũng rất quan trọng để đánh giá mức độ nghiêm trọng của một dịch bệnh. Bài hôm nay mình xin giới thiệu qua về hệ số lây nhiễm R và ứng dụng của chỉ số này.",,,,,
"Dường như xu thế sử dụng kiến trúc Transformers đang dần thay thế cho Convolution là không thể đảo ngược cho bài toán Image Recognition Tasks trong tương lai rất gần https://openreview.net/pdf?id=xTJEN-ggl1b#page4
Các bạn có thể xem YouTube giải thích về bài báo này thêm https://m.youtube.com/watch?feature=youtu.be&v=3qxJ2WD8p4w",Dường như xu thế sử dụng kiến trúc Transformers đang dần thay thế cho Convolution là không thể đảo ngược cho bài toán Image Recognition Tasks trong tương lai rất gần https://openreview.net/pdf?id=xTJEN-ggl1b#page4 Các bạn có thể xem YouTube giải thích về bài báo này thêm https://m.youtube.com/watch?feature=youtu.be&v=3qxJ2WD8p4w,,,,,
"🍎 IMAGE STITCHING - PANORAMA IMAGE
🍀 Làm việc trong lĩnh vực Computer Vision, chúng ta ít nhiều cũng phải động tới các thuật toán xử lí ảnh. Vậy nên hôm nay, ta sẽ đổi gió một chút với các xử lí ảnh nhé. Trong bài viết này, mình sẽ mô tả các thuật toán và code cho bài toán Image Stitching để ghép nối các tấm ảnh được chụp từ nhiều góc độ, vị trí khác nhau. Từ đó tạo nên 1 tấm ảnh có kích thước lớn và toàn cảnh.
🍀 Nội dung bài có viết về các thuật toán SIFT, SUFT, Match Agorithms, RANSAC, homography transform, ...
#image_processing , #feature_based, #feature_extraction, #homography
https://viblo.asia/p/image-stitching-thuat-toan-dung-dang-sau-cong-nghe-anh-panorama-LzD5dee4KjY","IMAGE STITCHING - PANORAMA IMAGE Làm việc trong lĩnh vực Computer Vision, chúng ta ít nhiều cũng phải động tới các thuật toán xử lí ảnh. Vậy nên hôm nay, ta sẽ đổi gió một chút với các xử lí ảnh nhé. Trong bài viết này, mình sẽ mô tả các thuật toán và code cho bài toán Image Stitching để ghép nối các tấm ảnh được chụp từ nhiều góc độ, vị trí khác nhau. Từ đó tạo nên 1 tấm ảnh có kích thước lớn và toàn cảnh. Nội dung bài có viết về các thuật toán SIFT, SUFT, Match Agorithms, RANSAC, homography transform, ... , https://viblo.asia/p/image-stitching-thuat-toan-dung-dang-sau-cong-nghe-anh-panorama-LzD5dee4KjY","#image_processing	#feature_based,	#feature_extraction,	#homography",,,,
"Cho mình xin nguồn tài liệu từ vựng tiếng anh về AI và thống kê ạ. Mình đang đọc tài liệu tiếng anh, cảm thấy tự dịch không có sát nghĩa lắm.","Cho mình xin nguồn tài liệu từ vựng tiếng anh về AI và thống kê ạ. Mình đang đọc tài liệu tiếng anh, cảm thấy tự dịch không có sát nghĩa lắm.",,,,,
"🍜 Kính chào cả nhà, em đang làm dự án cần camera IP nhưng do đang test nên chưa mua và giả lập trên Android cho tiện.
Thấy nhiều bạn cần nên em làm clip share mong giúp được các bạn mới học ah
Mong ad duyệt bài!","Kính chào cả nhà, em đang làm dự án cần camera IP nhưng do đang test nên chưa mua và giả lập trên Android cho tiện. Thấy nhiều bạn cần nên em làm clip share mong giúp được các bạn mới học ah Mong ad duyệt bài!",,,,,
"The ASGAARD lab ((University of Alberta, Canada) has 3 funded positions (2 PhD and 1 MSc) starting Summer or Fall 2021! If you are interested in working on:
(1) automated quality assurance processes for games and/or
(2) software engineering for the modern AI stack
check the website below:","The ASGAARD lab ((University of Alberta, Canada) has 3 funded positions (2 PhD and 1 MSc) starting Summer or Fall 2021! If you are interested in working on: (1) automated quality assurance processes for games and/or (2) software engineering for the modern AI stack check the website below:",,,,,
"Chào mọi người, mình là thành viên mới của nhóm . Hiện tại mình đang học môn machine learning và mình sẽ phải thi kết thúc môn bằng bài tập lớn. Các bạn có thể gợi ý cho mình 1 số đề tài không ạ ? Có tài liệu luôn thì càng tốt ạ 🤣
Mình xin cảm ơn 😊😊","Chào mọi người, mình là thành viên mới của nhóm . Hiện tại mình đang học môn machine learning và mình sẽ phải thi kết thúc môn bằng bài tập lớn. Các bạn có thể gợi ý cho mình 1 số đề tài không ạ ? Có tài liệu luôn thì càng tốt ạ Mình xin cảm ơn",,,,,
"Chào mọi người, dữ liệu đầu vào của em là string [họ tên người Việt] - là kết quả sau khi gọi text ocr detection, vì text ocr detection chưa tốt nên họ tên vẫn còn chưa chính xác, vd: 
result :""PHẠM LƯU HOANG"", thực tế là: ""PHẠM LƯU HOÀNG""
result: ""TRÂN THE YẾN THUY"", thực tế là ""TRẦN THỊ YẾN THÙY""
em muốn tìm một model có thể làm tăng khả năng chính xác cho text ocr detection, model không nhất thiết phải sửa toàn bộ string họ tên. vd:
result: ""TRÂN THE YẾN THUY"". ouput model: ""TRẦN THỊ YẾN THUY"". cụm từ ""TRẦN THE YẾN"" có thể dễ dàng sửa thành ""TRẦN THỊ YẾN"", nhưng cụm từ ""YẾN THUY"" có thể sửa thành ""YẾN THÙY"" hoặc ""YẾN THỦY"" - có độ phổ biến gần ngang nhau nên em không yêu cầu model recorrect cụm từ này (sẽ có một cách khác ngoài model để tiếp tục recorrect).
 đó là cách giải quyết mà em đang hướng tới, model đó là gì? hoặc có cách giải quyết nào tốt hơn mong mn giúp đỡ!","Chào mọi người, dữ liệu đầu vào của em là string [họ tên người Việt] - là kết quả sau khi gọi text ocr detection, vì text ocr detection chưa tốt nên họ tên vẫn còn chưa chính xác, vd: result :""PHẠM LƯU HOANG"", thực tế là: ""PHẠM LƯU HOÀNG"" result: ""TRÂN THE YẾN THUY"", thực tế là ""TRẦN THỊ YẾN THÙY"" em muốn tìm một model có thể làm tăng khả năng chính xác cho text ocr detection, model không nhất thiết phải sửa toàn bộ string họ tên. vd: result: ""TRÂN THE YẾN THUY"". ouput model: ""TRẦN THỊ YẾN THUY"". cụm từ ""TRẦN THE YẾN"" có thể dễ dàng sửa thành ""TRẦN THỊ YẾN"", nhưng cụm từ ""YẾN THUY"" có thể sửa thành ""YẾN THÙY"" hoặc ""YẾN THỦY"" - có độ phổ biến gần ngang nhau nên em không yêu cầu model recorrect cụm từ này (sẽ có một cách khác ngoài model để tiếp tục recorrect). đó là cách giải quyết mà em đang hướng tới, model đó là gì? hoặc có cách giải quyết nào tốt hơn mong mn giúp đỡ!",,,,,
"#hoidap
Em chào mọi người ạ. Hiện tại em đang có cần crawl các skill tiếng anh liên quan đến bài toán tuyển dụng. Mọi người có thể gợi ý cho em vài trang web tuyển dụng hay gì đó mà có thể crawl được các skill không ạ ? Em cảm ơn",Em chào mọi người ạ. Hiện tại em đang có cần crawl các skill tiếng anh liên quan đến bài toán tuyển dụng. Mọi người có thể gợi ý cho em vài trang web tuyển dụng hay gì đó mà có thể crawl được các skill không ạ ? Em cảm ơn,#hoidap,,,,
"Em có một số tấm hình chụp cây cải thảo như bên dưới và cần đếm số cây trên ruộng. Mọi người có ý tưởng nào để thực hiện không ạ.
Lượng ảnh có thể chụp thêm, em chỉ gửi 2 tấm mình họa ạ.
Cảm ơn mọi người!!!!!","Em có một số tấm hình chụp cây cải thảo như bên dưới và cần đếm số cây trên ruộng. Mọi người có ý tưởng nào để thực hiện không ạ. Lượng ảnh có thể chụp thêm, em chỉ gửi 2 tấm mình họa ạ. Cảm ơn mọi người!!!!!",,,,,
"Chào mọi người, mình đang muốn hỏi về sự khác nhau giữa validation, test và cross-validation. Theo mình hiểu, validation sử dụng để đo lường test statistics CV(n) để xem số lượng tham số của mô hình là bao nhiêu (ngăn ngừa overfit) còn tập test để đo lường true (test) MSE, chọn ra model tối ưu, đúng không nhỉ? Vậy thì sau khi tìm được số lượng tham số, mình lại chạy lại mô hình trên tập train để tìm ra mô hình tối ưu test lại trên tập test đúng không? (Sử dụng adjusted R square as a test statistic chẳng hạn?)","Chào mọi người, mình đang muốn hỏi về sự khác nhau giữa validation, test và cross-validation. Theo mình hiểu, validation sử dụng để đo lường test statistics CV(n) để xem số lượng tham số của mô hình là bao nhiêu (ngăn ngừa overfit) còn tập test để đo lường true (test) MSE, chọn ra model tối ưu, đúng không nhỉ? Vậy thì sau khi tìm được số lượng tham số, mình lại chạy lại mô hình trên tập train để tìm ra mô hình tối ưu test lại trên tập test đúng không? (Sử dụng adjusted R square as a test statistic chẳng hạn?)",,,,,
"Mình còn nhớ 2-3 năm trước bác ấy có nói tới trends in AI là unsupervised learning. Còn nay là:
1/ Self-supervised Learning;
2/ Federated Learning;
3/ Transformers architectures",Mình còn nhớ 2-3 năm trước bác ấy có nói tới trends in AI là unsupervised learning. Còn nay là: 1/ Self-supervised Learning; 2/ Federated Learning; 3/ Transformers architectures,,,,,
"Em là sinh viên và đang làm bài tập lớn về Neural Networks. Đề bài của em là ""Nhận dạng các chữ số từ 0 đến 9"", em sử dụng dữ liệu từ mnist dataset. Hiện em đang gặp khó khăn trong việc chọn mạng để xử lý bài toán. Mong mọi người cho thể cho em biết những loại mạng nào có thể giải quyết bài toán của em (trừ CNN và MLP ạ, 2 mạng này đã có nhóm dùng). Code với em không là vấn đề ạ","Em là sinh viên và đang làm bài tập lớn về Neural Networks. Đề bài của em là ""Nhận dạng các chữ số từ 0 đến 9"", em sử dụng dữ liệu từ mnist dataset. Hiện em đang gặp khó khăn trong việc chọn mạng để xử lý bài toán. Mong mọi người cho thể cho em biết những loại mạng nào có thể giải quyết bài toán của em (trừ CNN và MLP ạ, 2 mạng này đã có nhóm dùng). Code với em không là vấn đề ạ",,,,,
"Mời Anh EM Forum ML mình tham gia nghe và thảo luận online Tech Talk về các công nghệ AI, BigData, IoT, 5G-6G, Blockchain, Fintech và Robotics nhé.
. Trong Tech Talk này, các công nghệ trên sẽ được trình bày và thảo luận. Các speakers nổi bật trong mỗi topic sẽ đưa chúng ta tới những khía cạnh khác nhau về các công nghệ trên từ góc nhìn nghiên cứu, startup, và công nghiệp. Mọi người hãy cùng đăng ký tham gia thảo luận với các speakers về công nghệ yêu thích và triển vọng trong tương lai nhé.
Hãy chọn NÚT ""THAM GIA"" trong Facebook event dưới và đăng kí chọn Talk 6 chủ đề ""ICT CONVERGENCE – SHAPING THE FUTURE OF VIETNAM: HỘI TỤ CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG CHO SỰ PHÁT TRIỂN CỦA ĐẤT NƯỚC"" trên website http://trithuctrevietnam.vn để nhận được link của Online Talk của mình qua Zoom sớm nhất nhé!
Thời gian diễn ra Online Tech Talk: 12h00-14h10 (giờ Việt Nam), thứ bảy và chủ nhật, 17 và 18/10/2020","Mời Anh EM Forum ML mình tham gia nghe và thảo luận online Tech Talk về các công nghệ AI, BigData, IoT, 5G-6G, Blockchain, Fintech và Robotics nhé. . Trong Tech Talk này, các công nghệ trên sẽ được trình bày và thảo luận. Các speakers nổi bật trong mỗi topic sẽ đưa chúng ta tới những khía cạnh khác nhau về các công nghệ trên từ góc nhìn nghiên cứu, startup, và công nghiệp. Mọi người hãy cùng đăng ký tham gia thảo luận với các speakers về công nghệ yêu thích và triển vọng trong tương lai nhé. Hãy chọn NÚT ""THAM GIA"" trong Facebook event dưới và đăng kí chọn Talk 6 chủ đề ""ICT CONVERGENCE – SHAPING THE FUTURE OF VIETNAM: HỘI TỤ CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG CHO SỰ PHÁT TRIỂN CỦA ĐẤT NƯỚC"" trên website http://trithuctrevietnam.vn để nhận được link của Online Talk của mình qua Zoom sớm nhất nhé! Thời gian diễn ra Online Tech Talk: 12h00-14h10 (giờ Việt Nam), thứ bảy và chủ nhật, 17 và 18/10/2020",,,,,
"Hi mn.

Em đang dùng CNN để nhận dạng đèn giao thông.Mn ai từng làm đề tài có thể cho em xin Data được ko ạ ?Em cám ơn.",Hi mn. Em đang dùng CNN để nhận dạng đèn giao thông.Mn ai từng làm đề tài có thể cho em xin Data được ko ạ ?Em cám ơn.,,,,,
"Em có 1 thắc mắc này muốn nhờ các bác chỉ giáo.
Khi so sánh 2 model A (Mask R-CNN ResNeXt-101-32x4d) và B (Cascade Mask R-CNN ResNet-50) trên 1 tập khoảng ~ 1k ảnh, tỉ lệ train:val:test là 70:15:15, xảy ra vấn đề:
Khi training và validation: Precision và Recall của model B tốt hơn model A.
Tuy nhiên, khi testing: Precision và Recall của model A lại tốt hơn model B.
Các bác cho em xin lời giải thích và các khắc phục cho đúng lẽ ko ạ? 😢😢😢
P/s: ảnh testing sẽ khác chút ít so với train và val","Em có 1 thắc mắc này muốn nhờ các bác chỉ giáo. Khi so sánh 2 model A (Mask R-CNN ResNeXt-101-32x4d) và B (Cascade Mask R-CNN ResNet-50) trên 1 tập khoảng ~ 1k ảnh, tỉ lệ train:val:test là 70:15:15, xảy ra vấn đề: Khi training và validation: Precision và Recall của model B tốt hơn model A. Tuy nhiên, khi testing: Precision và Recall của model A lại tốt hơn model B. Các bác cho em xin lời giải thích và các khắc phục cho đúng lẽ ko ạ? P/s: ảnh testing sẽ khác chút ít so với train và val",,,,,
"Mọi người cho em hỏi có pretrained model nào cho bài toán phân loại (classification) ảnh 3D không ạ?
Bài toán của em là phân loại một người bệnh/không bệnh dựa trên các ảnh CT scans dạng 2D của bệnh nhân, và các ảnh này có thể dùng để reconstruct thành 3D. Tuy nhiên trước giờ em mới chỉ biết đến các 2D models (resnet, efficientnet, mobilenet, etc.) mà chưa thấy các ứng dụng tương tự cho 3D CNN. Cao nhân nào có thông tin cho em xin với ạ. Nếu model built trên keras/tensorflow thì càng tốt ạ. Em xin cảm ơn!","Mọi người cho em hỏi có pretrained model nào cho bài toán phân loại (classification) ảnh 3D không ạ? Bài toán của em là phân loại một người bệnh/không bệnh dựa trên các ảnh CT scans dạng 2D của bệnh nhân, và các ảnh này có thể dùng để reconstruct thành 3D. Tuy nhiên trước giờ em mới chỉ biết đến các 2D models (resnet, efficientnet, mobilenet, etc.) mà chưa thấy các ứng dụng tương tự cho 3D CNN. Cao nhân nào có thông tin cho em xin với ạ. Nếu model built trên keras/tensorflow thì càng tốt ạ. Em xin cảm ơn!",,,,,
"Gần đây mình có post bài báo dưới dạng open under review về Vision Transformers tại đây https://openreview.net/pdf?id=YicbFdNTTy; Vậy nên mình thử ứng dụng chúng vào giải quyết bài toán liên quan tới ảnh X quang phổi chẩn đoán Covid-19. Datasets cho việc này mình thu thập tại dựa trên nhiều nguồn mở https://github.com/lindawangg/COVID-Net; https://github.com/ieee8023/covid-chestxray-dataset; & https://figshare.com/articles/COVID-19_Chest_X-Ray_Image_Repository/12580328. Trong phần này mình dùng Hybrid model với backbone là EfficientNet-B0 kết hợp với Transformers. Và kết quả đạt được rất khả quan sau vài giờ huấn luyện (xem confusion matrix). Kết quả này tốt hơn so với kết quả gần đây của nhóm mình, các bạn có thể tham khảo tại đây https://www.medrxiv.org/content/10.1101/2020.08.13.20173997v1.full.pdf. Không hiểu sao stt trên Facebook không cho chèn ảnh nên mình đưa CM xuống phần comment","Gần đây mình có post bài báo dưới dạng open under review về Vision Transformers tại đây https://openreview.net/pdf?id=YicbFdNTTy; Vậy nên mình thử ứng dụng chúng vào giải quyết bài toán liên quan tới ảnh X quang phổi chẩn đoán Covid-19. Datasets cho việc này mình thu thập tại dựa trên nhiều nguồn mở https://github.com/lindawangg/COVID-Net; https://github.com/ieee8023/covid-chestxray-dataset; & https://figshare.com/articles/COVID-19_Chest_X-Ray_Image_Repository/12580328. Trong phần này mình dùng Hybrid model với backbone là EfficientNet-B0 kết hợp với Transformers. Và kết quả đạt được rất khả quan sau vài giờ huấn luyện (xem confusion matrix). Kết quả này tốt hơn so với kết quả gần đây của nhóm mình, các bạn có thể tham khảo tại đây https://www.medrxiv.org/content/10.1101/2020.08.13.20173997v1.full.pdf. Không hiểu sao stt trên Facebook không cho chèn ảnh nên mình đưa CM xuống phần comment",,,,,
"EM ĐANG LÀM VỀ TEXT TO IMAGE, ĐẦU VÀO CỦA EM LÀ 1 ĐOẠN, CÂU VĂN MÔ TẢ PHONG CẢNH, SỰ VẬT, ĐẦU RA LÀ 1 BỨC ẢNH. GIÚP CHO MÁY CÓ KHẢ NĂNG TƯỞNG TƯỢNG. HIỆN EM ĐANG GẶP KHÓ KHĂN TRONG VIỆC CHỌN MẠNG ĐỂ XỬ LÝ BÀI TOÁN.MONG MỌI NGƯỜI CHO THỂ CHO EM BIẾT NHỮNG LOẠI MẠNG NÀO CÓ THỂ GIẢI QUYẾT BÀI TOÁN CỦA EM","EM ĐANG LÀM VỀ TEXT TO IMAGE, ĐẦU VÀO CỦA EM LÀ 1 ĐOẠN, CÂU VĂN MÔ TẢ PHONG CẢNH, SỰ VẬT, ĐẦU RA LÀ 1 BỨC ẢNH. GIÚP CHO MÁY CÓ KHẢ NĂNG TƯỞNG TƯỢNG. HIỆN EM ĐANG GẶP KHÓ KHĂN TRONG VIỆC CHỌN MẠNG ĐỂ XỬ LÝ BÀI TOÁN.MONG MỌI NGƯỜI CHO THỂ CHO EM BIẾT NHỮNG LOẠI MẠNG NÀO CÓ THỂ GIẢI QUYẾT BÀI TOÁN CỦA EM",,,,,
"Kính chào các bác, chả là hôm rồi em làm một bài cần phải Custom Loss nên mạnh dạn note lại để chia sẻ cùng anh em newbie mới học.
Mong giúp được anh em và mong ad duyệt bài! Các bác cao thủ đi qua chỉ giáo thêm để em được mở mang kiến thức ah!","Kính chào các bác, chả là hôm rồi em làm một bài cần phải Custom Loss nên mạnh dạn note lại để chia sẻ cùng anh em newbie mới học. Mong giúp được anh em và mong ad duyệt bài! Các bác cao thủ đi qua chỉ giáo thêm để em được mở mang kiến thức ah!",,,,,
Cho em hỏi có ai đã từng tạo một cái environment trong zeppelin chưa ạ. Em tạo một cái và list ra thì thấy có mà không hiểu sao không thể chỉ tới môi trường đó làm việc là sao ạ? Cảm ơn cả nhà!,Cho em hỏi có ai đã từng tạo một cái environment trong zeppelin chưa ạ. Em tạo một cái và list ra thì thấy có mà không hiểu sao không thể chỉ tới môi trường đó làm việc là sao ạ? Cảm ơn cả nhà!,,,,,
"Anh chị cho em hỏi với ạ. Em đang tìm hiểu về Seq2Seq Encoder Decoder model ạ. Em muốn hỏi là:
1:
Input của phần Encoder là các embedding vector (như Word2Vec, Glove, ...) phải không ạ.

2:
<EOS> (EndOfSequence) ở đây cụ thể là gì ạ? <EOS> sẽ là một Embedding input vector như bao từ bình thường khác. Hay sẽ là một cách đánh dấu kết thúc câu nào đó ạ?

3:
Greedy search hay Beam search được dùng ở bước test phải không ạ. Tức là sau khi train xong rồi, thì mới dùng đến 2 giải thuật kia để quyết định dịch sang câu nào phải không ạ. 
Còn khi train thì đơn giản là tối ưu sao cho vị trí của từ mong muốn trong vector output của decoder có giá trí càng gần 1 càng tốt, các vị trí còn lại thì càng gần 0 là được phải không ạ. Tức là ở ngôn ngữ được dịch ra, ta không cần embedding kiểu word2vec, glove mà chỉ cần dạng one hot vector ạ.

Hơi dài ạ. Em mong anh, chị giúp em với ạ. Em cảm ơn anh chị nhiều ạ.","Anh chị cho em hỏi với ạ. Em đang tìm hiểu về Seq2Seq Encoder Decoder model ạ. Em muốn hỏi là: 1: Input của phần Encoder là các embedding vector (như Word2Vec, Glove, ...) phải không ạ. 2: <EOS> (EndOfSequence) ở đây cụ thể là gì ạ? <EOS> sẽ là một Embedding input vector như bao từ bình thường khác. Hay sẽ là một cách đánh dấu kết thúc câu nào đó ạ? 3: Greedy search hay Beam search được dùng ở bước test phải không ạ. Tức là sau khi train xong rồi, thì mới dùng đến 2 giải thuật kia để quyết định dịch sang câu nào phải không ạ. Còn khi train thì đơn giản là tối ưu sao cho vị trí của từ mong muốn trong vector output của decoder có giá trí càng gần 1 càng tốt, các vị trí còn lại thì càng gần 0 là được phải không ạ. Tức là ở ngôn ngữ được dịch ra, ta không cần embedding kiểu word2vec, glove mà chỉ cần dạng one hot vector ạ. Hơi dài ạ. Em mong anh, chị giúp em với ạ. Em cảm ơn anh chị nhiều ạ.",,,,,
"Hi mọi người,
Cho em hỏi là có cách nào để xài version control trên Google Colab như Git không ạ? Em cảm ơn ^^","Hi mọi người, Cho em hỏi là có cách nào để xài version control trên Google Colab như Git không ạ? Em cảm ơn ^^",,,,,
Anh chị cho em hỏi với ạ. Dạ em đang chạy code detect khuôn mặt thì bị lỗi này ạ. Anh chị giúp em với ạ.,Anh chị cho em hỏi với ạ. Dạ em đang chạy code detect khuôn mặt thì bị lỗi này ạ. Anh chị giúp em với ạ.,,,,,
"[OCR]
Hi mọi người,
Hiện tại em đang có bài toán nhận diện OCR hàng ngang với case giá trị số có thể bị lệch so với giá trị chữ. Ví dụ như khi detect Creatinine, thì thay vì nhận diện 0.8 cùng hàng nhưng lại bị lệch thành 0.5 hoặc 115.51. Mọi người có suggestion gì cho bài toán này không ạ, em xin cảm ơn ^^","[OCR] Hi mọi người, Hiện tại em đang có bài toán nhận diện OCR hàng ngang với case giá trị số có thể bị lệch so với giá trị chữ. Ví dụ như khi detect Creatinine, thì thay vì nhận diện 0.8 cùng hàng nhưng lại bị lệch thành 0.5 hoặc 115.51. Mọi người có suggestion gì cho bài toán này không ạ, em xin cảm ơn ^^",,,,,
"Hai chương chính thức cuối cùng của cuốn ""Dive into Deep Learning"" đã được dịch xong:
https://d2l.aivivn.com/chapter_recommender-systems/index_vn.html
Hai chương này trình bày các kỹ thuật trong Hệ thống Đề xuất (Recommendation System) và GAN. Phần còn lại của cuốn sách là các chương phụ lục.","Hai chương chính thức cuối cùng của cuốn ""Dive into Deep Learning"" đã được dịch xong: https://d2l.aivivn.com/chapter_recommender-systems/index_vn.html Hai chương này trình bày các kỹ thuật trong Hệ thống Đề xuất (Recommendation System) và GAN. Phần còn lại của cuốn sách là các chương phụ lục.",,,,,
"xin chào mọi người ạ
em mới tập code pytorch và tiện làm bài toán regression x -> y với y là 1 vector nhiều chiều e đã viết tự implement với hàm MSE thành công. nhưng để tăng độ chính xác em muốn thêm cả hàm tính cos vào nữa. Nhưng có vẻ hàm cos nó không tính backpropagation được hay sao ý. 
em đăng post này mong các cao thủ giúp em với ạ .
hàm cos em viết từng viết đây và nó không hoạt động :( 
import torch
def loss_cosin(y,y_pred):
    return -torch.nn.CosineSimilarity(y,y_pred).mean()","xin chào mọi người ạ em mới tập code pytorch và tiện làm bài toán regression x -> y với y là 1 vector nhiều chiều e đã viết tự implement với hàm MSE thành công. nhưng để tăng độ chính xác em muốn thêm cả hàm tính cos vào nữa. Nhưng có vẻ hàm cos nó không tính backpropagation được hay sao ý. em đăng post này mong các cao thủ giúp em với ạ . hàm cos em viết từng viết đây và nó không hoạt động :( import torch def loss_cosin(y,y_pred): return -torch.nn.CosineSimilarity(y,y_pred).mean()",,,,,
"https://blog.facebit.net/2018/09/07/zalo-ai-challenge-problems-and-solutions/?fbclid=IwAR0WkkWSurvGy5xm2uowPzgtc52dBB8ueHSPMGpK4iFMakM8dAKqm5jooG4
Anh chị cho e hỏi với ạ. Em có truy cập vào link zalo on chanllenge này để tải bộ dữ liệu về Challenge 3: Voice Gender/Accent Classification mà hiện tại link không tải được. Anh chị nào có bộ dữ liệu này cho e xin với ạ. Em cảm ơn.",https://blog.facebit.net/2018/09/07/zalo-ai-challenge-problems-and-solutions/?fbclid=IwAR0WkkWSurvGy5xm2uowPzgtc52dBB8ueHSPMGpK4iFMakM8dAKqm5jooG4 Anh chị cho e hỏi với ạ. Em có truy cập vào link zalo on chanllenge này để tải bộ dữ liệu về Challenge 3: Voice Gender/Accent Classification mà hiện tại link không tải được. Anh chị nào có bộ dữ liệu này cho e xin với ạ. Em cảm ơn.,,,,,
"Chào mọi người. E đang cần dataset để làm ứng dụng nhận dạng tiếng nói. Input file audio và out vùng miền nào ở VN, trai hay gái ( zalo al change 2018 có đề tài nayg mà e vào link tải không đx) anh chị nào có cho e xin với ạ. Em cảm ơn.","Chào mọi người. E đang cần dataset để làm ứng dụng nhận dạng tiếng nói. Input file audio và out vùng miền nào ở VN, trai hay gái ( zalo al change 2018 có đề tài nayg mà e vào link tải không đx) anh chị nào có cho e xin với ạ. Em cảm ơn.",,,,,
"Do tập những tập dữ liệu mình train hao hao giống nhau nênđể tăng tính chính xác và giảm nhiễu khi train nhiều class, mình đã tách 2 bộ dữ liệu khác nhau train thành 2 file weights.
Giờ để cho gọn model, mình muốn ghép 2 cái đó thành 1 file chung, liệu có model hỗ trợ cho việc đó không ạ, hay phải code kết hợp bên ngoài ạ. Cảm ơn mọi người.","Do tập những tập dữ liệu mình train hao hao giống nhau nênđể tăng tính chính xác và giảm nhiễu khi train nhiều class, mình đã tách 2 bộ dữ liệu khác nhau train thành 2 file weights. Giờ để cho gọn model, mình muốn ghép 2 cái đó thành 1 file chung, liệu có model hỗ trợ cho việc đó không ạ, hay phải code kết hợp bên ngoài ạ. Cảm ơn mọi người.",,,,,
"Mọi người cho em hỏi nếu học ML thì có cần biết về mạch điện tử ko ạ? Những công việc cụ thể để có thể làm sau khi tốt nghiệp, e đang phân vân giữa data analyst với computer vision, 1 câu hỏi nhạy cảm là mức lương nó ntn ạ? E là newbie, thank m.n","Mọi người cho em hỏi nếu học ML thì có cần biết về mạch điện tử ko ạ? Những công việc cụ thể để có thể làm sau khi tốt nghiệp, e đang phân vân giữa data analyst với computer vision, 1 câu hỏi nhạy cảm là mức lương nó ntn ạ? E là newbie, thank m.n",,,,,
Em hoàn thành xong khoá ML của thầy Andrew rồi giờ em muốn học sau hơn về phần toán thì nên học khoá nào nữa ạ?,Em hoàn thành xong khoá ML của thầy Andrew rồi giờ em muốn học sau hơn về phần toán thì nên học khoá nào nữa ạ?,,,"#Q&A, #math, #machine_learning",,
"#Object_Detection
Chào mọi người, em đang tìm hiểu về model mobilenet SSD v1. Theo em hiểu ưu điểm của model này kết hợp giữa mobilenet (phần embedding) và SSD giúp triển khai mô hình object detection nhanh hơn (độ chính xác kém hơn SSD nhưng k đáng kể), có thể chạy realtime trên các thiết bị như jetson nano. Không biết em hiểu đúng k, mong anh chị giúp em hiểu thêm với. nếu được thì nói giúp em thêm phần ưu điểm với. Em cảm ơn nhiều ạ.","Chào mọi người, em đang tìm hiểu về model mobilenet SSD v1. Theo em hiểu ưu điểm của model này kết hợp giữa mobilenet (phần embedding) và SSD giúp triển khai mô hình object detection nhanh hơn (độ chính xác kém hơn SSD nhưng k đáng kể), có thể chạy realtime trên các thiết bị như jetson nano. Không biết em hiểu đúng k, mong anh chị giúp em hiểu thêm với. nếu được thì nói giúp em thêm phần ưu điểm với. Em cảm ơn nhiều ạ.",#Object_Detection,,,,
"Chào các bạn, Vietnam Document Analysis group (VnDAG) & AiViVn đang lên kế hoạch tổ chức challenge cho nhận dạng hoá đơn tiếng Việt. Nhóm mong sự trợ giúp từ các bạn bằng cách chụp ảnh bất kỳ hoá đơn mua bán (ví dụ hoá đơn siêu thị) mà bạn có và upload qua dịch vụ của nhóm. Chi tiết mô tả như sau:
- 1. Hoá đơn dạng dài, nội dung là các sản phẩm mua sắm và giá tiền (xem ảnh ví dụ).
- 2. Hoá đơn cần bằng tiếng Việt, cần rõ nội dung, ko bị nhoè chữ. Mình có đính kèm 2 ảnh: 1 ảnh tiếng việt và 1 ảnh ví dụ tuy ko phẳng nhưng vẫn đạt chất lượng.
- 3. Bạn chụp và upload ảnh chụp qua website: http://vndag.vietnlp.com. Nếu bạn có nhiều hoá đơn, bạn có thể gửi nhiều lần hoặc nén và gửi file nén.
- 4. Thời gian đóng góp hoá đơn: 5 ngày (từ ngày Aug 13 - Aug 18).
- Để đủ dữ liệu cho challenge được thực hiện, nhóm xác định cần 2000 - 3000 hoá đơn. Nên nếu mỗi thành viên đọc được bài đóng góp 1 hoá đơn là nhóm đã có đủ số lượng để tiến hành annotate dữ liệu cho challenge.
Rất mong sự đóng góp dữ liệu và tham gia nhiệt tình từ các bạn cho challenge sắp tới.
PS. Nhóm VnDAG có tuyển 2 cộng tác viên là các bạn sinh viên nhiệt tình và có đam mê nghiên cứu lĩnh vực Document Analysis. Cơ hội để các bạn có thể tham gia vào nghiên cứu các challenges thực tế, tiếp cận và học hỏi / trao đổi các kiến thức mới nhất cũng như được mentor bởi các tiền bối trong domain này trên thế giới. Các bạn gửi email kèm CV đến vndag@vietnlp.com để được liên hệ và trao đổi chi tiết.","Chào các bạn, Vietnam Document Analysis group (VnDAG) & AiViVn đang lên kế hoạch tổ chức challenge cho nhận dạng hoá đơn tiếng Việt. Nhóm mong sự trợ giúp từ các bạn bằng cách chụp ảnh bất kỳ hoá đơn mua bán (ví dụ hoá đơn siêu thị) mà bạn có và upload qua dịch vụ của nhóm. Chi tiết mô tả như sau: - 1. Hoá đơn dạng dài, nội dung là các sản phẩm mua sắm và giá tiền (xem ảnh ví dụ). - 2. Hoá đơn cần bằng tiếng Việt, cần rõ nội dung, ko bị nhoè chữ. Mình có đính kèm 2 ảnh: 1 ảnh tiếng việt và 1 ảnh ví dụ tuy ko phẳng nhưng vẫn đạt chất lượng. - 3. Bạn chụp và upload ảnh chụp qua website: http://vndag.vietnlp.com. Nếu bạn có nhiều hoá đơn, bạn có thể gửi nhiều lần hoặc nén và gửi file nén. - 4. Thời gian đóng góp hoá đơn: 5 ngày (từ ngày Aug 13 - Aug 18). - Để đủ dữ liệu cho challenge được thực hiện, nhóm xác định cần 2000 - 3000 hoá đơn. Nên nếu mỗi thành viên đọc được bài đóng góp 1 hoá đơn là nhóm đã có đủ số lượng để tiến hành annotate dữ liệu cho challenge. Rất mong sự đóng góp dữ liệu và tham gia nhiệt tình từ các bạn cho challenge sắp tới. PS. Nhóm VnDAG có tuyển 2 cộng tác viên là các bạn sinh viên nhiệt tình và có đam mê nghiên cứu lĩnh vực Document Analysis. Cơ hội để các bạn có thể tham gia vào nghiên cứu các challenges thực tế, tiếp cận và học hỏi / trao đổi các kiến thức mới nhất cũng như được mentor bởi các tiền bối trong domain này trên thế giới. Các bạn gửi email kèm CV đến vndag@vietnlp.com để được liên hệ và trao đổi chi tiết.",,,,,
"Em chào các bác!
Hiện tại em tìm hiểu bài toán IDcard detection, với output như hình đính kèm.
Ban đầu em sử dụng Opencv để giải quyết nhưng không khả thi lắm, thấy nó chỉ tốt khi background không quá nhiều nhiễu.
Em tính dùng ML, trong quá trình tìm hiểu thì thấy có một vài hướng như sau:
1. Tìm 4 điểm góc (keypoint detection - Kiểu bài toán human pose estimation).
2. Segmentation.
3. Coi 4 góc là đối tượng => object detection (Giải pháp này em tham khảo bên FPT)
4. Text detection => bouding box
Và cần > 5 FPS (nếu mà trên mobile deivce thì càng tốt ạ)
Hiện tại em chưa đánh giá được phương pháp nào khả khi nhất, rất mong nhận được góp ý của các bác ạ.
Cám ơn các bác, chúc các bác một ngày làm việc hiệu quả :D","Em chào các bác! Hiện tại em tìm hiểu bài toán IDcard detection, với output như hình đính kèm. Ban đầu em sử dụng Opencv để giải quyết nhưng không khả thi lắm, thấy nó chỉ tốt khi background không quá nhiều nhiễu. Em tính dùng ML, trong quá trình tìm hiểu thì thấy có một vài hướng như sau: 1. Tìm 4 điểm góc (keypoint detection - Kiểu bài toán human pose estimation). 2. Segmentation. 3. Coi 4 góc là đối tượng => object detection (Giải pháp này em tham khảo bên FPT) 4. Text detection => bouding box Và cần > 5 FPS (nếu mà trên mobile deivce thì càng tốt ạ) Hiện tại em chưa đánh giá được phương pháp nào khả khi nhất, rất mong nhận được góp ý của các bác ạ. Cám ơn các bác, chúc các bác một ngày làm việc hiệu quả :D",,,,,
"#tensorflow
Em chào mọi người. Hiện tại em có 1 vấn đề liên quan dến tensorflow như sau mong có ai đã từng gặp phải cho em cách sửa lỗi.
Ảnh thứ nhất là dataloader của em,theo em hiểu thì sẽ tạo ra 1 data_generator.
ẢNh thứ 2 là hàm loss của em. Và khi em in ra y_true,y_pred như ảnh 2 có code thì nó ko in ra tensor chứa số liệu mà sẽ ra như hình thứ 3.
Em có xử lý điều đó bằng cách thêm 2 đoạn code ở hình thứ 4.Điều đó giúp em hoạt động ở 1 mạng khác.
Tuy nhiên ở mạng này của em thì nó gây ra lỗi ở hình thứ 5, có trỏ đến lỗi xuất phát ở việc gọi model EfficientNetB0 ở hình thứ 6.
2 cái bị config với nhau nên em ko biết xử lý thế nào. Ko biết có ai có cao kiến gì ko.
Vì vấn dề khá lằng nhằng nên hi vọng có người hiểu được ý e :)))
Em cảm ơn","Em chào mọi người. Hiện tại em có 1 vấn đề liên quan dến tensorflow như sau mong có ai đã từng gặp phải cho em cách sửa lỗi. Ảnh thứ nhất là dataloader của em,theo em hiểu thì sẽ tạo ra 1 data_generator. ẢNh thứ 2 là hàm loss của em. Và khi em in ra y_true,y_pred như ảnh 2 có code thì nó ko in ra tensor chứa số liệu mà sẽ ra như hình thứ 3. Em có xử lý điều đó bằng cách thêm 2 đoạn code ở hình thứ 4.Điều đó giúp em hoạt động ở 1 mạng khác. Tuy nhiên ở mạng này của em thì nó gây ra lỗi ở hình thứ 5, có trỏ đến lỗi xuất phát ở việc gọi model EfficientNetB0 ở hình thứ 6. 2 cái bị config với nhau nên em ko biết xử lý thế nào. Ko biết có ai có cao kiến gì ko. Vì vấn dề khá lằng nhằng nên hi vọng có người hiểu được ý e :))) Em cảm ơn",#tensorflow,,,,
"[Human-learn]
Human learn là một công cụ cho phép bạn vẽ qua các tập dữ liệu của mình. Những bản vẽ này sau đó có thể được chuyển đổi thành mô hình hoặc thành các công cụ tiền xử lý. Ngoài ra, Human learn có thể kết hợp với scikit-learn giúp cho việc tiền xử lí và xây dựng các mô hình dễ dàng hơn.
Một số mô hình được hỗ trợ:
- Classification Models
- Regression Models
- Outlier Detection Models
- Preprocessing Models
Github: https://github.com/koaning/human-learn
Document: https://koaning.github.io/human-learn/","[Human-learn] Human learn là một công cụ cho phép bạn vẽ qua các tập dữ liệu của mình. Những bản vẽ này sau đó có thể được chuyển đổi thành mô hình hoặc thành các công cụ tiền xử lý. Ngoài ra, Human learn có thể kết hợp với scikit-learn giúp cho việc tiền xử lí và xây dựng các mô hình dễ dàng hơn. Một số mô hình được hỗ trợ: - Classification Models - Regression Models - Outlier Detection Models - Preprocessing Models Github: https://github.com/koaning/human-learn Document: https://koaning.github.io/human-learn/",,,,,
"Chào các bác,
Em đang muốn sử dụng ML để phân tích các log của máy chủ (ví dụ: IIS log, ...) để phân biệt đâu là truy cập bình thường, đâu là truy cập bất thường nhưng em chưa có hướng đi nào cả.
Mong các bác gợi ý cụ thể giúp em ạ.
Em mới tiếp cận ML nên mong các bác gợi ý càng chi tiết càng tốt ạ. Em muốn từ bài toán này để có thể có động lực để đi sâu tìm hiểu ML ạ.
Cảm ơn các bác nhiều.","Chào các bác, Em đang muốn sử dụng ML để phân tích các log của máy chủ (ví dụ: IIS log, ...) để phân biệt đâu là truy cập bình thường, đâu là truy cập bất thường nhưng em chưa có hướng đi nào cả. Mong các bác gợi ý cụ thể giúp em ạ. Em mới tiếp cận ML nên mong các bác gợi ý càng chi tiết càng tốt ạ. Em muốn từ bài toán này để có thể có động lực để đi sâu tìm hiểu ML ạ. Cảm ơn các bác nhiều.",,,,,
"Chào mọi người, em đang muốn xuất frame ảnh từ videos trên google colab, code bên dưới chạy không ra kết quả, mong mọi người xem và hướng dẫn giúp, em cảm ơn ạ.
Edited: Code chạy được rồi, e cảm ơn mng nhiều ạ 😀","Chào mọi người, em đang muốn xuất frame ảnh từ videos trên google colab, code bên dưới chạy không ra kết quả, mong mọi người xem và hướng dẫn giúp, em cảm ơn ạ. Edited: Code chạy được rồi, e cảm ơn mng nhiều ạ",,,,,
"xin chào các ace, mình mới bắt đầu học về lĩnh vực này và bắt đầu học phần Neuron Network đầu tiên. Mình đang vướng ở 3 câu hỏi mà mình chưa hiểu và chưa làm được. Ace nào rảnh giúp mình với a?
1. Design a NAND-gate using a network of McCulloch and Pitts neurons
2. Create a 4-bit adder using McCulloch and Pitts neurons
3. Create a 4-bit subtractor using McCulloch and Pitts neurons
Mình cảm ơn ạ","xin chào các ace, mình mới bắt đầu học về lĩnh vực này và bắt đầu học phần Neuron Network đầu tiên. Mình đang vướng ở 3 câu hỏi mà mình chưa hiểu và chưa làm được. Ace nào rảnh giúp mình với a? 1. Design a NAND-gate using a network of McCulloch and Pitts neurons 2. Create a 4-bit adder using McCulloch and Pitts neurons 3. Create a 4-bit subtractor using McCulloch and Pitts neurons Mình cảm ơn ạ",,,,,
"[Góc xin data]
Em đang làm đồ án cần data về các hóa đơn: siêu thị,điện ,nước...Anh chị nào có data có thể public thì share cho em với được không ạ.
Cảm ơn mọi người !!!!","[Góc xin data] Em đang làm đồ án cần data về các hóa đơn: siêu thị,điện ,nước...Anh chị nào có data có thể public thì share cho em với được không ạ. Cảm ơn mọi người !!!!",,,,,
Chào mọi người ạ. Em là học sinh lớp 10 muốn học về Machine learning nhưng bị thiếu kiến thức toán thì em phải bổ sung ở đâu ạ? Em đọc sách Machine Learning cơ bản nhưng bị thiếu kiến thức toán nên em thấy khó hiểu quá ạ. Em phải bổ sung những kiến thức toán gì ạ? Em cảm ơn,Chào mọi người ạ. Em là học sinh lớp 10 muốn học về Machine learning nhưng bị thiếu kiến thức toán thì em phải bổ sung ở đâu ạ? Em đọc sách Machine Learning cơ bản nhưng bị thiếu kiến thức toán nên em thấy khó hiểu quá ạ. Em phải bổ sung những kiến thức toán gì ạ? Em cảm ơn,,,"#Q&A, #math, #machine_learning",,
"Chào anh chị, em có giá trị mean = 164, standard deviation = 6, một biến X có z-score = 1.67 và giá trị của X = 174, vậy làm sao để em tìm biết được điểm X này thuộc phân vị thứ mấy ạ, em cảm ơn mọi người.","Chào anh chị, em có giá trị mean = 164, standard deviation = 6, một biến X có z-score = 1.67 và giá trị của X = 174, vậy làm sao để em tìm biết được điểm X này thuộc phân vị thứ mấy ạ, em cảm ơn mọi người.",,,"#Q&A, #math",,
"Kính chào các bác, hiện nay kênh youtube Mì AI vừa ra thêm series Mì Úp để giải quyết nhanh các vướng mắc của các bạn mới học.
Đây là series gồm các clip ngắn (5,10 phút) để tập trung duy nhất vào một vấn đề nào đó.
Mong các bác ủng hộ và chỉ giáo!","Kính chào các bác, hiện nay kênh youtube Mì AI vừa ra thêm series Mì Úp để giải quyết nhanh các vướng mắc của các bạn mới học. Đây là series gồm các clip ngắn (5,10 phút) để tập trung duy nhất vào một vấn đề nào đó. Mong các bác ủng hộ và chỉ giáo!",,,,,
"hi mọi người, cho mình hỏi câu hỏi nhanh:
Nếu training data có labels nhưng test data thì lại không. Thì làm sao biết model perform thế nào trên test data?
Thanks, mọi người","hi mọi người, cho mình hỏi câu hỏi nhanh: Nếu training data có labels nhưng test data thì lại không. Thì làm sao biết model perform thế nào trên test data? Thanks, mọi người",,,,,
"Buổi hội thảo online trao đổi với Professor Alon Halevy, Director of Facebook AI, Professor tại University of Washington cho bạn nào quan tâm :)","Buổi hội thảo online trao đổi với Professor Alon Halevy, Director of Facebook AI, Professor tại University of Washington cho bạn nào quan tâm :)",,,,,
Chào anh chị ạ. Em đang tìm hiểu về Model Faster R-CNN và code bằng Python. Anh/chị nào am hiểu phần này thì có thể cho e hỏi một tý được k ạ. Vì có nhiều khúc mắc nên k tiện đăng lên để mọi người bàn luận. Nếu được thì cho em ib riêng để không phiền đến những người khác ạ. Em cảm ơn. Mong Ad duyệt bài ạ,Chào anh chị ạ. Em đang tìm hiểu về Model Faster R-CNN và code bằng Python. Anh/chị nào am hiểu phần này thì có thể cho e hỏi một tý được k ạ. Vì có nhiều khúc mắc nên k tiện đăng lên để mọi người bàn luận. Nếu được thì cho em ib riêng để không phiền đến những người khác ạ. Em cảm ơn. Mong Ad duyệt bài ạ,,,,,
"Chào mọi người. Mình có câu hỏi muốn đc giải đáp: trong quá trình chuyển task từ image retrieval sang image classification thì những bước cần làm là gì? Có ưu tiên cụ thể nào ko? Mặc định các khâu khác như data, model, loss function, training, retrieval function đều đã có sẵn.
Cám ơn mọi người trước nhé!","Chào mọi người. Mình có câu hỏi muốn đc giải đáp: trong quá trình chuyển task từ image retrieval sang image classification thì những bước cần làm là gì? Có ưu tiên cụ thể nào ko? Mặc định các khâu khác như data, model, loss function, training, retrieval function đều đã có sẵn. Cám ơn mọi người trước nhé!",,,,,
"[REGULARIZATION - OVERFITTING]
Em xin chào anh chị. Có một vài vấn đề trong phương pháp chính quy hóa trong series của Andrew Ng (regularization) trong việc xử lý overfiting em vẫn chưa hiểu lắm
1. Trong công thức mà Andrew Ng đưa ra ở Regularization cho hồi quy tuyến tính thì tại sao lại phải đưa vào một tham số lamda (em chưa hiểu rõ việc ""co"" cái model nghĩa là gì )
2. Vẫn trong công thức đó, thay vì là theta bình phương thì tại sao không phải là theta^3 ^4 hay các bậc khác.
Em xin cảm ơn mọi người chỉ bảo ạ.","[REGULARIZATION - OVERFITTING] Em xin chào anh chị. Có một vài vấn đề trong phương pháp chính quy hóa trong series của Andrew Ng (regularization) trong việc xử lý overfiting em vẫn chưa hiểu lắm 1. Trong công thức mà Andrew Ng đưa ra ở Regularization cho hồi quy tuyến tính thì tại sao lại phải đưa vào một tham số lamda (em chưa hiểu rõ việc ""co"" cái model nghĩa là gì ) 2. Vẫn trong công thức đó, thay vì là theta bình phương thì tại sao không phải là theta^3 ^4 hay các bậc khác. Em xin cảm ơn mọi người chỉ bảo ạ.",,,,,
"Dạo này em đang học về Pỵthon với OpenCV nên mạnh dạn làm clip vừa để ôn bài vừa chia sẻ cho các bạn newbie. Hi vọng giúp được các bạn.
Xin cảm ơn cả nhà!
Mọi thắc mắc xin mọi người cứ trao đổi ah!
Group trao đổi, chia sẻ: https://www.facebook.com/groups/miaigroup","Dạo này em đang học về Pỵthon với OpenCV nên mạnh dạn làm clip vừa để ôn bài vừa chia sẻ cho các bạn newbie. Hi vọng giúp được các bạn. Xin cảm ơn cả nhà! Mọi thắc mắc xin mọi người cứ trao đổi ah! Group trao đổi, chia sẻ: https://www.facebook.com/groups/miaigroup",,,,,
"Chào cả nhà , mọi người cho em hỏi chút sách Anh Việt về Machine Learning có bản PDF không ạ ? vì em hay di chuyển nên nếu có bản PDF đọc sẽ tốt hơn ạ","Chào cả nhà , mọi người cho em hỏi chút sách Anh Việt về Machine Learning có bản PDF không ạ ? vì em hay di chuyển nên nếu có bản PDF đọc sẽ tốt hơn ạ",,,,,
"Các anh chị trong group có ai từng tham gia vinai residency program chưa ạ?
Các anh/chị cho em hỏi chút kinh nghiệm về vòng phỏng vấn ạ? về kiến thức về toán? machine learning có yêu cầu cao không ạ?
Khi phỏng vấn sẽ hỏi bằng tiếng anh hay bằng tiếng việt ạ?
Em cảm ơn.",Các anh chị trong group có ai từng tham gia vinai residency program chưa ạ? Các anh/chị cho em hỏi chút kinh nghiệm về vòng phỏng vấn ạ? về kiến thức về toán? machine learning có yêu cầu cao không ạ? Khi phỏng vấn sẽ hỏi bằng tiếng anh hay bằng tiếng việt ạ? Em cảm ơn.,,,,,
Các bạn cho mình hỏi sao ở đây nó lại cho thêm một vector toàn số 1 vào Xbar rồi mới đi tính công thức vậy ?,Các bạn cho mình hỏi sao ở đây nó lại cho thêm một vector toàn số 1 vào Xbar rồi mới đi tính công thức vậy ?,,,"#Q&A, #math",,
"Mình thấy có rất nhiều bên đang đào tạo về Data Science, AI, ML như:
1. MindX
2. VietAI
3. BigO
4. Sagan School
5. Protonx
6. VEF
Có lẽ dạy AI đang là 1 mỏ vàng khi ngành đang hot mà tự học thì khó. Tuy nhiên, mình cũng đang có nhu cầu học thật nên nhờ ae review về các trung tâm này, càng chi tiết càng tốt,
Học có hiệu quả không?
Giảng viên quan tâm không?
Project được hướng dẫn có sát thực tế và có tính ứng dụng cao không?
Học xong bạn làm được việc không?
.....
Vì các khóa học trên đều rất đắt, phổ biến ở mức 5 triệu / course, cá biệt có Sagan School học phí lên tới 13.5 triệu nên mình cũng tâm tư nhiều. Mong ae review tận tình.","Mình thấy có rất nhiều bên đang đào tạo về Data Science, AI, ML như: 1. MindX 2. VietAI 3. BigO 4. Sagan School 5. Protonx 6. VEF Có lẽ dạy AI đang là 1 mỏ vàng khi ngành đang hot mà tự học thì khó. Tuy nhiên, mình cũng đang có nhu cầu học thật nên nhờ ae review về các trung tâm này, càng chi tiết càng tốt, Học có hiệu quả không? Giảng viên quan tâm không? Project được hướng dẫn có sát thực tế và có tính ứng dụng cao không? Học xong bạn làm được việc không? ..... Vì các khóa học trên đều rất đắt, phổ biến ở mức 5 triệu / course, cá biệt có Sagan School học phí lên tới 13.5 triệu nên mình cũng tâm tư nhiều. Mong ae review tận tình.",,,,,
https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/,https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/,,,,,
Cho mình hỏi group mình có ai có ebook “Raspberry for Computer Vision” của Adrian Rosebrock hoặc các tựa sách khác cùng tác giả không ạ? Pass lại cho mình với vì sách của Adrian mắc quá không đủ tiền mua 😭 (giá cả thương lượng ạ).,Cho mình hỏi group mình có ai có ebook “Raspberry for Computer Vision” của Adrian Rosebrock hoặc các tựa sách khác cùng tác giả không ạ? Pass lại cho mình với vì sách của Adrian mắc quá không đủ tiền mua (giá cả thương lượng ạ).,,,,,
This is a short demonstration of a face mask detector for COVID-19 using PyTorch Lightning running on videos. The article and the git are in description of the video.,This is a short demonstration of a face mask detector for COVID-19 using PyTorch Lightning running on videos. The article and the git are in description of the video.,,,,,
"Xin tài liệu C++

Mình đang học về C++. Mình cần cộng đồng khảo sát giúp về tài liệu nào hay với ah.

Xin cảm ơn!",Xin tài liệu C++ Mình đang học về C++. Mình cần cộng đồng khảo sát giúp về tài liệu nào hay với ah. Xin cảm ơn!,,,,,
"Chào mọi người.
Team em đang có một dự án về Speech To Text Tiếng Việt.
Mọi người có thể cho em gợi ý về mô hình mới nhất về Speech Recognize và có nhiều tài liệu nhất được không ạ?
Em xin cảm ơn.",Chào mọi người. Team em đang có một dự án về Speech To Text Tiếng Việt. Mọi người có thể cho em gợi ý về mô hình mới nhất về Speech Recognize và có nhiều tài liệu nhất được không ạ? Em xin cảm ơn.,,,,,
"Bài báo đang ở dạng open review kết hợp sức mạnh của CNN với Transformers cho kết quả rất khả quan, tất nhiên computation cost cũng không rẻ chút nào! https://openreview.net/pdf?id=YicbFdNTTy. Anh Ross Wrightman đã viết code tại đây: https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py
Hi vọng, chúng ta sẽ có GPT-3 trong ứng dụng Vision.
Ps. mình rất thích dạng open review, nếu VN áp dụng cách này cho các luận án tiến sĩ chắc chắn sẽ không có tiến sĩ chân vịt gì đó! Và còn hơn thế nữa","Bài báo đang ở dạng open review kết hợp sức mạnh của CNN với Transformers cho kết quả rất khả quan, tất nhiên computation cost cũng không rẻ chút nào! https://openreview.net/pdf?id=YicbFdNTTy. Anh Ross Wrightman đã viết code tại đây: https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py Hi vọng, chúng ta sẽ có GPT-3 trong ứng dụng Vision. Ps. mình rất thích dạng open review, nếu VN áp dụng cách này cho các luận án tiến sĩ chắc chắn sẽ không có tiến sĩ chân vịt gì đó! Và còn hơn thế nữa",,,,,
"Dear mọi người
Cho mình hỏi, trong bài article này có đoạn nhưng mình chưa hiểu ý cụ thể nói gì:
𝙏𝙝𝙞𝙨 𝙢𝙚𝙖𝙣𝙨 𝙞𝙙𝙚𝙣𝙩𝙞𝙛𝙮𝙞𝙣𝙜 𝙩𝙝𝙚 𝙧𝙚𝙡𝙖𝙩𝙞𝙤𝙣𝙨𝙝𝙞𝙥𝙨 𝙗𝙚𝙩𝙬𝙚𝙚𝙣 𝙞𝙣𝙙𝙚𝙥𝙚𝙣𝙙𝙚𝙣𝙩 𝙖𝙣𝙙 𝙙𝙚𝙥𝙚𝙣𝙙𝙚𝙣𝙩 𝙛𝙚𝙖𝙩𝙪𝙧𝙚𝙨. 𝙏𝙝𝙞𝙨 𝙞𝙨 𝙬𝙞𝙩𝙝 𝙩𝙝𝙚 𝙝𝙚𝙡𝙥 𝙤𝙛 𝙜𝙧𝙖𝙥𝙝𝙨 𝙡𝙞𝙠𝙚 𝙥𝙖𝙞𝙧 𝙥𝙡𝙤𝙩𝙨 𝙤𝙧 𝙘𝙤𝙧𝙧𝙚𝙡𝙖𝙩𝙞𝙤𝙣 𝙢𝙖𝙩𝙧𝙞𝙭. 𝙏𝙝𝙚𝙣 𝙩𝙝𝙚 𝙞𝙙𝙚𝙣𝙩𝙞𝙛𝙞𝙚𝙙 𝙧𝙚𝙡𝙖𝙩𝙞𝙤𝙣𝙨𝙝𝙞𝙥𝙨 𝙬𝙚 𝙘𝙖𝙣 𝙖𝙙𝙙 𝙖𝙨 𝙥𝙤𝙡𝙮𝙣𝙤𝙢𝙞𝙖𝙡 𝙤𝙧 𝙞𝙣𝙩𝙚𝙧𝙖𝙘𝙩𝙞𝙤𝙣 𝙛𝙚𝙖𝙩𝙪𝙧𝙚𝙨.
Source: https://towardsdatascience.com/supervised-machine-learning-model-validation-a-step-by-step-approach-771109ae0253
Vậy giả sử trong những columns ban đầu của dataset có những pairs mà highly correlated với nhau thì lúc làm feature selection mình sẽ đưa những pairs đó nhân với nhau đúng không?
Ví dụ như: dataset có A, B, C, D columns, A và C có correlation coefficent là 0.85 thì khi viết formula để build model:
Output ~ A*C + B + D hay nói cách khác là nhân các features có interaction với nhau (ở đây là correlation coefficient).
Thanks mọi người.","Dear mọi người Cho mình hỏi, trong bài article này có đoạn nhưng mình chưa hiểu ý cụ thể nói gì: . . . Source: https://towardsdatascience.com/supervised-machine-learning-model-validation-a-step-by-step-approach-771109ae0253 Vậy giả sử trong những columns ban đầu của dataset có những pairs mà highly correlated với nhau thì lúc làm feature selection mình sẽ đưa những pairs đó nhân với nhau đúng không? Ví dụ như: dataset có A, B, C, D columns, A và C có correlation coefficent là 0.85 thì khi viết formula để build model: Output ~ A*C + B + D hay nói cách khác là nhân các features có interaction với nhau (ở đây là correlation coefficient). Thanks mọi người.",,,,,
Mọi người cho mình hỏi cách để cắt frames ra trong 1 video và xử lí face detection bằng MTCNN sau đó lưu ra file ảnh như thế nào được ko ạ? Mình cảm ơn nhiều,Mọi người cho mình hỏi cách để cắt frames ra trong 1 video và xử lí face detection bằng MTCNN sau đó lưu ra file ảnh như thế nào được ko ạ? Mình cảm ơn nhiều,,,,,
"Mình đang nghiên cứu sử dụng machine learning để speech to text tiếng Việt. Trong group có bạn nào có tài liệu, kinh nghiệm, các dữ liệu mẫu thì chia sẻ nhé.
Nếu thiếu dữ liệu mẫu, anh em có thể chung tay xây dựng một website để mọi người đóng góp các mẫu dữ liệu có thể sử dụng chung cho tất cả. Rất anh em quan tâm, góp ý.","Mình đang nghiên cứu sử dụng machine learning để speech to text tiếng Việt. Trong group có bạn nào có tài liệu, kinh nghiệm, các dữ liệu mẫu thì chia sẻ nhé. Nếu thiếu dữ liệu mẫu, anh em có thể chung tay xây dựng một website để mọi người đóng góp các mẫu dữ liệu có thể sử dụng chung cho tất cả. Rất anh em quan tâm, góp ý.",,,,,
"Kính chào các anh em, hiện nay sao một thời gian tự học thì em thấy có nhiều bạn còn yếu Python. Trong khi đó Python là một công cụ sắc bén để tiếp cận AI, ML, DL....Do đó, em vừa học vừa có làm một khóa Python cơ bản theo phong cách Mì ăn liền để các bạn tiếp cận nhanh và chú trọng vào các phần chính liên quan đến AI (các cái khác ít dùng bỏ qua).
Khóa này là ONLINE và hoàn toàn MIỄN PHÍ ah.","Kính chào các anh em, hiện nay sao một thời gian tự học thì em thấy có nhiều bạn còn yếu Python. Trong khi đó Python là một công cụ sắc bén để tiếp cận AI, ML, DL....Do đó, em vừa học vừa có làm một khóa Python cơ bản theo phong cách Mì ăn liền để các bạn tiếp cận nhanh và chú trọng vào các phần chính liên quan đến AI (các cái khác ít dùng bỏ qua). Khóa này là ONLINE và hoàn toàn MIỄN PHÍ ah.",,,,,
"Cơ hội cho các bạn trẻ đam mê Data Engineering or Data Science muốn làm việc tại Facebook HQ, Mĩ. Mình chia sẻ thôi nhé, chứ ko tư vấn visa. 😅Bạn có thể đi theo diện H1B hoặc vào đây xem thêm: https://www.uscis.gov/working-in-the-united-states
(Nếu mà trúng tuyển và lấy visa qua Mĩ dc thì nhớ xuống LA đem theo quà cảm ơn mình nhoa. haha) Chúc các bạn trẻ may mắn! 🤓

Facebook University for Analytics is now accepting applications for Summer 2021. 
- We welcome Class of 2023 college students studying at a four-year university in the U.S to apply. 
-  FBU for Analytics is a paid, eight-week internship program designed to  provide exposure to students who are historically underrepresented in  this field.
- The application window will be open from 10/1/20-11/10/20. 
- Apply here: https://lnkd.in/gviayYf","Cơ hội cho các bạn trẻ đam mê Data Engineering or Data Science muốn làm việc tại Facebook HQ, Mĩ. Mình chia sẻ thôi nhé, chứ ko tư vấn visa. Bạn có thể đi theo diện H1B hoặc vào đây xem thêm: https://www.uscis.gov/working-in-the-united-states (Nếu mà trúng tuyển và lấy visa qua Mĩ dc thì nhớ xuống LA đem theo quà cảm ơn mình nhoa. haha) Chúc các bạn trẻ may mắn! Facebook University for Analytics is now accepting applications for Summer 2021. - We welcome Class of 2023 college students studying at a four-year university in the U.S to apply. - FBU for Analytics is a paid, eight-week internship program designed to provide exposure to students who are historically underrepresented in this field. - The application window will be open from 10/1/20-11/10/20. - Apply here: https://lnkd.in/gviayYf",,,,,
"Nhóm mình có ai học data analysis trên dataquest ko ạ?
Share mình vs ạ",Nhóm mình có ai học data analysis trên dataquest ko ạ? Share mình vs ạ,,,,,
Dạ các bác ơi giúp em vấn đề này với ạ. Em có 1 bảng gồm 4 chữ số đã được One hot ra. Bài toán của em là tìm những đoạn giống nhau bằng CNN ạ. Có bác nào biết tài liệu nào để đưa vào CNN hoặc đã làm vấn đề này cho em hướng đi với ạ. Thân,Dạ các bác ơi giúp em vấn đề này với ạ. Em có 1 bảng gồm 4 chữ số đã được One hot ra. Bài toán của em là tìm những đoạn giống nhau bằng CNN ạ. Có bác nào biết tài liệu nào để đưa vào CNN hoặc đã làm vấn đề này cho em hướng đi với ạ. Thân,,,,,
"Xin chào mọi người.
Cho em hỏi có ai đã từng làm việc với bộ dữ liệu 20BN-JESTER-V1 chưa ạ. Em đã tải xuống và làm theo hướng dẫn để giải nén (cat 20bn-jester-v1-?? | tar zx)nhưng có vẻ nhưng chỉ giải nén được 1/22 file . Em đang dùng dữ liệu để giải quyết bài toán Hand gesture recognition. Em cảm ơn ạ",Xin chào mọi người. Cho em hỏi có ai đã từng làm việc với bộ dữ liệu 20BN-JESTER-V1 chưa ạ. Em đã tải xuống và làm theo hướng dẫn để giải nén (cat 20bn-jester-v1-?? | tar zx)nhưng có vẻ nhưng chỉ giải nén được 1/22 file . Em đang dùng dữ liệu để giải quyết bài toán Hand gesture recognition. Em cảm ơn ạ,,,,,
kì này em đang làm đồ án nhận diện biển số xe bằng mạng cnn.Mọi người có source code hoặc tài liệu cho em xin với ạ,kì này em đang làm đồ án nhận diện biển số xe bằng mạng cnn.Mọi người có source code hoặc tài liệu cho em xin với ạ,,,,,
"Chào anh chị, em đang tìm hiểu phần phân phối xác suất hình học trong cuốn Head First Statistics, thì em nhận được cái hình dưới đây, cụ thể q = 0.8 là xác suất thất bại, p = 0.2 là xác suất thành công, thì cái cột x*P(X=x) thì em đã hiểu rồi, nhưng ở cái cột x*P(X<=x) thì em tính theo cách em như sau, em lấy cụ thể x = 2, thì P(X<=x) = 1 - q^x = 1 - 0.8^2 = 0.36 => x*P(X<=x) = 0.36*2 = 0.72 chứ, sao trong sách ghi là 0.52 vậy. Mong mọi người giúp em, cảm ơn mọi người ạ.","Chào anh chị, em đang tìm hiểu phần phân phối xác suất hình học trong cuốn Head First Statistics, thì em nhận được cái hình dưới đây, cụ thể q = 0.8 là xác suất thất bại, p = 0.2 là xác suất thành công, thì cái cột x*P(X=x) thì em đã hiểu rồi, nhưng ở cái cột x*P(X<=x) thì em tính theo cách em như sau, em lấy cụ thể x = 2, thì P(X<=x) = 1 - q^x = 1 - 0.8^2 = 0.36 => x*P(X<=x) = 0.36*2 = 0.72 chứ, sao trong sách ghi là 0.52 vậy. Mong mọi người giúp em, cảm ơn mọi người ạ.",,,"#Q&A, #math",,
"Hi mn,
Em đang làm nhận dang ô tô trên raspberry. Anh chị có tài liệu CNN, Lenet hoặc thuật toán DL nào để nhận dạng trên raspberry cho em xin với ạ . Em cảm ơn","Hi mn, Em đang làm nhận dang ô tô trên raspberry. Anh chị có tài liệu CNN, Lenet hoặc thuật toán DL nào để nhận dạng trên raspberry cho em xin với ạ . Em cảm ơn",,,,,
"#hoidap
Xin chào mọi người!
Mình đang làm đề tài về nhận dạng cử chỉ tay (Hand Getsture), nhưng đang vướng ngay ở bước chuẩn bị tập dữ liệu để học cho mạng Neural. Vậy mọi người có thể cho mình hỏi nguồn tải Dữ liệu ở đâu được không ạ?
Mình xin cảm ơn rất nhiều!","Xin chào mọi người! Mình đang làm đề tài về nhận dạng cử chỉ tay (Hand Getsture), nhưng đang vướng ngay ở bước chuẩn bị tập dữ liệu để học cho mạng Neural. Vậy mọi người có thể cho mình hỏi nguồn tải Dữ liệu ở đâu được không ạ? Mình xin cảm ơn rất nhiều!",#hoidap,,,,
"#ComputerVision #OpenCV #DeepLearning
--- EDITED ---
Cảm ơn mọi người, tụi mình đã quyết định thu hẹp bài toán 2 - detect màu bằng cách giả sử 2 chiếc giày sẽ khác một mảng màu đủ lớn để color histogram của 2 tấm ảnh chênh lệch rõ ràng (kết hợp với việc amplify sự khác biệt), từ đó khẳng định 2 chiếc giày có khác màu hay không 📷
Đối với những vệt màu nhỏ, team sẽ chuyển qua thành bài toán detect các vết bẩn và tạo tập data để train
---------------
Hiện tại tụi em đang làm đồ án với đề tài: detect các lỗi khác biệt trong dây chuyền sản xuất giày như khác size, khác màu, bẩn, dư/thiếu keo v.v...
Bài toán 1: detect size, chúng em sử dụng Canny edge detection để tìm ra đôi giày --> khá ổn
Bài toán 2: detect màu, chúng em sử dụng 2 phương pháp:
Cách 1: Sử dụng cv2.inRange để bắt các màu nằm trong một khoảng nào đó --> khá nhiễu và khó để xác định range màu.
Cách 2: preprocess tấm ảnh (blur: giảm nhiễu, thay đổi contrast: nhấn mạnh khác biệt), sau đó chia ảnh thành các ma trận 3x3 hoặc 4x4 để so sánh khác biệt từng phần. Thuật toán so sánh sử dụng là SSIM (Structural similarity index). 
Qua nhiều lần thử, cách 2 tỏ ra vượt trội hơn nhưng có một số hạn chế:
Khá nhạy về sự khác biệt về vị trí chiếc giày: 2 giày giống màu nhưng hơi lệch nhau chút thì SSIM cũng xác định là khác nhau
Không thể đảm bảo các lần chạy detect giày đều cùng nằm chính xác một vị trí.
✨✨✨ 
Mong nhận được ý kiến của mọi người về phương hướng cho bài toán thứ 2. Vì lượng data không nhiều nên team chưa nghĩ được cách sử dụng deep learning như thế nào.
Team xin cảm ơn ❤️❤️","--- EDITED --- Cảm ơn mọi người, tụi mình đã quyết định thu hẹp bài toán 2 - detect màu bằng cách giả sử 2 chiếc giày sẽ khác một mảng màu đủ lớn để color histogram của 2 tấm ảnh chênh lệch rõ ràng (kết hợp với việc amplify sự khác biệt), từ đó khẳng định 2 chiếc giày có khác màu hay không Đối với những vệt màu nhỏ, team sẽ chuyển qua thành bài toán detect các vết bẩn và tạo tập data để train --------------- Hiện tại tụi em đang làm đồ án với đề tài: detect các lỗi khác biệt trong dây chuyền sản xuất giày như khác size, khác màu, bẩn, dư/thiếu keo v.v... Bài toán 1: detect size, chúng em sử dụng Canny edge detection để tìm ra đôi giày --> khá ổn Bài toán 2: detect màu, chúng em sử dụng 2 phương pháp: Cách 1: Sử dụng cv2.inRange để bắt các màu nằm trong một khoảng nào đó --> khá nhiễu và khó để xác định range màu. Cách 2: preprocess tấm ảnh (blur: giảm nhiễu, thay đổi contrast: nhấn mạnh khác biệt), sau đó chia ảnh thành các ma trận 3x3 hoặc 4x4 để so sánh khác biệt từng phần. Thuật toán so sánh sử dụng là SSIM (Structural similarity index). Qua nhiều lần thử, cách 2 tỏ ra vượt trội hơn nhưng có một số hạn chế: Khá nhạy về sự khác biệt về vị trí chiếc giày: 2 giày giống màu nhưng hơi lệch nhau chút thì SSIM cũng xác định là khác nhau Không thể đảm bảo các lần chạy detect giày đều cùng nằm chính xác một vị trí. Mong nhận được ý kiến của mọi người về phương hướng cho bài toán thứ 2. Vì lượng data không nhiều nên team chưa nghĩ được cách sử dụng deep learning như thế nào. Team xin cảm ơn",#ComputerVision	#OpenCV	#DeepLearning,,,,
"Xin chào mọi người.
Em đang làm về nhận dạng biển số xe. Sau khi tách được biển số ra khỏi ảnh thì còn 1 công đoạn nữa là từ ảnh đó mà suy ra được text biển số xe (dạng như 63-B9 99999) thì hiện tại em không biết dùng cái gì để có thể nhận diện ra được như vậy. (trừ OCR).
Mọi người cho em ít lời khuyên và hướng dẫn với ạ. Em xin cảm ơn",Xin chào mọi người. Em đang làm về nhận dạng biển số xe. Sau khi tách được biển số ra khỏi ảnh thì còn 1 công đoạn nữa là từ ảnh đó mà suy ra được text biển số xe (dạng như 63-B9 99999) thì hiện tại em không biết dùng cái gì để có thể nhận diện ra được như vậy. (trừ OCR). Mọi người cho em ít lời khuyên và hướng dẫn với ạ. Em xin cảm ơn,,,,,
Mời mọi người cùng join chung nha,Mời mọi người cùng join chung nha,,,,,
"(Deep learning implementation on Pytorch)
Với một mạng neural nhiều lớp, thông thường khi ta muốn lấy output của các hidden layer thì phải đặt tên cho nó rồi mới dựa vào tên để lấy output của layer đó được.
Hiện tại mình đang train network và muốn lấy output từ một hidden layer như ở trên nhưng mình không muốn thay đổi bên trong network.
Bởi vì nếu thay đổi và đặt tên các layyer muốn lấy thông tin thì lúc load pretrained params sẽ có thể sinh ra lỗi ???. Mà mình cũng không muốn train lại từ đầu( mất gần cả tháng mới train được xíu)
Bạn nào có kinh nghiệm có thể chỉ cách cho mình lấy output từ một layer nào đó bên trong neural network được không ạ?
Cám ơn.","(Deep learning implementation on Pytorch) Với một mạng neural nhiều lớp, thông thường khi ta muốn lấy output của các hidden layer thì phải đặt tên cho nó rồi mới dựa vào tên để lấy output của layer đó được. Hiện tại mình đang train network và muốn lấy output từ một hidden layer như ở trên nhưng mình không muốn thay đổi bên trong network. Bởi vì nếu thay đổi và đặt tên các layyer muốn lấy thông tin thì lúc load pretrained params sẽ có thể sinh ra lỗi ???. Mà mình cũng không muốn train lại từ đầu( mất gần cả tháng mới train được xíu) Bạn nào có kinh nghiệm có thể chỉ cách cho mình lấy output từ một layer nào đó bên trong neural network được không ạ? Cám ơn.",,,,,
"Hi Everyone,
I'd like to introduce you a video from Deeplearning.AI about GANs. Experts will discuss some of their current projects and the importance and future of GANs and also provide practical career advice for ML practitioners.
Enjoy!
https://www.youtube.com/watch?v=9d4jmPmTWmc","Hi Everyone, I'd like to introduce you a video from Deeplearning.AI about GANs. Experts will discuss some of their current projects and the importance and future of GANs and also provide practical career advice for ML practitioners. Enjoy! https://www.youtube.com/watch?v=9d4jmPmTWmc",,,,,
"#lda2vec
Hi forum, mọi người trong nhóm có ai từng làm về lda2vec chưa ạ ? Em newbie có 1 số vấn đề còn thắc mắc, mong anh chị chỉ giáo thêm:
1, LDA và lda2vec sự khác nhau
2, Thuật toán của lda2vec
Mọi người chỉ cần chấm nhẹ bài viết thì em sẽ reply một số cái ngay ạ ! Cảm ơn mọi người.","Hi forum, mọi người trong nhóm có ai từng làm về lda2vec chưa ạ ? Em newbie có 1 số vấn đề còn thắc mắc, mong anh chị chỉ giáo thêm: 1, LDA và lda2vec sự khác nhau 2, Thuật toán của lda2vec Mọi người chỉ cần chấm nhẹ bài viết thì em sẽ reply một số cái ngay ạ ! Cảm ơn mọi người.",#lda2vec,,,,
"Kính chào các anh em, hôm nay mình học tiếp về Python cơ bản nên mạnh dạn làm clip chia sẻ cùng anh em. Hi vọng series 6 bài về Mì Python sẽ giúp anh em newbie còn yếu Python có cơ sở để dấn thân tiếp tục trên con đường AI, ML.
Cảm ơn MOD duyệt bài!","Kính chào các anh em, hôm nay mình học tiếp về Python cơ bản nên mạnh dạn làm clip chia sẻ cùng anh em. Hi vọng series 6 bài về Mì Python sẽ giúp anh em newbie còn yếu Python có cơ sở để dấn thân tiếp tục trên con đường AI, ML. Cảm ơn MOD duyệt bài!",,,,,
Cần tìm tài liệu Machine Learning hoặc deep learning trên matlab. Mong mọi người giới thiệu,Cần tìm tài liệu Machine Learning hoặc deep learning trên matlab. Mong mọi người giới thiệu,,,,,
Các anh chị làm công nghiệp rồi có thể chia sẻ có khi nào anh chị làm việc với các dự án thực tế mà làm mãi nhiều phương án mà accuracy của model vẫn không cải thiện không ạ. Nhất là dữ liệu dạng bảng số đấy ạ. Em thấy nhiều lúc nó mông lung kinh khủng khiếp ạ.,Các anh chị làm công nghiệp rồi có thể chia sẻ có khi nào anh chị làm việc với các dự án thực tế mà làm mãi nhiều phương án mà accuracy của model vẫn không cải thiện không ạ. Nhất là dữ liệu dạng bảng số đấy ạ. Em thấy nhiều lúc nó mông lung kinh khủng khiếp ạ.,,,,,
"chào mọi người!
Có bạn nào Training nhận diện mặt người bằng C++ chưa ạ?
Cho mình xin tài liệu hoặc hướng dẫn với ah,
mình có xem Opencv nhưng toàn là code python, chưa tìm được hướng dẫn nào về C++ cả.
cảm ơn mọi người!","chào mọi người! Có bạn nào Training nhận diện mặt người bằng C++ chưa ạ? Cho mình xin tài liệu hoặc hướng dẫn với ah, mình có xem Opencv nhưng toàn là code python, chưa tìm được hướng dẫn nào về C++ cả. cảm ơn mọi người!",,,,,
"Cho em hỏi một chút thông tin:
Em đang muốn làm học máy về stock market thì có sách nào cơ bản về này không ạ? Về khoản data thì nên lấy ở đâu ạ.
Anh chị có biết group nào chuyên về học máy về kinh tế hay tài chính không ạ?
Em đang chập chững bước vào tìm hiểu nên tinh thần vẫn là cày từ số 0 ạ.
Em trân trọng cảm ơn.",Cho em hỏi một chút thông tin: Em đang muốn làm học máy về stock market thì có sách nào cơ bản về này không ạ? Về khoản data thì nên lấy ở đâu ạ. Anh chị có biết group nào chuyên về học máy về kinh tế hay tài chính không ạ? Em đang chập chững bước vào tìm hiểu nên tinh thần vẫn là cày từ số 0 ạ. Em trân trọng cảm ơn.,,,,,
"[Chia sẻ lời giải cuộc thi Kalapa's CreditScoring Challenge For Students]
Cuộc thi Học máy dành cho sinh viên: Kalapa's CreditScoring For Students (https://challenge.kalapa.vn) đã kết thúc và tìm được chủ nhân các giải cao nhất.
Dưới đây là lời giải của các đội đạt giải, mời các bạn tham khảo:
+ Giải Đồng Đội - #1 - https://www.facebook.com/groups/615987485856981/permalink/791532624969132/
+ Giải Đồng Đội - #2 - https://www.facebook.com/groups/615987485856981/permalink/792334541555607/
+ Giải Đồng Đội - #3 - https://www.facebook.com/groups/615987485856981/permalink/791755111613550/
+ Giải Đồng Đội - #4 - https://www.facebook.com/groups/615987485856981/permalink/793195211469540/
+ Giải Đồng Đội - #5 - https://www.facebook.com/groups/615987485856981/permalink/793202004802194/
+ Giải Đồng Đội - #6 - https://www.facebook.com/groups/615987485856981/permalink/792307464891648/
+ Giải Đồng Đội - #7 - https://www.facebook.com/groups/615987485856981/permalink/793219171467144/
+ Giải Đồng Đội - #8 - https://www.facebook.com/groups/615987485856981/permalink/793121468143581/
+ Giải Đồng Đội - #9 - https://www.facebook.com/groups/615987485856981/permalink/793114414810953/
+ Giải Đồng Đội - #10 - https://www.facebook.com/groups/615987485856981/permalink/791563901632671/
+ Giải Cá Nhân - #1 - https://www.facebook.com/groups/615987485856981/permalink/791503054972089/
+ Giải Cá Nhân - #3 - https://www.facebook.com/groups/615987485856981/permalink/793796124742782/
+ Giải theo trường - #10 - https://www.facebook.com/groups/615987485856981/permalink/793881651400896/

Ngoài ra, Lễ trao giải cuộc thi cũng được tổ chức vào 8h30 thứ 7 tuần này (03/10/2020), mời các bạn tham dự.
📌 Link sự kiện: https://fb.me/e/4JKfP42S7
📌 Nội dung:
+ Các đội đạt giải trình bày lời giải. Hỏi đáp và giao lưu giữa các đội.
+ Giao lưu với các khách mời về lộ trình phát triển trong ngành Data Science.
📌 Các khách mời của chương trình:
👨‍💻 Anh Tuấn Nguyễn: Founder AI For Everyone
👨‍💻 Anh Khải Mai: Senior Data Engineer tại AImesoft
👨‍💻 Anh Phạm Đình Khánh: Founder Khanh Blog, DataScientist tại Vincomerce
📌 Đối tượng phù hợp tham gia chương trình:
Các đội tham gia cuộc thi Kalapa's CreditScoring Challenge For Students
Các bạn sinh viên/đã ra trường có mong muốn tìm hiểu và định hướng sẽ theo Machine Learning/Khoa học dữ liệu. Các khách mời sẽ giúp đỡ các bạn trong việc định hướng và xây dựng lộ trình.
📌📌📌 Link đăng ký tham gia sự kiện vẫn đang mở: https://forms.gle/9L7nDua3wjVY6YYe6.
 — với Tuan Nguyen và 3 người khác.","[Chia sẻ lời giải cuộc thi Kalapa's CreditScoring Challenge For Students] Cuộc thi Học máy dành cho sinh viên: Kalapa's CreditScoring For Students (https://challenge.kalapa.vn) đã kết thúc và tìm được chủ nhân các giải cao nhất. Dưới đây là lời giải của các đội đạt giải, mời các bạn tham khảo: + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/791532624969132/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/792334541555607/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/791755111613550/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/793195211469540/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/793202004802194/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/792307464891648/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/793219171467144/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/793121468143581/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/793114414810953/ + Giải Đồng Đội - - https://www.facebook.com/groups/615987485856981/permalink/791563901632671/ + Giải Cá Nhân - - https://www.facebook.com/groups/615987485856981/permalink/791503054972089/ + Giải Cá Nhân - - https://www.facebook.com/groups/615987485856981/permalink/793796124742782/ + Giải theo trường - - https://www.facebook.com/groups/615987485856981/permalink/793881651400896/ Ngoài ra, Lễ trao giải cuộc thi cũng được tổ chức vào 8h30 thứ 7 tuần này (03/10/2020), mời các bạn tham dự. Link sự kiện: https://fb.me/e/4JKfP42S7 Nội dung: + Các đội đạt giải trình bày lời giải. Hỏi đáp và giao lưu giữa các đội. + Giao lưu với các khách mời về lộ trình phát triển trong ngành Data Science. Các khách mời của chương trình: Anh Tuấn Nguyễn: Founder AI For Everyone Anh Khải Mai: Senior Data Engineer tại AImesoft Anh Phạm Đình Khánh: Founder Khanh Blog, DataScientist tại Vincomerce Đối tượng phù hợp tham gia chương trình: Các đội tham gia cuộc thi Kalapa's CreditScoring Challenge For Students Các bạn sinh viên/đã ra trường có mong muốn tìm hiểu và định hướng sẽ theo Machine Learning/Khoa học dữ liệu. Các khách mời sẽ giúp đỡ các bạn trong việc định hướng và xây dựng lộ trình. Link đăng ký tham gia sự kiện vẫn đang mở: https://forms.gle/9L7nDua3wjVY6YYe6. — với Tuan Nguyen và 3 người khác.",#1	#2	#3	#4	#5	#6	#7	#8	#9	#10	#1	#3	#10,,,,
"Chào mọi người, mình có trò chuyện với bác Jeff Dean (Giám đốc Google AI) và anh Bùi Hải Hưng (Viện trưởng VinAI) về một số chủ đề xoay quanh việc phát triển AI tại Việt Nam. Buổi nói chuyện hoàn toàn bằng tiếng Anh, có một số bạn không theo dõi được, nên chúng mình đã hoàn thành phụ đề tiếng Việt và đăng tải tại bit.ly/JeffDeanInVietnam. Nếu bạn có gợi ý gì cho bản dịch tốt hơn thì hãy nhắn mình nha! Buổi nói chuyện dài, nhiều từ chuyên ngành, và dịch từ văn nói tiếng Anh, nên không tránh khỏi sai sót. Cảm ơn mọi người.","Chào mọi người, mình có trò chuyện với bác Jeff Dean (Giám đốc Google AI) và anh Bùi Hải Hưng (Viện trưởng VinAI) về một số chủ đề xoay quanh việc phát triển AI tại Việt Nam. Buổi nói chuyện hoàn toàn bằng tiếng Anh, có một số bạn không theo dõi được, nên chúng mình đã hoàn thành phụ đề tiếng Việt và đăng tải tại bit.ly/JeffDeanInVietnam. Nếu bạn có gợi ý gì cho bản dịch tốt hơn thì hãy nhắn mình nha! Buổi nói chuyện dài, nhiều từ chuyên ngành, và dịch từ văn nói tiếng Anh, nên không tránh khỏi sai sót. Cảm ơn mọi người.",,,,,
"Mình xin chia sẻ Note cá nhân về ngành Bioinformatic và Big Data, Machine Learning.
Ít người biết rằng, hiện nay, Bioinfo đứng top đầu về khối lượng dữ liệu sinh ra và lưu trữ hàng năm trên thế giới. Các bài toán cần giải quyết trong Bioinfo thực ra rất cần những công cụ, những ý tưởng mới để giải quyết. Ví dụ như bài toán về Y học chính xác, khi cần tìm những important feature. Hoặc khi cần xây dựng bộ gen tham chiếu, bài toán string assembly.
Hi vọng note của mình mang lại thông tin cho các bạn mong muốn áp dụng ML/DL vào một lĩnh vực vừa mới mẻ, vừa thách thức.

https://www.facebook.com/notes/thanh-nguyen/big-data-trong-m%E1%BB%97i-ch%C3%BAng-ta/3609643259048929/

P/s: Ngày mai sẽ có toạ đàm về Y học chính xác và giới thiệu về cổng thông tin của dự án 1000 hệ gen Việt Nam (really really big data), bạn nào có nhu cầu có thể theo dõi tại đây nhé: https://www.facebook.com/events/384333912564581/388122885519017/","Mình xin chia sẻ Note cá nhân về ngành Bioinformatic và Big Data, Machine Learning. Ít người biết rằng, hiện nay, Bioinfo đứng top đầu về khối lượng dữ liệu sinh ra và lưu trữ hàng năm trên thế giới. Các bài toán cần giải quyết trong Bioinfo thực ra rất cần những công cụ, những ý tưởng mới để giải quyết. Ví dụ như bài toán về Y học chính xác, khi cần tìm những important feature. Hoặc khi cần xây dựng bộ gen tham chiếu, bài toán string assembly. Hi vọng note của mình mang lại thông tin cho các bạn mong muốn áp dụng ML/DL vào một lĩnh vực vừa mới mẻ, vừa thách thức. https://www.facebook.com/notes/thanh-nguyen/big-data-trong-m%E1%BB%97i-ch%C3%BAng-ta/3609643259048929/ P/s: Ngày mai sẽ có toạ đàm về Y học chính xác và giới thiệu về cổng thông tin của dự án 1000 hệ gen Việt Nam (really really big data), bạn nào có nhu cầu có thể theo dõi tại đây nhé: https://www.facebook.com/events/384333912564581/388122885519017/",,,,,
"chào các bạn, mình muốn tìm một quyển machine learning cơ bản để đọc từ đầu, mình là newbie. các bạn giới thiệu giúp nhé, thanks","chào các bạn, mình muốn tìm một quyển machine learning cơ bản để đọc từ đầu, mình là newbie. các bạn giới thiệu giúp nhé, thanks",,,,,
Question Answering with deep embedders (such as BERT),Question Answering with deep embedders (such as BERT),,,,,