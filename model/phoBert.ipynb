{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    t = text.replace('\\n', ' ')\n",
    "    t = t.lower()\n",
    "    return t\n",
    "\n",
    "def delete_hashtag(text):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def delete_link(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoj = re.compile(r\"\"\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251\\U0001f926-\\U0001f937\\U00010000-\\U0010ffff\\u200d\\u23cf\\u23e9\\u231a\\ufe0f\\u3030-]+(?<!\\n)\"\"\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n",
    "\n",
    "def encode_number(text):\n",
    "    t = text.split(' ')\n",
    "    t = map(lambda x: '<number>' if bool(re.match(r'^[0-9]+(\\.[0-9]+)?$', x)) else x, t)\n",
    "    return ' '.join(t)\n",
    "\n",
    "def delete_onelen_token(text):\n",
    "    t = text.split(' ')\n",
    "    t = filter(lambda x: len(x)>1, t)\n",
    "    return ' '.join(t)\n",
    "\n",
    "def preprocessing(text):\n",
    "    t = normalize(text)\n",
    "    t = delete_hashtag(t)\n",
    "    t = delete_link(t)\n",
    "    t = remove_emojis(t)\n",
    "    t = ViTokenizer.tokenize(t)\n",
    "    t = encode_number(t)\n",
    "    t = delete_onelen_token(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vuong\\AppData\\Local\\Temp\\ipykernel_36024\\3555114334.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchtext.transforms import ToTensor\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTag_Dataset(Dataset):\n",
    "    def __init__(self, root='../train_data.csv', max_length=128):\n",
    "        super(HashTag_Dataset, self).__init__()\n",
    "        self.classes = ['#Q&A', '#cv', '#data', '#deep_learning', '#machine_learning', '#math', '#nlp', '#python', '#sharing', '#webinar']\n",
    "        texts, labels = [], []\n",
    "\n",
    "        df = pd.read_csv(root, encoding='utf-8-sig')\n",
    "        texts = df['text']\n",
    "        labels = df['label']\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = self.make_vocab(texts)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def make_vocab(self, texts):\n",
    "      vocab = dict()\n",
    "      for text in texts:\n",
    "          words = text.split()\n",
    "          for word in words:\n",
    "              if word not in vocab:\n",
    "                  vocab[word] = 1\n",
    "              else:\n",
    "                  vocab[word] += 1\n",
    "      vocab = list(dict(filter(lambda x: x[1]>3, vocab.items())).keys())\n",
    "      vocab.append('<UNK>')\n",
    "      vocab.append('<PAD>')\n",
    "      vocab.append('<CLS>')\n",
    "      return vocab\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        words = text.split()\n",
    "        words = ['<CLS>'] + words\n",
    "        if len(words) > self.max_length:\n",
    "            words = words[:self.max_length]\n",
    "        else:\n",
    "            words += ['<PAD>']*(self.max_length-len(words))\n",
    "        enc = [self.vocab.index(w) if w in self.vocab else self.vocab.index('<UNK>') for w in words]\n",
    "        return enc\n",
    "\n",
    "    def encode_label(self, label):\n",
    "        enc = ast.literal_eval(label)\n",
    "        enc = [0.8 if l in enc else 0 for l in self.classes]\n",
    "        return enc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def len_vocab(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def num_classes(self):\n",
    "        return len(self.classes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encode = self.encode_text(text)\n",
    "        label = self.encode_label(label)\n",
    "        encode = torch.tensor(encode, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return encode, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2180,   66,   94,   95,   11,   81,   12,   30,  271,  120,  101,  196,\n",
       "          295,  296,  157,   54, 2178,  120,  115,   81,   12,   30,  133,   97,\n",
       "          297,  298,  117,  299, 2178,   39,   35,    0,  300,   32,   94,   95,\n",
       "          301,  302,   11,  127,   12,  303,  131,   84,  285,  117,  130,   84,\n",
       "          115,  112,   11,   81,   58,   62,   96,  102,   54,  304,  305,   14,\n",
       "          304,  306,  157,  307,  308,    0, 2178,  309,  310,  287,   11,   81,\n",
       "           32,  124,   35,  199,  287,   98,  127,  287,    3,  311,  102,   54,\n",
       "          304,  306,   98,  304,  305,  309,  175,    0,  108,   49,  144,  304,\n",
       "          306,   14,  304,  305,  307,    0,   86,  309,    0,  108,  312,  117,\n",
       "          307,    0,   81,   35,  313,   30,  133,   97,   88,   85,   61,  314,\n",
       "           54,  108,   39,   75,  199,  115,   58,   62]),\n",
       " tensor([0.8000, 0.8000, 0.0000, 0.8000, 0.0000, 0.0000, 0.8000, 0.0000, 0.0000,\n",
       "         0.0000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = HashTag_Dataset()\n",
    "train_set.__getitem__(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashtagRecommendation(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size, d_model=768, n_head=4):\n",
    "        super(HashtagRecommendation, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.embedding.weight.data.uniform_(-1, 1)\n",
    "\n",
    "        self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        for param in self.phobert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_labels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(768)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        attention_mask = (input_ids != 3800).float()\n",
    "        embedded = self.embedding(input_ids)\n",
    "        phobert_output = self.phobert(inputs_embeds=embedded, attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Lấy embedding của token [CLS]\n",
    "        cls_embedding = phobert_output[:, 0, :]\n",
    "\n",
    "        output = self.dropout(cls_embedding)\n",
    "        output = self.fc1(cls_embedding)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        output = self.fc2(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HashTag_Dataset()\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=2, drop_last=True)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "num_epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HashtagRecommendation(num_labels=train_set.num_classes(), vocab_size=train_set.len_vocab())\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "num_iters = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be44fc4cad54110935d5bf08c525081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, colour='green')\n",
    "    for iter, (texts, labels) in enumerate(progress_bar):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(dtype=torch.float).to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(texts)\n",
    "        loss_value = criterion(outputs, labels)\n",
    "        progress_bar.set_description(\"Epoch {}/{}. Iteration {}/{}. Loss {:.5f}\".format(epoch+1, num_epochs, iter+1, num_iters, loss_value))\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"Chào mọi người. Hiện tại em đang làm đồ án về truy vấn thông tin. Em làm về content based image retrieval.\n",
    "Em định làm thêm text based image retrieval nhưng em đang kẹt ở phần caption của image. Do dataset là phải tự scrape về nên chỉ có thể scrape được ảnh.\n",
    "Em có thử dùng 1 vài tool để tạo caption nhưng kết quả ra khá tệ.\n",
    " Giờ em phải làm như nào để tạo được caption ạ. Em cảm ơn mọi người.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocessing(test)\n",
    "test = train_set.encode_text(test)\n",
    "test = ToTensor()(test)[None, :]\n",
    "test = test.to(device)\n",
    "pred = model(test)[0].tolist()\n",
    "print(pred)\n",
    "pred = [train_set.classes[i] for i in range(10) if pred[i]>0.5]\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
